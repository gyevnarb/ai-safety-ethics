{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8582de802a03158d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Importing the necessary libraries\n",
    "We first import the necessary dependencies for our analysis as well as our data for further processing.\n",
    "\n",
    "The required libraries are imported below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abbe3e46bab47e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:33:06.212762Z",
     "start_time": "2024-07-19T14:32:37.782782Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/balint/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import wordcloud as wc\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bertopic\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "import rich\n",
    "try:\n",
    "    import networkx as nx\n",
    "    import graph_tool.all as gt\n",
    "except ImportError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from plotly import io as pio\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.chdir(\"/home/balint/ai-safety-ethics\")\n",
    "\n",
    "def get_authors(author_dict, fix_first_name=True):\n",
    "    aths = []\n",
    "    for author in author_dict:\n",
    "        if \"literal\" in author:\n",
    "            continue\n",
    "        if author[\"given\"] and fix_first_name:\n",
    "            if not author[\"given\"].isupper():\n",
    "                # Turn author given name into initials\n",
    "                author[\"given\"] = \"\".join([n[0] for n in author[\"given\"].split(\" \")])\n",
    "            name = author[\"family\"] + \", \" + \".\".join(author[\"given\"].replace(\".\", \"\")) + \".\"\n",
    "        elif author[\"given\"]:\n",
    "            name = author[\"family\"] + \", \" + author[\"given\"]\n",
    "        else:\n",
    "            name = author[\"family\"]\n",
    "        aths.append(name)\n",
    "    return \" and \".join(aths)\n",
    "\n",
    "\n",
    "def model_topics(\n",
    "        corpus: List[str],\n",
    "        dates,\n",
    "        path_to_data=\"output\",\n",
    "        fname=\"\",\n",
    "        min_cluster_size=3, rerun_embeddings=False, rerun_topic=True):\n",
    "    embeddings_path = os.path.join(path_to_data, f\"{fname}embeddings.p\")\n",
    "    if not os.path.exists(embeddings_path) or rerun_embeddings:\n",
    "        sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        embeddings = sentence_model.encode(corpus, show_progress_bar=True)\n",
    "        pickle.dump(embeddings, open(embeddings_path, \"wb\"))\n",
    "    else:\n",
    "        embeddings = pickle.load(open(embeddings_path, \"rb\"))\n",
    "\n",
    "    topic_model_path = os.path.join(path_to_data, f\"{fname}topic_model.p\")\n",
    "    if not os.path.exists(topic_model_path) or rerun_topic:\n",
    "        ctfidf_model = bertopic.vectorizers.ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "        topic_model = bertopic.BERTopic(\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            verbose=True)\n",
    "        topic_model.fit(corpus, embeddings)\n",
    "        topic_model.save(topic_model_path)\n",
    "    else:\n",
    "        topic_model = bertopic.BERTopic.load(topic_model_path)\n",
    "\n",
    "    return topic_model, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d1e4f",
   "metadata": {},
   "source": [
    "We then set local variables to determine which corpus ('safety' or 'ethics') we want to use with what date cutoff. If using the ethics corpus, then we may also specify which conference to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756c28bfbc12a6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:33:10.280247Z",
     "start_time": "2024-07-19T14:33:10.270637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set this variable to true if only want to process information for the titles but not the astracts. Otherwise, both the title and the abstract will be processed.\n",
    "use_title_only = False\n",
    "load_all = True  # If true then load all papers regardless of the below parameters\n",
    "\n",
    "conference = 'both' # Set this variable to choose which conference proceedings to process. The options are 'aies', 'facct', and 'both'.\n",
    "field = 'safety'  # Either 'safety' or 'ethics'\n",
    "if load_all:\n",
    "    field = \"combined\"\n",
    "year_cutoff = None  # 2022\n",
    "month_cutoff = None  # 11\n",
    "\n",
    "assert conference in ['aies', 'facct', 'both'], \"Invalid conference option. Choose 'aies', 'facct', or 'both'.\"\n",
    "assert field in ['safety', 'ethics', 'combined'], \"Invalid field option. Choose 'safety' or 'ethics'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8b473",
   "metadata": {},
   "source": [
    "Now we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b434da2be21079",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:38:24.291205Z",
     "start_time": "2024-07-19T14:38:24.264476Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 2655\n",
      "Split index: 1329\n",
      "Number of safety papers: 1329\n",
      "Number of ethics papers: 1326\n",
      "Load all is true!\n"
     ]
    }
   ],
   "source": [
    "# Load both JSON files for the AIES and FAccT proceedings and extract either the title or the abstract and title from the file.\n",
    "docs = []\n",
    "dates = []\n",
    "titles = []\n",
    "types = []\n",
    "\n",
    "if load_all:\n",
    "    ethics = json.load(open('data/ethics/all_ethics.json', \"r\", encoding=\"utf-8\"))\n",
    "    safety = json.load(open('data/safety/all_safety.json', \"r\", encoding=\"utf-8\"))\n",
    "    data = ethics + safety\n",
    "elif field == 'ethics':\n",
    "    aies = json.load(open('data/ethics/AIES.json', \"r\", encoding=\"utf-8\"))\n",
    "    facct = json.load(open('data/ethics/FACCT.json', \"r\", encoding=\"utf-8\"))\n",
    "    data = aies + facct if conference == 'both' else aies if conference == 'aies' else facct\n",
    "    safety = []\n",
    "else:\n",
    "    safety = json.load(open('data/safety/all_safety.json', \"r\", encoding=\"utf-8\"))\n",
    "    data = safety\n",
    "\n",
    "split_index = None\n",
    "used_ids = set()\n",
    "for i, paper in enumerate(data):\n",
    "    year = int(paper['issued']['date-parts'][0][0])\n",
    "    month = int(paper['issued']['date-parts'][0][1]) if len(paper['issued']['date-parts'][0]) > 1 else 13\n",
    "\n",
    "    if not year:\n",
    "        print(f\"Paper {paper['id']} has no year. Skipping.\")\n",
    "\n",
    "    if year_cutoff and year < year_cutoff:\n",
    "        continue\n",
    "    if month_cutoff and year == year_cutoff and month < month_cutoff:\n",
    "        continue\n",
    "\n",
    "    if paper['id'][-1] in ['a', 'b', 'c', 'd']:\n",
    "        paper['id'] = paper['id'][:-1]\n",
    "    pid = paper['id']\n",
    "    title = paper['title'].lower()\n",
    "    if title in map(str.lower, titles):\n",
    "        print(f\"Duplicate title: {paper['id']}\")\n",
    "    if pid in used_ids:\n",
    "        print(f\"Duplicate ID: {paper['id']}\")\n",
    "    used_ids.add(pid)\n",
    "\n",
    "    titles.append(title)\n",
    "    if use_title_only:\n",
    "        docs.append(title)\n",
    "        types.append(\"safety\" if paper in safety else \"ethics\")\n",
    "    elif not use_title_only and 'abstract' in paper:\n",
    "        text = title + ' ' + paper['abstract'].lower()\n",
    "        if \"©\" in text:\n",
    "            text = text[:text.index(\"©\")] # Remove copyright information\n",
    "        docs.append(text)\n",
    "        dates.append(year)\n",
    "        types.append(\"safety\" if paper in safety else \"ethics\")\n",
    "    else:\n",
    "        print(f\"Paper {paper['id']} has no abstract. Skipping.\")\n",
    "\n",
    "    if len(types) > 2 and types[-1] != types[-2]:\n",
    "        split_index = i\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Split index: {split_index}\")\n",
    "print(f\"Number of safety papers: {len([t for t in types if t == 'safety'])}\")\n",
    "print(f\"Number of ethics papers: {len([t for t in types if t == 'ethics'])}\")\n",
    "if load_all:\n",
    "    print(\"Load all is true!\")\n",
    "\n",
    "# Create output folders\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs(f'output/{field}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bf2a6",
   "metadata": {},
   "source": [
    "We preprocess the documents by removing URLs and symbols and then normalizing the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus for VOSviewer use\n",
    "\n",
    "scores = [\"0\"] * len([t for t in types if t == 'safety']) + [\"1\"] * len([t for t in types if t == 'ethics'])\n",
    "with open(\"data/scores.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(scores))\n",
    "with open(\"data/corpus.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(map(lambda x: x.replace(\"\\n\", \" \"), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from file...\n",
      "Running token post-processing...\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "from functools import partial\n",
    "from textacy import preprocessing\n",
    "\n",
    "\n",
    "def postproc(doc: spacy.tokens.Doc) -> str:\n",
    "    remove_list = [\"textit\", \"emph\", \"ieee\"]\n",
    "    filtered = filter(lambda t: not t.is_stop and t not in nltk.corpus.stopwords.words(\"english\") and t.lemma_ not in remove_list, doc)\n",
    "    lemmatized = map(lambda t: t.lemma_, filtered)\n",
    "    return list(lemmatized)\n",
    "\n",
    "preproc = preprocessing.make_pipeline(\n",
    "    partial(preprocessing.replace.urls, repl=\"\"),\n",
    "    partial(preprocessing.replace.currency_symbols, repl=\"\"),\n",
    "    preprocessing.remove.accents,\n",
    "    preprocessing.remove.html_tags,\n",
    "    preprocessing.remove.punctuation,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.quotation_marks,\n",
    "    preprocessing.normalize.hyphenated_words\n",
    ")\n",
    "\n",
    "rerun_corpus = False\n",
    "if os.path.exists(\"output/combined/corpus.p\") and not rerun_corpus:\n",
    "    print(\"Loading corpus from file...\")\n",
    "    corpus = pickle.load(open(\"output/combined/corpus.p\", \"rb\"))\n",
    "else:\n",
    "    try:\n",
    "        spacy.require_gpu()\n",
    "        print(f\"Creating corpus using GPU...\")\n",
    "    except ValueError:\n",
    "        print(f\"Creating corpus using CPU...\")\n",
    "    corpus = textacy.Corpus(\"en_core_web_sm\", map(preproc, docs))\n",
    "    pickle.dump(corpus, open(\"output/combined/corpus.p\", \"wb\"))\n",
    "\n",
    "print(\"Running token post-processing...\")\n",
    "corpus_tokens = []\n",
    "corpus_text = []\n",
    "for doc in corpus:\n",
    "    tokens = postproc(doc)\n",
    "    corpus_tokens.append(tokens)\n",
    "    corpus_text.append(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86277f2563d08415",
   "metadata": {},
   "source": [
    "# 2. Topic analysis with embeddings and BERTopic\n",
    "We will now perform topic analysis on the titles and abstracts of the papers using the BERTopic library.\n",
    "\n",
    "This uses a utility function from the file `util.py` to embed the data, model topics, and save everything in the appropriate location. Each visualisation will also be saved as an interactive HTML file under the folder `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:30:54,173 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-20 15:30:59,073 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-02-20 15:30:59,073 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-02-20 15:30:59,117 - BERTopic - Cluster - Completed ✓\n",
      "2025-02-20 15:30:59,119 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-20 15:30:59,272 - BERTopic - Representation - Completed ✓\n",
      "2025-02-20 15:30:59,415 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 39\n"
     ]
    }
   ],
   "source": [
    "rerun_embeddings = False\n",
    "min_cluster_size = 12\n",
    "topic_model, embeddings = model_topics(\n",
    "    corpus_text, [], path_to_data=f\"output/{field}\",\n",
    "    min_cluster_size=min_cluster_size, rerun_embeddings=rerun_embeddings)\n",
    "n_topics = len(topic_model.get_topics())\n",
    "print(f\"Number of topics: {n_topics}\")\n",
    "document_topics = topic_model.get_document_info(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6791686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:30:59,537 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-20 15:31:00,915 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-02-20 15:31:00,916 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-02-20 15:31:00,939 - BERTopic - Cluster - Completed ✓\n",
      "2025-02-20 15:31:00,941 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-20 15:31:01,033 - BERTopic - Representation - Completed ✓\n",
      "2025-02-20 15:31:01,131 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of safety topics: 35\n"
     ]
    }
   ],
   "source": [
    "min_cluster_size = 8\n",
    "safety_corpus = corpus_text[:split_index]\n",
    "safety_topic_model, safety_embeddings = model_topics(\n",
    "    safety_corpus, [], path_to_data=f\"output/{field}\", fname=\"safety_\",\n",
    "    min_cluster_size=min_cluster_size, rerun_embeddings=rerun_embeddings)\n",
    "n_topics = len(safety_topic_model.get_topics())\n",
    "print(f\"Number of safety topics: {n_topics}\")\n",
    "safety_document_topics = safety_topic_model.get_document_info(safety_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "498fc76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:31:01,223 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-20 15:31:02,616 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-02-20 15:31:02,616 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-02-20 15:31:02,638 - BERTopic - Cluster - Completed ✓\n",
      "2025-02-20 15:31:02,640 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-20 15:31:02,736 - BERTopic - Representation - Completed ✓\n",
      "2025-02-20 15:31:02,863 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ethics topics: 29\n"
     ]
    }
   ],
   "source": [
    "min_cluster_size = 8\n",
    "ethics_corpus = corpus_text[split_index:]\n",
    "ethics_topic_model, ethics_embeddings = model_topics(\n",
    "    ethics_corpus, [], path_to_data=f\"output/{field}\", fname=\"ethics_\",\n",
    "    min_cluster_size=min_cluster_size, rerun_embeddings=rerun_embeddings)\n",
    "n_topics = len(ethics_topic_model.get_topics())\n",
    "print(f\"Number of ethics topics: {n_topics}\")\n",
    "ethics_document_topics = ethics_topic_model.get_document_info(ethics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eee8fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "shared_embeddings = True\n",
    "most_similar = False  # Whether to find the most similar or least similar pairs\n",
    "\n",
    "if shared_embeddings:\n",
    "    sem = embeddings[:split_index]\n",
    "    eem = embeddings[split_index:]\n",
    "    dt = document_topics\n",
    "else:\n",
    "    sem = safety_embeddings\n",
    "    eem = ethics_embeddings\n",
    "    dt = pd.concat([safety_document_topics, ethics_document_topics], axis=0)\n",
    "\n",
    "similarity = cosine_similarity(sem, eem)\n",
    "if most_similar:\n",
    "    large_sim = similarity.argsort(axis=None)[::-1]\n",
    "    ixs = np.unravel_index(large_sim, similarity.shape)\n",
    "else:\n",
    "    low_sim = similarity.argsort(axis=None)\n",
    "    ixs = np.unravel_index(low_sim, similarity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570b017",
   "metadata": {},
   "source": [
    "The following will genertae a table of the top N most similar or dissimilar documents between the AI safety and ethics corpora as measure by their cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c96394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_limit = 50\n",
    "included_ids = set()\n",
    "ethics_df = []\n",
    "safety_df = []\n",
    "for safety_idx, ethics_idx in zip(ixs[0], split_index + ixs[1]):\n",
    "    if len(safety_df) >= n_limit and len(ethics_df) >= n_limit:\n",
    "        break\n",
    "\n",
    "    safety_paper = data[safety_idx]\n",
    "    if safety_paper['id'] not in included_ids and len(safety_df) < n_limit:\n",
    "        paper = {\n",
    "            \"Included\": True,\n",
    "            \"Ref\": safety_paper[\"id\"],\n",
    "            \"Title\": safety_paper[\"title\"],\n",
    "            \"Authors\": get_authors(safety_paper[\"author\"]),\n",
    "            \"Year\": safety_paper[\"issued\"][\"date-parts\"][0][0],\n",
    "            \"Field\": \"Safety\",\n",
    "            \"Link\": safety_paper.get(\"URL\", \"Unkown\"),\n",
    "            \"DOI\": safety_paper.get(\"DOI\", \"Unkown\"),\n",
    "            \"Citations\": 0,\n",
    "            \"Type\": safety_paper.get(\"type\", \"Unknown\")\n",
    "        }\n",
    "        safety_df.append(paper)\n",
    "        included_ids.add(safety_paper['id'])\n",
    "\n",
    "    ethics_paper = data[ethics_idx]\n",
    "    if ethics_paper['id'] not in included_ids and len(ethics_df) < n_limit:\n",
    "        paper = {\n",
    "            \"Included\": True,\n",
    "            \"Ref\": ethics_paper[\"id\"],\n",
    "            \"Title\": ethics_paper[\"title\"],\n",
    "            \"Authors\": get_authors(ethics_paper[\"author\"]),\n",
    "            \"Year\": ethics_paper[\"issued\"][\"date-parts\"][0][0],\n",
    "            \"Field\": \"Ethics\",\n",
    "            \"Link\": ethics_paper.get(\"URL\", \"Unkown\"),\n",
    "            \"DOI\": ethics_paper.get(\"DOI\", \"Unkown\"),\n",
    "            \"Citations\": 0,\n",
    "            \"Type\": ethics_paper.get(\"type\", \"Unknown\")\n",
    "        }\n",
    "        ethics_df.append(paper)\n",
    "        included_ids.add(ethics_paper['id'])\n",
    "\n",
    "safety_df = pd.DataFrame(safety_df)\n",
    "ethics_df = pd.DataFrame(ethics_df)\n",
    "df = pd.concat([safety_df, ethics_df], axis=0)\n",
    "fname = \"most_similar\" if most_similar else \"least_similar\"\n",
    "df.to_csv(f\"output/{fname}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e742461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lightman, H. and Kosaraju, V. and Burda, Y. and Edwards, H. and Baker, B. and Lee, T. and Leike, J. and Schulman, J. and Sutskever, I. and Cobbe, K.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_authors([s for s in safety if s[\"id\"] == \"lightmanLetVerifyStep2023\"][0][\"author\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca6849",
   "metadata": {},
   "source": [
    "Run the following code snippet to generate a topic and document map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "# fig.write_html(f\"output/{field}/documents-{conference}.html\", include_mathjax = 'cdn')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ff3b0",
   "metadata": {},
   "source": [
    "Run the following code to visualise which tokens are most frequent under each topic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_barchart(top_n_topics=12, n_words=6, title=f\"{field.capitalize()}: Top 12 Topics and Keywords\")\n",
    "# fig.write_html(f\"output/{field}/tokens-{conference}.html\", include_mathjax = 'cdn')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a6262",
   "metadata": {},
   "source": [
    "The code below generates a document and topic map with much more pleasing aesthetics, however, it does not provide interactivity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "\n",
    "# title = \"AIES and FAccT Topics and Documents\" if field == 'ethics' else \"Safety Topics and Documents\"\n",
    "title = \"Map of AI Ethics Topics and Documents\"\n",
    "fig = topic_model.visualize_document_datamap(docs,\n",
    "                                             title=\"\",\n",
    "                                            #  sub_title=\"A visualisation of document embeddings clustered by topic\",\n",
    "                                             embeddings=embeddings,\n",
    "                                             label_over_points=True,\n",
    "                                             dynamic_label_size=True,\n",
    "                                             max_font_size=24,\n",
    "                                             min_font_size=8,\n",
    "                                             custom_labels=True\n",
    "                                             )\n",
    "fig.savefig(f\"output/{field}/datamap.pdf\", bbox_inches='tight', pad_inches=0.0)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5315ba",
   "metadata": {},
   "source": [
    "# 3. Graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f918b",
   "metadata": {},
   "source": [
    "We build a graph of document similarities based on their embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_thershold = 0.67\n",
    "use_nx = False\n",
    "\n",
    "edge_list = []\n",
    "for i, row in enumerate(similarity):\n",
    "    for j, col in enumerate(row):\n",
    "        if col > sim_thershold:\n",
    "            edge_list.append((i, j + split_index, col))\n",
    "\n",
    "if use_nx:\n",
    "    g = nx.Graph()\n",
    "    g.add_weighted_edges_from(edge_list)\n",
    "    print(f\"Create graph with {g.number_of_nodes()} vertices and {g.number_of_edges()} edges.\")\n",
    "else:\n",
    "    g = gt.Graph(directed=False)\n",
    "    weight = g.new_edge_property(\"float\")\n",
    "    g.add_edge_list(edge_list, eprops=[weight], hashed=False)\n",
    "    g.ep[\"weight\"] = weight\n",
    "    doc_id = g.new_vertex_property(\"int\", range(g.num_vertices()))\n",
    "    g.vp[\"doc_id\"] = doc_id\n",
    "    g.remove_vertex([v for v in g.vertices() if g.vertex(v).out_degree() == 0 and g.vertex(v).in_degree() == 0])\n",
    "    print(f\"Create graph with {g.num_vertices()} vertices and {g.num_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.representations.network import rank_nodes_by_bestcoverage\n",
    "\n",
    "if use_nx:\n",
    "    ranking = rank_nodes_by_bestcoverage(g, c=1, alpha=0.1, k=len(g.nodes()))\n",
    "    rich.print([data[i][\"title\"] for i in ranking][:10])\n",
    "    nx.set_node_attributes(g, ranking, \"rank\")\n",
    "else:\n",
    "    vb, eb = gt.betweenness(g, weight=g.ep.weight)\n",
    "    g.vp[\"vb\"] = vb\n",
    "    g.ep[\"eb\"] = eb\n",
    "    rich.print([data[i][\"title\"] for i in vb.fa.argsort()[::-1][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b74b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_nx:\n",
    "    safety = g.new_vertex_property(\"bool\")\n",
    "    title = g.new_vertex_property(\"string\")\n",
    "    color = g.new_vertex_property(\"vector<double>\")\n",
    "    topic = g.new_vertex_property(\"int\")\n",
    "    topic_color = g.new_vertex_property(\"vector<double>\")\n",
    "\n",
    "    for i, v in enumerate(g.vertices()):\n",
    "        # title[v] = data[i][\"title\"][:20]\n",
    "        safety[v] = types[g.vp.doc_id[v]] == \"safety\"\n",
    "        alpha = gt.prop_to_size(g.vp.vb, 0.2, 1, power=.1)\n",
    "        color[v] = [0., 0.247, 0.361, alpha[v]] if types[g.vp.doc_id[v]] == \"safety\" \\\n",
    "            else [1., 0.388, 0.38, alpha[v]]\n",
    "        topic[v] = 1 + dt.iloc[g.vp.doc_id[v]][\"Topic\"]\n",
    "        topic_color[v] = plt.get_cmap(\"tab20\").colors[topic[v] % 20]\n",
    "\n",
    "    for i, v in enumerate(vb.fa.argsort()[::-1][:10]):\n",
    "        title[v] = data[g.vp.doc_id[v]][\"title\"][:40] + \"...\"\n",
    "\n",
    "    g.vp[\"safety\"] = safety\n",
    "    g.vp[\"title\"] = title\n",
    "    g.vp[\"color\"] = color\n",
    "    g.vp[\"topic\"] = topic\n",
    "    g.vp[\"topic_color\"] = topic_color\n",
    "\n",
    "    u = gt.extract_largest_component(g)\n",
    "    pos = gt.sfdp_layout(u, eweight=g.ep.weight, groups=topic, gamma=.01, kc=10, r=3, C=0.5, max_iter=1e5)\n",
    "    gt.graph_draw(u, pos, output=\"output/graph.png\", bg_color=\"white\", output_size=(800, 500), fit_view=True, adjust_aspect=False,\n",
    "           vertex_size=gt.prop_to_size(g.vp.vb, 2, 15, power=.4), vorder=g.vp.vb, vertex_fill_color=g.vp.color, vertex_color=g.vp.color,\n",
    "           vertex_text=title, vertex_text_color=color, vertex_text_position=0, vertex_text_rotation=0,\n",
    "           vertex_font_size=10, vertex_text_out_color=\"black\",\n",
    "           vertex_text_out_width=0.005, edge_pen_width=gt.prop_to_size(g.ep.weight, 0.1, 0.5, power=3)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ad8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.save(\"output/graph.graphml\")\n",
    "# np.unique_counts(safety.fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_nx:\n",
    "    u = gt.extract_largest_component(g)\n",
    "    gt.graph_draw(u, pos, output=\"output/graph_topic.png\", bg_color=\"white\", output_size=(1000, 1000), fit_view=True,\n",
    "           vertex_size=gt.prop_to_size(g.vp.vb, 2, 15, power=.3), vorder=g.vp.vb, vertex_fill_color=g.vp.topic_color, vertex_color=g.vp.color,\n",
    "           #vertex_text=titles, vertex_text_color=color, vertex_text_position=0, vertex_font_size=10,\n",
    "           edge_pen_width=gt.prop_to_size(g.ep.weight, 0.1, 0.15, power=2)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f82b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_nx:\n",
    "    u = gt.extract_largest_component(g)\n",
    "    state = gt.minimize_blockmodel_dl(g)\n",
    "    gt.mcmc_equilibrate(state, wait=1000, mcmc_args=dict(niter=10))\n",
    "\n",
    "    bs = [] # collect some partitions\n",
    "    def collect_partitions(s):\n",
    "        global bs\n",
    "        bs.append(s.b.a.copy())\n",
    "\n",
    "    gt.mcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10),\n",
    "                        callback=collect_partitions)\n",
    "\n",
    "    # # Disambiguate partitions and obtain marginals\n",
    "    pmode = gt.PartitionModeState(bs, converge=True)\n",
    "    pv = pmode.get_marginal(u)\n",
    "    state.draw(pos=pos, output=\"output/graph_alternate.png\", bg_color=\"white\", output_size=(1000, 1000),\n",
    "               vertex_shape=\"pie\", vertex_pie_fractions=pv, vertex_size=gt.prop_to_size(g.vp.vb, 3, 15, power=.3),\n",
    "               edge_pen_width=gt.prop_to_size(g.ep.weight, 0.1, 0.15, power=2)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce482e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_nx:\n",
    "    u = gt.extract_largest_component(g)\n",
    "    state = gt.minimize_blockmodel_dl(u)\n",
    "    state.draw(pos=pos, output=\"output/graph_clusters.png\", bg_color=\"white\", output_size=(1000, 1000),\n",
    "               vertex_size=gt.prop_to_size(g.vp.vb, 3, 15, power=.3),\n",
    "               edge_pen_width=gt.prop_to_size(g.ep.weight, 0.1, 0.15, power=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38f5ff",
   "metadata": {},
   "source": [
    "# 4. Wordcloud\n",
    "We can generate term frequencies with the unigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322109d4d4fb3e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:39:10.824440Z",
     "start_time": "2024-07-19T14:38:28.158760Z"
    }
   },
   "outputs": [],
   "source": [
    "from textacy.representations.vectorizers import GroupVectorizer\n",
    "\n",
    "vectorizer = vectorizer = GroupVectorizer(tf_type=\"linear\", idf_type=\"smooth\", norm=\"l2\", min_df=1, max_df=0.95)\n",
    "group_term_matrix = vectorizer.fit_transform(corpus_tokens, types)\n",
    "largest = np.argsort(group_term_matrix.toarray(), axis=1)\n",
    "for i, doc_type_largest in enumerate(largest):\n",
    "    print(f\"Document type: {vectorizer.grps_list[i]}\")\n",
    "    print([vectorizer.terms_list[j] for j in doc_type_largest[::-1][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f22d69cb9da35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T14:39:48.871075Z",
     "start_time": "2024-07-19T14:39:45.723766Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, doc_type_score in enumerate(group_term_matrix.toarray()):\n",
    "    doc_to_score = dict(zip(vectorizer.terms_list, doc_type_score))\n",
    "    wordcloud = wc.WordCloud(width=1000, height=1000, background_color=\"white\",\n",
    "                                colormap=\"tab10\", random_state=2).generate_from_frequencies(doc_to_score)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"output/{field}/wordcloud-{vectorizer.grps_list[i]}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e43ab",
   "metadata": {},
   "source": [
    "# 5. Topic Embedding Comparison\n",
    "\n",
    "In the following, we take both corpora and model them together to understand which topics are shared and which ones different between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ddad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and topic model for all data\n",
    "assert load_all, \"Must load all data for subsequent analysis\"\n",
    "\n",
    "output_path = os.path.join(\"output\", \"combined\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "topic_model, embeddings = model_topics(docs, dates, output_path, min_cluster_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb64ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Plot documents and topics for all data\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='euclidean').fit_transform(embeddings)\n",
    "\n",
    "# Convert data to DataFrame for easy plotting\n",
    "\n",
    "df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\n",
    "df[\"Type\"] = [t == \"safety\" for t in types]\n",
    "model = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "contour = px.density_contour(df, x=\"x\", y=\"y\", color=\"Type\")\n",
    "contour.update_traces(hovertemplate=None, hoverlabel=None)\n",
    "fig = go.Figure(data=contour.data + model.data, layout=model.layout)\n",
    "fig.write_html(os.path.join(output_path, \"documents-combined.html\"), include_mathjax = 'cdn')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "X = df.drop(columns=\"Type\").values\n",
    "y = df[\"Type\"].values\n",
    "# classifier = RandomForestClassifier(n_estimators=100, random_state=42).fit(X, y)\n",
    "classifier = SVC(kernel=\"rbf\", random_state=42).fit(X, y)\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    classifier, X, response_method=\"decision_function\",\n",
    "    xlabel=\"Dimension 1\", ylabel=\"Dimension 2\",\n",
    "    alpha=0.5, cmap=\"twilight_shifted\"\n",
    ")\n",
    "ax = disp.ax_\n",
    "for t in [\"safety\", \"ethics\"]:\n",
    "    ix = [tt == t for tt in types]\n",
    "    ax.scatter(X[ix, 0], X[ix, 1], c=\"lightsteelblue\" if t == \"ethics\" else \"darksalmon\", edgecolor=\"k\", label=t.capitalize())\n",
    "ax.legend(title=\"Classes\")\n",
    "# handles = [matplotlib.patches.Circle((0, 0), .0001, color=\"lightsteelblue\", edgecolor=\"k\", label=\"Ethics\"),\n",
    "        #    matplotlib.patches.Circle((0, 0), .0001, color=\"darksalmon\", edgecolor=\"k\", label=\"Safety\")]\n",
    "# legend = ax.legend(loc=\"lower left\", title=\"Classes\")\n",
    "# ax.add_artist(legend)\n",
    "ax.get_figure().tight_layout() \n",
    "ax.get_figure().savefig(os.path.join(output_path, \"decision_boundary.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b412e",
   "metadata": {},
   "source": [
    "# 6. Venue Analysis\n",
    "\n",
    "In this section, we extract and visualise the major venues for AI safety papers. As our AI ethics papers come from FAccT and AIES proceedings, we do not do this for the AI ethics corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import urllib\n",
    "\n",
    "# Create folder to save the data\n",
    "output_dir = os.path.join(\"data\", field, \"dblp\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Retrieve publications from DBLP by title\n",
    "dblp_data = {}\n",
    "not_found = {}  \n",
    "raised_error = {}\n",
    "multiple_hits = {}\n",
    "try:\n",
    "    dblp_data = json.load(open(f\"{output_dir}/dblp_data.json\", \"r\", encoding=\"utf-8\"))\n",
    "    not_found = json.load(open(f\"{output_dir}/not_found.json\", \"r\", encoding=\"utf-8\"))\n",
    "    raised_error = json.load(open(f\"{output_dir}/raised_error.json\", \"r\", encoding=\"utf-8\"))\n",
    "    multiple_hits = json.load(open(f\"{output_dir}/multiple_hits.json\", \"r\", encoding=\"utf-8\"))\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "BASE_API_URL = \"https://dblp.org/search/publ/api\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313623db",
   "metadata": {},
   "source": [
    "We can now retrieve all publication information from DBLP. Note, these may be incomplete so further processing by hand is required. However, the downloaded data is already located in the data folder so running this cell should not be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in (pbar := tqdm.tqdm(safety)):\n",
    "    title = paper['title']\n",
    "    pid = paper['id']\n",
    "\n",
    "    if pid in dblp_data or pid in not_found or pid in raised_error or pid in multiple_hits:\n",
    "        continue\n",
    "\n",
    "    request = f\"{BASE_API_URL}?q={title}&format=json\"\n",
    "    request = urllib.parse.quote(request, safe=':/?&=')\n",
    "\n",
    "    try:\n",
    "        data = json.load(urllib.request.urlopen(request))\n",
    "    except urllib.error.HTTPError as e:\n",
    "        if e.code == 429:\n",
    "            sleep_time = int(e.headers['Retry-After'])\n",
    "            print(f\"Rate limit exceeded. Sleeping for {sleep_time + 1} seconds.\")\n",
    "            time.sleep(sleep_time + 1.)\n",
    "            data = json.load(urllib.request.urlopen(request))\n",
    "        else:\n",
    "            raised_error.append(pid)\n",
    "            desc = f\"Error retrieving data for {title}: {e}\"\n",
    "    \n",
    "    hits = int(data['result']['hits']['@total'])\n",
    "    if hits == 0:\n",
    "        not_found.append(pid)\n",
    "        desc = f\"Title '{title}' not found\"\n",
    "    elif hits == 1:\n",
    "        data = data['result']['hits']['hit'][0]\n",
    "        dblp_data[pid] = data\n",
    "        desc = f\"Retrieved data for '{title}'\"\n",
    "    else:\n",
    "        not_corr = [hit for hit in data['result']['hits']['hit'] if 'venue' in hit['info'] and hit['info']['venue'] != \"CoRR\"]\n",
    "        if len(not_corr) == 1:\n",
    "            dblp_data[pid] = not_corr[0]\n",
    "            desc = f\"Retrieved data for '{title}'\"        \n",
    "        else:\n",
    "            desc = f\"Multiple hits for '{title}': {hits}\"\n",
    "            multiple_hits[pid] = data\n",
    "\n",
    "    json.dump(dblp_data, open(os.path.join(output_dir, \"dblp_data.json\"), \"w\", encoding=\"utf-8\"), indent=2)\n",
    "    json.dump(not_found, open(os.path.join(output_dir, \"not_found.json\"), \"w\", encoding=\"utf-8\"), indent=2)\n",
    "    json.dump(raised_error, open(os.path.join(output_dir, \"raised_error.json\"), \"w\", encoding=\"utf-8\"), indent=2)\n",
    "    json.dump(multiple_hits, open(os.path.join(output_dir, \"multiple_hits.json\"), \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "    pbar.set_postfix({\"result\": desc})\n",
    "\n",
    "    time.sleep(1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c180d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DBLP data\n",
    "dblp_data = json.load(open(os.path.join(output_dir, \"dblp_data.json\"), \"r\", encoding=\"utf-8\"))\n",
    "venues = pd.Series([data['info']['venue'] for data in dblp_data.values()])\n",
    "counts = venues.value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Plot the distribution of the top 20 most frequent venues\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(counts.index[:20], counts.values[:20], color=\"tab:blue\")\n",
    "plt.xlabel(\"Venue\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 20 Venues\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"output/{field}/venues.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0878ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
