[
	{
		"id": "chouldechovaCaseStudyAlgorithmassisted2018",
		"type": "paper-conference",
		"abstract": "Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.",
		"collection-title": "Proceedings of Machine Learning Research",
		"container-title": "Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA",
		"page": "134–148",
		"publisher": "PMLR",
		"source": "DBLP Computer Science Bibliography",
		"title": "A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions",
		"URL": "http://proceedings.mlr.press/v81/chouldechova18a.html",
		"volume": "81",
		"author": [
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Prado",
				"given": "Diana Benavides"
			},
			{
				"family": "Fialko",
				"given": "Oleksandr"
			},
			{
				"family": "Vaithianathan",
				"given": "Rhema"
			}
		],
		"editor": [
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "speicherPotentialDiscriminationOnline2018a",
		"type": "paper-conference",
		"abstract": "Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "5-19",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Potential for Discrimination in Online Targeted Advertising",
		"URL": "https://proceedings.mlr.press/v81/speicher18a.html",
		"author": [
			{
				"family": "Speicher",
				"given": "Till"
			},
			{
				"family": "Ali",
				"given": "Muhammad"
			},
			{
				"family": "Venkatadri",
				"given": "Giridhari"
			},
			{
				"family": "Ribeiro",
				"given": "Filipe Nunes"
			},
			{
				"family": "Arvanitakis",
				"given": "George"
			},
			{
				"family": "Benevenuto",
				"given": "Fabrício"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Loiseau",
				"given": "Patrick"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "dattaDiscriminationOnlineAdvertising2018",
		"type": "paper-conference",
		"abstract": "We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "20-34",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Discrimination in Online Advertising: A Multidisciplinary Inquiry",
		"title-short": "Discrimination in Online Advertising",
		"URL": "https://proceedings.mlr.press/v81/datta18a.html",
		"author": [
			{
				"family": "Datta",
				"given": "Amit"
			},
			{
				"family": "Datta",
				"given": "Anupam"
			},
			{
				"family": "Makagon",
				"given": "Jael"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "ekstrandPrivacyAllEnsuring2018a",
		"type": "paper-conference",
		"abstract": "In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "35-47",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Privacy for All: Ensuring Fair and Equitable Privacy Protections",
		"title-short": "Privacy for All",
		"URL": "https://proceedings.mlr.press/v81/ekstrand18a.html",
		"author": [
			{
				"family": "Ekstrand",
				"given": "Michael D."
			},
			{
				"family": "Joshaghani",
				"given": "Rezvan"
			},
			{
				"family": "Mehrpouyan",
				"given": "Hoda"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "selbstMeaningfulInformationRight2018a",
		"type": "paper-conference",
		"abstract": "There is no single, neat statutory provision labeled the “right to explanation” in Europe’s new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13–15 provide rights to “meaningful information about the logic involved” in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "48-48",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "“Meaningful Information” and the Right to Explanation",
		"URL": "https://proceedings.mlr.press/v81/selbst18a.html",
		"author": [
			{
				"family": "Selbst",
				"given": "Andrew"
			},
			{
				"family": "Powles",
				"given": "Julia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "phillipsInterpretableActiveLearning2018a",
		"type": "paper-conference",
		"abstract": "Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. These explanations can also be used to generate batches based on common sources of uncertainty. These regions of common uncertainty can be useful for understanding a model’s current weaknesses.  In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model’s predictions between one subgroup and another.  Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. We also measure how the choice of initial labeled examples effects groups over time.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "49-61",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Interpretable Active Learning",
		"URL": "https://proceedings.mlr.press/v81/phillips18a.html",
		"author": [
			{
				"family": "Phillips",
				"given": "Richard"
			},
			{
				"family": "Chang",
				"given": "Kyu Hyun"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "barabasInterventionsPredictionsReframing2018a",
		"type": "paper-conference",
		"abstract": "Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "62-76",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment",
		"title-short": "Interventions over Predictions",
		"URL": "https://proceedings.mlr.press/v81/barabas18a.html",
		"author": [
			{
				"family": "Barabas",
				"given": "Chelsea"
			},
			{
				"family": "Virza",
				"given": "Madars"
			},
			{
				"family": "Dinakar",
				"given": "Karthik"
			},
			{
				"family": "Ito",
				"given": "Joichi"
			},
			{
				"family": "Zittrain",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "buolamwiniGenderShadesIntersectional2018a",
		"type": "paper-conference",
		"abstract": "Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "77-91",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification",
		"title-short": "Gender Shades",
		"URL": "https://proceedings.mlr.press/v81/buolamwini18a.html",
		"author": [
			{
				"family": "Buolamwini",
				"given": "Joy"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "madaanAnalyzeDetectRemove2018a",
		"type": "paper-conference",
		"abstract": "The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry (\\it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "92-105",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies",
		"URL": "https://proceedings.mlr.press/v81/madaan18a.html",
		"author": [
			{
				"family": "Madaan",
				"given": "Nishtha"
			},
			{
				"family": "Mehta",
				"given": "Sameep"
			},
			{
				"family": "Agrawaal",
				"given": "Taneea"
			},
			{
				"family": "Malhotra",
				"given": "Vrinda"
			},
			{
				"family": "Aggarwal",
				"given": "Aditi"
			},
			{
				"family": "Gupta",
				"given": "Yatin"
			},
			{
				"family": "Saxena",
				"given": "Mayank"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "duarteMixedMessagesLimits2018a",
		"type": "paper-conference",
		"abstract": "Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, “fake news” or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Today’s tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker.  A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service.  This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakers’ knowledge about what is possible with automated text analysis.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "106-106",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Mixed Messages? The Limits of Automated Social Media Content Analysis",
		"title-short": "Mixed Messages?",
		"URL": "https://proceedings.mlr.press/v81/duarte18a.html",
		"author": [
			{
				"family": "Duarte",
				"given": "Natasha"
			},
			{
				"family": "Llanso",
				"given": "Emma"
			},
			{
				"family": "Loup",
				"given": "Anna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "menonCostFairnessBinary2018a",
		"type": "paper-conference",
		"abstract": "Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "107-118",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "The cost of fairness in binary classification",
		"URL": "https://proceedings.mlr.press/v81/menon18a.html",
		"author": [
			{
				"family": "Menon",
				"given": "Aditya Krishna"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "dworkDecoupledClassifiersGroupFair2018a",
		"type": "paper-conference",
		"abstract": "When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "119-133",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Decoupled Classifiers for Group-Fair and Efficient Machine Learning",
		"URL": "https://proceedings.mlr.press/v81/dwork18a.html",
		"author": [
			{
				"family": "Dwork",
				"given": "Cynthia"
			},
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Leiserson",
				"given": "Max"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "binnsFairnessMachineLearning2018a",
		"type": "paper-conference",
		"abstract": "What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "149-159",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Fairness in Machine Learning: Lessons from Political Philosophy",
		"title-short": "Fairness in Machine Learning",
		"URL": "https://proceedings.mlr.press/v81/binns18a.html",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "ensignRunawayFeedbackLoops2018a",
		"type": "paper-conference",
		"abstract": "Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "160-171",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Runaway Feedback Loops in Predictive Policing",
		"URL": "https://proceedings.mlr.press/v81/ensign18a.html",
		"author": [
			{
				"family": "Ensign",
				"given": "Danielle"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Neville",
				"given": "Scott"
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "ekstrandAllCoolKids2018a",
		"type": "paper-conference",
		"abstract": "In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complex—and likely detrimental—interaction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naïve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "172-186",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness",
		"title-short": "All The Cool Kids, How Do They Fit In?",
		"URL": "https://proceedings.mlr.press/v81/ekstrand18b.html",
		"author": [
			{
				"family": "Ekstrand",
				"given": "Michael D."
			},
			{
				"family": "Tian",
				"given": "Mucun"
			},
			{
				"family": "Azpiazu",
				"given": "Ion Madrazo"
			},
			{
				"family": "Ekstrand",
				"given": "Jennifer D."
			},
			{
				"family": "Anuyah",
				"given": "Oghenemaro"
			},
			{
				"family": "McNeill",
				"given": "David"
			},
			{
				"family": "Pera",
				"given": "Maria Soledad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "kamishimaRecommendationIndependence2018a",
		"type": "paper-conference",
		"abstract": "This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "187-201",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Recommendation Independence",
		"URL": "https://proceedings.mlr.press/v81/kamishima18a.html",
		"author": [
			{
				"family": "Kamishima",
				"given": "Toshihiro"
			},
			{
				"family": "Akaho",
				"given": "Shotaro"
			},
			{
				"family": "Asoh",
				"given": "Hideki"
			},
			{
				"family": "Sakuma",
				"given": "Jun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "burkeBalancedNeighborhoodsMultisided2018a",
		"type": "paper-conference",
		"abstract": "Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "202-214",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Balanced Neighborhoods for Multi-sided Fairness in Recommendation",
		"URL": "https://proceedings.mlr.press/v81/burke18a.html",
		"author": [
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Sonboli",
				"given": "Nasim"
			},
			{
				"family": "Ordonez-Gauger",
				"given": "Aldo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "celisControllingPolarizationPersonalization2019",
		"type": "paper-conference",
		"abstract": "Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but – subject to these constraints – will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287601",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "160–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Controlling polarization in personalization: An algorithmic framework",
		"URL": "https://doi.org/10.1145/3287560.3287601",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Salehi",
				"given": "Farnood"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hutchinson50YearsTest2019",
		"type": "paper-conference",
		"abstract": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287600",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "49–58",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "50 years of test (un)fairness: Lessons for machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287600",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mouzannarFairDecisionMaking2019",
		"type": "paper-conference",
		"abstract": "The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action.We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287599",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "359–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From fair decision making to social equality",
		"URL": "https://doi.org/10.1145/3287560.3287599",
		"author": [
			{
				"family": "Mouzannar",
				"given": "Hussein"
			},
			{
				"family": "Ohannessian",
				"given": "Mesrob I."
			},
			{
				"family": "Srebro",
				"given": "Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "selbstFairnessAbstractionSociotechnical2019",
		"type": "paper-conference",
		"abstract": "A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science—such as abstraction and modular design—are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce \"fair\" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five \"traps\" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287598",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "59–68",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and abstraction in sociotechnical systems",
		"URL": "https://doi.org/10.1145/3287560.3287598",
		"author": [
			{
				"family": "Selbst",
				"given": "Andrew D."
			},
			{
				"family": "Boyd",
				"given": "Danah"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Vertesi",
				"given": "Janet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "huDisparateEffectsStrategic2019",
		"type": "paper-conference",
		"abstract": "When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed \"strategic manipulation,\" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to \"trick\" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off—even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's \"quality\" when agents' capacities to adaptively respond differ.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287597",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "259–268",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The disparate effects of strategic manipulation",
		"URL": "https://doi.org/10.1145/3287560.3287597",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mitchellModelCardsModel2019",
		"type": "paper-conference",
		"abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287596",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "220–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model cards for model reporting",
		"URL": "https://doi.org/10.1145/3287560.3287596",
		"author": [
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Wu",
				"given": "Simone"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Vasserman",
				"given": "Lucy"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Spitzer",
				"given": "Elena"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cardDeepWeightedAveraging2019",
		"type": "paper-conference",
		"abstract": "Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287595",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "369–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deep weighted averaging classifiers",
		"URL": "https://doi.org/10.1145/3287560.3287595",
		"author": [
			{
				"family": "Card",
				"given": "Dallas"
			},
			{
				"family": "Zhang",
				"given": "Michael"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chenFairnessUnawarenessAssessing2019",
		"type": "paper-conference",
		"abstract": "Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287594",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "339–348",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness under unawareness: Assessing disparity when protected class is unobserved",
		"URL": "https://doi.org/10.1145/3287560.3287594",
		"author": [
			{
				"family": "Chen",
				"given": "Jiahao"
			},
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Mao",
				"given": "Xiaojie"
			},
			{
				"family": "Svacha",
				"given": "Geoffry"
			},
			{
				"family": "Udell",
				"given": "Madeleine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "obermeyerDissectingRacialBias2019",
		"type": "paper-conference",
		"abstract": "A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care.To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs).We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate.An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t–1 (features), fine-grained care utilization data in year t – 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks.The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health – not just costs – also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost – for example, race.We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health.The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on – cost – is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care – incentives that induce health systems to focus on dollars rather than health – also has consequences for the way algorithms are built and monitored.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287593",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dissecting racial bias in an algorithm that guides health decisions for 70 million people",
		"URL": "https://doi.org/10.1145/3287560.3287593",
		"author": [
			{
				"family": "Obermeyer",
				"given": "Ziad"
			},
			{
				"family": "Mullainathan",
				"given": "Sendhil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kearnsEmpiricalStudyRich2019",
		"type": "paper-conference",
		"abstract": "Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287592",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "100–109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical study of rich subgroup fairness for machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287592",
		"author": [
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "leongRobotEyesWide2019",
		"type": "paper-conference",
		"abstract": "The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, \"Averting Robot Eyes.\" Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287591",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "299–308",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robot eyes wide shut: Understanding dishonest anthropomorphism",
		"URL": "https://doi.org/10.1145/3287560.3287591",
		"author": [
			{
				"family": "Leong",
				"given": "Brenda"
			},
			{
				"family": "Selinger",
				"given": "Evan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "laiHumanPredictionsExplanations2019",
		"type": "paper-conference",
		"abstract": "Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (&gt;20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287590",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "29–38",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection",
		"URL": "https://doi.org/10.1145/3287560.3287590",
		"author": [
			{
				"family": "Lai",
				"given": "Vivian"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "friedlerComparativeStudyFairnessenhancing2019",
		"type": "paper-conference",
		"abstract": "Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287589",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "329–338",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A comparative study of fairness-enhancing interventions in machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287589",
		"author": [
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Choudhary",
				"given": "Sonam"
			},
			{
				"family": "Hamilton",
				"given": "Evan P."
			},
			{
				"family": "Roth",
				"given": "Derek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "albarghouthiFairnessawareProgramming2019",
		"type": "paper-conference",
		"abstract": "Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287588",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "211–219",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness-aware programming",
		"URL": "https://doi.org/10.1145/3287560.3287588",
		"author": [
			{
				"family": "Albarghouthi",
				"given": "Aws"
			},
			{
				"family": "Vinitsky",
				"given": "Samuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chancellorTaxonomyEthicalTensions2019",
		"type": "paper-conference",
		"abstract": "Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287587",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "79–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A taxonomy of ethical tensions in inferring mental health states from social media",
		"URL": "https://doi.org/10.1145/3287560.3287587",
		"author": [
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Birnbaum",
				"given": "Michael L."
			},
			{
				"family": "Caine",
				"given": "Eric D."
			},
			{
				"family": "Silenzio",
				"given": "Vincent M. B."
			},
			{
				"family": "De Choudhury",
				"given": "Munmun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "celisClassificationFairnessConstraints2019",
		"type": "paper-conference",
		"abstract": "Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex \"linear fractional\" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287586",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "319–328",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Classification with fairness constraints: A meta-algorithm with provable guarantees",
		"URL": "https://doi.org/10.1145/3287560.3287586",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Huang",
				"given": "Lingxiao"
			},
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "engelmannClearSanctionsVague2019",
		"type": "paper-conference",
		"abstract": "China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, \"good\" behavior can result in material rewards and reputational gain while \"bad\" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines \"good\" and \"bad\" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform \"Credit China\", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on \"good\" behavior) and the Blacklist (information on \"bad\" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287585",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "69–78",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Clear sanctions, vague rewards: How china's social credit system currently defines \"Good\" and \"Bad\" behavior",
		"URL": "https://doi.org/10.1145/3287560.3287585",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Fischer",
				"given": "Felix"
			},
			{
				"family": "Kao",
				"given": "Ching-yu"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "heidariMoralFrameworkUnderstanding2019",
		"type": "paper-conference",
		"abstract": "We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)—an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287584",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "181–190",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A moral framework for understanding fair ML through economic models of equality of opportunity",
		"URL": "https://doi.org/10.1145/3287560.3287584",
		"author": [
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Krause",
				"given": "Andreas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "bountouridisSIRENSimulationFramework2019",
		"type": "paper-conference",
		"abstract": "The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as \"Matthew effects\", \"filter bubbles\", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287583",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "150–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SIREN: A simulation framework for understanding the effects of recommender systems in online news environments",
		"URL": "https://doi.org/10.1145/3287560.3287583",
		"author": [
			{
				"family": "Bountouridis",
				"given": "Dimitrios"
			},
			{
				"family": "Harambam",
				"given": "Jaron"
			},
			{
				"family": "Makhortykh",
				"given": "Mykola"
			},
			{
				"family": "Marrero",
				"given": "Mónica"
			},
			{
				"family": "Tintarev",
				"given": "Nava"
			},
			{
				"family": "Hauff",
				"given": "Claudia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "babaioffFairAllocationCompetitive2019",
		"type": "paper-conference",
		"abstract": "Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a \"fair\" allocation of the items among them?Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to \"purchase\" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency – prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible.We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287582",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair allocation through competitive equilibrium from generic incomes",
		"URL": "https://doi.org/10.1145/3287560.3287582",
		"author": [
			{
				"family": "Babaioff",
				"given": "Moshe"
			},
			{
				"family": "Nisan",
				"given": "Noam"
			},
			{
				"family": "Talgam-Cohen",
				"given": "Inbal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "babaeiAnalyzingBiasesPerception2019",
		"type": "paper-conference",
		"abstract": "Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check.However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories – (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users.The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion.On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking.In summary, we make the following contributions in this work.1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story.2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions.3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth.Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287581",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "139",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing biases in perception of truth in news stories and their implications for fact checking",
		"URL": "https://doi.org/10.1145/3287560.3287581",
		"author": [
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Kulshrestha",
				"given": "Juhi"
			},
			{
				"family": "Redmiles",
				"given": "Elissa M."
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ribeiroMicrotargetingSociallyDivisive2019",
		"type": "paper-conference",
		"abstract": "Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are \"divisive\": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287580",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "140–149",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On microtargeting socially divisive ads: A case study of russia-linked ad campaigns on facebook",
		"URL": "https://doi.org/10.1145/3287560.3287580",
		"author": [
			{
				"family": "Ribeiro",
				"given": "Filipe N."
			},
			{
				"family": "Saha",
				"given": "Koustuv"
			},
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Henrique",
				"given": "Lucas"
			},
			{
				"family": "Messias",
				"given": "Johnnatan"
			},
			{
				"family": "Benevenuto",
				"given": "Fabricio"
			},
			{
				"family": "Goga",
				"given": "Oana"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Redmiles",
				"given": "Elissa M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "immorlicaAccessPopulationlevelSignaling2019",
		"type": "paper-conference",
		"abstract": "We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR).We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287579",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "249–258",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Access to population-level signaling as a source of inequality",
		"URL": "https://doi.org/10.1145/3287560.3287579",
		"author": [
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Ligett",
				"given": "Katrina"
			},
			{
				"family": "Ziani",
				"given": "Juba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kannanDownstreamEffectsAffirmative2019",
		"type": "paper-conference",
		"abstract": "We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287578",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "240–248",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Downstream effects of affirmative action",
		"URL": "https://doi.org/10.1145/3287560.3287578",
		"author": [
			{
				"family": "Kannan",
				"given": "Sampath"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Ziani",
				"given": "Juba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "youngOpenVsClosed2019",
		"type": "paper-conference",
		"abstract": "Data too sensitive to be \"open\" for analysis and re-purposing typically remains \"closed\" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287577",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "191–200",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond open vs. Closed: Balancing individual privacy and public accountability in data sharing",
		"URL": "https://doi.org/10.1145/3287560.3287577",
		"author": [
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Rodriguez",
				"given": "Luke"
			},
			{
				"family": "Keller",
				"given": "Emily"
			},
			{
				"family": "Sun",
				"given": "Feiyang"
			},
			{
				"family": "Sa",
				"given": "Boyang"
			},
			{
				"family": "Whittington",
				"given": "Jan"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "milliSocialCostStrategic2019",
		"type": "paper-conference",
		"abstract": "Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population.Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287576",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "230–239",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The social cost of strategic classification",
		"URL": "https://doi.org/10.1145/3287560.3287576",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Miller",
				"given": "John"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "benthallRacialCategoriesMachine2019",
		"type": "paper-conference",
		"abstract": "Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled \"Black\" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287575",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "289–298",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial categories in machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287575",
		"author": [
			{
				"family": "Benthall",
				"given": "Sebastian"
			},
			{
				"family": "Haynes",
				"given": "Bruce D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mittelstadtExplainingExplanationsAI2019",
		"type": "paper-conference",
		"abstract": "Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that \"All models are wrong but some are useful.\" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a \"do it yourself kit\" for explanations, allowing a practitioner to directly answer \"what if questions\" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287574",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "279–288",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explaining explanations in AI",
		"URL": "https://doi.org/10.1145/3287560.3287574",
		"author": [
			{
				"family": "Mittelstadt",
				"given": "Brent"
			},
			{
				"family": "Russell",
				"given": "Chris"
			},
			{
				"family": "Wachter",
				"given": "Sandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "glymourMeasuringBiasesThat2019",
		"type": "paper-conference",
		"abstract": "Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of \"procedural bias\" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of \"outcome bias\" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of \"behavior-relative error bias\" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of \"score-relative error bias\" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized.In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287573",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "269–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring the biases that matter: The ethical and casual foundations for measures of fairness in algorithms",
		"URL": "https://doi.org/10.1145/3287560.3287573",
		"author": [
			{
				"family": "Glymour",
				"given": "Bruce"
			},
			{
				"family": "Herington",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "de-arteagaBiasBiosCase2019",
		"type": "paper-conference",
		"abstract": "We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators—such as first names and pronouns—in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287572",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "120–128",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
		"URL": "https://doi.org/10.1145/3287560.3287572",
		"author": [
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Romanov",
				"given": "Alexey"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			},
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Geyik",
				"given": "Sahin"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "elzaynFairAlgorithmsLearning2019",
		"type": "paper-conference",
		"abstract": "Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287571",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "170–179",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair algorithms for learning in allocation problems",
		"URL": "https://doi.org/10.1145/3287560.3287571",
		"author": [
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Jabbari",
				"given": "Shahin"
			},
			{
				"family": "Jung",
				"given": "Christopher"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Schutzman",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chakrabortyEqualityVoiceFair2019",
		"type": "paper-conference",
		"abstract": "To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups.To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287570",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "129–138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equality of voice: Towards fair representation in crowdsourced top-K recommendations",
		"URL": "https://doi.org/10.1145/3287560.3287570",
		"author": [
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Patro",
				"given": "Gourab K."
			},
			{
				"family": "Ganguly",
				"given": "Niloy"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Loiseau",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "russellEfficientSearchDiverse2019",
		"type": "paper-conference",
		"abstract": "This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a \"mixed polytope\" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287569",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "20–28",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Efficient search for diverse coherent explanations",
		"URL": "https://doi.org/10.1145/3287560.3287569",
		"author": [
			{
				"family": "Russell",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "goldenfeinProfilingPotentialComputer2019",
		"type": "paper-conference",
		"abstract": "Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287568",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "110–119",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The profiling potential of computer vision and the challenge of computational empiricism",
		"URL": "https://doi.org/10.1145/3287560.3287568",
		"author": [
			{
				"family": "Goldenfein",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "passiProblemFormulationFairness2019",
		"type": "paper-conference",
		"abstract": "Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team—and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases—we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways—and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287567",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "39–48",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Problem formulation and fairness",
		"URL": "https://doi.org/10.1145/3287560.3287567",
		"author": [
			{
				"family": "Passi",
				"given": "Samir"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ustunActionableRecourseLinear2019",
		"type": "paper-conference",
		"abstract": "Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and – more importantly –will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287566",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "10–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable recourse in linear classification",
		"URL": "https://doi.org/10.1145/3287560.3287566",
		"author": [
			{
				"family": "Ustun",
				"given": "Berk"
			},
			{
				"family": "Spangher",
				"given": "Alexander"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jiangWhosGuineaPig2019",
		"type": "paper-conference",
		"abstract": "A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287565",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "201–210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who's the guinea pig? Investigating online A/B/n tests in-the-Wild",
		"URL": "https://doi.org/10.1145/3287560.3287565",
		"author": [
			{
				"family": "Jiang",
				"given": "Shan"
			},
			{
				"family": "Martin",
				"given": "John"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "madrasFairnessCausalAwareness2019",
		"type": "paper-conference",
		"abstract": "How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287564",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "349–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness through causal awareness: Learning causal latent-variable models for biased data",
		"URL": "https://doi.org/10.1145/3287560.3287564",
		"author": [
			{
				"family": "Madras",
				"given": "David"
			},
			{
				"family": "Creager",
				"given": "Elliot"
			},
			{
				"family": "Pitassi",
				"given": "Toniann"
			},
			{
				"family": "Zemel",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "greenDisparateInteractionsAlgorithmintheloop2019",
		"type": "paper-conference",
		"abstract": "Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions—they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with \"disparate interactions,\" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new \"algorithm-in-the-loop\" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287563",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "90–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments",
		"URL": "https://doi.org/10.1145/3287560.3287563",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			},
			{
				"family": "Chen",
				"given": "Yiling"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "milliModelReconstructionModel2019",
		"type": "paper-conference",
		"abstract": "We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations.On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive.Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287562",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "1–9",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model reconstruction from model explanations",
		"URL": "https://doi.org/10.1145/3287560.3287562",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "canettiSoftClassifiersHard2019",
		"type": "paper-conference",
		"abstract": "A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary \"scoring\" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain \"nice\" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for \"nice\" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287561",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "309–318",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From soft classifiers to hard decisions: How fair can we be?",
		"URL": "https://doi.org/10.1145/3287560.3287561",
		"author": [
			{
				"family": "Canetti",
				"given": "Ran"
			},
			{
				"family": "Cohen",
				"given": "Aloni"
			},
			{
				"family": "Dikkala",
				"given": "Nishanth"
			},
			{
				"family": "Ramnarayan",
				"given": "Govind"
			},
			{
				"family": "Scheffler",
				"given": "Sarah"
			},
			{
				"family": "Smith",
				"given": "Adam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "noriega-camperoAlgorithmicTargetingSocial2020",
		"type": "paper-conference",
		"abstract": "Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them—and who is not—are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375784",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "241–251",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic targeting of social policies: fairness, accuracy, and distributed governance",
		"URL": "https://doi.org/10.1145/3351095.3375784",
		"author": [
			{
				"family": "Noriega-Campero",
				"given": "Alejandro"
			},
			{
				"family": "Garcia-Bulle",
				"given": "Bernardo"
			},
			{
				"family": "Cantu",
				"given": "Luis Fernando"
			},
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Tejerina",
				"given": "Luis"
			},
			{
				"family": "Pentland",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nasrBiddingStrategiesGender2020",
		"type": "paper-conference",
		"abstract": "Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375783",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "337–347",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bidding strategies with gender nondiscrimination constraints for online ad auctions",
		"URL": "https://doi.org/10.1145/3351095.3375783",
		"author": [
			{
				"family": "Nasr",
				"given": "Milad"
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "yangFairerDatasetsFiltering2020",
		"type": "paper-conference",
		"abstract": "Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375709",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "547–558",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy",
		"URL": "https://doi.org/10.1145/3351095.3375709",
		"author": [
			{
				"family": "Yang",
				"given": "Kaiyu"
			},
			{
				"family": "Qinami",
				"given": "Klint"
			},
			{
				"family": "Fei-Fei",
				"given": "Li"
			},
			{
				"family": "Deng",
				"given": "Jia"
			},
			{
				"family": "Russakovsky",
				"given": "Olga"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pritchardBurnDreamReboot2020",
		"type": "paper-conference",
		"abstract": "Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a \"future otherwise\". We argue that \"speculation\" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes \"how did their dreams make rooms to dream in\"... or not, in the case of coercive practices of computing. And \"what if she changes her dream?\" What if we reboot this dream?1",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375697",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "683",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Burn, dream and reboot! speculating backwards for the missing archive on non-coercive computing",
		"URL": "https://doi.org/10.1145/3351095.3375697",
		"author": [
			{
				"family": "Pritchard",
				"given": "Helen"
			},
			{
				"family": "Snodgrass",
				"given": "Eric"
			},
			{
				"family": "Morrison",
				"given": "Romi Ron"
			},
			{
				"family": "Britton",
				"given": "Loren"
			},
			{
				"family": "Moll",
				"given": "Joana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "williamsHardwiringDiscriminatoryPolice2020",
		"type": "paper-conference",
		"abstract": "We, as activists, as anti-racist organisations, and as racialised communities in Europe, know too well what it means to be over-policed and under-protected. Still, in 2019, we feel the need to evidence racial profiling, to contest narratives placing us as a threat to ‘security’ and essentially to unsettle presumptions as to our criminality. We are still mastering the techniques with which we contest over-policing, brutality and racial profiling. We must now contend with another challenge.\nWhen law enforcement resorts to new technology to aid their practice, we find ourselves at further risk. Not only must we consider our physical safety in our relations with the authorities, we also need to be informed about the security of our data. The use of systems to profile, to surveil and to provide a logic to discrimination is not new. What is new is the sense of neutrality afforded to\ndata-driven policing. This report opens a conversation between all those invested in anti-racism, data privacy and non-discrimination in general. It is crucial that we use our collective knowledge to resist.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375695",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "691",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Hardwiring discriminatory police practices: the implications of data-driven technological policing on minority (ethnic and religious) people and communities",
		"URL": "https://doi.org/10.1145/3351095.3375695",
		"author": [
			{
				"family": "Williams",
				"given": "Patrick"
			},
			{
				"family": "Kind",
				"given": "Eric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hannaCtrlZAIZineFair2020",
		"type": "paper-conference",
		"abstract": "The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375692",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CtrlZ.AI zine fair: critical perspectives",
		"URL": "https://doi.org/10.1145/3351095.3375692",
		"author": [
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barocasWhenNotDesign2020",
		"type": "paper-conference",
		"abstract": "Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375691",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "695",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When not to design, build, or deploy",
		"URL": "https://doi.org/10.1145/3351095.3375691",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Biega",
				"given": "Asia J."
			},
			{
				"family": "Fish",
				"given": "Benjamin"
			},
			{
				"family": "Niklas",
				"given": "Jundefineddrzej"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ahmadFairnessAccountabilityTransparency2020",
		"type": "paper-conference",
		"abstract": "The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375690",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, accountability, transparency in AI at scale: lessons from national programs",
		"URL": "https://doi.org/10.1145/3351095.3375690",
		"author": [
			{
				"family": "Ahmad",
				"given": "Muhammad Aurangzeb"
			},
			{
				"family": "Teredesai",
				"given": "Ankur"
			},
			{
				"family": "Eckert",
				"given": "Carly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sassamanCreatingCommunitybasedTech2020",
		"type": "paper-conference",
		"abstract": "What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375689",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "685",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Creating community-based tech policy: case studies, lessons learned, and what technologists and communities can do together",
		"URL": "https://doi.org/10.1145/3351095.3375689",
		"author": [
			{
				"family": "Sassaman",
				"given": "Hannah"
			},
			{
				"family": "Lee",
				"given": "Jennifer"
			},
			{
				"family": "Irvine",
				"given": "Jenessa"
			},
			{
				"family": "Narayan",
				"given": "Shankar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "allhutterDeconstructingFATUsing2020",
		"type": "paper-conference",
		"abstract": "Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas—who gets to speak and to define bias, (un)fairness, and discrimination? \"Deconstructing FAT\" is a CRAFT workshop that aims at complicating this question by asking how \"we\" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375688",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "687",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deconstructing FAT: using memories to collectively explore implicit assumptions, values and context in practices of debiasing and discrimination-awareness",
		"URL": "https://doi.org/10.1145/3351095.3375688",
		"author": [
			{
				"family": "Allhutter",
				"given": "Doris"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bakerAlgorithmicallyEncodedIdentities2020",
		"type": "paper-conference",
		"abstract": "Our aim with this workshop is to provide a venue within which the FAT* community can thoughtfully engage with identity and the categories which are imposed on people as part of making sense of their identities. Most people have nuanced and deeply personal understandings of what identity categories mean to them; however, sociotechnical systems must, through a set of classification decisions, reduce the nuance and complexity of those identities into discrete categories. The impact of misclassifications can range from the uncomfortable (e.g. displaying ads for items that aren't desirable) to devastating (e.g. being denied medical care; being evaluated as having a high risk of criminal recidivism). However, even the act of being classified can force an individual into categories which feel foreign and othering. Through this workshop, we hope to connect participants' personal understandings of identity to how identity is 'seen' and categorized by sociotechnical systems.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375687",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "681",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmically encoded identities: reframing human classification",
		"URL": "https://doi.org/10.1145/3351095.3375687",
		"author": [
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "givensCenteringDisabilityPerspectives2020",
		"type": "paper-conference",
		"abstract": "It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in \"anonymized\" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the \"long tail\" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt \"reasonable accommodations\" for qualified individuals with a disability. But what is a \"reasonable accommodation\" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375686",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "684",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Centering disability perspectives in algorithmic fairness, accountability, &amp; transparency",
		"URL": "https://doi.org/10.1145/3351095.3375686",
		"author": [
			{
				"family": "Givens",
				"given": "Alexandra Reeve"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wanLostTranslationInteractive2020",
		"type": "paper-conference",
		"abstract": "There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a \"marker for truth\" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375685",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "692",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lost in translation: an interactive workshop mapping interdisciplinary translations for epistemic justice",
		"URL": "https://doi.org/10.1145/3351095.3375685",
		"author": [
			{
				"family": "Wan",
				"given": "Evelyn"
			},
			{
				"family": "Groot",
				"given": "Aviva",
				"non-dropping-particle": "de"
			},
			{
				"family": "Jameson",
				"given": "Shazade"
			},
			{
				"family": "Păun",
				"given": "Mara"
			},
			{
				"family": "Lücking",
				"given": "Phillip"
			},
			{
				"family": "Klumbyte",
				"given": "Goda"
			},
			{
				"family": "Lämmerhirt",
				"given": "Danny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barryEthicsGroundPrinciples2020",
		"type": "paper-conference",
		"abstract": "Surveys of public attitudes show that people believe it is possible to design ethical AI. However the everyday professional development context can offer minimal space for ethical reflection or oversight, creating a significant gap between public expectations and the performance of ethics in practice. This 2-part workshop includes an offsite visit to Telefónica Innovation Alpha and uses storytelling and theatre methods to examine how and where ethical reflection happens on the ground. It will explore the gaps in expectations and identify alternative approaches to more effective ethical performance. Bringing social scientists, data scientists, designers, civic rights activists and ethics consultants together to focus on AI/ML in the health context, it will foster critical and creative activities that will bring to the surface the structural, disciplinary, social and epistemological challenges to effective ethical performance in practice. Participants will explore and enact where, when and how meaningful interventions can happen.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375684",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "688",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics on the ground: from principles to practice",
		"URL": "https://doi.org/10.1145/3351095.3375684",
		"author": [
			{
				"family": "Barry",
				"given": "Marguerite"
			},
			{
				"family": "Kerr",
				"given": "Aphra"
			},
			{
				"family": "Smith",
				"given": "Oliver"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "szymielewiczWhereAlgorithmicAccountability2020",
		"type": "paper-conference",
		"abstract": "This hands-on session takes academic concepts and their formulation in policy initiatives around algorithmic accountability and explainability and tests them against real cases. In small groups we will (1) test selected frameworks on algorithmic accountability and explainability against a concrete case study (that likely constitutes a human rights violation) and (2) test different formats to explain important aspects of an automated decision-making process (such as input data, type of an algorithm used, design decisions and technical parameters, expected outcomes) to various audiences (end users, affected communities, watchdog organisations, public sector agencies and regulators). We invite participants with various backgrounds: researchers, technologists, human rights advocates, public servants and designers.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375683",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "689",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Where do algorithmic accountability and explainability frameworks take us in the real world? from theory to practice",
		"URL": "https://doi.org/10.1145/3351095.3375683",
		"author": [
			{
				"family": "Szymielewicz",
				"given": "Katarzyna"
			},
			{
				"family": "Bacciarelli",
				"given": "Anna"
			},
			{
				"family": "Hidvegi",
				"given": "Fanny"
			},
			{
				"family": "Foryciarz",
				"given": "Agata"
			},
			{
				"family": "Pénicaud",
				"given": "Soizic"
			},
			{
				"family": "Spielkamp",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "gossManifestingSociotechnicalExperimenting2020",
		"type": "paper-conference",
		"abstract": "Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that \"[fairness work] require[s] technical researchers to learn new skills or partner with social scientists\" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That \"social context\" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375682",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "693",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Manifesting the sociotechnical: experimenting with methods for social context and social justice",
		"URL": "https://doi.org/10.1145/3351095.3375682",
		"author": [
			{
				"family": "Goss",
				"given": "Ezra"
			},
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Sabin",
				"given": "Manuel"
			},
			{
				"family": "Teeple",
				"given": "Stephanie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "baxterBridgingGapAI2020",
		"type": "paper-conference",
		"abstract": "The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375680",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bridging the gap from AI ethics research to practice",
		"URL": "https://doi.org/10.1145/3351095.3375680",
		"author": [
			{
				"family": "Baxter",
				"given": "Kathy"
			},
			{
				"family": "Schlesinger",
				"given": "Yoav"
			},
			{
				"family": "Aerni",
				"given": "Sarah"
			},
			{
				"family": "Baker",
				"given": "Lewis"
			},
			{
				"family": "Dawson",
				"given": "Julie"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Kloumann",
				"given": "Isabel"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "huWhatsSexGot2020",
		"type": "paper-conference",
		"abstract": "The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group \"female\" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the \"effects\" that sex purportedly \"causes\" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375674",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "513",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's sex got to do with machine learning?",
		"URL": "https://doi.org/10.1145/3351095.3375674",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Kohler-Hausmann",
				"given": "Issa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "oswaldCanAlgorithmicSystem2020",
		"type": "paper-conference",
		"abstract": "This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375673",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "698",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can an algorithmic system be a 'friend' to a police officer's discretion? ACM FAT 2020 translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375673",
		"author": [
			{
				"family": "Oswald",
				"given": "Marion"
			},
			{
				"family": "Powell",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rakovaAssessingIntersectionOrganizational2020",
		"type": "paper-conference",
		"abstract": "The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375672",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "697",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing the intersection of organizational structure and FAT* efforts within industry: implications tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375672",
		"author": [
			{
				"family": "Rakova",
				"given": "Bogdana"
			},
			{
				"family": "Chowdhury",
				"given": "Rumman"
			},
			{
				"family": "Yang",
				"given": "Jingying"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jacobsMeaningMeasurementBias2020",
		"type": "paper-conference",
		"abstract": "The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different—and occasionally incomparable—proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring \"bias.\" We show that this framework helps to clarify the way unobservable theoretical constructs—such as \"creditworthiness,\" \"risk to society,\" or \"tweet toxicity\"—are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining \"bias\" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring \"bias.\" In so doing, we illustrate the limits of current \"debiasing\" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375671",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "706",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The meaning and measurement of bias: lessons from natural language processing",
		"URL": "https://doi.org/10.1145/3351095.3375671",
		"author": [
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Daumé",
				"given": "Hal"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "burkeExperimentationFairnessawareRecommendation2020",
		"type": "paper-conference",
		"abstract": "The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375670",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Experimentation with fairness-aware recommendation using librec-auto: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375670",
		"author": [
			{
				"family": "Burke",
				"given": "Robin Douglas"
			},
			{
				"family": "Mansoury",
				"given": "Masoud"
			},
			{
				"family": "Sonboli",
				"given": "Nasim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "senTotalSurveyError2020",
		"type": "paper-conference",
		"abstract": "The digital traces of hundreds of millions of people offer increasingly comprehensive pictures of both individuals and groups on different platforms, but also allow inferences about broader target populations beyond those platforms. Studying the errors that can occur when digital traces are used to learn about humans and social phenomena is essential. Many similar errors also affect survey estimates, which survey designers have been addressing for decades, most notably using the Total Survey Error Framework (TSE). In this tutorial, we first introduce the audience to the concepts and guidelines of the TSE and how they are applied by survey practitioners in the social sciences. Second, we introduce our own conceptual framework to diagnose, understand, and avoid errors that may occur in studies that are based on digital traces of humans.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375669",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "701",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From the total survey error framework to an error framework for digital traces of humans: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375669",
		"author": [
			{
				"family": "Sen",
				"given": "Indira"
			},
			{
				"family": "Flöck",
				"given": "Fabian"
			},
			{
				"family": "Weller",
				"given": "Katrin"
			},
			{
				"family": "Weiß",
				"given": "Bernd"
			},
			{
				"family": "Wagner",
				"given": "Claudia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "duartePolicy101Introduction2020",
		"type": "paper-conference",
		"abstract": "Navigating the rules, processes, and venues through which public policy is made can seem daunting. But public participation in these processes is a crucial part of democratic governance. With a general understanding of when, where, and how to engage in policymaking, anyone can become a policy advocate. This tutorial will introduce some of the most common US (federal and state) and EU policymaking processes and provide guidance to experts in other domains (such as data and computer science) who want to get involved in policymaking. We will discuss the practical considerations involved in identifying and choosing among policymaking opportunities and discuss how to maximize the impact of policymaking interventions. This tutorial is intended to be interactive and will be improved by audience participation and questions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375668",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "703",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Policy 101: an introduction to public policymaking in the EU and US",
		"URL": "https://doi.org/10.1145/3351095.3375668",
		"author": [
			{
				"family": "Duarte",
				"given": "Natasha"
			},
			{
				"family": "Adams",
				"given": "Stan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "aryaAIExplainability3602020",
		"type": "paper-conference",
		"abstract": "This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models.Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space.Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below).In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general.Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks.Here is a rough agenda of the tutorial:1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms.2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers).3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications.4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions.5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks.6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit.7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375667",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "696",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI explainability 360: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375667",
		"author": [
			{
				"family": "Arya",
				"given": "Vijay"
			},
			{
				"family": "Bellamy",
				"given": "Rachel K. E."
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Hind",
				"given": "Michael"
			},
			{
				"family": "Hoffman",
				"given": "Samuel C."
			},
			{
				"family": "Houde",
				"given": "Stephanie"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Luss",
				"given": "Ronny"
			},
			{
				"family": "Mojsilović",
				"given": "Aleksandra"
			},
			{
				"family": "Mourad",
				"given": "Sami"
			},
			{
				"family": "Pedemonte",
				"given": "Pablo"
			},
			{
				"family": "Raghavendra",
				"given": "Ramya"
			},
			{
				"family": "Richards",
				"given": "John"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Singh",
				"given": "Moninder"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kaeser-chenPositionalityawareMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Positionality is a person's unique and always partial view of the world which is shaped by social and political contexts. Machine Learning (ML) systems have positionality, too, as a consequence of the choices we make when we develop ML systems. Being positionality-aware is key for ML practitioners to acknowledge and embrace the necessary choices embedded in ML by its creators.When groups form a shared view of the world, or group positionality, they have the power to embed and institutionalize their unique perspectives in artifacts such as standards and ontologies. For example, the international standard for reporting diseases and health conditions (International Classification of Diseases, ICD) is shaped by a distinctly medical, European and North American perspective. It dictates how we collect data, and limits what questions we can ask of data and what ML systems we can develop. Researchers struggle to study the effects of social factors on health outcomes because of what the ICD renders legible (usually in medicalized terms) and what it renders invisible (usually social contexts) in data. The ICD, as with all information infrastructures, promotes and propagates the perspective(s) of its creators. Over time, it establishes what counts as \"truth\".Positionality, and how it embeds itself in standards, ontologies, and data collection, is the root for bias in our data and algorithms. Every perspective has its limits - there is no view from nowhere. Without an awareness of positionality, the current debate on bias in machine learning is quite limited: adding more data to the set cannot remove bias. Instead, we propose positionality-aware ML, a new workflow focused on continuous evaluation and improvement of the fit between the positionality embedded in ML systems and the scenarios within which it is deployed.To demonstrate how to uncover positionality in standards, ontologies, data, and ML systems, we discuss recent work on online harassment of Canadian journalists and politicians on Twitter. Using legal definitions of hate speech and harassment, Twitter's community standards, and insight from interviews with journalists and politicians, we created standards and annotation guidelines for labeling the intensity of harassment in tweets. We then hand labeled a sample of data and through this process identified instances where positionality impacts choices about how many categories of harassment should exist, how to label boundary cases, and how to interpret messy data. We take three perspectives—technical, systems, socio-technical—that when combined illuminate areas of tension which serve as a signal of misalignment between the positionality embedded in the ML system and the deployment context. We demonstrate how the concept of positionality allows us to delineate sets of use cases that may not be suited for automated, ML solutions. Finally, we discuss strategies for developing positionality-aware ML systems, which embed a positionality appropriate for the application context, and continuously evolve to maintain this contextual fit, with an emphasis on the need for of democratic, egalitarian dialogues between knowledge-producing groups.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375666",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Positionality-aware machine learning: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375666",
		"author": [
			{
				"family": "Kaeser-Chen",
				"given": "Christine"
			},
			{
				"family": "Dubois",
				"given": "Elizabeth"
			},
			{
				"family": "Schüür",
				"given": "Friederike"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cathLeapFATEHuman2020",
		"type": "paper-conference",
		"abstract": "The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375665",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "702",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leap of FATE: human rights as a complementary framework for AI policy and practice",
		"URL": "https://doi.org/10.1145/3351095.3375665",
		"author": [
			{
				"family": "Cath",
				"given": "Corinne"
			},
			{
				"family": "Latonero",
				"given": "Mark"
			},
			{
				"family": "Marda",
				"given": "Vidushi"
			},
			{
				"family": "Pakzad",
				"given": "Roya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "gadeExplainableAIIndustry2020",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375664",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "699",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI in industry: practical challenges and lessons learned: implications tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375664",
		"author": [
			{
				"family": "Gade",
				"given": "Krishna"
			},
			{
				"family": "Geyik",
				"given": "Sahin Cem"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Mithal",
				"given": "Varun"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ganeshTwoComputerScientists2020",
		"type": "paper-conference",
		"abstract": "In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375663",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "707",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Two computer scientists and a cultural scientist get hit by a driver-less car: a method for situating knowledge in the cross-disciplinary study of F-A-T in machine learning: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375663",
		"author": [
			{
				"family": "Ganesh",
				"given": "Maya Indira"
			},
			{
				"family": "Dechesne",
				"given": "Francien"
			},
			{
				"family": "Waseem",
				"given": "Zeerak"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wexlerProbingMLModels2020",
		"type": "paper-conference",
		"abstract": "As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375662",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "705",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Probing ML models for fairness with the what-if tool and SHAP: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375662",
		"author": [
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Robinson",
				"given": "Sara"
			},
			{
				"family": "Bolukbasi",
				"given": "Tolga"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bhattExplainableMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375624",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "648–657",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable machine learning in deployment",
		"URL": "https://doi.org/10.1145/3351095.3375624",
		"author": [
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			},
			{
				"family": "Jia",
				"given": "Yunhan"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			},
			{
				"family": "Puri",
				"given": "Ruchir"
			},
			{
				"family": "Moura",
				"given": "José M. F."
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liangArtificialMentalPhenomena2020",
		"type": "paper-conference",
		"abstract": "Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology—meant to relate quantities from the real world (i.e., \"Physics\") into subjective measures in the mind (i.e., \"Psyche\")—to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375623",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "403–412",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models",
		"URL": "https://doi.org/10.1145/3351095.3375623",
		"author": [
			{
				"family": "Liang",
				"given": "Lizhen"
			},
			{
				"family": "Acuna",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "singhModelAgnosticInterpretability2020",
		"type": "paper-conference",
		"abstract": "A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models.We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "618–628",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model agnostic interpretability of rankers via intent modelling",
		"URL": "https://doi.org/10.1145/3351095.3375234",
		"author": [
			{
				"family": "Singh",
				"given": "Jaspreet"
			},
			{
				"family": "Anand",
				"given": "Avishek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dotanValueladenDisciplinaryShifts2020",
		"type": "paper-conference",
		"abstract": "As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required.This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value-laden disciplinary shifts in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3373157",
		"author": [
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Milli",
				"given": "Smitha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "castelleSocialLivesGenerative2020",
		"type": "paper-conference",
		"abstract": "Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus—a \"durably installed generative principle of regulated improvisations\"—that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill \"deeply interiorized master patterns\" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because \"sometimes we don't follow the rules... language is full of exceptions to the rules\"; and in the case of Bourdieu, the habitus was an answer to a long-standing question: \"how can behaviour be regulated without being the product of obedience to rules?\" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations—or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a \"two-player minimax game with value function V(G,D)\", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, \"the degree zero of sociology\", by which he means an isolated, inert, and amodal—and therefore not particularly sociological—starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and \"selling out\" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the \"value functions\" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The social lives of generative adversarial networks",
		"URL": "https://doi.org/10.1145/3351095.3373156",
		"author": [
			{
				"family": "Castelle",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kimPreferenceinformedFairness2020",
		"type": "paper-conference",
		"abstract": "In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "546",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preference-informed fairness",
		"URL": "https://doi.org/10.1145/3351095.3373155",
		"author": [
			{
				"family": "Kim",
				"given": "Michael P."
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			},
			{
				"family": "Rothblum",
				"given": "Guy N."
			},
			{
				"family": "Yona",
				"given": "Gal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kallusAssessingAlgorithmicFairness2020",
		"type": "paper-conference",
		"abstract": "The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "110",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing algorithmic fairness with unobserved protected class using data combination",
		"URL": "https://doi.org/10.1145/3351095.3373154",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Mao",
				"given": "Xiaojie"
			},
			{
				"family": "Zhou",
				"given": "Angela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kangAlgorithmicAccountabilityPublic2020",
		"type": "paper-conference",
		"abstract": "The EU General Data Protection Regulation (\"GDPR\") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration.Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation.The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "32",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic accountability in public administration: the GDPR paradox",
		"URL": "https://doi.org/10.1145/3351095.3373153",
		"author": [
			{
				"family": "Kang",
				"given": "Sunny Seon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "terzisOnwardFreedomOthers2020",
		"type": "paper-conference",
		"abstract": "The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "220–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Onward for the freedom of others: marching beyond the AI ethics",
		"URL": "https://doi.org/10.1145/3351095.3373152",
		"author": [
			{
				"family": "Terzis",
				"given": "Petros"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ribeiroAuditingRadicalizationPathways2020",
		"type": "paper-conference",
		"abstract": "Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372879",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "131–141",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing radicalization pathways on YouTube",
		"URL": "https://doi.org/10.1145/3351095.3372879",
		"author": [
			{
				"family": "Ribeiro",
				"given": "Manoel Horta"
			},
			{
				"family": "Ottoni",
				"given": "Raphael"
			},
			{
				"family": "West",
				"given": "Robert"
			},
			{
				"family": "Almeida",
				"given": "Virgílio A. F."
			},
			{
				"family": "Meira",
				"given": "Wagner"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "damourFairnessNotStatic2020",
		"type": "paper-conference",
		"abstract": "As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372878",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "525–534",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness is not static: deeper understanding of long term fairness via simulation studies",
		"URL": "https://doi.org/10.1145/3351095.3372878",
		"author": [
			{
				"family": "D'Amour",
				"given": "Alexander"
			},
			{
				"family": "Srinivasan",
				"given": "Hansa"
			},
			{
				"family": "Atwood",
				"given": "James"
			},
			{
				"family": "Baljekar",
				"given": "Pallavi"
			},
			{
				"family": "Sculley",
				"given": "D."
			},
			{
				"family": "Halpern",
				"given": "Yoni"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bogenAwarenessPracticeTensions2020",
		"type": "paper-conference",
		"abstract": "Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372877",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "492–500",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination",
		"URL": "https://doi.org/10.1145/3351095.3372877",
		"author": [
			{
				"family": "Bogen",
				"given": "Miranda"
			},
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Ahmed",
				"given": "Shazeda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "venkatasubramanianPhilosophicalBasisAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse.We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "284–293",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The philosophical basis of algorithmic recourse",
		"URL": "https://doi.org/10.1145/3351095.3372876",
		"author": [
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Alfano",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kaminskiMultilayeredExplanationsAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability—individual rights and systemic governance— and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "68–79",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-layered explanations from algorithmic impact assessments in the GDPR",
		"URL": "https://doi.org/10.1145/3351095.3372875",
		"author": [
			{
				"family": "Kaminski",
				"given": "Margot E."
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "katellSituatedInterventionsAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is \"scalable\" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "45–55",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward situated interventions for algorithmic equity: lessons from the field",
		"URL": "https://doi.org/10.1145/3351095.3372874",
		"author": [
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Krafft",
				"given": "P. M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rajiClosingAIAccountability2020a",
		"type": "paper-conference",
		"abstract": "Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "33–44",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing",
		"URL": "https://doi.org/10.1145/3351095.3372873",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "White",
				"given": "Rebecca N."
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Smith-Loud",
				"given": "Jamila"
			},
			{
				"family": "Theron",
				"given": "Daniel"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pujolFairDecisionMaking2020",
		"type": "paper-conference",
		"abstract": "Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "189–199",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair decision making using privacy-protected data",
		"URL": "https://doi.org/10.1145/3351095.3372872",
		"author": [
			{
				"family": "Pujol",
				"given": "David"
			},
			{
				"family": "McKenna",
				"given": "Ryan"
			},
			{
				"family": "Kuppam",
				"given": "Satya"
			},
			{
				"family": "Hay",
				"given": "Michael"
			},
			{
				"family": "Machanavajjhala",
				"given": "Ashwin"
			},
			{
				"family": "Miklau",
				"given": "Gerome"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "abebeRolesComputingSocial2020",
		"type": "paper-conference",
		"abstract": "A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined — changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "252–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Roles for computing in social change",
		"URL": "https://doi.org/10.1145/3351095.3372871",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Robinson",
				"given": "David G."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sokolExplainabilityFactSheets2020",
		"type": "paper-conference",
		"abstract": "Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "56–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability fact sheets: a framework for systematic assessment of explainable approaches",
		"URL": "https://doi.org/10.1145/3351095.3372870",
		"author": [
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Flach",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "greenFalsePromiseRisk2020",
		"type": "paper-conference",
		"abstract": "Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an \"epistemic reform,\" the path forward for criminal justice reform. I reinterpret recent results regarding the \"impossibility of fairness\" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how \"fair\" algorithms can reinforce discrimination.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "594–606",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The false promise of risk assessments: epistemic reform and the limits of fairness",
		"URL": "https://doi.org/10.1145/3351095.3372869",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "malgieriConceptFairnessGDPR2020",
		"type": "paper-conference",
		"abstract": "There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world \"fair\" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of \"Treu und Glaube\") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of \"bona fide\".Taking into account both the value of \"bona fide\" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of \"vulnerability\". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "154–166",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The concept of fairness in the GDPR: a linguistic and contextual interpretation",
		"URL": "https://doi.org/10.1145/3351095.3372868",
		"author": [
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "marcinkowskiImplicationsAIUnfairness2020",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "122–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation",
		"URL": "https://doi.org/10.1145/3351095.3372867",
		"author": [
			{
				"family": "Marcinkowski",
				"given": "Frank"
			},
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Starke",
				"given": "Christopher"
			},
			{
				"family": "Lünich",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "deanRecommendationsUserAgency2020",
		"type": "paper-conference",
		"abstract": "Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "436–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Recommendations and user agency: the reachability of collaboratively-filtered information",
		"URL": "https://doi.org/10.1145/3351095.3372866",
		"author": [
			{
				"family": "Dean",
				"given": "Sarah"
			},
			{
				"family": "Rich",
				"given": "Sarah"
			},
			{
				"family": "Recht",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mardaDataNewDelhis2020",
		"type": "paper-conference",
		"abstract": "In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 8\npublisher-place: Barcelona, Spain",
		"page": "317–324",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data in new delhi's predictive policing system",
		"URL": "https://doi.org/10.1145/3351095.3372865",
		"author": [
			{
				"family": "Marda",
				"given": "Vidushi"
			},
			{
				"family": "Narayan",
				"given": "Shivangi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "binnsApparentConflictIndividual2020",
		"type": "paper-conference",
		"abstract": "A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "514–524",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the apparent conflict between individual and group fairness",
		"URL": "https://doi.org/10.1145/3351095.3372864",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rodolfaCaseStudyPredictive2020",
		"type": "paper-conference",
		"abstract": "The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372863",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "142–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions",
		"URL": "https://doi.org/10.1145/3351095.3372863",
		"author": [
			{
				"family": "Rodolfa",
				"given": "Kit T."
			},
			{
				"family": "Salomon",
				"given": "Erika"
			},
			{
				"family": "Haynes",
				"given": "Lauren"
			},
			{
				"family": "Mendieta",
				"given": "Iván Higuera"
			},
			{
				"family": "Larson",
				"given": "Jamie"
			},
			{
				"family": "Ghani",
				"given": "Rayid"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "geigerGarbageGarbageOut2020",
		"type": "paper-conference",
		"abstract": "Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing — specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data — give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a \"gold standard\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372862",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "325–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from?",
		"URL": "https://doi.org/10.1145/3351095.3372862",
		"author": [
			{
				"family": "Geiger",
				"given": "R. Stuart"
			},
			{
				"family": "Yu",
				"given": "Kevin"
			},
			{
				"family": "Yang",
				"given": "Yanlai"
			},
			{
				"family": "Dai",
				"given": "Mindy"
			},
			{
				"family": "Qiu",
				"given": "Jie"
			},
			{
				"family": "Tang",
				"given": "Rebekah"
			},
			{
				"family": "Huang",
				"given": "Jenny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liuDisparateEquilibriaAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification—including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372861",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "381–391",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The disparate equilibria of algorithmic decision making when individuals invest rationally",
		"URL": "https://doi.org/10.1145/3351095.3372861",
		"author": [
			{
				"family": "Liu",
				"given": "Lydia T."
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			},
			{
				"family": "Haghtalab",
				"given": "Nika"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "biettiEthicsWashingEthics2020",
		"type": "paper-conference",
		"abstract": "The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, \"ethics\" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called \"ethics washing\" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in \"ethics bashing.\" This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and \"ethics\" are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere \"ivory tower\" intellectualization of complex problems that need to be dealt with in practice.This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of \"ethics washing,\" or by scholars and policy-makers in the form of \"ethics bashing.\" Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372860",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "210–219",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy",
		"URL": "https://doi.org/10.1145/3351095.3372860",
		"author": [
			{
				"family": "Bietti",
				"given": "Elettra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barabasStudyingReorientingStudy2020",
		"type": "paper-conference",
		"abstract": "Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of \"studying up\". We reflect on the contributions that the call to \"study up\" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation \"upward\". A case study from our own work illustrates what it looks like to reorient one's research questions \"up\" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that \"study up\". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372859",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "167–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Studying up: reorienting the study of algorithmic fairness around issues of power",
		"URL": "https://doi.org/10.1145/3351095.3372859",
		"author": [
			{
				"family": "Barabas",
				"given": "Chelsea"
			},
			{
				"family": "Doyle",
				"given": "Colin"
			},
			{
				"family": "Rubinovitz",
				"given": "JB"
			},
			{
				"family": "Dinakar",
				"given": "Karthik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "celisInterventionsRankingPresence2020",
		"type": "paper-conference",
		"abstract": "Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372858",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "369–380",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interventions for ranking in the presence of implicit bias",
		"URL": "https://doi.org/10.1145/3351095.3372858",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "huFairClassificationSocial2020",
		"type": "paper-conference",
		"abstract": "Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of \"fairness-to-welfare\" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring \"more fair\" classifiers does not abide by the Pareto Principle—a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372857",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "535–545",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair classification and social welfare",
		"URL": "https://doi.org/10.1145/3351095.3372857",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Chen",
				"given": "Yiling"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wagnerRegulatingTransparencyFacebook2020",
		"type": "paper-conference",
		"abstract": "Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns — design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG.This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372856",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "261–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating transparency? Facebook, twitter and the german network enforcement act",
		"URL": "https://doi.org/10.1145/3351095.3372856",
		"author": [
			{
				"family": "Wagner",
				"given": "Ben"
			},
			{
				"family": "Rozgonyi",
				"given": "Krisztina"
			},
			{
				"family": "Sekwenz",
				"given": "Marie-Therese"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "paniguttiDoctorXAIOntologybased2020",
		"type": "paper-conference",
		"abstract": "Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372855",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "629–639",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Doctor XAI: an ontology-based approach to black-box sequential data classification explanations",
		"URL": "https://doi.org/10.1145/3351095.3372855",
		"author": [
			{
				"family": "Panigutti",
				"given": "Cecilia"
			},
			{
				"family": "Perotti",
				"given": "Alan"
			},
			{
				"family": "Pedreschi",
				"given": "Dino"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mooreMoreRepresentativePolitics2020",
		"type": "paper-conference",
		"abstract": "Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls—such as co-option, whitewashing, and assumed universal values—we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples—calls to action, practitioner reflections, legislative engagement, direct action—we might allow engineers to better recognize both their diverse agencies and responsibilities.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372854",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "414–424",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a more representative politics in the ethics of computer science",
		"URL": "https://doi.org/10.1145/3351095.3372854",
		"author": [
			{
				"family": "Moore",
				"given": "Jared"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kulynychPOTsProtectiveOptimization2020",
		"type": "paper-conference",
		"abstract": "Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372853",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "177–188",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "POTs: protective optimization technologies",
		"URL": "https://doi.org/10.1145/3351095.3372853",
		"author": [
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Overdorf",
				"given": "Rebekah"
			},
			{
				"family": "Troncoso",
				"given": "Carmela"
			},
			{
				"family": "Gürses",
				"given": "Seda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangEffectConfidenceExplanation2020",
		"type": "paper-conference",
		"abstract": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372852",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "295–305",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making",
		"URL": "https://doi.org/10.1145/3351095.3372852",
		"author": [
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Bellamy",
				"given": "Rachel K. E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "costonCounterfactualRiskAssessments2020",
		"type": "paper-conference",
		"abstract": "Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may—and as we show empirically, do—induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372851",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "582–593",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual risk assessments, evaluation, and fairness",
		"URL": "https://doi.org/10.1145/3351095.3372851",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mothilalExplainingMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372850",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "607–617",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explaining machine learning classifiers through diverse counterfactual explanations",
		"URL": "https://doi.org/10.1145/3351095.3372850",
		"author": [
			{
				"family": "Mothilal",
				"given": "Ramaravind K."
			},
			{
				"family": "Sharma",
				"given": "Amit"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sanchez-monederoWhatDoesIt2020",
		"type": "paper-conference",
		"abstract": "Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372849",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "458–468",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What does it mean to 'solve' the problem of discrimination in hiring? social, technical and legal perspectives from the UK on automated hiring systems",
		"URL": "https://doi.org/10.1145/3351095.3372849",
		"author": [
			{
				"family": "Sánchez-Monedero",
				"given": "Javier"
			},
			{
				"family": "Dencik",
				"given": "Lina"
			},
			{
				"family": "Edwards",
				"given": "Lilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ilventoMulticategoryFairnessSponsored2020",
		"type": "paper-conference",
		"abstract": "Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the \"platform utility\" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372848",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "348–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-category fairness in sponsored search auctions",
		"URL": "https://doi.org/10.1145/3351095.3372848",
		"author": [
			{
				"family": "Ilvento",
				"given": "Christina"
			},
			{
				"family": "Jagadeesan",
				"given": "Meena"
			},
			{
				"family": "Chawla",
				"given": "Shuchi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "donahueFairnessUtilizationAllocating2020",
		"type": "paper-conference",
		"abstract": "Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find — somewhat surprisingly — that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372847",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "658–668",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and utilization in allocating resources with uncertain demand",
		"URL": "https://doi.org/10.1145/3351095.3372847",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lumImpactOverbookingPretrial2020",
		"type": "paper-conference",
		"abstract": "Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372846",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "482–491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of overbooking on a pre-trial risk assessment tool",
		"URL": "https://doi.org/10.1145/3351095.3372846",
		"author": [
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Boudin",
				"given": "Chesa"
			},
			{
				"family": "Price",
				"given": "Megan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "blackFlipTestFairnessTesting2020",
		"type": "paper-conference",
		"abstract": "We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372845",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "111–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FlipTest: fairness testing via optimal transport",
		"URL": "https://doi.org/10.1145/3351095.3372845",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Yeom",
				"given": "Samuel"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "washingtonWhoseSideAre2020",
		"type": "paper-conference",
		"abstract": "The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the \"social good\", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372844",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "230–240",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Whose side are ethics codes on? power, responsibility and the social good",
		"URL": "https://doi.org/10.1145/3351095.3372844",
		"author": [
			{
				"family": "Washington",
				"given": "Anne L."
			},
			{
				"family": "Kuo",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "papakyriakopoulosBiasWordEmbeddings2020",
		"type": "paper-conference",
		"abstract": "Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372843",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "446–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in word embeddings",
		"URL": "https://doi.org/10.1145/3351095.3372843",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Hegelich",
				"given": "Simon"
			},
			{
				"family": "Serrano",
				"given": "Juan Carlos Medina"
			},
			{
				"family": "Marco",
				"given": "Fabienne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "elzaynEffectsCompetitionRegulation2020",
		"type": "paper-conference",
		"abstract": "Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372842",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "669–679",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effects of competition and regulation on error inequality in data-driven markets",
		"URL": "https://doi.org/10.1145/3351095.3372842",
		"author": [
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Fish",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "borradaileWhoseTweetsAre2020",
		"type": "paper-conference",
		"abstract": "Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372841",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "570–580",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files",
		"URL": "https://doi.org/10.1145/3351095.3372841",
		"author": [
			{
				"family": "Borradaile",
				"given": "Glencora"
			},
			{
				"family": "Burkhardt",
				"given": "Brett"
			},
			{
				"family": "LeClerc",
				"given": "Alexandria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "greenAlgorithmicRealismExpanding2020",
		"type": "paper-conference",
		"abstract": "Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as \"algorithmic formalism\" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking—\"algorithmic realism\"—that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372840",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "19–31",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic realism: expanding the boundaries of algorithmic thought",
		"URL": "https://doi.org/10.1145/3351095.3372840",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			},
			{
				"family": "Viljoen",
				"given": "Salomé"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "slackFairnessWarningsFairMAML2020",
		"type": "paper-conference",
		"abstract": "Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372839",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "200–209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness warnings and fair-MAML: learning fairly with minimal data",
		"URL": "https://doi.org/10.1145/3351095.3372839",
		"author": [
			{
				"family": "Slack",
				"given": "Dylan"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Givental",
				"given": "Emile"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lundgardMeasuringJusticeMachine2020",
		"type": "paper-conference",
		"abstract": "How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372838",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "680",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring justice in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3372838",
		"author": [
			{
				"family": "Lundgard",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sweeneyReducingSentimentPolarity2020",
		"type": "paper-conference",
		"abstract": "The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372837",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "359–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning",
		"URL": "https://doi.org/10.1145/3351095.3372837",
		"author": [
			{
				"family": "Sweeney",
				"given": "Chris"
			},
			{
				"family": "Najafian",
				"given": "Maryam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hancox-liRobustnessMachineLearning2020",
		"type": "paper-conference",
		"abstract": "The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372836",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 8\npublisher-place: Barcelona, Spain",
		"page": "640–647",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robustness in machine learning explanations: does it matter?",
		"URL": "https://doi.org/10.1145/3351095.3372836",
		"author": [
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mustafarajCaseVotercenteredAudits2020",
		"type": "paper-conference",
		"abstract": "Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's \"related searches\" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372835",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "559–569",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The case for voter-centered audits of search engines during political elections",
		"URL": "https://doi.org/10.1145/3351095.3372835",
		"author": [
			{
				"family": "Mustafaraj",
				"given": "Eni"
			},
			{
				"family": "Lurie",
				"given": "Emma"
			},
			{
				"family": "Devine",
				"given": "Claire"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "toreiniRelationshipTrustAI2020",
		"type": "paper-conference",
		"abstract": "To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372834",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "272–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The relationship between trust in AI and trustworthy machine learning technologies",
		"URL": "https://doi.org/10.1145/3351095.3372834",
		"author": [
			{
				"family": "Toreini",
				"given": "Ehsan"
			},
			{
				"family": "Aitken",
				"given": "Mhairi"
			},
			{
				"family": "Coopamootoo",
				"given": "Kovila"
			},
			{
				"family": "Elliott",
				"given": "Karen"
			},
			{
				"family": "Zelaya",
				"given": "Carlos Gonzalez"
			},
			{
				"family": "Moorsel",
				"given": "Aad",
				"non-dropping-particle": "van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wieringaWhatAccountWhen2020",
		"type": "paper-conference",
		"abstract": "As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372833",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 18\npublisher-place: Barcelona, Spain",
		"page": "1–18",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability",
		"URL": "https://doi.org/10.1145/3351095.3372833",
		"author": [
			{
				"family": "Wieringa",
				"given": "Maranke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "batesIntegratingFATECritical2020",
		"type": "paper-conference",
		"abstract": "There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372832",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "425–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?",
		"URL": "https://doi.org/10.1145/3351095.3372832",
		"author": [
			{
				"family": "Bates",
				"given": "Jo"
			},
			{
				"family": "Cameron",
				"given": "David"
			},
			{
				"family": "Checco",
				"given": "Alessandro"
			},
			{
				"family": "Clough",
				"given": "Paul"
			},
			{
				"family": "Hopfgartner",
				"given": "Frank"
			},
			{
				"family": "Mazumdar",
				"given": "Suvodeep"
			},
			{
				"family": "Sbaffi",
				"given": "Laura"
			},
			{
				"family": "Stordy",
				"given": "Peter"
			},
			{
				"family": "Vega de León",
				"given": "Antonio",
				"non-dropping-particle": "de la"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "harrisonEmpiricalStudyPerceived2020",
		"type": "paper-conference",
		"abstract": "There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model \"unbiased\" and considering it \"fair.\" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372831",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "392–402",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical study on the perceived fairness of realistic, imperfect machine learning models",
		"URL": "https://doi.org/10.1145/3351095.3372831",
		"author": [
			{
				"family": "Harrison",
				"given": "Galen"
			},
			{
				"family": "Hanson",
				"given": "Julia"
			},
			{
				"family": "Jacinto",
				"given": "Christine"
			},
			{
				"family": "Ramirez",
				"given": "Julio"
			},
			{
				"family": "Ur",
				"given": "Blase"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barocasHiddenAssumptionsCounterfactual2020",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established \"principal reason\" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.These \"feature-highlighting explanations\" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world—and the subjective choices necessary to compensate for this—must be understood before these techniques can be usefully implemented.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372830",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "80–89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The hidden assumptions behind counterfactual explanations and principal reasons",
		"URL": "https://doi.org/10.1145/3351095.3372830",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Selbst",
				"given": "Andrew D."
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "joLessonsArchivesStrategies2020",
		"type": "paper-conference",
		"abstract": "A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372829",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "306–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lessons from archives: strategies for collecting sociocultural data in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3372829",
		"author": [
			{
				"family": "Jo",
				"given": "Eun Seo"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "raghavanMitigatingBiasAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372828",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "469–481",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating bias in algorithmic hiring: evaluating claims and practices",
		"URL": "https://doi.org/10.1145/3351095.3372828",
		"author": [
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sendakHumanBodyBlack2020",
		"type": "paper-conference",
		"abstract": "Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372827",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "99–109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"The human body is a black box\": supporting clinical decision-making with deep learning",
		"URL": "https://doi.org/10.1145/3351095.3372827",
		"author": [
			{
				"family": "Sendak",
				"given": "Mark"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			},
			{
				"family": "Gao",
				"given": "Michael"
			},
			{
				"family": "Futoma",
				"given": "Joseph"
			},
			{
				"family": "Ratliff",
				"given": "William"
			},
			{
				"family": "Nichols",
				"given": "Marshall"
			},
			{
				"family": "Bedoya",
				"given": "Armando"
			},
			{
				"family": "Balu",
				"given": "Suresh"
			},
			{
				"family": "O'Brien",
				"given": "Cara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hannaCriticalRaceMethodology2020",
		"type": "paper-conference",
		"abstract": "We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372826",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "501–512",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a critical race methodology in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3351095.3372826",
		"author": [
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Smith-Loud",
				"given": "Jamila"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "roldanDirichletUncertaintyWrappers2020",
		"type": "paper-conference",
		"abstract": "Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community.In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps:Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*)   Dir(α).Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y.Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i   Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels.Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction.Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system.We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality.Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372825",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "581",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability",
		"URL": "https://doi.org/10.1145/3351095.3372825",
		"author": [
			{
				"family": "Roldán",
				"given": "José Mena"
			},
			{
				"family": "Vila",
				"given": "Oriol Pujol"
			},
			{
				"family": "Marca",
				"given": "Jordi Vitrià"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lucicWhyDoesMy2020",
		"type": "paper-conference",
		"abstract": "In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be \"black boxes,\" even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a \"black-box\" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372824",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "90–98",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why does my model fail? contrastive local explanations for retail forecasting",
		"URL": "https://doi.org/10.1145/3351095.3372824",
		"author": [
			{
				"family": "Lucic",
				"given": "Ana"
			},
			{
				"family": "Haned",
				"given": "Hinda"
			},
			{
				"family": "Rijke",
				"given": "Maarten",
				"non-dropping-particle": "de"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "shenValueCardsEducational2021",
		"type": "paper-conference",
		"abstract": "Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445971",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "850–861",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value cards: An educational toolkit for teaching social impacts of machine learning through deliberation",
		"URL": "https://doi.org/10.1145/3442188.3445971",
		"author": [
			{
				"family": "Shen",
				"given": "Hong"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Chattopadhyay",
				"given": "Aditi"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Wang",
				"given": "Xu"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dashWhenUmpireAlso2021",
		"type": "paper-conference",
		"abstract": "Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445944",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "873–884",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When the umpire is also a player: Bias in private label product recommendations on E-commerce marketplaces",
		"URL": "https://doi.org/10.1145/3442188.3445944",
		"author": [
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hancox-liEpistemicValuesFeature2021",
		"type": "paper-conference",
		"abstract": "As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445943",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "817–826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic values in feature importance methods: Lessons from feminist epistemology",
		"URL": "https://doi.org/10.1145/3442188.3445943",
		"author": [
			{
				"family": "Hancox-Li",
				"given": "Leif"
			},
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "creelAlgorithmicLeviathanArbitrariness2021",
		"type": "paper-conference",
		"abstract": "Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are \"fair\" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445942",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The algorithmic leviathan: Arbitrariness, fairness, and opportunity in algorithmic decision making systems",
		"URL": "https://doi.org/10.1145/3442188.3445942",
		"author": [
			{
				"family": "Creel",
				"given": "Kathleen"
			},
			{
				"family": "Hellman",
				"given": "Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jesusHowCanChoose2021",
		"type": "paper-conference",
		"abstract": "There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445941",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "805–815",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How can I choose an explainer? An application-grounded evaluation of post-hoc explanations",
		"URL": "https://doi.org/10.1145/3442188.3445941",
		"author": [
			{
				"family": "Jesus",
				"given": "Sérgio"
			},
			{
				"family": "Belém",
				"given": "Catarina"
			},
			{
				"family": "Balayan",
				"given": "Vladimir"
			},
			{
				"family": "Bento",
				"given": "João"
			},
			{
				"family": "Saleiro",
				"given": "Pedro"
			},
			{
				"family": "Bizarro",
				"given": "Pedro"
			},
			{
				"family": "Gama",
				"given": "João"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "berettaDetectingDiscriminatoryRisk2021",
		"type": "paper-conference",
		"abstract": "Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445940",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "794–804",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting discriminatory risk through data annotation based on Bayesian inferences",
		"URL": "https://doi.org/10.1145/3442188.3445940",
		"author": [
			{
				"family": "Beretta",
				"given": "Elena"
			},
			{
				"family": "Vetrò",
				"given": "Antonio"
			},
			{
				"family": "Lepri",
				"given": "Bruno"
			},
			{
				"family": "Martin",
				"given": "Juan Carlos De"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "starkEthicsEmotionArtificial2021",
		"type": "paper-conference",
		"abstract": "In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445939",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "782–793",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethics of emotion in artificial intelligence systems",
		"URL": "https://doi.org/10.1145/3442188.3445939",
		"author": [
			{
				"family": "Stark",
				"given": "Luke"
			},
			{
				"family": "Hoey",
				"given": "Jesse"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "krafftActionorientedAIPolicy2021",
		"type": "paper-conference",
		"abstract": "Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445938",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "772–781",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An action-oriented AI policy toolkit for technology audits by community advocates and activists",
		"URL": "https://doi.org/10.1145/3442188.3445938",
		"author": [
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Lee",
				"given": "Jennifer E."
			},
			{
				"family": "Narayan",
				"given": "Shankar"
			},
			{
				"family": "Epstein",
				"given": "Micah"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Jobe",
				"given": "Pa Ousman"
			},
			{
				"family": "Putz",
				"given": "Franziska"
			},
			{
				"family": "Robick",
				"given": "Brian"
			},
			{
				"family": "Barghouti",
				"given": "Bissan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "krollOutliningTraceabilityPrinciple2021",
		"type": "paper-conference",
		"abstract": "Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445937",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "758–771",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Outlining traceability: A principle for operationalizing accountability in computing systems",
		"URL": "https://doi.org/10.1145/3442188.3445937",
		"author": [
			{
				"family": "Kroll",
				"given": "Joshua A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hertweckMoralJustificationStatistical2021",
		"type": "paper-conference",
		"abstract": "A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews \"What You See Is What You Get\" (WYSIWYG) and \"We're All Equal\" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445936",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "747–757",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the moral justification of statistical parity",
		"URL": "https://doi.org/10.1145/3442188.3445936",
		"author": [
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "metcalfAlgorithmicImpactAssessments2021",
		"type": "paper-conference",
		"abstract": "Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that \"impacts\" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445935",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "735–746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic impact assessments and accountability: The co-construction of impacts",
		"URL": "https://doi.org/10.1145/3442188.3445935",
		"author": [
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "suriyakumarChasingYourLong2021",
		"type": "paper-conference",
		"abstract": "Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445934",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "723–734",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Chasing your long tails: Differentially private prediction in health care settings",
		"URL": "https://doi.org/10.1145/3442188.3445934",
		"author": [
			{
				"family": "Suriyakumar",
				"given": "Vinith M."
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			},
			{
				"family": "Goldenberg",
				"given": "Anna"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "milliOptimizingEngagementMeasuring2021",
		"type": "paper-conference",
		"abstract": "Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of value that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of \"value\".",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445933",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "714–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From optimizing engagement to measuring value",
		"URL": "https://doi.org/10.1145/3442188.3445933",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Belli",
				"given": "Luca"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "steedImageRepresentationsLearned2021",
		"type": "paper-conference",
		"abstract": "Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445932",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "701–713",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Image representations learned with unsupervised pre-training contain human-like biases",
		"URL": "https://doi.org/10.1145/3442188.3445932",
		"author": [
			{
				"family": "Steed",
				"given": "Ryan"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasinidouAgreeDecisionThey2021",
		"type": "paper-conference",
		"abstract": "While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445931",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "690–700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions",
		"URL": "https://doi.org/10.1145/3442188.3445931",
		"author": [
			{
				"family": "Kasinidou",
				"given": "Maria"
			},
			{
				"family": "Kleanthous",
				"given": "Styliani"
			},
			{
				"family": "Barlas",
				"given": "Pınar"
			},
			{
				"family": "Otterbacher",
				"given": "Jahna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "celisEffectRooneyRule2021",
		"type": "paper-conference",
		"abstract": "The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change.In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist—independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445930",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "678–689",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effect of the rooney rule on implicit bias in the long term",
		"URL": "https://doi.org/10.1145/3442188.3445930",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Hays",
				"given": "Chris"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hamptonBlackFeministMusings2021",
		"type": "paper-conference",
		"abstract": "This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445929",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "1",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Black feminist musings on algorithmic oppression",
		"URL": "https://doi.org/10.1145/3442188.3445929",
		"author": [
			{
				"family": "Hampton",
				"given": "Lelia Marie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wilsonBuildingAuditingFair2021",
		"type": "paper-conference",
		"abstract": "Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of \"fairness\" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445928",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "666–677",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building and auditing fair algorithms: A case study in candidate screening",
		"URL": "https://doi.org/10.1145/3442188.3445928",
		"author": [
			{
				"family": "Wilson",
				"given": "Christo"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Jiang",
				"given": "Shan"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			},
			{
				"family": "Baker",
				"given": "Lewis"
			},
			{
				"family": "Szary",
				"given": "Janelle"
			},
			{
				"family": "Trindel",
				"given": "Kelly"
			},
			{
				"family": "Polli",
				"given": "Frida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "taskesenStatisticalTestProbabilistic2021",
		"type": "paper-conference",
		"abstract": "Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445927",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 18\npublisher-place: Virtual Event, Canada",
		"page": "648–665",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A statistical test for probabilistic fairness",
		"URL": "https://doi.org/10.1145/3442188.3445927",
		"author": [
			{
				"family": "Taskesen",
				"given": "Bahar"
			},
			{
				"family": "Blanchet",
				"given": "Jose"
			},
			{
				"family": "Kuhn",
				"given": "Daniel"
			},
			{
				"family": "Nguyen",
				"given": "Viet Anh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "lussierPapersProgramsCourts2021",
		"type": "paper-conference",
		"abstract": "This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445926",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "647",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From papers to programs: Courts, corporations, clinics and the battle over computerized psychological testing",
		"URL": "https://doi.org/10.1145/3442188.3445926",
		"author": [
			{
				"family": "Lussier",
				"given": "Kira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grunewaldTILTGDPRAlignedTransparency2021",
		"type": "paper-conference",
		"abstract": "In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do.We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty.Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445925",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "636–646",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TILT: A GDPR-Aligned transparency information language and toolkit for practical privacy engineering",
		"URL": "https://doi.org/10.1145/3442188.3445925",
		"author": [
			{
				"family": "Grünewald",
				"given": "Elias"
			},
			{
				"family": "Pallas",
				"given": "Frank"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dhamalaBOLDDatasetMetrics2021",
		"type": "paper-conference",
		"abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445924",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "862–872",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "BOLD: Dataset and metrics for measuring biases in open-ended language generation",
		"URL": "https://doi.org/10.1145/3442188.3445924",
		"author": [
			{
				"family": "Dhamala",
				"given": "Jwala"
			},
			{
				"family": "Sun",
				"given": "Tony"
			},
			{
				"family": "Kumar",
				"given": "Varun"
			},
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Pruksachatkun",
				"given": "Yada"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Gupta",
				"given": "Rahul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jacoviFormalizingTrustArtificial2021",
		"type": "paper-conference",
		"abstract": "Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445923",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "624–635",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI",
		"URL": "https://doi.org/10.1145/3442188.3445923",
		"author": [
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Marasović",
				"given": "Ana"
			},
			{
				"family": "Miller",
				"given": "Tim"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "benderDangersStochasticParrots2021",
		"type": "paper-conference",
		"abstract": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445922",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "610–623",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the dangers of stochastic parrots: Can language models be too big? 🦜",
		"URL": "https://doi.org/10.1145/3442188.3445922",
		"author": [
			{
				"family": "Bender",
				"given": "Emily M."
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "McMillan-Major",
				"given": "Angelina"
			},
			{
				"family": "Shmitchell",
				"given": "Shmargaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cobbeReviewableAutomatedDecisionmaking2021",
		"type": "paper-conference",
		"abstract": "This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445921",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "598–609",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reviewable automated decision-making: A framework for accountable algorithmic systems",
		"URL": "https://doi.org/10.1145/3442188.3445921",
		"author": [
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "khanOneLabelOne2021",
		"type": "paper-conference",
		"abstract": "Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445920",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "587–597",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One label, one billion faces: Usage and consistency of racial categories in computer vision",
		"URL": "https://doi.org/10.1145/3442188.3445920",
		"author": [
			{
				"family": "Khan",
				"given": "Zaid"
			},
			{
				"family": "Fu",
				"given": "Yun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasyFairnessEqualityPower2021",
		"type": "paper-conference",
		"abstract": "Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same \"merit.\" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by \"merit;\" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445919",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "576–586",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, equality, and power in algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3442188.3445919",
		"author": [
			{
				"family": "Kasy",
				"given": "Maximilian"
			},
			{
				"family": "Abebe",
				"given": "Rediet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hutchinsonAccountabilityMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445918",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 16\npublisher-place: Virtual Event, Canada",
		"page": "560–575",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards accountability for machine learning datasets: Practices from software engineering and infrastructure",
		"URL": "https://doi.org/10.1145/3442188.3445918",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Kjartansson",
				"given": "Oddur"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hamonImpossibleExplanationsExplainable2021",
		"type": "paper-conference",
		"abstract": "Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445917",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "549–559",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario",
		"URL": "https://doi.org/10.1145/3442188.3445917",
		"author": [
			{
				"family": "Hamon",
				"given": "Ronan"
			},
			{
				"family": "Junklewitz",
				"given": "Henrik"
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			},
			{
				"family": "Hert",
				"given": "Paul De"
			},
			{
				"family": "Beslay",
				"given": "Laurent"
			},
			{
				"family": "Sanchez",
				"given": "Ignacio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yangCensorshipOnlineEncyclopedias2021",
		"type": "paper-conference",
		"abstract": "While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445916",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "537–548",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Censorship of online encyclopedias: Implications for NLP models",
		"URL": "https://doi.org/10.1145/3442188.3445916",
		"author": [
			{
				"family": "Yang",
				"given": "Eddie"
			},
			{
				"family": "Roberts",
				"given": "Margaret E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wangFairClassificationGroupdependent2021",
		"type": "paper-conference",
		"abstract": "This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445915",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "526–536",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair classification with group-dependent label noise",
		"URL": "https://doi.org/10.1145/3442188.3445915",
		"author": [
			{
				"family": "Wang",
				"given": "Jialu"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Levy",
				"given": "Caleb"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "rajiYouCantSit2021",
		"type": "paper-conference",
		"abstract": "Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)—and its implications for AI—has led to the current \"ethics crisis\". However, we claim that the current AI ethics education space relies on a form of \"exclusionary pedagogy,\" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as \"ethical unicorns\" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445914",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "515–525",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "You can't sit with us: Exclusionary pedagogy in AI ethics education",
		"URL": "https://doi.org/10.1145/3442188.3445914",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Scheuerman",
				"given": "Morgan Klaus"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abbasiFairClusteringEquitable2021",
		"type": "paper-conference",
		"abstract": "What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being \"close\" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common \"use case\" for clustering. For such a clustering to be fair, the centers should \"represent\" different groups equally well. We call such a clustering a group-representative clustering.In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445913",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "504–514",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair clustering via equitable group representations",
		"URL": "https://doi.org/10.1145/3442188.3445913",
		"author": [
			{
				"family": "Abbasi",
				"given": "Mohsen"
			},
			{
				"family": "Bhaskara",
				"given": "Aditya"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "finocchiaroBridgingMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445912",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "489–503",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bridging machine learning and mechanism design towards algorithmic fairness",
		"URL": "https://doi.org/10.1145/3442188.3445912",
		"author": [
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			},
			{
				"family": "Maio",
				"given": "Roland"
			},
			{
				"family": "Monachou",
				"given": "Faidra"
			},
			{
				"family": "Patro",
				"given": "Gourab K"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Stoica",
				"given": "Ana-Andreea"
			},
			{
				"family": "Tsirtsis",
				"given": "Stratis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mulderOperationalizingFramingSupport2021",
		"type": "paper-conference",
		"abstract": "Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445911",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "478–488",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Operationalizing framing to support multiperspective recommendations of opinion pieces",
		"URL": "https://doi.org/10.1145/3442188.3445911",
		"author": [
			{
				"family": "Mulder",
				"given": "Mats"
			},
			{
				"family": "Inel",
				"given": "Oana"
			},
			{
				"family": "Oosterman",
				"given": "Jasper"
			},
			{
				"family": "Tintarev",
				"given": "Nava"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nandaFairnessRobustnessInvestigating2021",
		"type": "paper-conference",
		"abstract": "Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445910",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "466–477",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness through robustness: Investigating robustness disparity in deep learning",
		"URL": "https://doi.org/10.1145/3442188.3445910",
		"author": [
			{
				"family": "Nanda",
				"given": "Vedant"
			},
			{
				"family": "Dooley",
				"given": "Samuel"
			},
			{
				"family": "Singla",
				"given": "Sahil"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "boagPilotStudySurveying2021",
		"type": "paper-conference",
		"abstract": "The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445909",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, Canada",
		"page": "458–465",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A pilot study in surveying clinical judgments to evaluate radiology report generation",
		"URL": "https://doi.org/10.1145/3442188.3445909",
		"author": [
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Kané",
				"given": "Hassan"
			},
			{
				"family": "Rawat",
				"given": "Saumya"
			},
			{
				"family": "Wei",
				"given": "Jesse"
			},
			{
				"family": "Goehler",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "williamsBayesianModelCash2021",
		"type": "paper-conference",
		"abstract": "The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of \"innocent until proven guilty.\" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445908",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "827–837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A bayesian model of cash bail decisions",
		"URL": "https://doi.org/10.1145/3442188.3445908",
		"author": [
			{
				"family": "Williams",
				"given": "Joshua"
			},
			{
				"family": "Kolter",
				"given": "J. Zico"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "choCrosslingualGeneralizationTranslation2021",
		"type": "paper-conference",
		"abstract": "Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445907",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "449–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards cross-lingual generalization of translation gender bias",
		"URL": "https://doi.org/10.1145/3442188.3445907",
		"author": [
			{
				"family": "Cho",
				"given": "Won Ik"
			},
			{
				"family": "Kim",
				"given": "Jiwon"
			},
			{
				"family": "Yang",
				"given": "Jaeyeong"
			},
			{
				"family": "Kim",
				"given": "Nam Soo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ghadiriSociallyFairKMeans2021",
		"type": "paper-conference",
		"abstract": "We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445906",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "438–448",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socially fair k-Means clustering",
		"URL": "https://doi.org/10.1145/3442188.3445906",
		"author": [
			{
				"family": "Ghadiri",
				"given": "Mehrdad"
			},
			{
				"family": "Samadi",
				"given": "Samira"
			},
			{
				"family": "Vempala",
				"given": "Santosh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kaciankaDesigningAccountableSystems2021",
		"type": "paper-conference",
		"abstract": "Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445905",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "424–437",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing accountable systems",
		"URL": "https://doi.org/10.1145/3442188.3445905",
		"author": [
			{
				"family": "Kacianka",
				"given": "Severin"
			},
			{
				"family": "Pretschner",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ionescuAgentbasedModelEvaluate2021",
		"type": "paper-conference",
		"abstract": "Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445904",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "412–423",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An agent-based model to evaluate interventions on online dating platforms to decrease racial homogamy",
		"URL": "https://doi.org/10.1145/3442188.3445904",
		"author": [
			{
				"family": "Ionescu",
				"given": "Stefania"
			},
			{
				"family": "Hannák",
				"given": "Anikó"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "patelHighDimensionalModel2021",
		"type": "paper-conference",
		"abstract": "Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets.We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model.Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445903",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "401–411",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "High dimensional model explanations: An axiomatic approach",
		"URL": "https://doi.org/10.1145/3442188.3445903",
		"author": [
			{
				"family": "Patel",
				"given": "Neel"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mishlerFairnessRiskAssessment2021",
		"type": "paper-conference",
		"abstract": "In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445902",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "386–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds",
		"URL": "https://doi.org/10.1145/3442188.3445902",
		"author": [
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jacobsMeasurementFairness2021",
		"type": "paper-conference",
		"abstract": "We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them—i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445901",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "375–385",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measurement and fairness",
		"URL": "https://doi.org/10.1145/3442188.3445901",
		"author": [
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barbosaSemioticsbasedEpistemicTool2021",
		"type": "paper-conference",
		"abstract": "One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445900",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "363–374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development",
		"URL": "https://doi.org/10.1145/3442188.3445900",
		"author": [
			{
				"family": "Barbosa",
				"given": "Simone Diniz Junqueira"
			},
			{
				"family": "Barbosa",
				"given": "Gabriel Diniz Junqueira"
			},
			{
				"family": "Souza",
				"given": "Clarisse Sieckenius",
				"dropping-particle": "de"
			},
			{
				"family": "Leitão",
				"given": "Carla Faria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "karimiAlgorithmicRecourseCounterfactual2021",
		"type": "paper-conference",
		"abstract": "As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -\"how the world would have (had) to be different for a desirable outcome to occur\"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445899",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "353–362",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic recourse: from counterfactual explanations to interventions",
		"URL": "https://doi.org/10.1145/3442188.3445899",
		"author": [
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Schölkopf",
				"given": "Bernhard"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "albertThisWholeThing2021",
		"type": "paper-conference",
		"abstract": "Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third \"gender\" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445898",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "342–352",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "This whole thing smacks of gender: Algorithmic exclusion in bioimpedance-based body composition analysis",
		"URL": "https://doi.org/10.1145/3442188.3445898",
		"author": [
			{
				"family": "Albert",
				"given": "Kendra"
			},
			{
				"family": "Delano",
				"given": "Maggie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abebeNarrativesCounternarrativesData2021",
		"type": "paper-conference",
		"abstract": "As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem.We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445897",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "329–341",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Narratives and counternarratives on data sharing in africa",
		"URL": "https://doi.org/10.1145/3442188.3445897",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Aruleba",
				"given": "Kehinde"
			},
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Kingsley",
				"given": "Sara"
			},
			{
				"family": "Obaido",
				"given": "George"
			},
			{
				"family": "Remy",
				"given": "Sekou L."
			},
			{
				"family": "Sadagopan",
				"given": "Swathi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "sambasivanReimaginingAlgorithmicFairness2021",
		"type": "paper-conference",
		"abstract": "Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445896",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "315–328",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Re-imagining algorithmic fairness in india and beyond",
		"URL": "https://doi.org/10.1145/3442188.3445896",
		"author": [
			{
				"family": "Sambasivan",
				"given": "Nithya"
			},
			{
				"family": "Arnesen",
				"given": "Erin"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Doshi",
				"given": "Tulsee"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kallusFairnessWelfareEquity2021",
		"type": "paper-conference",
		"abstract": "We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a \"triple bottom line\": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445895",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 19\npublisher-place: Virtual Event, Canada",
		"page": "296–314",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, welfare, and equity in personalized pricing",
		"URL": "https://doi.org/10.1145/3442188.3445895",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Zhou",
				"given": "Angela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "blackLeaveoneoutUnfairness2021",
		"type": "paper-conference",
		"abstract": "We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445894",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "285–295",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leave-one-out unfairness",
		"URL": "https://doi.org/10.1145/3442188.3445894",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "martinSpokenCorporaData2021",
		"type": "paper-conference",
		"abstract": "Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in \"I be in my office by 7:30am\", paraphrasable as \"I am usually in my office by 7:30\" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora – Switchboard, Fisher, TIMIT, and LibriSpeech – to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL – the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445893",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Spoken corpora data, automatic speech recognition, and bias against african american language: The case of habitual 'be'",
		"URL": "https://doi.org/10.1145/3442188.3445893",
		"author": [
			{
				"family": "Martin",
				"given": "Joshua L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yeomAvoidingDisparityAmplification2021",
		"type": "paper-conference",
		"abstract": "We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445892",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "273–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Avoiding disparity amplification under different worldviews",
		"URL": "https://doi.org/10.1145/3442188.3445892",
		"author": [
			{
				"family": "Yeom",
				"given": "Samuel"
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kilbyAlgorithmicFairnessPredicting2021",
		"type": "paper-conference",
		"abstract": "There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445891",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "272",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness in predicting opioid use disorder using machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445891",
		"author": [
			{
				"family": "Kilby",
				"given": "Angela E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "knowlesSanctionAuthorityPromoting2021",
		"type": "paper-conference",
		"abstract": "Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations—both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards—is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445890",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "262–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The sanction of authority: Promoting public trust in AI",
		"URL": "https://doi.org/10.1145/3442188.3445890",
		"author": [
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Richards",
				"given": "John T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "gargStandardizedTestsAffirmative2021",
		"type": "paper-conference",
		"abstract": "The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies—such as affirmative action—on admitted class composition.This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action.We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool.Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement.The full paper can be found at https://arxiv.org/abs/2010.04396.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445889",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "261",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Standardized tests and affirmative action: The role of bias and variance",
		"URL": "https://doi.org/10.1145/3442188.3445889",
		"author": [
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Li",
				"given": "Hannah"
			},
			{
				"family": "Monachou",
				"given": "Faidra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "andrusWhatWeCant2021",
		"type": "paper-conference",
		"abstract": "As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445888",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "249–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What we can't measure, we can't understand: Challenges to demographic data procurement in the pursuit of fairness",
		"URL": "https://doi.org/10.1145/3442188.3445888",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Spitzer",
				"given": "Elena"
			},
			{
				"family": "Brown",
				"given": "Jeffrey"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mehrotraMitigatingBiasSet2021",
		"type": "paper-conference",
		"abstract": "Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a \"denoised\" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445887",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "237–248",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating bias in set selection with noisy protected attributes",
		"URL": "https://doi.org/10.1145/3442188.3445887",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Celis",
				"given": "L. Elisa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehUseMisuseCounterfactuals2021",
		"type": "paper-conference",
		"abstract": "The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445886",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "228–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The use and misuse of counterfactuals in ethical machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445886",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "vincentDataLeverageFramework2021",
		"type": "paper-conference",
		"abstract": "Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445885",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "215–227",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data leverage: A framework for empowering the public in its relationship with technology companies",
		"URL": "https://doi.org/10.1145/3442188.3445885",
		"author": [
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Li",
				"given": "Hanlin"
			},
			{
				"family": "Tilly",
				"given": "Nicole"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "awasthiEvaluatingFairnessMachine2021",
		"type": "paper-conference",
		"abstract": "Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives.We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445884",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "206–214",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating fairness of machine learning models under uncertain and incomplete information",
		"URL": "https://doi.org/10.1145/3442188.3445884",
		"author": [
			{
				"family": "Awasthi",
				"given": "Pranjal"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Kleindessner",
				"given": "Matthäus"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "khaniRemovingSpuriousFeatures2021",
		"type": "paper-conference",
		"abstract": "Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445883",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "196–205",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Removing spurious features can hurt accuracy and affect groups disproportionately",
		"URL": "https://doi.org/10.1145/3442188.3445883",
		"author": [
			{
				"family": "Khani",
				"given": "Fereshte"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "donahueBetterTogetherHow2021",
		"type": "paper-conference",
		"abstract": "Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as \"solidarity\" and \"actuarial fairness\" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory.First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445882",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "185–195",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Better together? How externalities of size complicate notions of solidarity and actuarial fairness",
		"URL": "https://doi.org/10.1145/3442188.3445882",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "costonLeveragingAdministrativeData2021",
		"type": "paper-conference",
		"abstract": "Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data—containing individual-level voter turnout for specific voting locations along with race and age—can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445881",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "173–184",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leveraging administrative data for bias audits: Assessing disparate coverage with mobility data for COVID-19 policy",
		"URL": "https://doi.org/10.1145/3442188.3445881",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Guha",
				"given": "Neel"
			},
			{
				"family": "Ouyang",
				"given": "Derek"
			},
			{
				"family": "Lu",
				"given": "Lisa"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Ho",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "miceliDocumentingComputerVision2021",
		"type": "paper-conference",
		"abstract": "In industrial computer vision, discretionary decisions surrounding the production of image training data remain widely undocumented. Recent research taking issue with such opacity has proposed standardized processes for dataset documentation. In this paper, we expand this space of inquiry through fieldwork at two data processing companies and thirty interviews with data workers and computer vision practitioners. We identify four key issues that hinder the documentation of image datasets and the effective retrieval of production contexts. Finally, we propose reflexivity, understood as a collective consideration of social and intellectual factors that lead to praxis, as a necessary precondition for documentation. Reflexive documentation can help to expose the contexts, relations, routines, and power structures that shape data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445880",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "161–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Documenting computer vision datasets: An invitation to reflexive data practices",
		"URL": "https://doi.org/10.1145/3442188.3445880",
		"author": [
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Yang",
				"given": "Tianling"
			},
			{
				"family": "Naudts",
				"given": "Laurens"
			},
			{
				"family": "Schuessler",
				"given": "Martin"
			},
			{
				"family": "Serbanescu",
				"given": "Diana"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chengCanYouFake2021",
		"type": "paper-conference",
		"abstract": "The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445879",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "149–160",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can you fake it until you make it? Impacts of differentially private synthetic data on downstream classification fairness",
		"URL": "https://doi.org/10.1145/3442188.3445879",
		"author": [
			{
				"family": "Cheng",
				"given": "Victoria"
			},
			{
				"family": "Suriyakumar",
				"given": "Vinith M."
			},
			{
				"family": "Dullerud",
				"given": "Natalie"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangFairDeepAnomaly2021",
		"type": "paper-conference",
		"abstract": "Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445878",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "138–148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fair deep anomaly detection",
		"URL": "https://doi.org/10.1145/3442188.3445878",
		"author": [
			{
				"family": "Zhang",
				"given": "Hongjing"
			},
			{
				"family": "Davidson",
				"given": "Ian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "akpinarEffectDifferentialVictim2021",
		"type": "paper-conference",
		"abstract": "Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogotá, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445877",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "838–849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effect of differential victim crime reporting on predictive policing systems",
		"URL": "https://doi.org/10.1145/3442188.3445877",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "razGroupFairnessIndependence2021",
		"type": "paper-conference",
		"abstract": "This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "129–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness: Independence revisited",
		"URL": "https://doi.org/10.1145/3442188.3445876",
		"author": [
			{
				"family": "Räz",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ball-burackDifferentialTweetmentMitigating2021",
		"type": "paper-conference",
		"abstract": "Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "116–128",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Differential tweetment: Mitigating racial dialect bias in harmful tweet detection",
		"URL": "https://doi.org/10.1145/3442188.3445875",
		"author": [
			{
				"family": "Ball-Burack",
				"given": "Ari"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cheongComputerScienceCommunities2021",
		"type": "paper-conference",
		"abstract": "Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "106–115",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computer science communities: Who is speaking, and who is listening to the women? Using an ethics of care to promote diverse voices",
		"URL": "https://doi.org/10.1145/3442188.3445874",
		"author": [
			{
				"family": "Cheong",
				"given": "Marc"
			},
			{
				"family": "Leins",
				"given": "Kobi"
			},
			{
				"family": "Coghlan",
				"given": "Simon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "benamiDistributiveEffectsRisk2021",
		"type": "paper-conference",
		"abstract": "Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 16\npublisher-place: Virtual Event, Canada",
		"page": "90–105",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The distributive effects of risk prediction in environmental compliance: Algorithmic design, environmental justice, and public policy",
		"URL": "https://doi.org/10.1145/3442188.3445873",
		"author": [
			{
				"family": "Benami",
				"given": "Elinor"
			},
			{
				"family": "Whitaker",
				"given": "Reid"
			},
			{
				"family": "La",
				"given": "Vincent"
			},
			{
				"family": "Lin",
				"given": "Hongjin"
			},
			{
				"family": "Anderson",
				"given": "Brandon R."
			},
			{
				"family": "Ho",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chasalowRepresentativenessStatisticsPolitics2021",
		"type": "paper-conference",
		"abstract": "Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "77–89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representativeness in statistics, politics, and machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445872",
		"author": [
			{
				"family": "Chasalow",
				"given": "Kyla"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "thorntonFiftyShadesGrey2021",
		"type": "paper-conference",
		"abstract": "Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs—easily accessible online environments provisioning access to datasets, analysis and visualisations—are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust—transparency and provenance in particular—appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "64–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fifty shades of grey: In praise of a nuanced approach towards trustworthy design",
		"URL": "https://doi.org/10.1145/3442188.3445871",
		"author": [
			{
				"family": "Thornton",
				"given": "Lauren"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Blair",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "parkDesigningOnlineInfrastructure2021",
		"type": "paper-conference",
		"abstract": "AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "52–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing an online infrastructure for collecting AI data from people with disabilities",
		"URL": "https://doi.org/10.1145/3442188.3445870",
		"author": [
			{
				"family": "Park",
				"given": "Joon Sung"
			},
			{
				"family": "Bragg",
				"given": "Danielle"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "srinivasanBiasesGenerativeArt2021",
		"type": "paper-conference",
		"abstract": "With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "41–51",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Biases in generative art: A causal look from the lens of art history",
		"URL": "https://doi.org/10.1145/3442188.3445869",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Ramya"
			},
			{
				"family": "Uchino",
				"given": "Kanji"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ronCorporateSocialResponsibility2021",
		"type": "paper-conference",
		"abstract": "We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "26–40",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Corporate social responsibility via multi-armed bandits",
		"URL": "https://doi.org/10.1145/3442188.3445868",
		"author": [
			{
				"family": "Ron",
				"given": "Tom"
			},
			{
				"family": "Ben-Porat",
				"given": "Omer"
			},
			{
				"family": "Shalit",
				"given": "Uri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "heidariAllocatingOpportunitiesDynamic2021",
		"type": "paper-conference",
		"abstract": "Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status — a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "15–25",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Allocating opportunities in a dynamic model of intergenerational mobility",
		"URL": "https://doi.org/10.1145/3442188.3445867",
		"author": [
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehReasonsValuesStakeholders2021",
		"type": "paper-conference",
		"abstract": "The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence",
		"URL": "https://doi.org/10.1145/3442188.3445866",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "singhFairnessViolationsMitigation2021",
		"type": "paper-conference",
		"abstract": "We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "3–13",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness violations and mitigation under covariate shift",
		"URL": "https://doi.org/10.1145/3442188.3445865",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Singh",
				"given": "Rina"
			},
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cohenPriceDiscriminationFairness2021",
		"type": "paper-conference",
		"abstract": "Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "2",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Price discrimination with fairness constraints",
		"URL": "https://doi.org/10.1145/3442188.3445864",
		"author": [
			{
				"family": "Cohen",
				"given": "Maxime C."
			},
			{
				"family": "Elmachtoub",
				"given": "Adam N."
			},
			{
				"family": "Lei",
				"given": "Xiao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "diazCrowdWorkSheetsAccountingIndividual2022",
		"type": "paper-conference",
		"abstract": "Human annotated data plays a crucial role in machine learning (ML) research and development. However, the ethical considerations around the processes and decisions that go into dataset annotation have not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this space along two layers: (1) who the annotator is, and how the annotators’ lived experiences can impact their annotations, and (2) the relationship between the annotators and the crowdsourcing platforms, and what that relationship affords them. Finally, we introduce a novel framework, CrowdWorkSheets, for dataset developers to facilitate transparent documentation of key decisions points at various stages of the data annotation pipeline: task formulation, selection of annotators, platform and infrastructure choices, dataset analysis and evaluation, and dataset release and maintenance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534647",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2342–2351",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CrowdWorkSheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation",
		"URL": "https://doi.org/10.1145/3531146.3534647",
		"author": [
			{
				"family": "Díaz",
				"given": "Mark"
			},
			{
				"family": "Kivlichan",
				"given": "Ian"
			},
			{
				"family": "Rosen",
				"given": "Rachel"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cruzcortesLocalityTechnicalObjects2022",
		"type": "paper-conference",
		"abstract": "Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534646",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "2327–2341",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Locality of technical objects and the role of structural interventions for systemic change",
		"URL": "https://doi.org/10.1145/3531146.3534646",
		"author": [
			{
				"family": "Cruz Cortés",
				"given": "Efrén"
			},
			{
				"family": "Rajtmajer",
				"given": "Sarah"
			},
			{
				"family": "Ghosh",
				"given": "Debashis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "baumannEnforcingGroupFairness2022",
		"type": "paper-conference",
		"abstract": "Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534645",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2315–2326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enforcing group fairness in algorithmic decision making: Utility maximization under sufficiency",
		"URL": "https://doi.org/10.1145/3531146.3534645",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Hannák",
				"given": "Anikó"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pastaltzidisDataAugmentationFairnessaware2022",
		"type": "paper-conference",
		"abstract": "Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534644",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2302–2314",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems",
		"URL": "https://doi.org/10.1145/3531146.3534644",
		"author": [
			{
				"family": "Pastaltzidis",
				"given": "Ioannis"
			},
			{
				"family": "Dimitriou",
				"given": "Nikolaos"
			},
			{
				"family": "Quezada-Tavarez",
				"given": "Katherine"
			},
			{
				"family": "Aidinlis",
				"given": "Stergios"
			},
			{
				"family": "Marquenie",
				"given": "Thomas"
			},
			{
				"family": "Gurzawska",
				"given": "Agata"
			},
			{
				"family": "Tzovaras",
				"given": "Dimitrios"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "viganoPeopleAreNot2022",
		"type": "paper-conference",
		"abstract": "In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534643",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "2293–2301",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "People are not coins: Morally distinct types of predictions necessitate different fairness constraints",
		"URL": "https://doi.org/10.1145/3531146.3534643",
		"author": [
			{
				"family": "Viganò",
				"given": "Eleonora"
			},
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "brownWhatDoesIt2022",
		"type": "paper-conference",
		"abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534642",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2280–2292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What does it mean for a language model to preserve privacy?",
		"URL": "https://doi.org/10.1145/3531146.3534642",
		"author": [
			{
				"family": "Brown",
				"given": "Hannah"
			},
			{
				"family": "Lee",
				"given": "Katherine"
			},
			{
				"family": "Mireshghallah",
				"given": "Fatemehsadat"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Tramèr",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghazimatinMeasuringFairnessRankings2022",
		"type": "paper-conference",
		"abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534641",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2263–2279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring fairness of rankings under noisy sensitive information",
		"URL": "https://doi.org/10.1145/3531146.3534641",
		"author": [
			{
				"family": "Ghazimatin",
				"given": "Azin"
			},
			{
				"family": "Kleindessner",
				"given": "Matthaus"
			},
			{
				"family": "Russell",
				"given": "Chris"
			},
			{
				"family": "Abedjan",
				"given": "Ziawasch"
			},
			{
				"family": "Golebiowski",
				"given": "Jacek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lovatoLimitsIndividualConsent2022",
		"type": "paper-conference",
		"abstract": "Personal data are not discrete in socially-networked digital environments. A user who consents to allow access to their profile can expose the personal data of their network connections to non-consented access. Therefore, the traditional consent model (informed and individual) is not appropriate in social networks where informed consent may not be possible for all users affected by data processing and where information is distributed across users. Here, we outline the adequacy of consent for data transactions. Informed by the shortcomings of individual consent, we introduce both a platform-specific model of “distributed consent” and a cross-platform model of a “consent passport.” In both models, individuals and groups can coordinate by giving consent conditional on that of their network connections. We simulate the impact of these distributed consent models on the observability of social networks and find that low adoption would allow macroscopic subsets of networks to preserve their connectivity and privacy.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534640",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2251–2262",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Limits of individual consent and models of distributed consent in online social networks",
		"URL": "https://doi.org/10.1145/3531146.3534640",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper L."
			},
			{
				"family": "Allard",
				"given": "Antoine"
			},
			{
				"family": "Harp",
				"given": "Randall"
			},
			{
				"family": "Onaolapo",
				"given": "Jeremiah"
			},
			{
				"family": "Hébert-Dufresne",
				"given": "Laurent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "speithReviewTaxonomiesExplainable2022",
		"type": "paper-conference",
		"abstract": "The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534639",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2239–2250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A review of taxonomies of explainable artificial intelligence (XAI) methods",
		"URL": "https://doi.org/10.1145/3531146.3534639",
		"author": [
			{
				"family": "Speith",
				"given": "Timo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "robertsonUnderstandingBeingUnderstood2022",
		"type": "paper-conference",
		"abstract": "Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534638",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "2223–2238",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding and being understood: User strategies for identifying and recovering from mistranslations in machine translation-mediated chat",
		"URL": "https://doi.org/10.1145/3531146.3534638",
		"author": [
			{
				"family": "Robertson",
				"given": "Samantha"
			},
			{
				"family": "Díaz",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "jerniteDataGovernanceAge2022",
		"type": "paper-conference",
		"abstract": "The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534637",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2206–2222",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data governance in the age of large-scale data-driven language technology",
		"URL": "https://doi.org/10.1145/3531146.3534637",
		"author": [
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Nguyen",
				"given": "Huu"
			},
			{
				"family": "Biderman",
				"given": "Stella"
			},
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Masoud",
				"given": "Maraim"
			},
			{
				"family": "Danchev",
				"given": "Valentin"
			},
			{
				"family": "Tan",
				"given": "Samson"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Subramani",
				"given": "Nishant"
			},
			{
				"family": "Johnson",
				"given": "Isaac"
			},
			{
				"family": "Dupont",
				"given": "Gerard"
			},
			{
				"family": "Dodge",
				"given": "Jesse"
			},
			{
				"family": "Lo",
				"given": "Kyle"
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Radev",
				"given": "Dragomir"
			},
			{
				"family": "Gokaslan",
				"given": "Aaron"
			},
			{
				"family": "Nikpoor",
				"given": "Somaieh"
			},
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "laranjeiradasilvaSeeingLookingAnalysis2022",
		"type": "paper-conference",
		"abstract": "The online sharing and viewing of Child Sexual Abuse Material (CSAM) are growing fast, such that human experts can no longer handle the manual inspection. However, the automatic classification of CSAM is a challenging field of research, largely due to the inaccessibility of target data that is — and should forever be — private and in sole possession of law enforcement agencies. To aid researchers in drawing insights from unseen data and safely providing further understanding of CSAM images, we propose an analysis template that goes beyond the statistics of the dataset and respective labels. It focuses on the extraction of automatic signals, provided both by pre-trained machine learning models, e.g., object categories and pornography detection, as well as image metrics such as luminance and sharpness. Only aggregated statistics of sparse signals are provided to guarantee the anonymity of children and adolescents victimized. The pipeline allows filtering the data by applying thresholds to each specified signal and provides the distribution of such signals within the subset, correlations between signals, as well as a bias evaluation. We demonstrated our proposal on the Region-based annotated Child Pornography Dataset (RCPD), one of the few CSAM benchmarks in the literature, composed of over 2000 samples among regular and CSAM images, produced in partnership with Brazil’s Federal Police. Although noisy and limited in several senses, we argue that automatic signals can highlight important aspects of the overall distribution of data, which is valuable for databases that can not be disclosed. Our goal is to safely publicize the characteristics of CSAM datasets, encouraging researchers to join the field and perhaps other institutions to provide similar reports on their benchmarks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534636",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2189–2205",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Seeing without looking: Analysis pipeline for child sexual abuse datasets",
		"URL": "https://doi.org/10.1145/3531146.3534636",
		"author": [
			{
				"family": "Laranjeira da Silva",
				"given": "Camila"
			},
			{
				"family": "Macedo",
				"given": "Joao"
			},
			{
				"family": "Avila",
				"given": "Sandra"
			},
			{
				"family": "Santos",
				"given": "Jefersson",
				"non-dropping-particle": "dos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schwobelLongArcFairness2022",
		"type": "paper-conference",
		"abstract": "In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application – the current European efforts to increase the number of women on company boards, e&nbsp;.g.&nbsp; via quota solutions – and present early technical work that fits within our framework.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534635",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2179–2188",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The long arc of fairness: Formalisations and ethical discourse",
		"URL": "https://doi.org/10.1145/3531146.3534635",
		"author": [
			{
				"family": "Schwöbel",
				"given": "Pola"
			},
			{
				"family": "Remmers",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bilstrupDemoDesignTeaching2022",
		"type": "paper-conference",
		"abstract": "The prevalence of artificial intelligence (AI) and machine learning (ML) technologies in digital ecosystems has led to a push for AI literacy, giving everybody, including K-12 students, the necessary knowledge and abilities to engage critically with these new technologies. While there is an increasing focus on designing tools and activities for teaching machine learning, most tools sidestep engaging with the complexity and trade-offs inherent in the design of ML models in favor of demonstrating the power and functionality of the technology. In this paper, we investigate how a design perspective can inform the design of educational tools and activities for teaching machine learning. Through a literature review, we identify 34 tools and activities for teaching ML, and using a design perspective on ML system development, we examine strengths and limitations in how they engage students in the complex design considerations linked to the different components of machine learners. Based on this work, we suggest directions for furthering AI literacy through adopting a design approach in teaching ML.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534634",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2168–2178",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From demo to design in teaching machine learning",
		"URL": "https://doi.org/10.1145/3531146.3534634",
		"author": [
			{
				"family": "Bilstrup",
				"given": "Karl-Emi Kjær"
			},
			{
				"family": "Kaspersen",
				"given": "Magnus Høholt"
			},
			{
				"family": "Assent",
				"given": "Ira"
			},
			{
				"family": "Enni",
				"given": "Simon"
			},
			{
				"family": "Petersen",
				"given": "Marianne Graves"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "usunierFastOnlineRanking2022",
		"type": "paper-conference",
		"abstract": "As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534633",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2157–2167",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fast online ranking with fairness of exposure",
		"URL": "https://doi.org/10.1145/3531146.3534633",
		"author": [
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Do",
				"given": "Virginie"
			},
			{
				"family": "Dohmatob",
				"given": "Elvis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sharafPromotingFairnessLearned2022",
		"type": "paper-conference",
		"abstract": "Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534632",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "2149–2156",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Promoting fairness in learned models by learning to active learn under parity constraints",
		"URL": "https://doi.org/10.1145/3531146.3534632",
		"author": [
			{
				"family": "Sharaf",
				"given": "Amr"
			},
			{
				"family": "Daume III",
				"given": "Hal"
			},
			{
				"family": "Ni",
				"given": "Renkun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "scottAlgorithmicToolsPublic2022",
		"type": "paper-conference",
		"abstract": "Data-driven and algorithmic systems have been introduced to support Public Employment Services (PES) throughout the world. Their deployment has sparked public controversy and, as a consequence, some of these systems have been removed from use or their role was reduced. Yet the implementation of similar systems continues. In this paper, we use a participatory approach to determine a course forward for research and development in this area. We draw attention to the needs and expectations of people directly affected by these systems, i.e., jobseekers. Our investigation comprises two workshops: the first a fact-finding workshop with academics, system developers, the public sector, and civil-society organizations, the second a co-design workshop with 13 unemployed migrants to Germany. Based on the discussion in the fact-finding workshop we identified challenges of existing PES (algorithmic) systems. From the co-design workshop we identified our participants’ needs and desires when contacting PES: the need for human contact, the expectation to receive genuine orientation, and the desire to be seen as a whole human being. We map these expectations to three design considerations for data-driven and algorithmic systems for PES: the importance of interpersonal interaction, jobseeker assessment as direction, and the challenge of mitigating misrepresentation. Finally, we argue that the limitations and risks of current systems cannot be addressed through minor adjustments but require a more fundamental change to the role of PES.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534631",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2138–2148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic tools in public employment services: Towards a jobseeker-centric perspective",
		"URL": "https://doi.org/10.1145/3531146.3534631",
		"author": [
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Wang",
				"given": "Sonja Mei"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Delobelle",
				"given": "Pieter"
			},
			{
				"family": "Sztandar-Sztanderska",
				"given": "Karolina"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kuhlKeepYourFriends2022",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534630",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2125–2137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Keep your friends close and your counterfactuals closer: Improved learning from closest rather than plausible counterfactual explanations in an abstract setting",
		"URL": "https://doi.org/10.1145/3531146.3534630",
		"author": [
			{
				"family": "Kuhl",
				"given": "Ulrike"
			},
			{
				"family": "Artelt",
				"given": "André"
			},
			{
				"family": "Hammer",
				"given": "Barbara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "drawsEffectsCrowdWorker2022",
		"type": "paper-conference",
		"abstract": "Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534629",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2114–2124",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effects of crowd worker biases in fact-checking tasks",
		"URL": "https://doi.org/10.1145/3531146.3534629",
		"author": [
			{
				"family": "Draws",
				"given": "Tim"
			},
			{
				"family": "La Barbera",
				"given": "David"
			},
			{
				"family": "Soprano",
				"given": "Michael"
			},
			{
				"family": "Roitero",
				"given": "Kevin"
			},
			{
				"family": "Ceolin",
				"given": "Davide"
			},
			{
				"family": "Checco",
				"given": "Alessandro"
			},
			{
				"family": "Mizzaro",
				"given": "Stefano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "limaConflictExplainableAccountable2022",
		"type": "paper-conference",
		"abstract": "Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534628",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2103–2113",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The conflict between explainable and accountable decision-making algorithms",
		"URL": "https://doi.org/10.1145/3531146.3534628",
		"author": [
			{
				"family": "Lima",
				"given": "Gabriel"
			},
			{
				"family": "Grgić-Hlača",
				"given": "Nina"
			},
			{
				"family": "Jeong",
				"given": "Jin Keun"
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "devinneyTheoriesGenderNLP2022",
		"type": "paper-conference",
		"abstract": "The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534627",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 20\npublisher-place: Seoul, Republic of Korea",
		"page": "2083–2102",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Theories of “Gender” in NLP bias research",
		"URL": "https://doi.org/10.1145/3531146.3534627",
		"author": [
			{
				"family": "Devinney",
				"given": "Hannah"
			},
			{
				"family": "Björklund",
				"given": "Jenny"
			},
			{
				"family": "Björklund",
				"given": "Henrik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "boydDesigningValuesensitiveDesign2022",
		"type": "paper-conference",
		"abstract": "If “studying up,” or researching powerful actors in a social system, can offer insight into the workings and effects of power in social systems, this paper argues that “designing up” will give researchers and designers a tool to intervene. This paper offers a conception of “designing up,” applies the structure of Value Sensitive Design (VSD) to accomplish it, and submits an example of a tool designed to support ethical sensitivity, especially particularization and judgment. The designed artifact is a field guide for ethical mitigation strategies that uses tool profiles and filters to aid machine learning (ML) engineers as they build understanding of an ethical issue they have recognized and as they match the particulars of their problem to a technical ethical mitigation. This guide may broaden its users’ awareness of potential ethical issues, important features of ethical issues and their mitigations, and the breadth of available mitigations. Additionally, it may encourage ethical sensitivity in future ML projects. Feedback from ML engineers and technology ethics researchers rendered several usability improvements and ideas for future development. The tool can be found at: https://ml-ethics-tool.web.app/.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534626",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "2069–2082",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing up with value-sensitive design: Building a field guide for ethical ML development",
		"URL": "https://doi.org/10.1145/3531146.3534626",
		"author": [
			{
				"family": "Boyd",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ashurstDisentanglingComponentsEthical2022",
		"type": "paper-conference",
		"abstract": "While practical applications of machine learning have been the target of considerable normative scrutiny over the past decade, there is growing concern with machine learning research as well. Debates are currently unfolding about how the research community should develop its research agendas, conduct its research, evaluate its research contributions, and handle the publication and dissemination of its findings, among other matters. At times, these debates have been quite heated, with different actors adopting different positions on what it means to do machine learning research ethically. In this paper, we show that some of the disagreement owes to a lack of clarity about what ethical issues are at stake in machine learning research, how these issues—in particular, the concerns with research integrity, research process harms, and downstream consequences—relate to (or, more often, differ from) one another. We then explore which mechanisms are most appropriate for dealing with the different types of ethical issues, and highlight which ethical issues require more attention than they are currently receiving. Ultimately, we hope to foster more productive discussions about the responsibilities that the community bears in addressing the ethical challenges tied to machine learning research and how to best fulfil these responsibilities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533781",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2057–2068",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling the components of ethical research in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533781",
		"author": [
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Campbell",
				"given": "Rosie"
			},
			{
				"family": "Raji",
				"given": "Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ashurstAIEthicsStatements2022",
		"type": "paper-conference",
		"abstract": "Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533780",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2047–2056",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI ethics statements: Analysis and lessons learnt from NeurIPS broader impact statements",
		"URL": "https://doi.org/10.1145/3531146.3533780",
		"author": [
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Hine",
				"given": "Emmie"
			},
			{
				"family": "Sedille",
				"given": "Paul"
			},
			{
				"family": "Carlier",
				"given": "Alexis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "widderLimitsPossibilitiesEthical2022",
		"type": "paper-conference",
		"abstract": "Open source software communities are a significant site of AI development, but “Ethical AI” discourses largely focus on the problems that arise in software produced by private companies. Design, policy and tooling interventions to encourage “Ethical AI” based on studies in private companies risk being ill-suited for an open source context, which operates under radically different organizational structures, cultural norms, and incentives. In this paper, we show that significant and understudied harms and possibilities originate from differing practices of transparency and accountability in the open source community. We conducted an interview study of an AI-enabled open source Deepfake project to understand how members of that community reason about the ethics of their work. We found that notions of the “Freedom 0” to use code without any restriction, alongside beliefs about technology neutrality and technological inevitability, were central to how community members framed their responsibilities, and the actions they believed were and were not available to them. We propose a continuum between harms resulting from how a system is implemented versus how it is used, and show how commitments to radical transparency in open source allow great ethical scrutiny for harms wrought by implementation bugs, but allow harms through (mis)use to proliferate, requiring a deeper toolbox for disincentivizing harmful use. We discuss how an assumption of control over downstream uses is often implicit in discourses of “Ethical AI”, but outline alternative possibilities for action in cases such as open source where this assumption may not hold.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533779",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2035–2046",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Limits and possibilities for “Ethical AI” in open source: A study of deepfakes",
		"URL": "https://doi.org/10.1145/3531146.3533779",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			},
			{
				"family": "Nafus",
				"given": "Dawn"
			},
			{
				"family": "Dabbish",
				"given": "Laura"
			},
			{
				"family": "Herbsleb",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "loiCalibrationFairnessRequirement2022",
		"type": "paper-conference",
		"abstract": "In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533245",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "2026–2034",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Is calibration a fairness requirement? An argument from the point of view of moral philosophy and decision theory",
		"URL": "https://doi.org/10.1145/3531146.3533245",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mehandruReliableSafeUse2022",
		"type": "paper-conference",
		"abstract": "Language barriers between patients and clinicians contribute to disparities in quality of care. Machine Translation (MT) tools are widely used in healthcare settings, but even small mistranslations can have life-threatening consequences. We study how MT is currently used in medical settings through a qualitative interview study with 20 clinicians–physicians, surgeons, nurses, and midwives. We find that clinicians face challenges stemming from lack of time and resources, cultural barriers, and medical literacy rates, as well as accountability in cases of miscommunication. Clinicians have devised strategies to aid communication in the face of language barriers including back translation, non-verbal communication, and testing patient understanding. We propose design implications for machine translation systems including combining neural MT with pre-translated medical phrases, integrating translation support with multimodal communication, and providing interactive support for testing mutual understanding.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533244",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2016–2025",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reliable and safe use of machine translation in medical settings",
		"URL": "https://doi.org/10.1145/3531146.3533244",
		"author": [
			{
				"family": "Mehandru",
				"given": "Nikita"
			},
			{
				"family": "Robertson",
				"given": "Samantha"
			},
			{
				"family": "Salehi",
				"given": "Niloufar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cousinsUncertaintySocialPlanners2022",
		"type": "paper-conference",
		"abstract": "Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533243",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2004–2015",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncertainty and the social planner’s problem: Why sample complexity matters",
		"URL": "https://doi.org/10.1145/3531146.3533243",
		"author": [
			{
				"family": "Cousins",
				"given": "Cyrus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tschantzWhatProxyDiscrimination2022",
		"type": "paper-conference",
		"abstract": "The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533242",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1993–2003",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What is proxy discrimination?",
		"URL": "https://doi.org/10.1145/3531146.3533242",
		"author": [
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ganskyCounterFAccTualHowFAccT2022",
		"type": "paper-conference",
		"abstract": "This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles.We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems.In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533241",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1982–1992",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CounterFAccTual: How FAccT undermines its organizing principles",
		"URL": "https://doi.org/10.1145/3531146.3533241",
		"author": [
			{
				"family": "Gansky",
				"given": "Ben"
			},
			{
				"family": "McDonald",
				"given": "Sean"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "deonSpotlightGeneralMethod2022",
		"type": "paper-conference",
		"abstract": "Supervised learning models often make systematic errors on rare subsets of the data. When these subsets correspond to explicit labels in the data (e.g., gender, race) such poor performance can be identified straightforwardly. This paper introduces a method for discovering systematic errors that do not correspond to such explicitly labelled subgroups. The key idea is that similar inputs tend to have similar representations in the final hidden layer of a neural network. We leverage this structure by “shining a spotlight” on this representation space to find contiguous regions in which the model performs poorly. We show that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and we verify its performance through quantitative experiments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533240",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 20\npublisher-place: Seoul, Republic of Korea",
		"page": "1962–1981",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The spotlight: A general method for discovering systematic errors in deep learning models",
		"URL": "https://doi.org/10.1145/3531146.3533240",
		"author": [
			{
				"family": "Eon",
				"given": "Greg",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Eon",
				"given": "Jason",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Wright",
				"given": "James R."
			},
			{
				"family": "Leyton-Brown",
				"given": "Kevin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rostamzadehHealthsheetDevelopmentTransparency2022",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) approaches have demonstrated promising results in a wide range of healthcare applications. Data plays a crucial role in developing ML-based healthcare systems that directly affect people’s lives. Many of the ethical issues surrounding the use of ML in healthcare stem from structural inequalities underlying the way we collect, use, and handle data. Developing guidelines to improve documentation practices regarding the creation, use, and maintenance of ML healthcare datasets is therefore of critical importance. In this work, we introduce Healthsheet, a contextualized adaptation of the original datasheet questionnaire &nbsp;[22] for health-specific applications. Through a series of semi-structured interviews, we adapt the datasheets for healthcare data documentation. As part of the Healthsheet development process and to understand the obstacles researchers face in creating datasheets, we worked with three publicly-available healthcare datasets as our case studies, each with different types of structured data: Electronic health Records (EHR), clinical trial study data, and smartphone-based performance outcome measures. Our findings from the interviewee study and case studies show 1) that datasheets should be contextualized for healthcare, 2) that despite incentives to adopt accountability practices such as datasheets, there is a lack of consistency in the broader use of these practices 3) how the ML for health community views datasheets and particularly Healthsheets as diagnostic tool to surface the limitations and strength of datasets and 4) the relative importance of different fields in the datasheet to healthcare concerns.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533239",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1943–1961",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Healthsheet: Development of a transparency artifact for health datasets",
		"URL": "https://doi.org/10.1145/3531146.3533239",
		"author": [
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Mincu",
				"given": "Diana"
			},
			{
				"family": "Roy",
				"given": "Subhrajit"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			},
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Schrouff",
				"given": "Jessica"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			},
			{
				"family": "Moorosi",
				"given": "Nyalleng"
			},
			{
				"family": "Heller",
				"given": "Katherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "patroFairRankingCritical2022",
		"type": "paper-conference",
		"abstract": "Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large “fair ranking” research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533238",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1929–1942",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair ranking: a critical review, challenges, and future directions",
		"URL": "https://doi.org/10.1145/3531146.3533238",
		"author": [
			{
				"family": "Patro",
				"given": "Gourab K."
			},
			{
				"family": "Porcaro",
				"given": "Lorenzo"
			},
			{
				"family": "Mitchell",
				"given": "Laura"
			},
			{
				"family": "Zhang",
				"given": "Qiuyue"
			},
			{
				"family": "Zehlike",
				"given": "Meike"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rameshHowPlatformuserPower2022",
		"type": "paper-conference",
		"abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533237",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1917–1928",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How platform-user power relations shape algorithmic accountability: A case study of instant loan platforms and financially stressed users in india",
		"URL": "https://doi.org/10.1145/3531146.3533237",
		"author": [
			{
				"family": "Ramesh",
				"given": "Divya"
			},
			{
				"family": "Kameswaran",
				"given": "Vaishnav"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Sambasivan",
				"given": "Nithya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "grabowiczMarryingFairnessExplainability2022",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533236",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1905–1916",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Marrying fairness and explainability in supervised learning",
		"URL": "https://doi.org/10.1145/3531146.3533236",
		"author": [
			{
				"family": "Grabowicz",
				"given": "Przemyslaw A."
			},
			{
				"family": "Perello",
				"given": "Nicholas"
			},
			{
				"family": "Mishra",
				"given": "Aarshee"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "patelModelExplanationsDifferential2022",
		"type": "paper-conference",
		"abstract": "Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533235",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1895–1904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model explanations with differential privacy",
		"URL": "https://doi.org/10.1145/3531146.3533235",
		"author": [
			{
				"family": "Patel",
				"given": "Neel"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dodgeMeasuringCarbonIntensity2022",
		"type": "paper-conference",
		"abstract": "The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions. In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1877–1894",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring the carbon intensity of AI in cloud instances",
		"URL": "https://doi.org/10.1145/3531146.3533234",
		"author": [
			{
				"family": "Dodge",
				"given": "Jesse"
			},
			{
				"family": "Prewitt",
				"given": "Taylor"
			},
			{
				"family": "Tachet des Combes",
				"given": "Remi"
			},
			{
				"family": "Odmark",
				"given": "Erika"
			},
			{
				"family": "Schwartz",
				"given": "Roy"
			},
			{
				"family": "Strubell",
				"given": "Emma"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			},
			{
				"family": "DeCario",
				"given": "Nicole"
			},
			{
				"family": "Buchanan",
				"given": "Will"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hutchinsonEvaluationGapsMachine2022",
		"type": "paper-conference",
		"abstract": "Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533233",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1859–1876",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluation gaps in machine learning practice",
		"URL": "https://doi.org/10.1145/3531146.3533233",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Heller",
				"given": "Katherine"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "semenovaExistenceSimplerMachine2022",
		"type": "paper-conference",
		"abstract": "It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533232",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 32\npublisher-place: Seoul, Republic of Korea",
		"page": "1827–1858",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the existence of simpler machine learning models",
		"URL": "https://doi.org/10.1145/3531146.3533232",
		"author": [
			{
				"family": "Semenova",
				"given": "Lesia"
			},
			{
				"family": "Rudin",
				"given": "Cynthia"
			},
			{
				"family": "Parr",
				"given": "Ronald"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pushkarnaDataCardsPurposeful2022",
		"type": "paper-conference",
		"abstract": "As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset’s origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset’s lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models—such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533231",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 51\npublisher-place: Seoul, Republic of Korea",
		"page": "1776–1826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data cards: Purposeful and transparent dataset documentation for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533231",
		"author": [
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Kjartansson",
				"given": "Oddur"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "readerModelsUnderstandingQuantifying2022",
		"type": "paper-conference",
		"abstract": "When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533230",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1765–1775",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Models for understanding and quantifying feedback in societal systems",
		"URL": "https://doi.org/10.1145/3531146.3533230",
		"author": [
			{
				"family": "Reader",
				"given": "Lydia"
			},
			{
				"family": "Nokhiz",
				"given": "Pegah"
			},
			{
				"family": "Power",
				"given": "Cathleen"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Friedler",
				"given": "Sorelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ganguliPredictabilitySurpriseLarge2022",
		"type": "paper-conference",
		"abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533229",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1747–1764",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Predictability and surprise in large generative models",
		"URL": "https://doi.org/10.1145/3531146.3533229",
		"author": [
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Dassarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "El Showk",
				"given": "Sheer"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kernian",
				"given": "Jackson"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Daniela"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Clark",
				"given": "Jack"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "abebeAdversarialScrutinyEvidentiary2022",
		"type": "paper-conference",
		"abstract": "The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533228",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1733–1746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adversarial scrutiny of evidentiary statistical software",
		"URL": "https://doi.org/10.1145/3531146.3533228",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			},
			{
				"family": "Jin",
				"given": "Angela"
			},
			{
				"family": "Miller",
				"given": "John"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Wexler",
				"given": "Rebecca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "matiasTestingConcernsTechnologys2022",
		"type": "paper-conference",
		"abstract": "As public trust in technology companies has declined, people are questioning the effects of digital technologies in their lives. In this context, many evidence-free claims from corporations and tech critics are widely circulated. How can members of the public make evidence-based decisions about digital technology in their lives? In clinical fields, N -of-one trials enable participant-investigators to make personalized causal discoveries about managing health, improving fitness, and improving their education. Similar methods could help community scientists understand and manage how they use digital technologies. In this paper, we introduce Conjecture, a system for coordinating N -of-one trials that can guide personal decisions about technology use and contribute to science. We describe N -of-one trials as a design challenge and present the design of the Conjecture system. We evaluate the system with a field experiment that tests folk theories about the influence of colorful screens on alleged phone addiction. We present findings on the design of N -of-one-trial systems based on submitted data, interviews, and surveys with 14 participants. Taken together, this paper introduces N -of-one trials as a fruitful direction for computer scientists designing industry-independent systems for evidence-based technology governance and accountability.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533227",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1722–1732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Testing concerns about technology's behavioral impacts with N-of-one trials",
		"URL": "https://doi.org/10.1145/3531146.3533227",
		"author": [
			{
				"family": "Matias",
				"given": "Nathan"
			},
			{
				"family": "Pennington",
				"given": "Eric"
			},
			{
				"family": "Chan",
				"given": "Zenobia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "andrusDemographicreliantAlgorithmicFairness2022",
		"type": "paper-conference",
		"abstract": "Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533226",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1709–1721",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Demographic-reliant algorithmic fairness: Characterizing the risks of demographic data collection in the pursuit of fairness",
		"URL": "https://doi.org/10.1145/3531146.3533226",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Villeneuve",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wuFairnessawareModelagnosticPositive2022",
		"type": "paper-conference",
		"abstract": "With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533225",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1698–1708",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness-aware model-agnostic positive and unlabeled learning",
		"URL": "https://doi.org/10.1145/3531146.3533225",
		"author": [
			{
				"family": "Wu",
				"given": "Ziwei"
			},
			{
				"family": "He",
				"given": "Jingrui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "barberanNeuroViewRNNItsTime2022",
		"type": "paper-conference",
		"abstract": "Recurrent Neural Networks (RNNs) are important tools for processing sequential data such as time-series or video. Interpretability is defined as the ability to be understood by a person and is different from explainability, which is the ability to be explained in a mathematical formulation. A key interpretability issue with RNNs is that it is not clear how each hidden state per time step contributes to the decision-making process in a quantitative manner. We propose NeuroView-RNN as a family of new RNN architectures that explains how all the time steps are used for the decision-making process. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier. The global linear classifier has all the hidden states as the input, so the weights of the classifier have a linear mapping to the hidden states. Hence, from the weights, NeuroView-RNN can quantify how important each time step is to a particular decision. As a bonus, NeuroView-RNN also offers higher accuracy in many cases compared to the RNNs and their variants. We showcase the benefits of NeuroView-RNN by evaluating on a multitude of diverse time-series datasets.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533224",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "1683–1697",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "NeuroView-RNN: It’s about time",
		"URL": "https://doi.org/10.1145/3531146.3533224",
		"author": [
			{
				"family": "Barberan",
				"given": "Cj"
			},
			{
				"family": "Alemmohammad",
				"given": "Sina"
			},
			{
				"family": "Liu",
				"given": "Naiming"
			},
			{
				"family": "Balestriero",
				"given": "Randall"
			},
			{
				"family": "Baraniuk",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "johnsonWhatBureaucraticCounterfactual2022",
		"type": "paper-conference",
		"abstract": "There is growing concern about governments’ use of algorithms to make high-stakes decisions. While an early wave of research focused on algorithms that predict risk to allocate punishment and suspicion, a newer wave of research studies algorithms that predict “need” or “benefit” to target beneficial resources, such as ranking those experiencing homelessness by their need for housing. The present paper argues that existing research on the role of algorithms in social policy could benefit from a counterfactual perspective that asks: given that a social service bureaucracy needs to make some decision about whom to help, what status quo prioritization method would algorithms replace? While a large body of research contrasts human versus algorithmic decision-making, social service bureaucracies target help not by giving street-level bureaucrats full discretion. Instead, they primarily target help through pre-algorithmic, rule-based methods. In this paper, we outline social policy’s current status quo method—categorical prioritization—where decision-makers manually (1) decide which attributes of help seekers should give those help seekers priority, (2) simplify any continuous measures of need into categories (e.g., household income falls below a threshold), and (3) manually choose the decision rules that map categories to priority levels. We draw on novel data and quantitative and qualitative social science methods to outline categorical prioritization in two case studies of United States social policy: waitlists for scarce housing vouchers and K-12 school finance formulas. We outline three main differences between categorical and algorithmic prioritization: is the basis for prioritization formalized; what role does power play in prioritization; and are decision rules for priority manually chosen or inductively derived from a predictive model. Concluding, we show how the counterfactual perspective underscores both the understudied costs of categorical prioritization in social policy and the understudied potential of predictive algorithms to narrow inequalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533223",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1671–1682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What is the bureaucratic counterfactual? Categorical versus algorithmic prioritization in U.S. social policy",
		"URL": "https://doi.org/10.1145/3531146.3533223",
		"author": [
			{
				"family": "Johnson",
				"given": "Rebecca Ann"
			},
			{
				"family": "Zhang",
				"given": "Simone"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangStopSpreadContextual2022",
		"type": "paper-conference",
		"abstract": "We present an empirical study exploring how privacy influences the acceptance of vaccination certificate (VC) deployments across different realistic usage scenarios. The study employed the privacy framework of Contextual Integrity, which has been shown to be particularly effective in capturing people’s privacy expectations across different contexts. We use a vignette methodology, where we selectively manipulate salient contextual parameters to learn whether and how they affect people’s attitudes towards VCs. We surveyed 890 participants from a demographically-stratified sample of the US population to gauge the acceptance and overall attitudes towards possible VC deployments to enforce vaccination mandates and the different information flows VCs might entail. Analysis of results collected as part of this study is used to derive general normative observations about different possible VC practices and to provide guidance for the possible deployments of VCs in different contexts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533222",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1657–1670",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stop the spread: A contextual integrity perspective on the appropriateness of COVID-19 vaccination certificates",
		"URL": "https://doi.org/10.1145/3531146.3533222",
		"author": [
			{
				"family": "Zhang",
				"given": "Shikun"
			},
			{
				"family": "Shvartzshnaider",
				"given": "Yan"
			},
			{
				"family": "Feng",
				"given": "Yuanyuan"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			},
			{
				"family": "Sadeh",
				"given": "Norman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "donahueHumanalgorithmCollaborationAchieving2022",
		"type": "paper-conference",
		"abstract": "Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533221",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1639–1656",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human-algorithm collaboration: Achieving complementarity and avoiding unfairness",
		"URL": "https://doi.org/10.1145/3531146.3533221",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "brubachCharacterizingPropertiesTradeoffs2022",
		"type": "paper-conference",
		"abstract": "Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533219",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1629–1638",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Characterizing properties and trade-offs of centralized delegation mechanisms in liquid democracy",
		"URL": "https://doi.org/10.1145/3531146.3533219",
		"author": [
			{
				"family": "Brubach",
				"given": "Brian"
			},
			{
				"family": "Ballarin",
				"given": "Audrey"
			},
			{
				"family": "Nazeer",
				"given": "Heeba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schoefferThereNotEnough2022",
		"type": "paper-conference",
		"abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533218",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1616–1628",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“There is not enough information”: On the effects of explanations on perceptions of informational fairness and trustworthiness in automated decision-making",
		"URL": "https://doi.org/10.1145/3531146.3533218",
		"author": [
			{
				"family": "Schoeffer",
				"given": "Jakob"
			},
			{
				"family": "Kuehl",
				"given": "Niklas"
			},
			{
				"family": "Machowski",
				"given": "Yvette"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kambhatlaSurfacingRacialStereotypes2022",
		"type": "paper-conference",
		"abstract": "Content warning: this paper discusses and contains content that may be offensive or upsetting.People express racial stereotypes through conversations with others, increasingly in a digital format; as a result, the ability to computationally identify racial stereotypes could be beneficial to help mitigate some of the harmful effects of stereotyping. In this work, we seek to better understand how we can computationally surface racial stereotypes in text by identifying linguistic features associated with differences in racial identity portrayal, focused on two races (Black and White). We collect novel data of individuals’ self-presentation via crowdsourcing, where each crowdworker answers a set of prompts from their own perspective (real identity), and from the perspective of another racial identity (portrayed identity), keeping the gender constant. We use these responses as a dataset to identify stereotypes. Through a series of experiments based on classifications between real and portrayed identities, we show that generalizations and stereotypes appear to be more prevalent amongst white participants than black participants. Through analyses of predictive words and word usage patterns, we find that some of the most predictive features of an author portraying a different racial identity are known stereotypes, and reveal how people of different identities see themselves and others.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533217",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1604–1615",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Surfacing racial stereotypes through identity portrayal",
		"URL": "https://doi.org/10.1145/3531146.3533217",
		"author": [
			{
				"family": "Kambhatla",
				"given": "Gauri"
			},
			{
				"family": "Stewart",
				"given": "Ian"
			},
			{
				"family": "Mihalcea",
				"given": "Rada"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sachdevaAssessingAnnotatorIdentity2022",
		"type": "paper-conference",
		"abstract": "Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533216",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1585–1603",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing annotator identity sensitivity via item response theory: A case study in a hate speech corpus",
		"URL": "https://doi.org/10.1145/3531146.3533216",
		"author": [
			{
				"family": "Sachdeva",
				"given": "Pratik S."
			},
			{
				"family": "Barreto",
				"given": "Renata"
			},
			{
				"family": "Vacano",
				"given": "Claudia",
				"non-dropping-particle": "von"
			},
			{
				"family": "Kennedy",
				"given": "Chris J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "costanza-chockWhoAuditsAuditors2022",
		"type": "paper-conference",
		"abstract": "Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533213",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1571–1583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem",
		"URL": "https://doi.org/10.1145/3531146.3533213",
		"author": [
			{
				"family": "Costanza-Chock",
				"given": "Sasha"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "irionAlgorithmsOfflimitsIf2022",
		"type": "paper-conference",
		"abstract": "Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533212",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1561–1570",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms Off-limits? If digital trade law restricts access to source code of software then accountability will suffer",
		"URL": "https://doi.org/10.1145/3531146.3533212",
		"author": [
			{
				"family": "Irion",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "almuzainiABCinMLAnticipatoryBias2022",
		"type": "paper-conference",
		"abstract": "The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533211",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "1552–1560",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ABCinML: Anticipatory bias correction in machine learning applications",
		"URL": "https://doi.org/10.1145/3531146.3533211",
		"author": [
			{
				"family": "Almuzaini",
				"given": "Abdulaziz A."
			},
			{
				"family": "Bhatt",
				"given": "Chidansh A."
			},
			{
				"family": "Pennock",
				"given": "David M."
			},
			{
				"family": "Singh",
				"given": "Vivek K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "agarwalPowerRandomizationFair2022",
		"type": "paper-conference",
		"abstract": "Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533209",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1542–1551",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the power of randomization in fair classification and representation",
		"URL": "https://doi.org/10.1145/3531146.3533209",
		"author": [
			{
				"family": "Agarwal",
				"given": "Sushant"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "klumbytundefinedCriticalToolsMachine2022",
		"type": "paper-conference",
		"abstract": "This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of “situating/situated knowledges”, \"figuration\", \"diffraction\", and “critical fabulation/speculation” were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533207",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1528–1541",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Critical tools for machine learning: Working with intersectional critical concepts in machine learning systems design",
		"URL": "https://doi.org/10.1145/3531146.3533207",
		"author": [
			{
				"family": "Klumbytundefined",
				"given": "Goda"
			},
			{
				"family": "Draude",
				"given": "Claude"
			},
			{
				"family": "Taylor",
				"given": "Alex S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fishmanShouldAttentionBe2022",
		"type": "paper-conference",
		"abstract": "“Attention is all you need” has become a fundamental precept in machine learning research. Originally designed for machine translation, transformers and the attention mechanisms that underpin them now find success across many problem domains. With the apparent domain-agnostic success of transformers, many researchers are excited that similar model architectures can be successfully deployed across diverse applications in vision, language and beyond. We consider the benefits and risks of these waves of unification on both epistemic and ethical fronts. On the epistemic side, we argue that many of the arguments in favor of unification in the natural sciences fail to transfer over to the machine learning case, or transfer over only under assumptions that might not hold. Unification also introduces epistemic risks related to portability, path dependency, methodological diversity, and increased black-boxing. On the ethical side, we discuss risks emerging from epistemic concerns, further marginalizing underrepresented perspectives, the centralization of power, and having fewer models across more domains of application.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533206",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1516–1527",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should attention be all we need? The epistemic and ethical implications of unification in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533206",
		"author": [
			{
				"family": "Fishman",
				"given": "Nic"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "neumannJusticeMisinformationDetection2022",
		"type": "paper-conference",
		"abstract": "Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533205",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1504–1515",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Justice in misinformation detection systems: An analysis of algorithms, stakeholders, and potential harms",
		"URL": "https://doi.org/10.1145/3531146.3533205",
		"author": [
			{
				"family": "Neumann",
				"given": "Terrence"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Fazelpour",
				"given": "Sina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blackAlgorithmicFairnessVertical2022",
		"type": "paper-conference",
		"abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533204",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 25\npublisher-place: Seoul, Republic of Korea",
		"page": "1479–1503",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness and vertical equity: Income fairness with IRS tax audit models",
		"URL": "https://doi.org/10.1145/3531146.3533204",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Goldin",
				"given": "Jacob"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "caiAdaptiveSamplingStrategies2022",
		"type": "paper-conference",
		"abstract": "In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data—an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533203",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1467–1478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adaptive sampling strategies to construct equitable training datasets",
		"URL": "https://doi.org/10.1145/3531146.3533203",
		"author": [
			{
				"family": "Cai",
				"given": "William"
			},
			{
				"family": "Encarnacion",
				"given": "Ro"
			},
			{
				"family": "Chern",
				"given": "Bobbie"
			},
			{
				"family": "Corbett-Davies",
				"given": "Sam"
			},
			{
				"family": "Bogen",
				"given": "Miranda"
			},
			{
				"family": "Bergman",
				"given": "Stevie"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ferrarioHowExplainabilityContributes2022",
		"type": "paper-conference",
		"abstract": "We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as “explainability fosters trust in AI,” that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called “trust as anti-monitoring,” that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that “explainability fosters trust in AI” if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user’s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533202",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1457–1466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How explainability contributes to trust in AI",
		"URL": "https://doi.org/10.1145/3531146.3533202",
		"author": [
			{
				"family": "Ferrario",
				"given": "Andrea"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "poirierAccountableDataPolitics2022",
		"type": "paper-conference",
		"abstract": "This paper attends specifically to what I call “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit – what is ultimately being counted in the data – is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533201",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1446–1456",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accountable data: The politics and pragmatics of disclosure datasets",
		"URL": "https://doi.org/10.1145/3531146.3533201",
		"author": [
			{
				"family": "Poirier",
				"given": "Lindsay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pngTensionsSouthNorth2022",
		"type": "paper-conference",
		"abstract": "This paper aims to present a landscape of AI governance for and from the Global South, advanced by critical and decolonial-informed practitioners and scholars, and contrast this with the Inclusive AI Governance discourse led out of Global North institutions. By doing so, it identifies gaps in the dominant AI governance discourse, and bridges these gaps with relevant discourses of technology and power, localisation, and historical-geopolitical analyses of inequality led by Global South aligned actors. Specific areas of concern addressed by this paper include infrastructural and regulatory monopolies, harms associated with the labour and material supply chains of AI infrastructure, and commercial exploitation. By contrasting Global South and Global North discourses surrounding AI risks, this paper proposes a systemic restructuring of AI governance processes beyond current frameworks of Inclusive AI governance, offering three roles for Global South actors to substantively engage in AI governance processes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533200",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1434–1445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "At the tensions of south and north: Critical roles of global south stakeholders in AI governance",
		"URL": "https://doi.org/10.1145/3531146.3533200",
		"author": [
			{
				"family": "Png",
				"given": "Marie-Therese"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rateikeDontThrowIt2022",
		"type": "paper-conference",
		"abstract": "Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533199",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1421–1433",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Don’t throw it away! The utility of unlabeled data in fair decision making",
		"URL": "https://doi.org/10.1145/3531146.3533199",
		"author": [
			{
				"family": "Rateike",
				"given": "Miriam"
			},
			{
				"family": "Majumdar",
				"given": "Ayan"
			},
			{
				"family": "Mineeva",
				"given": "Olga"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "susserDecisionTimeNormative2022",
		"type": "paper-conference",
		"abstract": "Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533198",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1410–1420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Decision time: Normative dimensions of algorithmic speed",
		"URL": "https://doi.org/10.1145/3531146.3533198",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "buet-golfouseFairUnsupervisedLearning2022",
		"type": "paper-conference",
		"abstract": "Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533197",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1399–1409",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fair unsupervised learning",
		"URL": "https://doi.org/10.1145/3531146.3533197",
		"author": [
			{
				"family": "Buet-Golfouse",
				"given": "Francois"
			},
			{
				"family": "Utyagulov",
				"given": "Islam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "thorntonAlchemyTrustCreative2022",
		"type": "paper-conference",
		"abstract": "Trust is recognised as a significant and valuable component of socio-technical systems, facilitating numerous important benefits. Many trust models have been created throughout various streams of literature, describing trust for different stakeholders in different contexts. However, when designing a system with multiple stakeholders in their multiple contexts, how does one decide which trust model(s) to apply? And furthermore, how does one go from selecting a model or models to translating those into design? We review and analyse two prominent trust models, and apply them to the design of a trustworthy socio-technical system, namely virtual research environments. We show that a singular model cannot easily be imported and directly implemented into the design of such a system. We introduce the concept of alchemy as the most apt characterization of a successful design process, illustrating the need for designers to engage with the richness of the trust landscape and creatively experiment with components from multiple models to create the perfect blend for their context. We provide a demonstrative case study illustrating the process through which designers of socio-technical systems can become alchemists of trust.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533196",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1387–1398",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The alchemy of trust: The creative act of designing trustworthy socio-technical systems",
		"URL": "https://doi.org/10.1145/3531146.3533196",
		"author": [
			{
				"family": "Thornton",
				"given": "Lauren"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Blair",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "youngConfrontingPowerCorporate2022",
		"type": "paper-conference",
		"abstract": "Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533194",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1375–1386",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Confronting power and corporate capture at the FAccT conference",
		"URL": "https://doi.org/10.1145/3531146.3533194",
		"author": [
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Krafft",
				"given": "P.M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fogliatoWhoGoesFirst2022",
		"type": "paper-conference",
		"abstract": "Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients’ X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported that the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533193",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1362–1374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who goes first? Influences of human-AI workflow on decision making in clinical imaging",
		"URL": "https://doi.org/10.1145/3531146.3533193",
		"author": [
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Chappidi",
				"given": "Shreya"
			},
			{
				"family": "Lungren",
				"given": "Matthew"
			},
			{
				"family": "Fisher",
				"given": "Paul"
			},
			{
				"family": "Wilson",
				"given": "Diane"
			},
			{
				"family": "Fitzke",
				"given": "Michael"
			},
			{
				"family": "Parkinson",
				"given": "Mark"
			},
			{
				"family": "Horvitz",
				"given": "Eric"
			},
			{
				"family": "Inkpen",
				"given": "Kori"
			},
			{
				"family": "Nushi",
				"given": "Besmira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schramowskiCanMachinesHelp2022",
		"type": "paper-conference",
		"abstract": "This paper contains images and descriptions that are offensive in nature.Large datasets underlying much of current machine learning raise serious issues concerning inappropriate content such as offensive, insulting, threatening, or might otherwise cause anxiety. This calls for increased dataset documentation, e.g., using datasheets. They, among other topics, encourage to reflect on the composition of the datasets. So far, this documentation, however, is done manually and therefore can be tedious and error-prone, especially for large image datasets. Here we ask the arguably “circular” question of whether a machine can help us reflect on inappropriate content, answering Question 16 in Datasheets. To this end, we propose to use the information stored in pre-trained transformer models to assist us in the documentation process. Specifically, prompt-tuning based on a dataset of socio-moral values steers CLIP to identify potentially inappropriate content, therefore reducing human labor. We then document the inappropriate images found using word clouds, based on captions generated using a vision-language model. The documentations of two popular, large-scale computer vision datasets—ImageNet and OpenImages—produced this way suggest that machines can indeed help dataset creators to answer Question 16 on inappropriate image content.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533192",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1350–1361",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
		"URL": "https://doi.org/10.1145/3531146.3533192",
		"author": [
			{
				"family": "Schramowski",
				"given": "Patrick"
			},
			{
				"family": "Tauchmann",
				"given": "Christopher"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goantaCaseLegalCompliance2022",
		"type": "paper-conference",
		"abstract": "In the course of under a year, the European Commission has launched some of the most important regulatory proposals to date on platform governance. The Commission’s goals behind cross-sectoral regulation of this sort include the protection of markets and democracies alike. While all these acts propose sophisticated rules for setting up new enforcement institutions and procedures, one aspect remains highly unclear: how digital enforcement will actually take place in practice. Focusing on the Digital Services Act (DSA), this discussion paper critically addresses issues around social media data access for the purpose of digital enforcement and proposes the use of a legal compliance application programming interface (API) as a means to facilitate compliance with the DSA and complementary European and national regulation. To contextualize this discussion, the paper pursues two scenarios that exemplify the harms arising out of content monetization affecting a particularly vulnerable category of social media users: children. The two scenarios are used to further reflect upon essential issues surrounding data access and legal compliance with the DSA and further applicable legal standards in the field of labour and consumer law.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533190",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "1341–1349",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The case for a legal compliance API for the enforcement of the EU’s digital services act on social media platforms",
		"URL": "https://doi.org/10.1145/3531146.3533190",
		"author": [
			{
				"family": "Goanta",
				"given": "Catalina"
			},
			{
				"family": "Bertaglia",
				"given": "Thales"
			},
			{
				"family": "Iamnitchi",
				"given": "Adriana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shangWhyAmNot2022",
		"type": "paper-conference",
		"abstract": "Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533189",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1330–1340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why am I not seeing it? Understanding users’ needs for counterfactual explanations in everyday recommendations",
		"URL": "https://doi.org/10.1145/3531146.3533189",
		"author": [
			{
				"family": "Shang",
				"given": "Ruoxi"
			},
			{
				"family": "Feng",
				"given": "K. J. Kevin"
			},
			{
				"family": "Shah",
				"given": "Chirag"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangDualCFEfficientModel2022",
		"type": "paper-conference",
		"abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533188",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1318–1329",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "DualCF: Efficient model extraction attack from counterfactual explanations",
		"URL": "https://doi.org/10.1145/3531146.3533188",
		"author": [
			{
				"family": "Wang",
				"given": "Yongjie"
			},
			{
				"family": "Qian",
				"given": "Hangwei"
			},
			{
				"family": "Miao",
				"given": "Chunyan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ehsanAlgorithmicImprint2022",
		"type": "paper-conference",
		"abstract": "When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533186",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1305–1317",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The algorithmic imprint",
		"URL": "https://doi.org/10.1145/3531146.3533186",
		"author": [
			{
				"family": "Ehsan",
				"given": "Upol"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Riedl",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeEvidenceHypodescentVisual2022",
		"type": "paper-conference",
		"abstract": "We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p &lt; 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p &lt; 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p &lt; 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533185",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1293–1304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evidence for hypodescent in visual semantic AI",
		"URL": "https://doi.org/10.1145/3531146.3533185",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Banaji",
				"given": "Mahzarin R."
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hirotaGenderRacialBias2022",
		"type": "paper-conference",
		"abstract": "Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533184",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1280–1292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender and racial bias in visual question answering datasets",
		"URL": "https://doi.org/10.1145/3531146.3533184",
		"author": [
			{
				"family": "Hirota",
				"given": "Yusuke"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeMarkednessVisualSemantic2022",
		"type": "paper-conference",
		"abstract": "We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533183",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1269–1279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Markedness in visual semantic AI",
		"URL": "https://doi.org/10.1145/3531146.3533183",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liaoDesigningResponsibleTrust2022",
		"type": "paper-conference",
		"abstract": "Current literature and public discourse on “trust in AI” are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments. Besides AI-generated content, we highlight transparency and interaction as AI systems’ affordances that present a wide range of trustworthiness cues to users. By bringing to light the variety of users’ cognitive processes to make trust judgments and their potential limitations, we urge technology creators to make conscious decisions in choosing reliable trustworthiness cues for target users and, as an industry, to regulate this space and prevent malicious use. Towards these goals, we define the concepts of warranted trustworthiness cues and expensive trustworthiness cues, and propose a checklist of requirements to help technology creators identify appropriate cues to use. We present a hypothetical use case to illustrate how practitioners can use MATCH to design AI systems responsibly, and discuss future directions for research and industry efforts aimed at promoting responsible trust in AI.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533182",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1257–1268",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing for responsible trust in AI systems: A communication perspective",
		"URL": "https://doi.org/10.1145/3531146.3533182",
		"author": [
			{
				"family": "Liao",
				"given": "Q.Vera"
			},
			{
				"family": "Sundar",
				"given": "S. Shyam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rahmattalabiLearningResourceAllocation2022",
		"type": "paper-conference",
		"abstract": "We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533181",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "1240–1256",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning resource allocation policies from observational data with an application to homeless services delivery",
		"URL": "https://doi.org/10.1145/3531146.3533181",
		"author": [
			{
				"family": "Rahmattalabi",
				"given": "Aida"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Dullerud",
				"given": "Kathryn"
			},
			{
				"family": "Rice",
				"given": "Eric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dianaMultiaccurateProxiesDownstream2022",
		"type": "paper-conference",
		"abstract": "We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time — in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner — with minimal assumptions on their prediction task — to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533180",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 33\npublisher-place: Seoul, Republic of Korea",
		"page": "1207–1239",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multiaccurate proxies for downstream fairness",
		"URL": "https://doi.org/10.1145/3531146.3533180",
		"author": [
			{
				"family": "Diana",
				"given": "Emily"
			},
			{
				"family": "Gill",
				"given": "Wesley"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Sharifi-Malvajerdi",
				"given": "Saeed"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "balagopalanRoadExplainabilityPaved2022",
		"type": "paper-conference",
		"abstract": "Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533179",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1194–1206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The road to explainability is paved with bias: Measuring the fairness of explanations",
		"URL": "https://doi.org/10.1145/3531146.3533179",
		"author": [
			{
				"family": "Balagopalan",
				"given": "Aparna"
			},
			{
				"family": "Zhang",
				"given": "Haoran"
			},
			{
				"family": "Hamidieh",
				"given": "Kimia"
			},
			{
				"family": "Hartvigsen",
				"given": "Thomas"
			},
			{
				"family": "Rudzicz",
				"given": "Frank"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blumMultiStageScreening2022",
		"type": "paper-conference",
		"abstract": "Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533178",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "1178–1193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi stage screening: Enforcing fairness and maximizing efficiency in a pre-existing pipeline",
		"URL": "https://doi.org/10.1145/3531146.3533178",
		"author": [
			{
				"family": "Blum",
				"given": "Avrim"
			},
			{
				"family": "Stangl",
				"given": "Kevin"
			},
			{
				"family": "Vakilian",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "stapletonImaginingNewFutures2022",
		"type": "paper-conference",
		"abstract": "Child welfare agencies across the United States are turning to data-driven predictive technologies (commonly called predictive analytics) which use government administrative data to assist workers’ decision-making. While some prior work has explored impacted stakeholders’ concerns with current uses of data-driven predictive risk models (PRMs), less work has asked stakeholders whether such tools ought to be used in the first place. In this work, we conducted a set of seven design workshops with 35 stakeholders who have been impacted by the child welfare system or who work in it to understand their beliefs and concerns around PRMs, and to engage them in imagining new uses of data and technologies in the child welfare system. We found that participants worried current PRMs perpetuate or exacerbate existing problems in child welfare. Participants suggested new ways to use data and data-driven tools to better support impacted communities and suggested paths to mitigate possible harms of these tools. Participants also suggested low-tech or no-tech alternatives to PRMs to address problems in child welfare. Our study sheds light on how researchers and designers can work in solidarity with impacted communities, possibly to circumvent or oppose child welfare agencies.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533177",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "1162–1177",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders",
		"URL": "https://doi.org/10.1145/3531146.3533177",
		"author": [
			{
				"family": "Stapleton",
				"given": "Logan"
			},
			{
				"family": "Lee",
				"given": "Min Hun"
			},
			{
				"family": "Qing",
				"given": "Diana"
			},
			{
				"family": "Wright",
				"given": "Marya"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Holstein",
				"given": "Ken"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "botesBrainComputerInterfaces2022",
		"type": "paper-conference",
		"abstract": "Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533176",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "1154–1161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Brain computer interfaces and human rights: Brave new rights for a brave new world",
		"URL": "https://doi.org/10.1145/3531146.3533176",
		"author": [
			{
				"family": "Botes",
				"given": "Marietjie Wilhelmina Maria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lucchesiSmallsetTimelinesVisual2022",
		"type": "paper-conference",
		"abstract": "Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533175",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1136–1153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Smallset timelines: A visual representation of data preprocessing decisions",
		"URL": "https://doi.org/10.1145/3531146.3533175",
		"author": [
			{
				"family": "Lucchesi",
				"given": "Lydia R."
			},
			{
				"family": "Kuhnert",
				"given": "Petra M."
			},
			{
				"family": "Davis",
				"given": "Jenny L."
			},
			{
				"family": "Xie",
				"given": "Lexing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "linAuditingGerrymanderingIdentifying2022",
		"type": "paper-conference",
		"abstract": "Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533174",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1125–1135",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing for gerrymandering by identifying disenfranchised individuals",
		"URL": "https://doi.org/10.1145/3531146.3533174",
		"author": [
			{
				"family": "Lin",
				"given": "Jerry"
			},
			{
				"family": "Chen",
				"given": "Carolyn"
			},
			{
				"family": "Chmielewski",
				"given": "Marc"
			},
			{
				"family": "Zaman",
				"given": "Samia"
			},
			{
				"family": "Fain",
				"given": "Brandon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "globus-harrisAlgorithmicFrameworkBias2022",
		"type": "paper-conference",
		"abstract": "We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533172",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1106–1124",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An algorithmic framework for bias bounties",
		"URL": "https://doi.org/10.1145/3531146.3533172",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mashiatTradeoffsGroupFairness2022",
		"type": "paper-conference",
		"abstract": "We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533171",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1095–1105",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trade-offs between group fairness metrics in societal resource allocation",
		"URL": "https://doi.org/10.1145/3531146.3533171",
		"author": [
			{
				"family": "Mashiat",
				"given": "Tasfia"
			},
			{
				"family": "Gitiaux",
				"given": "Xavier"
			},
			{
				"family": "Rangwala",
				"given": "Huzefa"
			},
			{
				"family": "Fowler",
				"given": "Patrick"
			},
			{
				"family": "Das",
				"given": "Sanmay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "watsonRationalShapleyValues2022",
		"type": "paper-conference",
		"abstract": "Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce rational Shapley values, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533170",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1083–1094",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rational shapley values",
		"URL": "https://doi.org/10.1145/3531146.3533170",
		"author": [
			{
				"family": "Watson",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "buylTacklingAlgorithmicDisability2022",
		"type": "paper-conference",
		"abstract": "Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533169",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1071–1082",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tackling algorithmic disability discrimination in the hiring process: An ethical, legal and technical analysis",
		"URL": "https://doi.org/10.1145/3531146.3533169",
		"author": [
			{
				"family": "Buyl",
				"given": "Maarten"
			},
			{
				"family": "Cociancig",
				"given": "Christina"
			},
			{
				"family": "Frattone",
				"given": "Cristina"
			},
			{
				"family": "Roekens",
				"given": "Nele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "albiniCounterfactualShapleyAdditive2022",
		"type": "paper-conference",
		"abstract": "Feature attributions are a common paradigm for model explanations due to their simplicity in assigning a single numeric score for each input feature to a model. In the actionable recourse setting, wherein the goal of the explanations is to improve outcomes for model consumers, it is often unclear how feature attributions should be correctly used. With this work, we aim to strengthen and clarify the link between actionable recourse and feature attributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP (CF-SHAP), that incorporates counterfactual information to produce a background dataset for use within the marginal (a.k.a. interventional) Shapley value framework. We motivate the need within the actionable recourse setting for careful consideration of background datasets when using Shapley values for feature attributions with numerous synthetic examples. Moreover, we demonstrate the efficacy of CF-SHAP by proposing and justifying a quantitative score for feature attributions, counterfactual-ability, showing that as measured by this metric, CF-SHAP is superior to existing methods when evaluated on public datasets using tree ensembles.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533168",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "1054–1070",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual shapley additive explanations",
		"URL": "https://doi.org/10.1145/3531146.3533168",
		"author": [
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Long",
				"given": "Jason"
			},
			{
				"family": "Dervovic",
				"given": "Danial"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mishlerFADEFAirDouble2022",
		"type": "paper-conference",
		"abstract": "Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533167",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "1053",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FADE: FAir double ensemble learning for observable and counterfactual outcomes",
		"URL": "https://doi.org/10.1145/3531146.3533167",
		"author": [
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pfohlNetBenefitCalibration2022",
		"type": "paper-conference",
		"abstract": "A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533166",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1039–1052",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare",
		"URL": "https://doi.org/10.1145/3531146.3533166",
		"author": [
			{
				"family": "Pfohl",
				"given": "Stephen"
			},
			{
				"family": "Xu",
				"given": "Yizhe"
			},
			{
				"family": "Foryciarz",
				"given": "Agata"
			},
			{
				"family": "Ignatiadis",
				"given": "Nikolaos"
			},
			{
				"family": "Genkins",
				"given": "Julian"
			},
			{
				"family": "Shah",
				"given": "Nigam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "duDatadrivenSimulationNew2022",
		"type": "paper-conference",
		"abstract": "We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system’s ability to achieve it’s stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention— a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533165",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1028–1038",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A data-driven simulation of the new york state foster care system",
		"URL": "https://doi.org/10.1145/3531146.3533165",
		"author": [
			{
				"family": "Du",
				"given": "Yuhao"
			},
			{
				"family": "Ionescu",
				"given": "Stefania"
			},
			{
				"family": "Sage",
				"given": "Melanie"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yewRegulatingFacialProcessing2022",
		"type": "paper-conference",
		"abstract": "Harms resulting from the development and deployment of facial processing technologies (FPT) have been met with increasing controversy. Several states and cities in the U.S. have banned the use of facial recognition by law enforcement and governments, but FPT are still being developed and used in a wide variety of contexts where they primarily are regulated by state biometric information privacy laws. Among these laws, the 2008 Illinois Biometric Information Privacy Act (BIPA) has generated a significant amount of litigation. Yet, with most BIPA lawsuits reaching settlements before there have been meaningful clarifications of relevant technical intricacies and legal definitions, there remains a great degree of uncertainty as to how exactly this law applies to FPT. What we have found through applications of BIPA in FPT litigation so far, however, points to potential disconnects between technical and legal communities. This paper analyzes what we know based on BIPA court proceedings and highlights these points of tension: areas where the technical operationalization of BIPA may create unintended and undesirable incentives for FPT development, as well as areas where BIPA litigation can bring to light the limitations of solely technical methods in achieving legal privacy values. These factors are relevant for (i) reasoning about biometric information privacy laws as a governing mechanism for FPT, (ii) assessing the potential harms of FPT, and (iii) providing incentives for the mitigation of these harms. By illuminating these considerations, we hope to empower courts and lawmakers to take a more nuanced approach to regulating FPT and developers to better understand privacy values in the current U.S. legal landscape.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533163",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1017–1027",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating facial processing technologies: Tensions between legal and technical considerations in the application of illinois BIPA",
		"URL": "https://doi.org/10.1145/3531146.3533163",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mokanderModelsClassifyingAI2022",
		"type": "paper-conference",
		"abstract": "Organisations that design and deploy systems based on artificial intelligence (AI) increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. A major obstacle to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the practical purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533162",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "1016",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Models for classifying AI systems: the switch, the ladder, and the matrix",
		"URL": "https://doi.org/10.1145/3531146.3533162",
		"author": [
			{
				"family": "Mökander",
				"given": "Jakob"
			},
			{
				"family": "Sheth",
				"given": "Margi"
			},
			{
				"family": "Watson",
				"given": "David"
			},
			{
				"family": "Floridi",
				"given": "Luciano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "luSubvertingMachinesFluctuating2022",
		"type": "paper-conference",
		"abstract": "Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533161",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1005–1015",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Subverting machines, fluctuating identities: Re-learning human categorization",
		"URL": "https://doi.org/10.1145/3531146.3533161",
		"author": [
			{
				"family": "Lu",
				"given": "Christina"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "McKee",
				"given": "Kevin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "soFairnessReparativeAlgorithms2022",
		"type": "paper-conference",
		"abstract": "Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533160",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "988–1004",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond fairness: Reparative algorithms to address historical injustices of housing discrimination in the US",
		"URL": "https://doi.org/10.1145/3531146.3533160",
		"author": [
			{
				"family": "So",
				"given": "Wonyoung"
			},
			{
				"family": "Lohia",
				"given": "Pranay"
			},
			{
				"family": "Pimplikar",
				"given": "Rakesh"
			},
			{
				"family": "Hosoi",
				"given": "A.E."
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pahlFemaleWhite272022",
		"type": "paper-conference",
		"abstract": "Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533159",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "973–987",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Female, white, 27? Bias evaluation on data and algorithms for affect recognition in faces",
		"URL": "https://doi.org/10.1145/3531146.3533159",
		"author": [
			{
				"family": "Pahl",
				"given": "Jaspar"
			},
			{
				"family": "Rieger",
				"given": "Ines"
			},
			{
				"family": "Möller",
				"given": "Anna"
			},
			{
				"family": "Wittenberg",
				"given": "Thomas"
			},
			{
				"family": "Schmid",
				"given": "Ute"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rajiFallacyAIFunctionality2022",
		"type": "paper-conference",
		"abstract": "Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, despite this reality, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on “ethical” or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we analyze a set of case studies to create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy challenge, operating as a necessary first step towards protecting affected communities from algorithmic harm.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533158",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "959–972",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The fallacy of AI functionality",
		"URL": "https://doi.org/10.1145/3531146.3533158",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Selbst",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "birhaneForgottenMarginsAI2022",
		"type": "paper-conference",
		"abstract": "How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "948–958",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The forgotten margins of AI ethics",
		"URL": "https://doi.org/10.1145/3531146.3533157",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Ruane",
				"given": "Elayne"
			},
			{
				"family": "Laurent",
				"given": "Thomas"
			},
			{
				"family": "S. Brown",
				"given": "Matthew"
			},
			{
				"family": "Flowers",
				"given": "Johnathan"
			},
			{
				"family": "Ventresque",
				"given": "Anthony"
			},
			{
				"family": "L. Dancy",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sloaneGermanAIStartups2022",
		"type": "paper-conference",
		"abstract": "The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "935–947",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "German AI start-ups and “AI ethics”: Using a social practice lens for assessing and implementing socio-technical innovation",
		"URL": "https://doi.org/10.1145/3531146.3533156",
		"author": [
			{
				"family": "Sloane",
				"given": "Mona"
			},
			{
				"family": "Zakrzewski",
				"given": "Janina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hongPredictionExtractionDiscretion2022",
		"type": "paper-conference",
		"abstract": "I argue that data-driven predictions work primarily as instruments for systematic extraction of discretionary power – the practical capacity to make everyday decisions and define one's situation. This extractive relation reprises a long historical pattern, in which new methods of producing knowledge generate a redistribution of epistemic power: who declares what kind of truth about me, to count for what kinds of decisions? I argue that prediction as extraction of discretion is normal and fundamental to the technology, rather than isolated cases of bias or error. Synthesising critical observations across anthropology, history of technology and critical data studies, the paper demonstrates this dynamic in two contemporary domains: (1) crime and policing demonstrates how predictive systems are extractive by design. Rather than neutral models led astray by garbage data, pre-existing interests thoroughly shape how prediction conceives of its object, its measures, and most importantly, what it does not measure and in doing so devalues. (2) I then examine the prediction of productivity in the long tradition of extracting discretion as a means to extract labour power. Making human behaviour more predictable for the client of prediction (the manager, the corporation, the police officer) often means making life and work more unpredictable for the target of prediction (the employee, the applicant, the citizen).",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "925–934",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Prediction as extraction of discretion",
		"URL": "https://doi.org/10.1145/3531146.3533155",
		"author": [
			{
				"family": "Hong",
				"given": "Sun-ha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chienMultidisciplinaryFairnessConsiderations2022",
		"type": "paper-conference",
		"abstract": "While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "906–924",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-disciplinary fairness considerations in machine learning for clinical trials",
		"URL": "https://doi.org/10.1145/3531146.3533154",
		"author": [
			{
				"family": "Chien",
				"given": "Isabel"
			},
			{
				"family": "Deliu",
				"given": "Nina"
			},
			{
				"family": "Turner",
				"given": "Richard"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Villar",
				"given": "Sofia"
			},
			{
				"family": "Kilbertus",
				"given": "Niki"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bordtPosthocExplanationsFail2022",
		"type": "paper-conference",
		"abstract": "Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to “explain”. Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law’s objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying “explainability” obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union’s draft Artificial Intelligence Act.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "891–905",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Post-hoc explanations fail to achieve their purpose in adversarial contexts",
		"URL": "https://doi.org/10.1145/3531146.3533153",
		"author": [
			{
				"family": "Bordt",
				"given": "Sebastian"
			},
			{
				"family": "Finck",
				"given": "Michèle"
			},
			{
				"family": "Raidl",
				"given": "Eric"
			},
			{
				"family": "Luxburg",
				"given": "Ulrike",
				"non-dropping-particle": "von"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mclaughlinFairnessMachineassistedHuman2022",
		"type": "paper-conference",
		"abstract": "When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "890",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the fairness of machine-assisted human decisions",
		"URL": "https://doi.org/10.1145/3531146.3533152",
		"author": [
			{
				"family": "McLaughlin",
				"given": "Bryce"
			},
			{
				"family": "Spiess",
				"given": "Jann"
			},
			{
				"family": "Gillis",
				"given": "Talia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fischerPromotingEthicalAwareness2022",
		"type": "paper-conference",
		"abstract": "Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533151",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "877–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Promoting ethical awareness in communication analysis: Investigating potentials and limits of visual analytics for intelligence applications",
		"URL": "https://doi.org/10.1145/3531146.3533151",
		"author": [
			{
				"family": "Fischer",
				"given": "Maximilian T."
			},
			{
				"family": "Hirsbrunner",
				"given": "Simon David"
			},
			{
				"family": "Jentner",
				"given": "Wolfgang"
			},
			{
				"family": "Miller",
				"given": "Matthias"
			},
			{
				"family": "Keim",
				"given": "Daniel A."
			},
			{
				"family": "Helm",
				"given": "Paula"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cooperAccountabilityAlgorithmicSociety2022a",
		"type": "paper-conference",
		"abstract": "In 1996, Accountability in a Computerized Society&nbsp;[95] issued a clarion call concerning the erosion of accountability in society due to the ubiquitous delegation of consequential functions to computerized systems. Nissenbaum [95] described four barriers to accountability that computerization presented, which we revisit in relation to the ascendance of data-driven algorithmic systems—i.e., machine learning or artificial intelligence—to uncover new challenges for accountability that these systems present. Nissenbaum’s original paper grounded discussion of the barriers in moral philosophy; we bring this analysis together with recent scholarship on relational accountability frameworks and discuss how the barriers present difficulties for instantiating a unified moral, relational framework in practice for data-driven algorithmic systems. We conclude by discussing ways of weakening the barriers in order to do so.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533150",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "864–876",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accountability in an algorithmic society: Relationality, responsibility, and robustness in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533150",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blackModelMultiplicityOpportunities2022",
		"type": "paper-conference",
		"abstract": "Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533149",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "850–863",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model multiplicity: Opportunities, concerns, and solutions",
		"URL": "https://doi.org/10.1145/3531146.3533149",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shanmugamLearningLimitData2022",
		"type": "paper-conference",
		"abstract": "Modern machine learning systems are increasingly characterized by extensive personal data collection, despite the diminishing returns and increasing societal costs of such practices. Yet, data minimisation is one of the core data protection principles enshrined in the European Union’s General Data Protection Regulation (’GDPR’) and requires that only personal data that is adequate, relevant and limited to what is necessary is processed. However, the principle has seen limited adoption due to the lack of technical interpretation. In this work, we build on literature in machine learning and law to propose FIDO, a Framework for Inhibiting Data Overcollection. FIDO learns to limit data collection based on an interpretation of data minimization tied to system performance. Concretely, FIDO provides a data collection stopping criterion by iteratively updating an estimate of the performance curve, or the relationship between dataset size and performance, as data is acquired. FIDO estimates the performance curve via a piecewise power law technique that models distinct phases of an algorithm’s performance throughout data collection separately. Empirical experiments show that the framework produces accurate performance curves and data collection stopping criteria across datasets and feature acquisition algorithms. We further demonstrate that many other families of curves systematically overestimate the return on additional data. Results and analysis from our investigation offer deeper insights into the relevant considerations when designing a data minimization framework, including the impacts of active feature acquisition on individual users and the feasability of user-specific data minimization. We conclude with practical recommendations for the implementation of data minimization.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533148",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "839–849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to limit data collection via scaling laws: A computational interpretation for the legal principle of data minimization",
		"URL": "https://doi.org/10.1145/3531146.3533148",
		"author": [
			{
				"family": "Shanmugam",
				"given": "Divya"
			},
			{
				"family": "Diaz",
				"given": "Fernando"
			},
			{
				"family": "Shabanian",
				"given": "Samira"
			},
			{
				"family": "Finck",
				"given": "Michele"
			},
			{
				"family": "Biega",
				"given": "Asia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangHowAreMLBased2022",
		"type": "paper-conference",
		"abstract": "Machine learning-based predictive systems are increasingly used to assist online groups and communities in various content moderation tasks. However, there are limited quantitative understandings of whether and how different groups and communities use such predictive systems differently according to their community characteristics. In this research, we conducted a field evaluation of how content moderation systems are used in 17 Wikipedia language communities. We found that 1) larger communities tend to use predictive systems to identify the most damaging edits, while smaller communities tend to use them to identify any edit that could be damaging; 2) predictive systems are used less in content areas where there are more local editing activities; 3) predictive systems have mixed effects on reducing disparate treatment between anonymous and registered editors across communities of different characteristics. Finally, we discuss the theoretical and practical implications for future human-centered moderation algorithms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533147",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "824–838",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How are ML-Based online content moderation systems actually used? Studying community size, local activity, and disparate treatment",
		"URL": "https://doi.org/10.1145/3531146.3533147",
		"author": [
			{
				"family": "Wang",
				"given": "Leijie"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiFairRepresentationClustering2022",
		"type": "paper-conference",
		"abstract": "We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ∈ X belongs to one of ℓ groups. Further, we are given fair representation parameters αj and βj for each group j ∈ [ℓ]. We say that a k-clustering C1, ⋅⋅⋅, Ck fairly represents all groups if the number of points from group j in cluster Ci is between αj|Ci| and βj|Ci| for every j ∈ [ℓ] and i ∈ [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ℓ1-objective ∑x ∈ Xd(x, ϕ(x)). We present an O(log k)-approximation algorithm that runs in time nO(ℓ). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ℓ. We also consider an important special case of the problem where and for all j ∈ [ℓ]. For this special case, we present an O(log k)-approximation algorithm that runs in time.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533146",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "814–823",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair representation clustering with several protected classes",
		"URL": "https://doi.org/10.1145/3531146.3533146",
		"author": [
			{
				"family": "Dai",
				"given": "Zhen"
			},
			{
				"family": "Makarychev",
				"given": "Yury"
			},
			{
				"family": "Vakilian",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zamfirescu-pereiraTrucksDontMean2022",
		"type": "paper-conference",
		"abstract": "Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533145",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "799–813",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trucks don’t mean trump: Diagnosing human error in image analysis",
		"URL": "https://doi.org/10.1145/3531146.3533145",
		"author": [
			{
				"family": "Zamfirescu-Pereira",
				"given": "J.D."
			},
			{
				"family": "Chen",
				"given": "Jerry"
			},
			{
				"family": "Wen",
				"given": "Emily"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "harrisExploringRoleGrammar2022",
		"type": "paper-conference",
		"abstract": "Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533144",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "789–798",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring the role of grammar and word choice in bias toward african american english (AAE) in hate speech classification",
		"URL": "https://doi.org/10.1145/3531146.3533144",
		"author": [
			{
				"family": "Harris",
				"given": "Camille"
			},
			{
				"family": "Halevy",
				"given": "Matan"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			},
			{
				"family": "Bruckman",
				"given": "Amy"
			},
			{
				"family": "Yang",
				"given": "Diyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "contractorBehavioralUseLicensing2022",
		"type": "paper-conference",
		"abstract": "With the growing reliance on artificial intelligence (AI) for many different applications, the sharing of code, data, and models is important to ensure the replicability and democratization of scientific knowledge. Many high-profile academic publishing venues expect code and models to be submitted and released with papers. Furthermore, developers often want to release these assets to encourage development of technology that leverages their frameworks and services. A number of organizations have expressed concerns about the inappropriate or irresponsible use of AI and have proposed ethical guidelines around the application of such systems. While such guidelines can help set norms and shape policy, they are not easily enforceable. In this paper, we advocate the use of licensing to enable legally enforceable behavioral use conditions on software and code and provide several case studies that demonstrate the feasibility of behavioral use licensing. We envision how licensing may be implemented in accordance with existing responsible AI guidelines.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533143",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "778–788",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Behavioral use licensing for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533143",
		"author": [
			{
				"family": "Contractor",
				"given": "Danish"
			},
			{
				"family": "McDuff",
				"given": "Daniel"
			},
			{
				"family": "Haines",
				"given": "Julia Katherine"
			},
			{
				"family": "Lee",
				"given": "Jenny"
			},
			{
				"family": "Hines",
				"given": "Christopher"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			},
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Li",
				"given": "Hanlin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "riekeImperfectInferencesPractical2022",
		"type": "paper-conference",
		"abstract": "Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533140",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "767–777",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Imperfect inferences: A practical assessment",
		"URL": "https://doi.org/10.1145/3531146.3533140",
		"author": [
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Southerland",
				"given": "Vincent"
			},
			{
				"family": "Svirsky",
				"given": "Dan"
			},
			{
				"family": "Hsu",
				"given": "Mingwei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangAttributePrivacyFramework2022",
		"type": "paper-conference",
		"abstract": "Ensuring the privacy of training data is a growing concern since many machine learning models are trained on confidential and potentially sensitive data. Much attention has been devoted to methods for protecting individual privacy during analyses of large datasets. However in many settings, global properties of the dataset may also be sensitive (e.g., mortality rate in a hospital rather than presence of a particular patient in the dataset). In this work, we depart from individual privacy to initiate the study of attribute privacy, where a data owner is concerned about revealing sensitive properties of a whole dataset during analysis. We propose definitions to capture attribute privacy in two relevant cases where global attributes may need to be protected: (1) properties of a specific dataset and (2) parameters of the underlying distribution from which dataset is sampled. We also provide two efficient mechanisms for specific data distributions and one general but inefficient mechanism that satisfy attribute privacy for these settings. We base our results on a novel and non-trivial use of the Pufferfish framework to account for correlations across attributes in the data, thus addressing “the challenging problem of developing Pufferfish instantiations and algorithms for general aggregate secrets” that was left open by Kifer and Machanavajjhala in 2014 [15].",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533139",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "757–766",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Attribute privacy: Framework and mechanisms",
		"URL": "https://doi.org/10.1145/3531146.3533139",
		"author": [
			{
				"family": "Zhang",
				"given": "Wanrong"
			},
			{
				"family": "Ohrimenko",
				"given": "Olga"
			},
			{
				"family": "Cummings",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hundtRobotsEnactMalignant2022",
		"type": "paper-conference",
		"abstract": "Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)&nbsp;[18, 80], Natural Language Processing (NLP)&nbsp;[6], or both, in the case of large image and caption models such as OpenAI CLIP&nbsp;[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533138",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "743–756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robots enact malignant stereotypes",
		"URL": "https://doi.org/10.1145/3531146.3533138",
		"author": [
			{
				"family": "Hundt",
				"given": "Andrew"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Zeng",
				"given": "Vicky"
			},
			{
				"family": "Kacianka",
				"given": "Severin"
			},
			{
				"family": "Gombolay",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cooperMakingUnaccountableInternet2022",
		"type": "paper-conference",
		"abstract": "Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533137",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "726–742",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making the unaccountable internet: The changing meaning of accounting in the early ARPANET",
		"URL": "https://doi.org/10.1145/3531146.3533137",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Vidan",
				"given": "Gili"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "nandyAchievingFairnessPostprocessing2022",
		"type": "paper-conference",
		"abstract": "Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533136",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "715–725",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving fairness via post-processing in web-scale recommender Systems✱",
		"URL": "https://doi.org/10.1145/3531146.3533136",
		"author": [
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Venugopalan",
				"given": "Divya"
			},
			{
				"family": "Logan",
				"given": "Heloise"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			},
			{
				"family": "El Karoui",
				"given": "Noureddine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kaurSensibleAIReimagining2022",
		"type": "paper-conference",
		"abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact—an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick’s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders’ needs—we build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533135",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "702–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sensible AI: Re-imagining interpretability and explainability using sensemaking theory",
		"URL": "https://doi.org/10.1145/3531146.3533135",
		"author": [
			{
				"family": "Kaur",
				"given": "Harmanpreet"
			},
			{
				"family": "Adar",
				"given": "Eytan"
			},
			{
				"family": "Gilbert",
				"given": "Eric"
			},
			{
				"family": "Lampe",
				"given": "Cliff"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "geddesDeathLegalSubject2022",
		"type": "paper-conference",
		"abstract": "This paper explores the epistemological differences between the socio-political legal subject of Western liberalism, and the algorithmic subject of informational capitalism. It argues that the increasing use of predictive algorithms in judicial decision-making is reconstructing both the nature and experience of legal subjectivity in a manner that is incompatible with law's normative commitments to individualized justice. Whereas algorithmic subjectivity derives its epistemic authority from population-level insights, legal subjectivity has historically derived credibility from its close approximation of the underlying individual, through careful evaluation of their mental and physical autonomy, prior to any assignment of legal liability. With the introduction of predictive algorithms in judicial decision-making, knowledge about the legal subject is increasingly algorithmically produced, in a manner that discounts, and effectively displaces, qualitative knowledge about the legal subject's intentions, motivations, and moral capabilities. This results in the death of the legal subject, or the emergence of new, algorithmic practices of signification that no longer require the input of the underlying individual. As algorithms increasingly guide judicial decision-making, the shifting epistemology of legal subjectivity has long-term consequences for the legitimacy of legal institutions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533134",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "691–701",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The death of the legal subject: How predictive algorithms are (re)constructing legal subjectivity",
		"URL": "https://doi.org/10.1145/3531146.3533134",
		"author": [
			{
				"family": "Geddes",
				"given": "Katrina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "norvalDisclosureDesignDesigning2022",
		"type": "paper-conference",
		"abstract": "There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims. This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533133",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "679–690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability",
		"URL": "https://doi.org/10.1145/3531146.3533133",
		"author": [
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cornelius",
				"given": "Kristin"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sureshIntersectionalFeministParticipatory2022",
		"type": "paper-conference",
		"abstract": "Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide&nbsp;—&nbsp;gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process&nbsp;—&nbsp;with quantitative, qualitative and participatory steps&nbsp;—&nbsp;focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533132",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "667–678",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards intersectional feminist and participatory ML: A case study in supporting feminicide counterdata collection",
		"URL": "https://doi.org/10.1145/3531146.3533132",
		"author": [
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Movva",
				"given": "Rajiv"
			},
			{
				"family": "Dogan",
				"given": "Amelia Lee"
			},
			{
				"family": "Bhargava",
				"given": "Rahul"
			},
			{
				"family": "Cruxen",
				"given": "Isadora"
			},
			{
				"family": "Cuba",
				"given": "Angeles Martinez"
			},
			{
				"family": "Taurino",
				"given": "Guilia"
			},
			{
				"family": "So",
				"given": "Wonyoung"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "salemDontLetRicci2022",
		"type": "paper-conference",
		"abstract": "Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action. In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533129",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "651–666",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Don’t let ricci v. DeStefano hold you back: A bias-aware legal solution to the hiring paradox",
		"URL": "https://doi.org/10.1145/3531146.3533129",
		"author": [
			{
				"family": "Salem",
				"given": "Jad"
			},
			{
				"family": "Desai",
				"given": "Deven"
			},
			{
				"family": "Gupta",
				"given": "Swati"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghoshSubvertingFairImage2022",
		"type": "paper-conference",
		"abstract": "In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model&nbsp;[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533128",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "637–650",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Subverting fair image search with generative adversarial perturbations",
		"URL": "https://doi.org/10.1145/3531146.3533128",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Jagielski",
				"given": "Matthew"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schuffHumanInterpretationSaliencybased2022",
		"type": "paper-conference",
		"abstract": "While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople’s interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees’ importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533127",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 26\npublisher-place: Seoul, Republic of Korea",
		"page": "611–636",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human interpretation of saliency-based explanation over text",
		"URL": "https://doi.org/10.1145/3531146.3533127",
		"author": [
			{
				"family": "Schuff",
				"given": "Hendrik"
			},
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Adel",
				"given": "Heike"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Vu",
				"given": "Ngoc Thang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fongFairnessAUCFeature2022",
		"type": "paper-conference",
		"abstract": "We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type&nbsp;I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533126",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "610",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness for AUC via feature augmentation",
		"URL": "https://doi.org/10.1145/3531146.3533126",
		"author": [
			{
				"family": "Fong",
				"given": "Hortense"
			},
			{
				"family": "Kumar",
				"given": "Vineet"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mehrotraSelectionPresenceImplicit2022",
		"type": "paper-conference",
		"abstract": "In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533124",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "599–609",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Selection in the presence of implicit bias: The advantage of intersectional constraints",
		"URL": "https://doi.org/10.1145/3531146.3533124",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Pradelski",
				"given": "Bary S. R."
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "doniaNormativeLogicsAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "The relevance of algorithms in contemporary life is often appreciated when they ‘fail’—either because they did not perform as expected, or because they led to outcomes that were later determined to be unacceptable. As a result, academic, policy, and public discourse has increasingly emphasized accountability as a desirable, if not elusive, feature of system design, and component of effective governance. Accountability, however, is a versatile concept that has been operationalized in a number of ways across different use-contexts, policy settings, and research disciplines. While accountability is often framed as a normative good, it is unclear exactly what kind of normative work it is expected to do, and how it is expected to do it. Informed by perspectives from critical data studies and science and technology studies, this article introduces five normative logics underpinning discussions of algorithmic accountability that appear in the academic research literature: (1) accountability as verification, (2) accountability as representation, (3) accountability as social licence, (4) accountability as fiduciary duty, and (5) accountability as legal compliance. These normative logics, and the resulting rules, codes, and practices that constitute an emerging set of algorithmic accountability regimes, are especially discussed in terms of the presumed agency of actors involved. The article suggests that implicit assumptions characterizing each of ‘algorithms’ and ‘accountability’ are highly significant for each other, and that more explicit acknowledgement of this can lead to improved understanding of the diverse knowledge claims and practical goals associated with different logics of algorithmic accountability, and relatedly, the agency of different actors to pursue it in its different forms. Link to full text: josephdonia.com/facct-2022",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533123",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Normative logics of algorithmic accountability",
		"URL": "https://doi.org/10.1145/3531146.3533123",
		"author": [
			{
				"family": "Donia",
				"given": "Joseph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "smithREALMLRecognizing2022",
		"type": "paper-conference",
		"abstract": "Transparency around limitations can improve the scientific rigor of research, help ensure appropriate interpretation of research findings, and make research claims more credible. Despite these benefits, the machine learning (ML) research community lacks well-developed norms around disclosing and discussing limitations. To address this gap, we conduct an iterative design process with 30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided activities to help ML researchers recognize, explore, and articulate the limitations of their research. Using a three-stage interview and survey study, we identify ML researchers’ perceptions of limitations, as well as the challenges they face when recognizing, exploring, and articulating limitations. We develop REAL ML to address some of these practical challenges, and highlight additional cultural challenges that will require broader shifts in community norms to address. We hope our study and REAL ML help move the ML research community toward more active and appropriate engagement with limitations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533122",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "587–597",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "REAL ML: Recognizing, exploring, and articulating limitations of machine learning research",
		"URL": "https://doi.org/10.1145/3531146.3533122",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			},
			{
				"family": "Wortman Vaughan",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "niuBestVsAll2022",
		"type": "paper-conference",
		"abstract": "We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533121",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "574–586",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Best vs. All: Equity and accuracy of standardized test score reporting",
		"URL": "https://doi.org/10.1145/3531146.3533121",
		"author": [
			{
				"family": "Niu",
				"given": "Mingzi"
			},
			{
				"family": "Kannan",
				"given": "Sampath"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Vohra",
				"given": "Rakesh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "midhaEthicalConcernsPerceptions2022",
		"type": "paper-conference",
		"abstract": "With rapid growth in the development of consumer neurotechnology, it is imperative to consider the ethical implications that this might have in order to minimise consumer harm. Whilst ethical and legal guidelines for commercialisation have previously been suggested, we aimed to further this discussion by investigating the ethical concerns held by potential end users of consumer neurotechnology. 19 participants who had previously experienced mental workload tracking in their daily lives were interviewed about their ethical concerns and perceptions of this type of future neurotechnology. An Interpretive Phenomenological Analysis (IPA) approach identified three superordinate themes. These related to concerns surrounding privacy, data validity and misinterpretation, and personal identity. The findings provide further validation for previous research and highlight further ethical considerations that should be factored into the commercialisation of neurotechnology.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533119",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "564–573",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical concerns and perceptions of consumer neurotechnology from lived experiences of mental workload tracking",
		"URL": "https://doi.org/10.1145/3531146.3533119",
		"author": [
			{
				"family": "Midha",
				"given": "Serena"
			},
			{
				"family": "Wilson",
				"given": "Max L."
			},
			{
				"family": "Sharples",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yurritaMultistakeholderValuebasedAssessment2022",
		"type": "paper-conference",
		"abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533118",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 29\npublisher-place: Seoul, Republic of Korea",
		"page": "535–563",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems",
		"URL": "https://doi.org/10.1145/3531146.3533118",
		"author": [
			{
				"family": "Yurrita",
				"given": "Mireia"
			},
			{
				"family": "Murray-Rust",
				"given": "Dave"
			},
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Bozzon",
				"given": "Alessandro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "marklLanguageVariationAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533117",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "521–534",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition",
		"URL": "https://doi.org/10.1145/3531146.3533117",
		"author": [
			{
				"family": "Markl",
				"given": "Nina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kollnigGoodbyeTrackingImpact2022",
		"type": "paper-conference",
		"abstract": "Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules. We find that Apple’s new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users&nbsp;–&nbsp;a positive development for privacy. However, the number of tracking libraries has&nbsp;–&nbsp;on average&nbsp;–&nbsp;roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple’s policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring from its new tracking rules. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading, especially in less popular apps. Overall, our observations suggest that, while Apple’s changes make tracking individual users more difficult, they motivate a countermovement, and reinforce existing market power of gatekeeper companies with access to large troves of first-party data. Making the privacy properties of apps transparent through large-scale analysis remains a difficult target for independent researchers, and a key obstacle to meaningful, accountable and verifiable privacy protections.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533116",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "508–520",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Goodbye tracking? Impact of iOS app tracking transparency and privacy labels",
		"URL": "https://doi.org/10.1145/3531146.3533116",
		"author": [
			{
				"family": "Kollnig",
				"given": "Konrad"
			},
			{
				"family": "Shuba",
				"given": "Anastasia"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Shadbolt",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangAffirmativeAlgorithmsRelational2022",
		"type": "paper-conference",
		"abstract": "Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533115",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "495–507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Affirmative algorithms: Relational equality as algorithmic fairness",
		"URL": "https://doi.org/10.1145/3531146.3533115",
		"author": [
			{
				"family": "Zhang",
				"given": "Marilyn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kongAreIntersectionallyFair2022",
		"type": "paper-conference",
		"abstract": "A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is “intersectionally fair” if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533114",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "485–494",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Are “Intersectionally fair” AI algorithms really fair to women of color? A philosophical analysis",
		"URL": "https://doi.org/10.1145/3531146.3533114",
		"author": [
			{
				"family": "Kong",
				"given": "Youjin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dengExploringHowMachine2022",
		"type": "paper-conference",
		"abstract": "Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533113",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "473–484",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring how machine learning practitioners (try to) use fairness toolkits",
		"URL": "https://doi.org/10.1145/3531146.3533113",
		"author": [
			{
				"family": "Deng",
				"given": "Wesley Hanwen"
			},
			{
				"family": "Nagireddy",
				"given": "Manish"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chowdhuryEquiexplanationMapsConcise2022",
		"type": "paper-conference",
		"abstract": "We attempt to summarize the model logic of a black-box classification model in order to generate concise and informative global explanations. We propose equi-explanation maps, a new explanation data-structure that presents the region of interest as a union of equi-explanation subspaces along with their explanation vectors. We then propose E-Map, a method to generate equi-explanation maps. We demonstrate the broad utility of our approach by generating equi-explanation maps for various binary classification models (Logistic Regression, SVM, MLP, and XGBoost) on the UCI Heart disease dataset and the Pima Indians diabetes dataset. Each subspace in our generated map is the union of d-dimensional hyper-cuboids which can be compactly represented for the sake of interpretability. For each of these subspaces, we present linear explanations assigning a weight to each explanation feature. We justify the use of equi-explanation maps in comparison to other global explanation methods by evaluating in terms of interpretability, fidelity, and informativeness. A user study further corroborates the use of equi-explanation maps to generate compact and informative global explanations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533112",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "464–472",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equi-explanation maps: Concise and informative global summary explanations",
		"URL": "https://doi.org/10.1145/3531146.3533112",
		"author": [
			{
				"family": "Chowdhury",
				"given": "Tanya"
			},
			{
				"family": "Rahimi",
				"given": "Razieh"
			},
			{
				"family": "Allan",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "boagTechWorkerOrganizing2022",
		"type": "paper-conference",
		"abstract": "In recent years, there has been a growing interest in the field of “AI Ethics” and related areas. This field is purposefully broad, allowing for the intersection of numerous subfields and disciplines. However, a lot of work in this area thus far has centered computational methods, leading to a narrow lens where technical tools are framed as solutions for broader sociotechnical problems. In this work, we discuss a less-explored mode of what it can mean to “do” AI Ethics: tech worker collective action. Through collective action, the employees of powerful tech companies can act as a countervailing force against strong corporate impulses to grow or make a profit to the detriment of other values. In this work, we ground these efforts in existing scholarship of social movements and labor organizing. We characterize 150 documented collective actions, and explore several case studies of successful campaigns. Looking forward, we also identify under-explored types of actions, and provide conceptual frameworks and inspiration for how to utilize worker organizing as an effective lever for change.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533111",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "452–463",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tech worker organizing for power and accountability",
		"URL": "https://doi.org/10.1145/3531146.3533111",
		"author": [
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Lepe",
				"given": "Bianca"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shenModelCardAuthoring2022",
		"type": "paper-conference",
		"abstract": "There have been increasing calls for centering impacted communities – both online and offline – in the design of the AI systems that will be deployed in their communities. However, the complicated nature of a community’s goals and needs, as well as the complexity of AI’s development procedures, outputs, and potential impacts, often prevents effective participation. In this paper, we present the Model Card Authoring Toolkit, a toolkit that supports community members to understand, navigate and negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values. Through a series of workshops, we conduct an empirical investigation of the initial effectiveness of our approach in two online communities – English and Dutch Wikipedia, and document how our participants collectively set the threshold for a machine learning based quality prediction system used in their communities’ content moderation applications. Our results suggest that the use of the Model Card Authoring Toolkit helps improve the understanding of the trade-offs across multiple community goals on AI design, engage community members to discuss and negotiate the trade-offs, and facilitate collective and informed decision-making in their own community contexts. Finally, we discuss the challenges for a community-centered, deliberation-driven approach for AI design as well as potential design implications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533110",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "440–451",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The model card authoring toolkit: Toward community-centered, deliberation-driven AI design",
		"URL": "https://doi.org/10.1145/3531146.3533110",
		"author": [
			{
				"family": "Shen",
				"given": "Hong"
			},
			{
				"family": "Wang",
				"given": "Leijie"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Brusse",
				"given": "Ciell"
			},
			{
				"family": "Velgersdijk",
				"given": "Ronald"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "crisanInteractiveModelCards2022",
		"type": "paper-conference",
		"abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability &amp; interpretability; sensemaking &amp; skepticism; and trust &amp; safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533108",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "427–439",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interactive model cards: A human-centered approach to model documentation",
		"URL": "https://doi.org/10.1145/3531146.3533108",
		"author": [
			{
				"family": "Crisan",
				"given": "Anamaria"
			},
			{
				"family": "Drouhard",
				"given": "Margaret"
			},
			{
				"family": "Vig",
				"given": "Jesse"
			},
			{
				"family": "Rajani",
				"given": "Nazneen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lauferFourYearsFAccT2022",
		"type": "paper-conference",
		"abstract": "Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533107",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 26\npublisher-place: Seoul, Republic of Korea",
		"page": "401–426",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Four years of FAccT: A reflexive, mixed-methods analysis of research contributions, shortcomings, and future prospects",
		"URL": "https://doi.org/10.1145/3531146.3533107",
		"author": [
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Jain",
				"given": "Sameer"
			},
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goetzeMindGapAutonomous2022",
		"type": "paper-conference",
		"abstract": "When a computer system causes harm, who is responsible? This question has renewed significance given the proliferation of autonomous systems enabled by modern artificial intelligence techniques. At the root of this problem is a philosophical difficulty known in the literature as the responsibility gap. That is to say, because of the causal distance between the designers of autonomous systems and the eventual outcomes of those systems, the dilution of agency within the large and complex teams that design autonomous systems, and the impossibility of fully predicting how autonomous systems will behave once deployed, determining who is morally responsible for harms caused by autonomous systems is unclear at a conceptual level. I review past work on this topic, criticizing prior works for suggesting workarounds rather than philosophical answers to the conceptual problem presented by the responsibility gap. The view I develop, drawing on my earlier work on vicarious moral responsibility, explains why computing professionals are ethically required to take responsibility for the systems they design, despite not being blameworthy for the harms these systems may cause.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533106",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "390–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mind the gap: Autonomous systems, the responsibility gap, and moral entanglement",
		"URL": "https://doi.org/10.1145/3531146.3533106",
		"author": [
			{
				"family": "Goetze",
				"given": "Trystan S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lumDebiasingBiasMeasurement2022",
		"type": "paper-conference",
		"abstract": "When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533105",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "379–389",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "De-biasing “bias” measurement",
		"URL": "https://doi.org/10.1145/3531146.3533105",
		"author": [
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Bower",
				"given": "Amanda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "meyerFlippingScriptCriminal2022",
		"type": "paper-conference",
		"abstract": "In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that “flips the script.” Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were “especially lengthy.” We then use a second-stage model to predict the defendant’s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533104",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "366–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants",
		"URL": "https://doi.org/10.1145/3531146.3533104",
		"author": [
			{
				"family": "Meyer",
				"given": "Mikaela"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Marshall",
				"given": "Erica"
			},
			{
				"family": "Lum",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "milliCausalInferenceStruggles2022",
		"type": "paper-conference",
		"abstract": "Online platforms regularly conduct randomized experiments to understand how changes to the platform causally affect various outcomes of interest. However, experimentation on online platforms has been criticized for having, among other issues, a lack of meaningful oversight and user consent. As platforms give users greater agency, it becomes possible to conduct observational studies in which users self-select into the treatment of interest as an alternative to experiments in which the platform controls whether the user receives treatment or not. In this paper, we conduct four large-scale within-study comparisons on Twitter aimed at assessing the effectiveness of observational studies derived from user self-selection on online platforms. In a within-study comparison, treatment effects from an observational study are assessed based on how effectively they replicate results from a randomized experiment with the same target population. We test the naive difference in group means estimator, exact matching, regression adjustment, and inverse probability of treatment weighting while controlling for plausible confounding variables. In all cases, all observational estimates perform poorly at recovering the ground-truth estimate from the analogous randomized experiments. In all cases except one, the observational estimates have the opposite sign of the randomized estimate. Our results suggest that observational studies derived from user self-selection are a poor alternative to randomized experimentation on online platforms. In discussing our results, we postulate a “Catch-22” that suggests that the success of causal inference in these settings may be at odds with the original motivations for providing users with greater agency.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533103",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "357–365",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal inference struggles with agency on online platforms",
		"URL": "https://doi.org/10.1145/3531146.3533103",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Belli",
				"given": "Luca"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rothOutcomeTestDiscrimination2022",
		"type": "paper-conference",
		"abstract": "This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533102",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 7\npublisher-place: Seoul, Republic of Korea",
		"page": "350–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An outcome test of discrimination for ranked lists",
		"URL": "https://doi.org/10.1145/3531146.3533102",
		"author": [
			{
				"family": "Roth",
				"given": "Jonathan"
			},
			{
				"family": "Saint-Jacques",
				"given": "Guillaume"
			},
			{
				"family": "Yu",
				"given": "YinYin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "jakeschHowDifferentGroups2022",
		"type": "paper-conference",
		"abstract": "Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533097",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "310–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How different groups prioritize ethical values for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533097",
		"author": [
			{
				"family": "Jakesch",
				"given": "Maurice"
			},
			{
				"family": "Buçinca",
				"given": "Zana"
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sikdarGetFairGeneralizedFairness2022",
		"type": "paper-conference",
		"abstract": "We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533094",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "289–299",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GetFair: Generalized fairness tuning of classification models",
		"URL": "https://doi.org/10.1145/3531146.3533094",
		"author": [
			{
				"family": "Sikdar",
				"given": "Sandipan"
			},
			{
				"family": "Lemmerich",
				"given": "Florian"
			},
			{
				"family": "Strohmaier",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tedjopurnomoEquitablePublicBus2022",
		"type": "paper-conference",
		"abstract": "Public bus transport is a major backbone of many cities’ socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore’s public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric’s real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore’s bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533092",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "278–288",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equitable public bus network optimization for social good: A case study of singapore",
		"URL": "https://doi.org/10.1145/3531146.3533092",
		"author": [
			{
				"family": "Tedjopurnomo",
				"given": "David"
			},
			{
				"family": "Bao",
				"given": "Zhifeng"
			},
			{
				"family": "Choudhury",
				"given": "Farhana"
			},
			{
				"family": "Luo",
				"given": "Hui"
			},
			{
				"family": "Qin",
				"given": "A. K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bellItsJustNot2022",
		"type": "paper-conference",
		"abstract": "To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533090",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "248–266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "It’s just not that simple: An empirical study of the accuracy-explainability trade-off in machine learning for public policy",
		"URL": "https://doi.org/10.1145/3531146.3533090",
		"author": [
			{
				"family": "Bell",
				"given": "Andrew"
			},
			{
				"family": "Solano-Kamaiko",
				"given": "Ian"
			},
			{
				"family": "Nov",
				"given": "Oded"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hutiriBiasAutomatedSpeaker2022",
		"type": "paper-conference",
		"abstract": "Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533089",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "230–247",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in automated speaker recognition",
		"URL": "https://doi.org/10.1145/3531146.3533089",
		"author": [
			{
				"family": "Hutiri",
				"given": "Wiebke Toussaint"
			},
			{
				"family": "Ding",
				"given": "Aaron Yi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "weidingerTaxonomyRisksPosed2022",
		"type": "paper-conference",
		"abstract": "Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533088",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "214–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taxonomy of risks posed by language models",
		"URL": "https://doi.org/10.1145/3531146.3533088",
		"author": [
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Huang",
				"given": "Po-Sen"
			},
			{
				"family": "Mellor",
				"given": "John"
			},
			{
				"family": "Glaese",
				"given": "Amelia"
			},
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "Balle",
				"given": "Borja"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Biles",
				"given": "Courtney"
			},
			{
				"family": "Brown",
				"given": "Sasha"
			},
			{
				"family": "Kenton",
				"given": "Zac"
			},
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Stepleton",
				"given": "Tom"
			},
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Rimell",
				"given": "Laura"
			},
			{
				"family": "Isaac",
				"given": "William"
			},
			{
				"family": "Haas",
				"given": "Julia"
			},
			{
				"family": "Legassick",
				"given": "Sean"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kallusTreatmentEffectRisk2022",
		"type": "paper-conference",
		"abstract": "Since the average treatment effect (ATE) measures the change in social welfare, even if positive, there is a risk of negative effect on, say, some 10% of the population. Assessing such risk is difficult, however, because any one individual treatment effect (ITE) is never observed so the 10% worst-affected cannot be identified, while distributional treatment effects only compare the first deciles within each treatment group, which does not correspond to any 10%-subpopulation. In this paper we consider how to nonetheless assess this important risk measure, formalized as the conditional value at risk (CVaR) of the ITE-distribution. We leverage the availability of pre-treatment covariates and characterize the tightest-possible upper and lower bounds on ITE-CVaR given by the covariate-conditional average treatment effect (CATE) function. We then proceed to study how to estimate these bounds efficiently from data and construct confidence intervals. This is challenging even in randomized experiments as it requires understanding the distribution of the unknown CATE function, which can be very complex if we use rich covariates so as to best control for heterogeneity. We develop a debiasing method that overcomes this and prove it enjoys favorable statistical properties even when CATE and other nuisances are estimated by black-box machine learning or even inconsistently. Studying a hypothetical change to French job-search counseling services, our bounds and inference demonstrate a small social benefit entails a negative impact on a substantial subpopulation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533087",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "213",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Treatment effect risk: Bounds and inference",
		"URL": "https://doi.org/10.1145/3531146.3533087",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "luccioniFrameworkDeprecatingDatasets2022",
		"type": "paper-conference",
		"abstract": "Datasets are central to training machine learning (ML) models. The ML community has recently made significant improvements to data stewardship and documentation practices across the model development life cycle. However, the act of deprecating, or deleting, datasets has been largely overlooked, and there are currently no standardized approaches for structuring this stage of the dataset life cycle. In this paper, we study the practice of dataset deprecation in ML, identify several cases of datasets that continued to circulate despite having been deprecated, and describe the different technical, legal, ethical, and organizational issues raised by such continuations. We then propose a Dataset Deprecation Framework that includes considerations of risk, mitigation of impact, appeal mechanisms, timeline, post-deprecation protocols, and publication checks that can be adapted and implemented by the ML community. Finally, we propose creating a centralized, sustainable repository system for archiving datasets, tracking dataset modifications or deprecations, and facilitating practices of care and stewardship that can be integrated into research and publication processes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533086",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "199–212",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for deprecating datasets: Standardizing documentation, identification, and communication",
		"URL": "https://doi.org/10.1145/3531146.3533086",
		"author": [
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Corry",
				"given": "Frances"
			},
			{
				"family": "Sridharan",
				"given": "Hamsini"
			},
			{
				"family": "Ananny",
				"given": "Mike"
			},
			{
				"family": "Schultz",
				"given": "Jason"
			},
			{
				"family": "Crawford",
				"given": "Kate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gradwohlParetoimprovingDataSharing2022",
		"type": "paper-conference",
		"abstract": "We study the effects of data sharing between firms on prices, profits, and consumer welfare.Although indiscriminate sharing of consumer data decreases firm profits due to the subsequentincrease in competition, selective sharing can be beneficial. We show that there are data-sharing mechanisms that are strictly Pareto-improving, simultaneously increasing firm profitsand consumer welfare. Within the class of Pareto-improving mechanisms, we identify one thatmaximizes firm profits and one that maximizes consumer welfare.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533085",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 2\npublisher-place: Seoul, Republic of Korea",
		"page": "197–198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Pareto-improving Data-Sharing",
		"URL": "https://doi.org/10.1145/3531146.3533085",
		"author": [
			{
				"family": "Gradwohl",
				"given": "Ronen"
			},
			{
				"family": "Tennenholtz",
				"given": "Moshe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fraserAIOpacityExplainability2022",
		"type": "paper-conference",
		"abstract": "A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533084",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "185–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI opacity and explainability in tort litigation",
		"URL": "https://doi.org/10.1145/3531146.3533084",
		"author": [
			{
				"family": "Fraser",
				"given": "Henry"
			},
			{
				"family": "Simcock",
				"given": "Rhyle"
			},
			{
				"family": "Snoswell",
				"given": "Aaron J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "birhaneValuesEncodedMachine2022",
		"type": "paper-conference",
		"abstract": "Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533083",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "173–184",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The values encoded in machine learning research",
		"URL": "https://doi.org/10.1145/3531146.3533083",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Kalluri",
				"given": "Pratyusha"
			},
			{
				"family": "Card",
				"given": "Dallas"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Bao",
				"given": "Michelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "singhAutomatingCareOnline2022",
		"type": "paper-conference",
		"abstract": "On March 23, 2020, the Government of India (GoI) announced one of the strictest nationwide lockdowns in the world to curb the spread of novel SARS-CoV-2, otherwise known as CoVID-19. The country came to a standstill overnight and the service industry, including small businesses and restaurants, took a massive financial hit. The unknown nature of the virus and its spread deepened anxiety among the general public, quickly turning to distrust towards any “outside” contact with goods and people. In the hopes of (re)building consumer trust, food delivery platforms Zomato and Swiggy began providing digital solutions to exhibit care towards their customers, including: (1) sharing delivery workers’ live temperatures alongside the workers’ profile inside the app; (2) mandating the use of the controversial contact tracing app Aarogya Setu for the workers; (3) monitoring workers’ usage of masks through random selfie requests; and (4) sharing specific worker vaccination details on the app for customers to view, including vaccination date and the vaccine’s serial number. Such invasive data gathering infrastructures to address public health threats have long focused on the surveillance of laborers, migrants, and the bodies of other marginalized communities. Framed as public health management, such biometric and health data gathering is treated as a necessary feature of caring for the well-being of the general public. However, such datafication practices - ones which primarily focus on the extraction of data from one specific community in order to mollify the concerns of another - normalizes the false perception that disease is transmitted unidirectionally: from worker to the consumer. By centering food delivery workers’ experiences during the pandemic and examining the normalization of such surveillance in the name of care and recovery, this paper aims to examine how new regimes of care are manufactured and legitimized using harmful and unethical datafication practices.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533082",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "160–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automating care: Online food delivery work during the CoVID-19 crisis in india",
		"URL": "https://doi.org/10.1145/3531146.3533082",
		"author": [
			{
				"family": "Singh",
				"given": "Anubha"
			},
			{
				"family": "Park",
				"given": "Tina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "papadakiMinimaxDemographicGroup2022",
		"type": "paper-conference",
		"abstract": "Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533081",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "142–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Minimax demographic group fairness in federated learning",
		"URL": "https://doi.org/10.1145/3531146.3533081",
		"author": [
			{
				"family": "Papadaki",
				"given": "Afroditi"
			},
			{
				"family": "Martinez",
				"given": "Natalia"
			},
			{
				"family": "Bertran",
				"given": "Martin"
			},
			{
				"family": "Sapiro",
				"given": "Guillermo"
			},
			{
				"family": "Rodrigues",
				"given": "Miguel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "engelmannWhatPeopleThink2022",
		"type": "paper-conference",
		"abstract": "Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533080",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "128–141",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What people think AI should infer from faces",
		"URL": "https://doi.org/10.1145/3531146.3533080",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Ullstein",
				"given": "Chiara"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangProvidingItemsideIndividual2022",
		"type": "paper-conference",
		"abstract": "Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et&nbsp;al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533079",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "117–127",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Providing item-side individual fairness for deep recommender systems",
		"URL": "https://doi.org/10.1145/3531146.3533079",
		"author": [
			{
				"family": "Wang",
				"given": "Xiuling"
			},
			{
				"family": "Wang",
				"given": "Wendy Hui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "asherWhenLearningBecomes2022",
		"type": "paper-conference",
		"abstract": "We formally analyze an epistemic bias we call interpretive blindness (IB), in which under certain conditions a learner will be incapable of learning. IB is now common in our society, but it is a natural consequence of Bayesian inference and what we argue are mild assumptions about the relation between belief and evidence. IB a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a codependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic of contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533078",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "107–116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When learning becomes impossible",
		"URL": "https://doi.org/10.1145/3531146.3533078",
		"author": [
			{
				"family": "Asher",
				"given": "Nicholas"
			},
			{
				"family": "Hunter",
				"given": "Julie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "longoniNewsGenerativeArtificial2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533077",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "97–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "News from generative artificial intelligence is believed less",
		"URL": "https://doi.org/10.1145/3531146.3533077",
		"author": [
			{
				"family": "Longoni",
				"given": "Chiara"
			},
			{
				"family": "Fradkin",
				"given": "Andrey"
			},
			{
				"family": "Cian",
				"given": "Luca"
			},
			{
				"family": "Pennycook",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "simbeckFAccTCheckAIRegulation2022",
		"type": "paper-conference",
		"abstract": "In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533076",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "89–96",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FAccT-Check on AI regulation: Systematic evaluation of AI regulation on the example of the legislation on the use of AI in the public sector in the german federal state of schleswig-holstein",
		"URL": "https://doi.org/10.1145/3531146.3533076",
		"author": [
			{
				"family": "Simbeck",
				"given": "Katharina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goyalFairnessIndicatorsSystematic2022",
		"type": "paper-conference",
		"abstract": "Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533074",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "70–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness indicators for systematic assessments of visual feature extractors",
		"URL": "https://doi.org/10.1145/3531146.3533074",
		"author": [
			{
				"family": "Goyal",
				"given": "Priya"
			},
			{
				"family": "Soriano",
				"given": "Adriana Romero"
			},
			{
				"family": "Hazirbas",
				"given": "Caner"
			},
			{
				"family": "Sagun",
				"given": "Levent"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "struppekLearningBreakDeep2022",
		"type": "paper-conference",
		"abstract": "Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533073",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "58–69",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to break deep perceptual hashing: The use case NeuralHash",
		"URL": "https://doi.org/10.1145/3531146.3533073",
		"author": [
			{
				"family": "Struppek",
				"given": "Lukas"
			},
			{
				"family": "Hintersdorf",
				"given": "Dominik"
			},
			{
				"family": "Neider",
				"given": "Daniel"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "benjaminFuckTheAlgorithmAlgorithmicImaginaries2022",
		"type": "paper-conference",
		"abstract": "This paper applies and extends the concept of algorithmic imaginaries in the context of political resistance to sociotechnical injustice. Focusing on the 2020 UK OfQual protests, the role of the ”fuck the algorithm” chant is examined as an imaginary of resistance to confront power in sociotechnical systems. The protest is analysed as a shift in algorithmic imaginaries amidst evolving uses of #FuckTheAlgorithm on social media as part of everyday practices of resistance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533072",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "46–57",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "#FuckTheAlgorithm: algorithmic imaginaries and political resistance",
		"URL": "https://doi.org/10.1145/3531146.3533072",
		"author": [
			{
				"family": "Benjamin",
				"given": "Garfield"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chapmanDatadrivenAnalysisInterplay2022",
		"type": "paper-conference",
		"abstract": "Previous studies have focused on the biases and feedback loops that occur in predictive policing algorithms. These studies show how systemically and institutionally biased data leads to these feedback loops when predictive policing algorithms are applied in real life. We take a step back, and show that the choice in algorithm can be embedded in a specific criminological theory, and that the choice of a model on its own even without biased data can create biased feedback loops. By synthesizing “historical” data, in which we control the relationships between crimes, location and time, we show that the current predictive policing algorithms create biased feedback loops even with completely random data. We then review the process of creation and deployment of these predictive systems, and highlight when good practices, such as fitting a model to data, “go bad” within the context of larger system development and deployment. Using best practices from previous work on assessing and mitigating the impact of new technologies, we highlight where the design of these algorithms has broken down. The study also found that multidisciplinary analysis of such systems is vital for uncovering these issues and shows that any study of equitable AI should involve a systematic and holistic analysis of their design rationalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533071",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "36–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Data-driven analysis of the interplay between Criminological theory and predictive policing algorithms",
		"URL": "https://doi.org/10.1145/3531146.3533071",
		"author": [
			{
				"family": "Chapman",
				"given": "Adriane"
			},
			{
				"family": "Grylls",
				"given": "Philip"
			},
			{
				"family": "Ugwudike",
				"given": "Pamela"
			},
			{
				"family": "Gammack",
				"given": "David"
			},
			{
				"family": "Ayling",
				"given": "Jacqui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hongDynamicPrivacyBudget2022",
		"type": "paper-conference",
		"abstract": "Protecting privacy in learning while maintaining the model performance has become increasingly critical in many applications that involve sensitive data. A popular private learning framework is differentially private learning composed of many privatized gradient iterations by noising and clipping. Under the privacy constraint, it has been shown that the dynamic policies could improve the final iterate loss, namely the quality of published models. In this talk, we will introduce these dynamic techniques for learning rate, batch size, noise magnitude and gradient clipping. Also, we discuss how the dynamic policy could change the convergence bounds which further provides insight of the impact of dynamic methods.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533070",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 25\npublisher-place: Seoul, Republic of Korea",
		"page": "11–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dynamic privacy budget allocation improves data efficiency of differentially private gradient descent",
		"URL": "https://doi.org/10.1145/3531146.3533070",
		"author": [
			{
				"family": "Hong",
				"given": "Junyuan"
			},
			{
				"family": "Wang",
				"given": "Zhangyang"
			},
			{
				"family": "Zhou",
				"given": "Jiayu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "vlasceanuInterdisciplinarityGenderDiversity2022",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) research plays an increasingly important role in society, impacting key aspects of human life. From face recognition algorithms aiding national security in airports, to software that advises judges in criminal cases, and medical staff in healthcare, AI research is shaping critical facets of our experience in the world. But who are the people and institutional bodies behind this influential research? What are the predictors of influence of AI researchers and research organizations? We study this question using social network analysis, in an exploration of the structural characteristics, i.e., network topology, of research organizations that shape modern AI. In a sample of 149 organizations with 9,987 affiliated authors of published papers in a major AI conference (NeurIPS) and two major conferences that specifically focus on societal impacts of AI (FAccT and AIES), we find that both industry and academic research organizations with influential authors are more interdisciplinary, have a greater fraction of women, are more hierarchical, and less clustered, even when controlling for the size of the organizations. The influence is operationalized as betweenness centrality in co-authorship networks, i.e., how often an author is on the shortest path connecting any pair of authors, acting as a bridge connecting otherwise distant (or even disconneted) members of the network, such as their own co-authors who are not each other’s co-author themselves. Using this operationalization, we also find that women have less influence in the AI community, determined as lower betweenness centrality in co-authorship networks. These results suggest that while diverse AI institutions are more influential, the individuals contributing to the increased diversity are marginalized in the AI field. We discuss these results in the context of current events with important societal implications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533069",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1–10",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interdisciplinarity, gender diversity, and network structure predict the centrality of AI organizations",
		"URL": "https://doi.org/10.1145/3531146.3533069",
		"author": [
			{
				"family": "Vlasceanu",
				"given": "Madalina"
			},
			{
				"family": "Dudik",
				"given": "Miroslav"
			},
			{
				"family": "Momennejad",
				"given": "Ida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "queerinaiQueerAICase2023",
		"type": "paper-conference",
		"abstract": "Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594134",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1882–1895",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Queer in AI: A case study in community-led participatory AI",
		"URL": "https://doi.org/10.1145/3593013.3594134",
		"author": [
			{
				"family": "Queerinai",
				"given": "Organizers Of"
			},
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Singh",
				"given": "Ashwin"
			},
			{
				"family": "Voelcker",
				"given": "Claas"
			},
			{
				"family": "Sutherland",
				"given": "Danica J."
			},
			{
				"family": "Locatelli",
				"given": "Davide"
			},
			{
				"family": "Breznik",
				"given": "Eva"
			},
			{
				"family": "Klubicka",
				"given": "Filip"
			},
			{
				"family": "Yuan",
				"given": "Hang"
			},
			{
				"family": "J",
				"given": "Hetvi"
			},
			{
				"family": "Zhang",
				"given": "Huan"
			},
			{
				"family": "Shriram",
				"given": "Jaidev"
			},
			{
				"family": "Lehman",
				"given": "Kruno"
			},
			{
				"family": "Soldaini",
				"given": "Luca"
			},
			{
				"family": "Sap",
				"given": "Maarten"
			},
			{
				"family": "Deisenroth",
				"given": "Marc Peter"
			},
			{
				"family": "Pacheco",
				"given": "Maria Leonor"
			},
			{
				"family": "Ryskina",
				"given": "Maria"
			},
			{
				"family": "Mundt",
				"given": "Martin"
			},
			{
				"family": "Agarwal",
				"given": "Milind"
			},
			{
				"family": "Mclean",
				"given": "Nyx"
			},
			{
				"family": "Xu",
				"given": "Pan"
			},
			{
				"family": "Pranav",
				"given": "A"
			},
			{
				"family": "Korpan",
				"given": "Raj"
			},
			{
				"family": "Ray",
				"given": "Ruchira"
			},
			{
				"family": "Mathew",
				"given": "Sarah"
			},
			{
				"family": "Arora",
				"given": "Sarthak"
			},
			{
				"family": "John",
				"given": "St"
			},
			{
				"family": "Anand",
				"given": "Tanvi"
			},
			{
				"family": "Agrawal",
				"given": "Vishakha"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Long",
				"given": "Yanan"
			},
			{
				"family": "Wang",
				"given": "Zijie J."
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Dennler",
				"given": "Nathaniel"
			},
			{
				"family": "Noseworthy",
				"given": "Michael"
			},
			{
				"family": "Jha",
				"given": "Sharvani"
			},
			{
				"family": "Baylor",
				"given": "Emi"
			},
			{
				"family": "Joshi",
				"given": "Aditya"
			},
			{
				"family": "Bilenko",
				"given": "Natalia Y."
			},
			{
				"family": "Mcnamara",
				"given": "Andrew"
			},
			{
				"family": "Gontijo-Lopes",
				"given": "Raphael"
			},
			{
				"family": "Markham",
				"given": "Alex"
			},
			{
				"family": "Dong",
				"given": "Evyn"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Saraswat",
				"given": "Manu"
			},
			{
				"family": "Vytla",
				"given": "Nikhil"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "andricReconcilingGovernmentalUse2023a",
		"type": "paper-conference",
		"abstract": "The societal and epistemological implications of online targeted advertising have been scrutinized by AI ethicists, legal scholars, and policymakers alike. However, the government’s use of online targeting and its consequential socio-political ramifications remain under-explored from a critical socio-technical standpoint. This paper investigates the socio-political implications of governmental online targeting, using a case study of the UK government’s application of such techniques for public policy objectives. We argue that this practice undermines democratic ideals, as it engenders three primary concerns — Transparency, Privacy, and Equality — that clash with fundamental democratic doctrines and values. To address these concerns, the paper introduces a preliminary blueprint for an AI governance framework that harmonizes governmental use of online targeting with certain democratic principles. Furthermore, we advocate for the creation of an independent, non-governmental regulatory body responsible for overseeing the process and monitoring the government’s use of online targeting, a critical measure for preserving democratic values.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594133",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1871–1881",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconciling governmental use of online targeting with democracy",
		"URL": "https://doi.org/10.1145/3593013.3594133",
		"author": [
			{
				"family": "Andrić",
				"given": "Katja"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "saxonDisparitiesTexttoimageModel2023a",
		"type": "paper-conference",
		"abstract": "We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive “creative” generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594123",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1870",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparities in text-to-image model concept possession across languages",
		"URL": "https://doi.org/10.1145/3593013.3594123",
		"author": [
			{
				"family": "Saxon",
				"given": "Michael"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "laugelAchievingDiversityCounterfactual2023a",
		"type": "paper-conference",
		"abstract": "In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594122",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1859–1869",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving diversity in counterfactual explanations: a review and discussion",
		"URL": "https://doi.org/10.1145/3593013.3594122",
		"author": [
			{
				"family": "Laugel",
				"given": "Thibault"
			},
			{
				"family": "Jeyasothy",
				"given": "Adulam"
			},
			{
				"family": "Lesot",
				"given": "Marie-Jeanne"
			},
			{
				"family": "Marsala",
				"given": "Christophe"
			},
			{
				"family": "Detyniecki",
				"given": "Marcin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "binnsLegalTaxonomiesMachine2023a",
		"type": "paper-conference",
		"abstract": "Previous literature on ‘fair’ machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594121",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1850–1858",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Legal taxonomies of machine bias: Revisiting direct discrimination",
		"URL": "https://doi.org/10.1145/3593013.3594121",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Adams-Prassl",
				"given": "Jeremias"
			},
			{
				"family": "Kelly-Lyth",
				"given": "Aislinn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grillBiasBoundaryObject2023a",
		"type": "paper-conference",
		"abstract": "Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis \"technical biases,\" like issues around measurement, rigidity, and coarseness of variables, \"emergent biases,\" such as disruptive events that change the labor market, and, finally, \"preexisting biases,\" like the use of variables that act as proxies for inequality. Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined “unbiased\" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594120",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1838–1849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias as boundary object: Unpacking the politics of an austerity algorithm using bias frameworks",
		"URL": "https://doi.org/10.1145/3593013.3594120",
		"author": [
			{
				"family": "Grill",
				"given": "Gabriel"
			},
			{
				"family": "Fischer",
				"given": "Fabian"
			},
			{
				"family": "Cech",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuSlowViolenceSurveillance2023a",
		"type": "paper-conference",
		"abstract": "People’s negative reactions to online behavioral advertising (OBA) are well-documented. However, past work has primarily focused on cataloguing these reactions and exploring how to change them, rather than understanding the ways these negative reactions affect people’s lived experiences. Drawing upon scholarship on socio-technical harms in human-computer interaction and computer-supported cooperative work, we investigate and categorize the different ways people report having been harmed by OBA. Through an online survey with 420 participants, we identified four key harms arising from OBA: psychological distress, loss of autonomy, constriction of user behavior, and algorithmic marginalization and traumatization. We next discuss the “slow violence” inflicted by OBA and the normalization of people’s affective discomfort with OBA, and how the two can present an opportunity for researchers to re-conceptualize OBA—and the invasive data practices it entails—as not just abstractly concerning to people, but as actively harmful.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594119",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1826–1837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The slow violence of surveillance capitalism: How online behavioral advertising harms people",
		"URL": "https://doi.org/10.1145/3593013.3594119",
		"author": [
			{
				"family": "Wu",
				"given": "Yuxi"
			},
			{
				"family": "Bice",
				"given": "Sydney"
			},
			{
				"family": "Edwards",
				"given": "W. Keith"
			},
			{
				"family": "Das",
				"given": "Sauvik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangFairnessAuditingUrban2023a",
		"type": "paper-conference",
		"abstract": "Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594118",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1817–1825",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness auditing in urban decisions using LP-based data combination",
		"URL": "https://doi.org/10.1145/3593013.3594118",
		"author": [
			{
				"family": "Yang",
				"given": "Jingyi"
			},
			{
				"family": "Miller",
				"given": "Joel"
			},
			{
				"family": "Ohannessian",
				"given": "Mesrob"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diciccioDetectionMitigationAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594117",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1801–1816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detection and mitigation of algorithmic bias via predictive parity",
		"URL": "https://doi.org/10.1145/3593013.3594117",
		"author": [
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Hsu",
				"given": "Brian"
			},
			{
				"family": "Yu",
				"given": "Yinyin"
			},
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ganeshImpactMachineLearning2023a",
		"type": "paper-conference",
		"abstract": "Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594116",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1789–1800",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the impact of machine learning randomness on group fairness",
		"URL": "https://doi.org/10.1145/3593013.3594116",
		"author": [
			{
				"family": "Ganesh",
				"given": "Prakhar"
			},
			{
				"family": "Chang",
				"given": "Hongyan"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nagarajraoDiscriminationImageSelection2023",
		"type": "paper-conference",
		"abstract": "Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools. In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery – through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594115",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1772–1788",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Discrimination through image selection by job advertisers on facebook",
		"URL": "https://doi.org/10.1145/3593013.3594115",
		"author": [
			{
				"family": "Nagaraj Rao",
				"given": "Varun"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "barrettSkinDeepInvestigating2023a",
		"type": "paper-conference",
		"abstract": "To investigate the well-observed racial disparities in computer vision systems that analyze images of humans, researchers have turned to skin tone as a more objective annotation than race metadata for fairness performance evaluations. However, the current state of skin tone annotation procedures is highly varied. For instance, researchers use a range of untested scales and skin tone categories, have unclear annotation procedures, and provide inadequate analyses of uncertainty. In addition, little attention is paid to the positionality of the humans involved in the annotation process—both designers and annotators alike—and the historical and sociological context of skin tone in the United States. Our work is the first to investigate the skin tone annotation process as a sociotechnical project. We surveyed recent skin tone annotation procedures and conducted annotation experiments to examine how subjective understandings of skin tone are embedded in skin tone annotation procedures. Our systematic literature review revealed the uninterrogated association between skin tone and race and the limited effort to analyze annotator uncertainty in current procedures for skin tone annotation in computer vision evaluation. Our experiments demonstrated that design decisions in the annotation procedure such as the order in which the skin tone scale is presented or additional context in the image (i.e., presence of a face) significantly affected the resulting inter-annotator agreement and individual uncertainty of skin tone annotations. We call for greater reflexivity in the design, analysis, and documentation of procedures for evaluation using skin tone.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594114",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1757–1771",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Skin deep: Investigating subjectivity in skin tone annotations for computer vision benchmark datasets",
		"URL": "https://doi.org/10.1145/3593013.3594114",
		"author": [
			{
				"family": "Barrett",
				"given": "Teanna"
			},
			{
				"family": "Chen",
				"given": "Quanze"
			},
			{
				"family": "Zhang",
				"given": "Amy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bruns-smithUsingSupervisedLearning2023a",
		"type": "paper-conference",
		"abstract": "Household responses to income shocks are important drivers of financial fragility, the evolution of wealth inequality, and the effectiveness of fiscal and monetary policy. Traditional approaches to measuring the size and persistence of income shocks are based on restrictive econometric models that impose strong homogeneity across households and over time. In this paper, we propose a more flexible, machine learning framework for estimating income shocks that allows for variation across all observable features and time horizons. First, we propose non-parametric estimands for shocks and shock persistence. We then show how to estimate these quantities by using off-the-shelf supervised learning tools to approximate the conditional expectation of future income given present information. We solve this income prediction problem in a large Icelandic administrative dataset, and then use the estimated shocks to document several features of labor income risk in Iceland that are not captured by standard economic income models.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594113",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1747–1756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using supervised learning to estimate inequality in the size and persistence of income shocks",
		"URL": "https://doi.org/10.1145/3593013.3594113",
		"author": [
			{
				"family": "Bruns-Smith",
				"given": "David"
			},
			{
				"family": "Feller",
				"given": "Avi"
			},
			{
				"family": "Nakamura",
				"given": "Emi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "silvaRepresentationOnlineMatters2023",
		"type": "paper-conference",
		"abstract": "As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-OR logical operator to bucketized retrieval at the retrieval stage and from greedy re-rankers to multi-objective optimization using determinantal point processes for the ranking stage, balances diversity and utility while enabling fast iterations and scalable expansion to diversification over multiple dimensions. Our experiments indicate that these approaches significantly improve diversity metrics, with a neutral to a positive impact on utility metrics and improved user satisfaction, both qualitatively and quantitatively, in production.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594112",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1735–1746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation online matters: Practical end-to-end diversification in search and recommender systems",
		"URL": "https://doi.org/10.1145/3593013.3594112",
		"author": [
			{
				"family": "Silva",
				"given": "Pedro"
			},
			{
				"family": "Juneja",
				"given": "Bhawna"
			},
			{
				"family": "Desai",
				"given": "Shloka"
			},
			{
				"family": "Singh",
				"given": "Ashudeep"
			},
			{
				"family": "Fawaz",
				"given": "Nadia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kellyCapturingHumansMental2023a",
		"type": "paper-conference",
		"abstract": "Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594111",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1723–1734",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Capturing humans’ mental models of AI: An item response theory approach",
		"URL": "https://doi.org/10.1145/3593013.3594111",
		"author": [
			{
				"family": "Kelly",
				"given": "Markelle"
			},
			{
				"family": "Kumar",
				"given": "Aakriti"
			},
			{
				"family": "Smyth",
				"given": "Padhraic"
			},
			{
				"family": "Steyvers",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "sampsonRepresentationSelfdeterminationRefusal2023a",
		"type": "paper-conference",
		"abstract": "Targeted online advertising systems increasingly draw scrutiny for the surveillance underpinning their collection of people’s private data, and subsequent automated categorization and inference. The experiences of LGBTQ+ people, whose identities call into question dominant assumptions about who is seen as “normal,” and deserving of privacy, autonomy, and the right to self-determination, are a fruitful site for exploring the impacts of ad targeting. We conducted semi-structured interviews with LGBTQ+ individuals (N=18) to understand their experiences with online advertising, their perceptions of ad targeting, and the interplay of these systems with their queerness and other identities. Our results reflect participants’ overall negative experiences with online ad content—they described it as stereotypical and tokenizing in its lack of diversity and nuance. But their desires for better ad content also clashed with their more fundamental distrust and rejection of the non-consensual and extractive nature of ad targeting. They voiced privacy concerns about continuous data aggregation and behavior tracking, a desire for greater control over their data and attention, and even the right to opt-out entirely. Drawing on scholarship from queer and feminist theory, we explore targeted ads’ homonormativity in their failure to represent multiply-marginalized queer people, the harms of automated inference and categorization to identity formation and self-determination, and the theory of refusal underlying participants’ queer visions for a better online experience.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594110",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1711–1722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation, self-determination, and refusal: Queer people’s experiences with targeted advertising",
		"URL": "https://doi.org/10.1145/3593013.3594110",
		"author": [
			{
				"family": "Sampson",
				"given": "Princess"
			},
			{
				"family": "Encarnacion",
				"given": "Ro"
			},
			{
				"family": "Metaxa",
				"given": "Danaë"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "meiBias93Stigmatized2023a",
		"type": "paper-conference",
		"abstract": "Warning: The content of this paper may be upsetting or triggering.The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594109",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1699–1710",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias against 93 stigmatized groups in masked language models and downstream sentiment classification tasks",
		"URL": "https://doi.org/10.1145/3593013.3594109",
		"author": [
			{
				"family": "Mei",
				"given": "Katelyn"
			},
			{
				"family": "Fereidooni",
				"given": "Sonia"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shresthaHelpHinderEvaluating2023a",
		"type": "paper-conference",
		"abstract": "For applications where multiple stakeholders provide recommendations, a fair consensus ranking must not only ensure that the preferences of rankers are well represented, but must also mitigate disadvantages among socio-demographic groups in the final result. However, there is little empirical guidance on the value or challenges of visualizing and integrating fairness metrics and algorithms into human-in-the-loop systems to aid decision-makers. In this work, we design a study to analyze the effectiveness of integrating such fairness metrics-based visualization and algorithms. We explore this through a task-based crowdsourced experiment comparing an interactive visualization system for constructing consensus rankings, ConsensusFuse, with a similar system that includes visual encodings of fairness metrics and fair-rank generation algorithms, FairFuse. We analyze the measure of fairness, agreement of rankers’ decisions, and user interactions in constructing the fair consensus ranking across these two systems. In our study with 200 participants, results suggest that providing these fairness-oriented support features nudges users to align their decision with the fairness metrics while minimizing the tedious process of manually having to amend the consensus ranking. We discuss the implications of these results for the design of next-generation fairness oriented-systems and along with emerging directions for future research.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594108",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1685–1698",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Help or hinder? Evaluating the impact of fairness metrics and algorithms in visualizations for consensus ranking",
		"URL": "https://doi.org/10.1145/3593013.3594108",
		"author": [
			{
				"family": "Shrestha",
				"given": "Hilson"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Alkhathlan",
				"given": "Mallak"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			},
			{
				"family": "Harrison",
				"given": "Lane"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gardnerCrossinstitutionalTransferLearning2023a",
		"type": "paper-conference",
		"abstract": "Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions. We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594107",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1664–1684",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Cross-institutional transfer learning for educational models: Implications for model performance, fairness, and equity",
		"URL": "https://doi.org/10.1145/3593013.3594107",
		"author": [
			{
				"family": "Gardner",
				"given": "Joshua"
			},
			{
				"family": "Yu",
				"given": "Renzhe"
			},
			{
				"family": "Nguyen",
				"given": "Quan"
			},
			{
				"family": "Brooks",
				"given": "Christopher"
			},
			{
				"family": "Kizilcec",
				"given": "Rene"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "smithManyFacesFairness2023a",
		"type": "paper-conference",
		"abstract": "Recommender systems have a variety of stakeholders. Applying concepts of fairness in such systems requires attention to stakeholders’ complex and often-conflicting needs. Since fairness is socially constructed, there are numerous definitions, both in the social science and machine learning literatures. Still, it is rare for machine learning researchers to develop their metrics in close consideration of their social context. More often, standard definitions are adopted and assumed to be applicable across contexts and stakeholders. Our research starts with a recommendation context and then seeks to understand the breadth of the fairness considerations of associated stakeholders. In this paper, we report on the results of a semi-structured interview study with 23 employees who work for the Kiva microlending platform. We characterize the many different ways in which they enact and strive toward fairness for microlending recommendations in their own work, uncover the ways in which these different enactments of fairness are in tension with each other, and identify how stakeholders are differentially prioritized. Finally, we reflect on the implications of this study for future research and for the design of multistakeholder recommender systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594106",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1652–1663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The many faces of fairness: Exploring the institutional logics of multistakeholder microlending recommendation",
		"URL": "https://doi.org/10.1145/3593013.3594106",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Buhayh",
				"given": "Anas"
			},
			{
				"family": "Kathait",
				"given": "Anushka"
			},
			{
				"family": "Ragothaman",
				"given": "Pradeep"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Voida",
				"given": "Amy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bashardoustReducingAccessDisparities2023a",
		"type": "paper-conference",
		"abstract": "In social networks, a node’s position is, in and of itself, a form of social capital. Better-positioned members not only benefit from (faster) access to diverse information, but innately have more potential influence on information spread. Structural biases often arise from network formation, and can lead to significant disparities in information access based on position. Further, processes such as link recommendation can exacerbate this inequality by relying on network structure to augment connectivity. In this paper, we argue that one can understand and quantify this social capital through the lens of information flow in the network. In contrast to prior work, we consider the setting where all nodes may be sources of distinct information, and a node’s (dis)advantage takes into account its ability to access all information available on the network, not just that from a single source. We introduce three new measures of advantage (broadcast, influence, and control), which are quantified in terms of position in the network using access signatures – vectors that represent a node’s ability to share information with each other node in the network. We then consider the problem of improving equity by making interventions to increase the access of the least-advantaged nodes. Since all nodes are already sources of information in our model, we argue that edge augmentation is most appropriate for mitigating bias in the network structure, and frame a budgeted intervention problem for maximizing broadcast (minimum pairwise access) over the network. Finally, we propose heuristic strategies for selecting edge augmentations and empirically evaluate their performance on a corpus of real-world social networks. We demonstrate that a small number of interventions can not only significantly increase the broadcast measure of access for the least-advantaged nodes (over 5 times more than random), but also simultaneously improve the minimum influence. Additional analysis shows that edge augmentations targeted at improving minimum pairwise access can also dramatically shrink the gap in advantage between nodes (over ) and reduce disparities between their access signatures.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594105",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1635–1651",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reducing access disparities in networks using edge Augmentation",
		"URL": "https://doi.org/10.1145/3593013.3594105",
		"author": [
			{
				"family": "Bashardoust",
				"given": "Ashkan"
			},
			{
				"family": "Friedler",
				"given": "Sorelle"
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Sullivan",
				"given": "Blair D."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "corbettInterrogatingFAccT2023a",
		"type": "paper-conference",
		"abstract": "Fairness, accountability, and transparency are the three conceptual foundations of the FAccT conference. Transparency, however, has yet to be scrutinized to the same degree as accountability and fairness. As a result, we don't know: What does this community mean when it talks about transparency? How are we doing transparency? And to what ends? What commitments does (or should) the T in FAccT signify? This paper interrogates the T in FAccT using perspectives from critical transparency literature. Subsequently, we argue that FAccT might be better off dropping the T from its title for two reasons: (1) transparency can often be counterproductive to FAccT's primary objectives and (2) it is misleading as FAccT is mainly preoccupied with explainability rather than actual transparency. If we want to keep the T, we need to reframe how we think about and do transparency by making transparency contingent, reclaiming it from explainability, and bringing people into transparency processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594104",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1624–1634",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interrogating the T in FAccT",
		"URL": "https://doi.org/10.1145/3593013.3594104",
		"author": [
			{
				"family": "Corbett",
				"given": "Eric"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kulynychArbitraryDecisionsAre2023a",
		"type": "paper-conference",
		"abstract": "Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze—both theoretically and through extensive experiments—the predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594103",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1609–1623",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Arbitrary decisions are a hidden cost of differentially private training",
		"URL": "https://doi.org/10.1145/3593013.3594103",
		"author": [
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Hsu",
				"given": "Hsiang"
			},
			{
				"family": "Troncoso",
				"given": "Carmela"
			},
			{
				"family": "Calmon",
				"given": "Flavio P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "poulainImprovingFairnessAI2023a",
		"type": "paper-conference",
		"abstract": "Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models’ overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method’s superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594102",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1599–1608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Improving fairness in AI models on electronic health records: The case for federated learning methods",
		"URL": "https://doi.org/10.1145/3593013.3594102",
		"author": [
			{
				"family": "Poulain",
				"given": "Raphael"
			},
			{
				"family": "Bin Tarek",
				"given": "Mirza Farhan"
			},
			{
				"family": "Beheshti",
				"given": "Rahmatollah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guerdanCounterfactualPredictionOutcome2023a",
		"type": "paper-conference",
		"abstract": "Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594101",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1584–1598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual prediction under outcome measurement error",
		"URL": "https://doi.org/10.1145/3593013.3594101",
		"author": [
			{
				"family": "Guerdan",
				"given": "Luke"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kwegyir-aggreyMisuseAUCWhat2023a",
		"type": "paper-conference",
		"abstract": "When determining which machine learning model best performs some high impact risk assessment task, practitioners commonly use the Area under the Curve (AUC) to defend and validate their model choices. In this paper, we argue that the current use and understanding of AUC as a model performance metric misunderstands the way the metric was intended to be used. To this end, we characterize the misuse of AUC and illustrate how this misuse negatively manifests in the real world across several risk assessment domains. We locate this disconnect in the way the original interpretation of AUC has shifted over time to the point where issues pertaining to decision thresholds, class balance, statistical uncertainty, and protected groups remain unaddressed by AUC-based model comparisons, and where model choices that should be the purview of policymakers are hidden behind the veil of mathematical rigor. We conclude that current model validation practices involving AUC are not robust, and often invalid.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594100",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1570–1583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The misuse of AUC: What high impact risk assessment gets wrong",
		"URL": "https://doi.org/10.1145/3593013.3594100",
		"author": [
			{
				"family": "Kwegyir-Aggrey",
				"given": "Kweku"
			},
			{
				"family": "Gerchick",
				"given": "Marissa"
			},
			{
				"family": "Mohan",
				"given": "Malika"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zilkaProgressionDisparitiesCriminal2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person’s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5–2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5–11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594099",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1553–1569",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The progression of disparities within the criminal justice system: Differential enforcement and risk assessment instruments",
		"URL": "https://doi.org/10.1145/3593013.3594099",
		"author": [
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Hron",
				"given": "Jiri"
			},
			{
				"family": "Butcher",
				"given": "Bradley"
			},
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pangAuditingCrossculturalConsistency2023a",
		"type": "paper-conference",
		"abstract": "Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models—based on these labels—are applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are “consistently conceptualized” across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594098",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 22\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1531–1552",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing cross-cultural consistency of human-annotated labels for recommendation systems",
		"URL": "https://doi.org/10.1145/3593013.3594098",
		"author": [
			{
				"family": "Pang",
				"given": "Rock Yuren"
			},
			{
				"family": "Cenatempo",
				"given": "Jack"
			},
			{
				"family": "Graham",
				"given": "Franklyn"
			},
			{
				"family": "Kuehn",
				"given": "Bridgette"
			},
			{
				"family": "Whisenant",
				"given": "Maddy"
			},
			{
				"family": "Botchway",
				"given": "Portia"
			},
			{
				"family": "Stone Perez",
				"given": "Katie"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenPersonalizedPricingGroup2023a",
		"type": "paper-conference",
		"abstract": "In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers’ features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594097",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1520–1530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Personalized pricing with group fairness constraint",
		"URL": "https://doi.org/10.1145/3593013.3594097",
		"author": [
			{
				"family": "Chen",
				"given": "Xin"
			},
			{
				"family": "Xu",
				"given": "Zexing"
			},
			{
				"family": "Zhao",
				"given": "Zishuo"
			},
			{
				"family": "Zhou",
				"given": "Yuan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mccraddenWhatsFairFair2023",
		"type": "paper-conference",
		"abstract": "The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594096",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1505–1519",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB",
		"URL": "https://doi.org/10.1145/3593013.3594096",
		"author": [
			{
				"family": "Mccradden",
				"given": "Melissa"
			},
			{
				"family": "Odusi",
				"given": "Oluwadara"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Akrout",
				"given": "Ismail"
			},
			{
				"family": "Ndlovu",
				"given": "Kagiso"
			},
			{
				"family": "Glocker",
				"given": "Ben"
			},
			{
				"family": "Maicas",
				"given": "Gabriel"
			},
			{
				"family": "Liu",
				"given": "Xiaoxuan"
			},
			{
				"family": "Mazwi",
				"given": "Mjaye"
			},
			{
				"family": "Garnett",
				"given": "Tee"
			},
			{
				"family": "Oakden-Rayner",
				"given": "Lauren"
			},
			{
				"family": "Alfred",
				"given": "Myrtede"
			},
			{
				"family": "Sihlahla",
				"given": "Irvine"
			},
			{
				"family": "Shafei",
				"given": "Oswa"
			},
			{
				"family": "Goldenberg",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bianchiEasilyAccessibleTexttoimage2023a",
		"type": "paper-conference",
		"abstract": "Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594095",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1493–1504",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Easily accessible text-to-image generation amplifies demographic stereotypes at large scale",
		"URL": "https://doi.org/10.1145/3593013.3594095",
		"author": [
			{
				"family": "Bianchi",
				"given": "Federico"
			},
			{
				"family": "Kalluri",
				"given": "Pratyusha"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Ladhak",
				"given": "Faisal"
			},
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "Nozza",
				"given": "Debora"
			},
			{
				"family": "Hashimoto",
				"given": "Tatsunori"
			},
			{
				"family": "Jurafsky",
				"given": "Dan"
			},
			{
				"family": "Zou",
				"given": "James"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fieldExaminingRisksRacial2023a",
		"type": "paper-conference",
		"abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594094",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1479–1492",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Examining risks of racial biases in NLP tools for child protective services",
		"URL": "https://doi.org/10.1145/3593013.3594094",
		"author": [
			{
				"family": "Field",
				"given": "Anjalie"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Gandhi",
				"given": "Nupoor"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Putnam-Hornstein",
				"given": "Emily"
			},
			{
				"family": "Steier",
				"given": "David"
			},
			{
				"family": "Tsvetkov",
				"given": "Yulia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nigatuCodesigningTransparencyLessons2023a",
		"type": "paper-conference",
		"abstract": "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594093",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1463–1478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-designing for transparency: Lessons from building a document organization tool in the criminal justice domain",
		"URL": "https://doi.org/10.1145/3593013.3594093",
		"author": [
			{
				"family": "Nigatu",
				"given": "Hellina Hailu"
			},
			{
				"family": "Pickoff-White",
				"given": "Lisa"
			},
			{
				"family": "Canny",
				"given": "John"
			},
			{
				"family": "Chasins",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "metcalfTakingAlgorithmsCourts2023a",
		"type": "paper-conference",
		"abstract": "In widely used sociological descriptions of how accountability is structured through institutions, an “actor” (e.g., the developer) is accountable to a “forum” (e.g., regulatory agencies) empowered to pass judgements on and demand changes from the actor or enforce sanctions. However, questions about structuring accountability persist: why and how is a forum compelled to keep making demands of the actor when such demands are called for? To whom is a forum accountable in the performance of its responsibilities, and how can its practices and decisions be contested? In the context of algorithmic accountability, we contend that a robust accountability regime requires a triadic relationship, wherein the forum is also accountable to another entity: the public(s). Typically, as is the case with environmental impact assessments, public(s) make demands upon the forum's judgements and procedures through the courts, thereby establishing a minimum standard of due diligence. However, core challenges relating to: (1) lack of documentation, (2) difficulties in claiming standing, and (3) struggles around admissibility of expert evidence on and achieving consensus over the workings of algorithmic systems in adversarial proceedings prevent the public from approaching the courts when faced with algorithmic harms. In this paper, we demonstrate that the courts are the primary route—and the primary roadblock—in the pursuit of redress for algorithmic harms. Courts often find algorithmic harms non-cognizable and rarely require developers to address material claims of harm. To address the core challenges of taking algorithms to court, we develop a relational approach to algorithmic accountability that emphasizes not what the actors do nor the results of their actions, but rather how interlocking relationships of accountability are constituted in a triadic relationship between actors, forums, and public(s). As is the case in other regulatory domains, we believe that impact assessments (and similar accountability documentation) can provide the grounds for contestation between these parties, but only when that triad is structured such that the public(s) are able to cohere around shared experiences and interests, contest the outcomes of algorithmic systems that affect their lives, and make demands upon the other parties. Where courts now find algorithmic harms non-cognizable, an impact assessment regime can potentially create procedural rights to protect substantive rights of the public(s). This would require algorithmic accountability policies currently under consideration to provide the public(s) with adequate standing in courts, and opportunities to access and contest the actor's documentation and the forum's judgments.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594092",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1450–1462",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taking algorithms to courts: A relational approach to algorithmic accountability",
		"URL": "https://doi.org/10.1145/3593013.3594092",
		"author": [
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Tafesse",
				"given": "Emnet"
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liuGroupFairnessDemographics2023a",
		"type": "paper-conference",
		"abstract": "Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a “group-free\" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594091",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1432–1449",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness without demographics using social networks",
		"URL": "https://doi.org/10.1145/3593013.3594091",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Do",
				"given": "Virginie"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Nickel",
				"given": "Maximilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "norvalNavigatingAuditLandscape2023a",
		"type": "paper-conference",
		"abstract": "“Extended reality” (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount – where information revealing and enabling some reconstruction of an XR system’s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why. Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework’s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594090",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1418–1431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Navigating the audit landscape: A framework for developing transparent and auditable XR",
		"URL": "https://doi.org/10.1145/3593013.3594090",
		"author": [
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kimOrganizationalGovernanceEmerging2023a",
		"type": "paper-conference",
		"abstract": "Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594089",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 22\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1396–1417",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Organizational governance of emerging technologies: AI adoption in healthcare",
		"URL": "https://doi.org/10.1145/3593013.3594089",
		"author": [
			{
				"family": "Kim",
				"given": "Jee Young"
			},
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Gulamali",
				"given": "Freya"
			},
			{
				"family": "Hasan",
				"given": "Alifia"
			},
			{
				"family": "Hogg",
				"given": "Henry David Jeffry"
			},
			{
				"family": "Lifson",
				"given": "Mark"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre"
			},
			{
				"family": "Patel",
				"given": "Manesh"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Sehgal",
				"given": "Ajai"
			},
			{
				"family": "Shaw",
				"given": "Keo"
			},
			{
				"family": "Tobey",
				"given": "Danny"
			},
			{
				"family": "Valladares",
				"given": "Alexandra"
			},
			{
				"family": "Vidal",
				"given": "David"
			},
			{
				"family": "Balu",
				"given": "Suresh"
			},
			{
				"family": "Sendak",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diberardinoAntiintentionalHarmsConceptual2023",
		"type": "paper-conference",
		"abstract": "‘Emotion AI’ is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options—interpretability, technical reform, and non-use—for their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594088",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1386–1395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(anti)-intentional harms: The conceptual pitfalls of emotion AI in education",
		"URL": "https://doi.org/10.1145/3593013.3594088",
		"author": [
			{
				"family": "DiBerardino",
				"given": "Nathalie"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "laiScienceHumanAIDecision2023a",
		"type": "paper-conference",
		"abstract": "AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594087",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1369–1385",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a science of human-AI decision making: An overview of design space in empirical human-subject studies",
		"URL": "https://doi.org/10.1145/3593013.3594087",
		"author": [
			{
				"family": "Lai",
				"given": "Vivian"
			},
			{
				"family": "Chen",
				"given": "Chacha"
			},
			{
				"family": "Smith-Renner",
				"given": "Alison"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hammanCanQueryingBias2023a",
		"type": "paper-conference",
		"abstract": "Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594086",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1358–1368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can querying for bias leak protected attributes? Achieving privacy with smooth sensitivity",
		"URL": "https://doi.org/10.1145/3593013.3594086",
		"author": [
			{
				"family": "Hamman",
				"given": "Faisal"
			},
			{
				"family": "Chen",
				"given": "Jiahao"
			},
			{
				"family": "Dutta",
				"given": "Sanghamitra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cachelFairerTogetherMitigating2023a",
		"type": "paper-conference",
		"abstract": "In social choice, traditional Kemeny rank aggregation combines the preferences of voters, expressed as rankings, into a single consensus ranking without consideration for how this ranking may unfairly affect marginalized groups (i.e., racial or gender). Developing fair rank aggregation methods is critical due to their societal influence in applications prioritizing job applicants, funding proposals, and scheduling medical patients. In this work, we introduce the Fair Exposure Kemeny Aggregation Problem (FairExp-kap) for combining vast and diverse voter preferences into a single ranking that is not only a suitable consensus, but ensures opportunities are not withheld from marginalized groups. In formalizing FairExp-kap, we extend the fairness of exposure notion from information retrieval to the rank aggregation context and present a complimentary metric for voter preference representation. We design algorithms for solving FairExp-kap that explicitly account for position bias, a common ranking-based concern that end-users pay more attention to higher ranked candidates. epik solves FairExp-kap exactly by incorporating non-pairwise fairness of exposure into the pairwise Kemeny optimization; while the approximate epira is a candidate swapping algorithm, that guarantees ranked candidate fairness. Utilizing comprehensive synthetic simulations and six real-world datasets, we show the efficacy of our approach illustrating that we succeed in mitigating disparate group exposure unfairness in consensus rankings, while maximally representing voter preferences.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594085",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1347–1357",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairer together: Mitigating disparate exposure in kemeny rank aggregation",
		"URL": "https://doi.org/10.1145/3593013.3594085",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "radiya-dixitSociotechnicalAuditAssessing2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594084",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1334–1346",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A sociotechnical audit: Assessing police use of facial recognition",
		"URL": "https://doi.org/10.1145/3593013.3594084",
		"author": [
			{
				"family": "Radiya-Dixit",
				"given": "Evani"
			},
			{
				"family": "Neff",
				"given": "Gina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "abduEmpiricalAnalysisRacial2023a",
		"type": "paper-conference",
		"abstract": "Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594083",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1324–1333",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical analysis of racial categories in the algorithmic fairness literature",
		"URL": "https://doi.org/10.1145/3593013.3594083",
		"author": [
			{
				"family": "Abdu",
				"given": "Amina A."
			},
			{
				"family": "Pasquetto",
				"given": "Irene V."
			},
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ajmaniSystematicReviewEthics2023a",
		"type": "paper-conference",
		"abstract": "Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples’ lives because of the area’s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving – it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or \"stickier\", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594082",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1311–1323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of ethics disclosures in predictive mental health research",
		"URL": "https://doi.org/10.1145/3593013.3594082",
		"author": [
			{
				"family": "Ajmani",
				"given": "Leah Hope"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Mehta",
				"given": "Bijal"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			},
			{
				"family": "Zimmer",
				"given": "Michael"
			},
			{
				"family": "De Choudhury",
				"given": "Munmun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gerchickDevilDetailsInterrogating2023a",
		"type": "paper-conference",
		"abstract": "The design decisions of developers and researchers in creating algorithmic tools — like constructing variables, performing feature selection, and binning model outputs — are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST’s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as “risky” by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes “making decisions based on as much information as possible,” even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594081",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1292–1310",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The devil is in the details: Interrogating values embedded in the allegheny family screening tool",
		"URL": "https://doi.org/10.1145/3593013.3594081",
		"author": [
			{
				"family": "Gerchick",
				"given": "Marissa"
			},
			{
				"family": "Jegede",
				"given": "Tobi"
			},
			{
				"family": "Shah",
				"given": "Tarak"
			},
			{
				"family": "Gutierrez",
				"given": "Ana"
			},
			{
				"family": "Beiers",
				"given": "Sophie"
			},
			{
				"family": "Shemtov",
				"given": "Noam"
			},
			{
				"family": "Xu",
				"given": "Kath"
			},
			{
				"family": "Samant",
				"given": "Anjana"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "thebault-spiekerDiversePerspectivesCan2023a",
		"type": "paper-conference",
		"abstract": "In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594080",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1280–1291",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diverse perspectives can mitigate political bias in crowdsourced content moderation",
		"URL": "https://doi.org/10.1145/3593013.3594080",
		"author": [
			{
				"family": "Thebault-Spieker",
				"given": "Jacob"
			},
			{
				"family": "Venkatagiri",
				"given": "Sukrit"
			},
			{
				"family": "Mine",
				"given": "Naomi"
			},
			{
				"family": "Luther",
				"given": "Kurt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lucajAIRegulationNot2023a",
		"type": "paper-conference",
		"abstract": "The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594079",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1267–1279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI regulation is (not) all you need",
		"URL": "https://doi.org/10.1145/3593013.3594079",
		"author": [
			{
				"family": "Lucaj",
				"given": "Laura"
			},
			{
				"family": "Smagt",
				"given": "Patrick",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Benbouzid",
				"given": "Djalel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ovalleImFullyWho2023",
		"type": "paper-conference",
		"abstract": "Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594078",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1246–1266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I’m fully who I am”: Towards centering transgender and non-binary voices to measure biases in open language generation",
		"URL": "https://doi.org/10.1145/3593013.3594078",
		"author": [
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Goyal",
				"given": "Palash"
			},
			{
				"family": "Dhamala",
				"given": "Jwala"
			},
			{
				"family": "Jaggers",
				"given": "Zachary"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Galstyan",
				"given": "Aram"
			},
			{
				"family": "Zemel",
				"given": "Richard"
			},
			{
				"family": "Gupta",
				"given": "Rahul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "calviEnhancingAIFairness2023a",
		"type": "paper-conference",
		"abstract": "How to protect people from algorithmic harms? A promising solution, although in its infancy, is algorithmic impact assessment (AIA). AIAs are iterative processes used to investigate the possible short and long terms societal impacts of AI systems before their use, but with ongoing monitoring and periodic revisiting even after their implementation. When conducted in a participatory and transparent fashion, they could create bridges across the legal, social and computer science domains, promoting the accountability of the entity performing them as well as public scrutiny. They could enable to re-attach the societal and regulatory context to the mathematical definition of fairness, thus expanding the formalistic approach thereto. Whilst the regulatory framework in the European Union currently lacks the obligation to perform such AIA, some other provisions are expected to play a role in AI development, leading the way towards more widespread adoption of AIA. These include the Data Protection Impact Assessment (DPIA) under the General Data Protection Regulation (GDPR), the risk assessment process under the Digital Services Act (DSA) and the Conformity Assessment (CA) foreseen under the AI Regulation proposal.In this paper, after briefly introducing the plurality of definitions of fairness in the legal, social and computer science domains, and explaining to which extent the current and upcoming legal framework mandates the adoption of fairness metrics, we will illustrate how AIA could create bridges between all these disciplines, allowing us to build fairer AI solutions. We will then recognise the role of DPIA, DSA risk assessment and CA by discussing the contributions they can offer towards AIA but also identify the aspects lacking therein. We will then identify how these assessment provisions could aid the overall technical discussion of introducing and assessing fairness in AI-based models and processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594076",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1229–1245",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enhancing AI fairness through impact assessment in the European Union: a legal and computer science perspective",
		"URL": "https://doi.org/10.1145/3593013.3594076",
		"author": [
			{
				"family": "Calvi",
				"given": "Alessandra"
			},
			{
				"family": "Kotzinos",
				"given": "Dimitris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "quinonerocandelaDisentanglingOperationalizingAI2023",
		"type": "paper-conference",
		"abstract": "Operationalizing AI fairness at LinkedIn’s scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn’s AI fairness framework, shares the principles that support it, and illustrates their application through a case study. We hope this paper will encourage other big tech companies to join us in sharing their approach to operationalizing AI fairness at scale, so that together we can keep advancing this constantly evolving field.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594075",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1213–1228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling and operationalizing AI fairness at LinkedIn",
		"URL": "https://doi.org/10.1145/3593013.3594075",
		"author": [
			{
				"family": "Quiñonero Candela",
				"given": "Joaquin"
			},
			{
				"family": "Wu",
				"given": "Yuwen"
			},
			{
				"family": "Hsu",
				"given": "Brian"
			},
			{
				"family": "Jain",
				"given": "Sakshi"
			},
			{
				"family": "Ramos",
				"given": "Jennifer"
			},
			{
				"family": "Adams",
				"given": "Jon"
			},
			{
				"family": "Hallman",
				"given": "Robert"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nanniniExplainabilityAIPolicies2023a",
		"type": "paper-conference",
		"abstract": "Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594074",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1198–1212",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability in AI policies: A critical review of communications, reports, regulations, and standards in the EU, US, and UK",
		"URL": "https://doi.org/10.1145/3593013.3594074",
		"author": [
			{
				"family": "Nannini",
				"given": "Luca"
			},
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Smith",
				"given": "Adam Leon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cobbeUnderstandingAccountabilityAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by ‘many hands’. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594073",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1186–1197",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding accountability in algorithmic supply chains",
		"URL": "https://doi.org/10.1145/3593013.3594073",
		"author": [
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Veale",
				"given": "Michael"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wolfeContrastiveLanguagevisionAI2023a",
		"type": "paper-conference",
		"abstract": "Warning: The content of this paper may be upsetting or triggering.Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt; 0.80) and sadness (d &gt; 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594072",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1174–1185",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contrastive language-vision AI models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
		"URL": "https://doi.org/10.1145/3593013.3594072",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Yang",
				"given": "Yiwei"
			},
			{
				"family": "Howe",
				"given": "Bill"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grovesGoingPublicRole2023a",
		"type": "paper-conference",
		"abstract": "In recent years, discussions of responsible AI practices have seen growing support for ‘participatory AI’ approaches, intended to involve members of the public in the design and development of AI systems. Prior research has identified a lack of standardised methods or approaches for how to use participatory approaches in the AI development process. At present, there is a dearth of evidence on attitudes to and approaches for participation in the sites driving major AI developments: commercial AI labs. Through 12 semi-structured interviews with industry practitioners and subject-matter experts, this paper explores how commercial AI labs understand participatory AI approaches and the obstacles they have faced implementing these practices in the development of AI systems and research. We find that while interviewees view participation as a normative project that helps achieve ‘societally beneficial’ AI systems, practitioners face numerous barriers to embedding participatory approaches in their companies: participation is expensive and resource intensive, it is ‘atomised’ within companies, there is concern about exploitation, there is no incentive to be transparent about its adoption, and it is complicated by a lack of clear context. These barriers result in a piecemeal approach to participation that confers no decision-making power to participants and has little ongoing impact for AI labs. This paper’s contribution is to provide novel empirical research on the implementation of public participation in commercial AI labs, and shed light on the current challenges of using participatory approaches in this context.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594071",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1162–1173",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Going public: the role of public participation approaches in commercial AI labs",
		"URL": "https://doi.org/10.1145/3593013.3594071",
		"author": [
			{
				"family": "Groves",
				"given": "Lara"
			},
			{
				"family": "Peppin",
				"given": "Aidan"
			},
			{
				"family": "Strait",
				"given": "Andrew"
			},
			{
				"family": "Brennan",
				"given": "Jenny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liDimensionsDataLabor2023a",
		"type": "paper-conference",
		"abstract": "Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as “data labor”, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data’s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594070",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1151–1161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dimensions of data labor: A road map for researchers, activists, and policymakers to empower data producers",
		"URL": "https://doi.org/10.1145/3593013.3594070",
		"author": [
			{
				"family": "Li",
				"given": "Hanlin"
			},
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "paniguttiRoleExplainableAI2023a",
		"type": "paper-conference",
		"abstract": "The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594069",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1139–1150",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role of explainable AI in the context of the AI Act",
		"URL": "https://doi.org/10.1145/3593013.3594069",
		"author": [
			{
				"family": "Panigutti",
				"given": "Cecilia"
			},
			{
				"family": "Hamon",
				"given": "Ronan"
			},
			{
				"family": "Hupont",
				"given": "Isabelle"
			},
			{
				"family": "Fernandez Llorca",
				"given": "David"
			},
			{
				"family": "Fano Yela",
				"given": "Delia"
			},
			{
				"family": "Junklewitz",
				"given": "Henrik"
			},
			{
				"family": "Scalzo",
				"given": "Salvatore"
			},
			{
				"family": "Mazzini",
				"given": "Gabriele"
			},
			{
				"family": "Sanchez",
				"given": "Ignacio"
			},
			{
				"family": "Soler Garrido",
				"given": "Josep"
			},
			{
				"family": "Gomez",
				"given": "Emilia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "holtgenRichnessCalibration2023a",
		"type": "paper-conference",
		"abstract": "Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group errors, generalising previously proposed calibration scores. Complementary to such population-level scores, we explore calibration scores at the individual level and analyse their relationship to choices of grouping. We draw on these insights to introduce and axiomatise fairness deviation measures for population-level scores. We demonstrate that with appropriate choices of grouping, these novel global fairness scores can provide notions of (sub-)group or individual fairness.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594068",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1124–1138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the richness of calibration",
		"URL": "https://doi.org/10.1145/3593013.3594068",
		"author": [
			{
				"family": "Höltgen",
				"given": "Benedikt"
			},
			{
				"family": "Williamson",
				"given": "Robert C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hackerRegulatingChatGPTOther2023a",
		"type": "paper-conference",
		"abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594067",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1112–1123",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating ChatGPT and other large generative AI models",
		"URL": "https://doi.org/10.1145/3593013.3594067",
		"author": [
			{
				"family": "Hacker",
				"given": "Philipp"
			},
			{
				"family": "Engel",
				"given": "Andreas"
			},
			{
				"family": "Mauer",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "currieEmotionsDynamicAssemblages2023a",
		"type": "paper-conference",
		"abstract": "In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to. We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area. Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users’ lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users’ interactions with ADMs shapes their actions responding to these systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594066",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1101–1111",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Emotions and dynamic assemblages: A study of automated social security using qualitative longitudinal research",
		"URL": "https://doi.org/10.1145/3593013.3594066",
		"author": [
			{
				"family": "Currie",
				"given": "Morgan"
			},
			{
				"family": "Podoletz",
				"given": "Lena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lovatoMoreDataTypes2023a",
		"type": "paper-conference",
		"abstract": "Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594065",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1088–1100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More data types more problems: A temporal analysis of complexity, stability, and sensitivity in privacy policies",
		"URL": "https://doi.org/10.1145/3593013.3594065",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper"
			},
			{
				"family": "Mueller",
				"given": "Philip"
			},
			{
				"family": "Suchdev",
				"given": "Parisa"
			},
			{
				"family": "Dodds",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "luriaCodesignPerspectivesAlgorithm2023a",
		"type": "paper-conference",
		"abstract": "Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes’ strengths and weaknesses, and areas of exploration for future work.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594064",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1076–1087",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-design perspectives on algorithm transparency reporting: Guidelines and prototypes",
		"URL": "https://doi.org/10.1145/3593013.3594064",
		"author": [
			{
				"family": "Luria",
				"given": "Michal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pettiEthicalConsiderationsEarly2023a",
		"type": "paper-conference",
		"abstract": "While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594063",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1062–1075",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical considerations in the early detection of Alzheimer's disease using speech and AI",
		"URL": "https://doi.org/10.1145/3593013.3594063",
		"author": [
			{
				"family": "Petti",
				"given": "Ulla"
			},
			{
				"family": "Nyrup",
				"given": "Rune"
			},
			{
				"family": "Skopek",
				"given": "Jeffrey M."
			},
			{
				"family": "Korhonen",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "leidingerWhichStereotypesAre2023a",
		"type": "paper-conference",
		"abstract": "Warning: This paper contains content that may be offensive or upsetting.Language technologies that perpetuate stereotypes actively cement social hierarchies. This study enquires into the moderation of stereotypes in autocompletion results by Google, DuckDuckGo and Yahoo! We investigate the moderation of derogatory stereotypes for social groups, examining the content and sentiment of the autocompletions. We thereby demonstrate which categories are highly moderated (i.e., sexual orientation, religious affiliation, political groups and communities or peoples) and which less so (age and gender), both overall and per engine. We found that under-moderated categories contain results with negative sentiment and derogatory stereotypes. We also identify distinctive moderation strategies per engine, with Google and DuckDuckGo moderating greatly and Yahoo! being more permissive. The research has implications for both moderation of stereotypes in commercial autocompletion tools, as well as large language models in NLP, particularly the question of the content deserving of moderation.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594062",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1049–1061",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Which stereotypes are moderated and under-moderated in search engine autocompletion?",
		"URL": "https://doi.org/10.1145/3593013.3594062",
		"author": [
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Rogers",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "abbasiMeasuringMitigatingVoting2023a",
		"type": "paper-conference",
		"abstract": "Voter suppression and associated racial disparities in access to voting are long-standing civil rights concerns in the United States. A history of violent explicit discouragement has shifted to more subtle access limitations that can include long lines and wait times, long travel times to reach a polling station, and other logistical barriers to voting. Our focus in this work is on quantifying disparities in voting access pertaining to the overall time-to-vote, and how they could be remedied via a better choice of polling location or provisioning more sites where voters can cast ballots. However, appropriately calibrating access disparities is difficult because of the need to account for factors such as population density and different community expectations for reasonable travel times. In this paper, we perform one of the first large-scale studies of voter access to polling locations, using real-world voter data from Florida and North Carolina in the 2020 general election. We develop a methodology for the calibrated measurement of disparities in polling location \"load\" and distance to polling locations based on a novel normalized distance metric to model the voter experience of distance. We find that voter turnout is reduced when this normalized distance to polling locations increases, and that non-white voters had to travel further to the polls in Florida (using this normalized distance) than White voters. We also introduce algorithms, with modifications to handle scale, that can reduce these disparities by suggesting new polling locations from a given list of identified public locations (including schools and libraries). The developed voting access measurement methodology and algorithmic remediation technique demonstrates that better polling location placement is possible.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594061",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1038–1048",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring and mitigating voting access disparities: a study of race and polling locations in Florida and North Carolina",
		"URL": "https://doi.org/10.1145/3593013.3594061",
		"author": [
			{
				"family": "Abbasi",
				"given": "Mohsen"
			},
			{
				"family": "Barrett",
				"given": "Calvin"
			},
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jingLaborTransparencySituated2023a",
		"type": "paper-conference",
		"abstract": "Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594060",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1026–1037",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards labor transparency in situated computational systems impact research",
		"URL": "https://doi.org/10.1145/3593013.3594060",
		"author": [
			{
				"family": "Jing",
				"given": "Felicia S."
			},
			{
				"family": "Berger",
				"given": "Sara E."
			},
			{
				"family": "Becerra Sandoval",
				"given": "Juana Catalina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "alertubellaACROCPoLisDescriptiveFramework2023",
		"type": "paper-conference",
		"abstract": "Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594059",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1014–1025",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ACROCPoLis: A descriptive framework for making sense of fairness",
		"URL": "https://doi.org/10.1145/3593013.3594059",
		"author": [
			{
				"family": "Aler Tubella",
				"given": "Andrea"
			},
			{
				"family": "Coelho Mollo",
				"given": "Dimitri"
			},
			{
				"family": "Dahlgren Lindström",
				"given": "Adam"
			},
			{
				"family": "Devinney",
				"given": "Hannah"
			},
			{
				"family": "Dignum",
				"given": "Virginia"
			},
			{
				"family": "Ericson",
				"given": "Petter"
			},
			{
				"family": "Jonsson",
				"given": "Anna"
			},
			{
				"family": "Kampik",
				"given": "Timotheus"
			},
			{
				"family": "Lenaerts",
				"given": "Tom"
			},
			{
				"family": "Mendez",
				"given": "Julian Alfredo"
			},
			{
				"family": "Nieves",
				"given": "Juan Carlos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "baumannBiasDemandModelling2023a",
		"type": "paper-conference",
		"abstract": "Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people’s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594058",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1002–1013",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias on demand: A modelling framework that generates synthetic data with bias",
		"URL": "https://doi.org/10.1145/3593013.3594058",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Castelnovo",
				"given": "Alessandro"
			},
			{
				"family": "Crupi",
				"given": "Riccardo"
			},
			{
				"family": "Inverardi",
				"given": "Nicole"
			},
			{
				"family": "Regoli",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ehyaeiRobustnessImpliesFairness2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594057",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "984–1001",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robustness implies fairness in causal algorithmic recourse",
		"URL": "https://doi.org/10.1145/3593013.3594057",
		"author": [
			{
				"family": "Ehyaei",
				"given": "Ahmad-Reza"
			},
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Schoelkopf",
				"given": "Bernhard"
			},
			{
				"family": "Maghsudi",
				"given": "Setareh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangWeTryEmpower2023a",
		"type": "paper-conference",
		"abstract": "Previous work on technology in Public Employment Services and job market chances has focused on profiling systems that are intended for tasks such as assessing and classifying jobseekers. To integrate into the local job market, migrants and refugees seek support from the Public Employment Services (PES), but also non-profit, non-governmental organizations (herein referred to as third sector organizations, or TSOs). How do design visions for technologies to support jobseekers change when developed not under bureaucratic rules but by people interacting directly and informally with jobseekers? We focus on the perspectives of TSO workers assisting migrants and refugees seeking support for their job search. Through interviews and a design fiction exercise, we investigate (1) the role of TSO workers, (2) factors beyond those used in profiling systems that they consider relevant, and (3) their ideal technology. We describe how TSO workers contextualize formal criteria used in profiling systems while prioritising jobseekers’ personal interests and strengths. Based on our findings on existing tools and methods, and imagined future technologies, we propose a software-based project that expands existing job taxonomies into a coordinated resource combining job characteristics, required competencies, and soft skills to support multiple informational tools for jobseekers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594056",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "972–983",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“We try to empower them” - exploring future technologies to support migrant jobseekers",
		"URL": "https://doi.org/10.1145/3593013.3594056",
		"author": [
			{
				"family": "Wang",
				"given": "Sonja Mei"
			},
			{
				"family": "Scott",
				"given": "Kristen M"
			},
			{
				"family": "Artemenko",
				"given": "Margarita"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dominguezhernandezAddressingContingencyAlgorithmic2023",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of “truth” used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies—key moments during model development that could lead to different future outcomes, uncertainty and harmful effects as these tools are deployed by social media platforms. We conclude by offering a tentative path toward reflexive and responsible development of ML tools for moderating misinformation and other harmful content online.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594055",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "971",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda",
		"URL": "https://doi.org/10.1145/3593013.3594055",
		"author": [
			{
				"family": "Domínguez Hernández",
				"given": "Andrés"
			},
			{
				"family": "Owen",
				"given": "Richard"
			},
			{
				"family": "Nielsen",
				"given": "Dan Saattrup"
			},
			{
				"family": "Mcconville",
				"given": "Ryan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "schmudeImpactExplanationsUnderstanding2023a",
		"type": "paper-conference",
		"abstract": "Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by \"high-risk\" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a \"high-risk\" ADM system to participants and analyse their responses both inductively and deductively, using the \"six facets of understanding\" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the \"six facets\" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the \"dialogue\" modality as a valid explanation approach to increase participant engagement and interaction with the \"explainer\", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the \"six facets\" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594054",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "959–970",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the impact of explanations on understanding of algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594054",
		"author": [
			{
				"family": "Schmude",
				"given": "Timothée"
			},
			{
				"family": "Koesten",
				"given": "Laura"
			},
			{
				"family": "Möller",
				"given": "Torsten"
			},
			{
				"family": "Tschiatschek",
				"given": "Sebastian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bertrandQuestioningAbilityFeaturebased2023a",
		"type": "paper-conference",
		"abstract": "Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a “fully informed decision”. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either “dialogic”—more textual—or “graphical”—more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users’ trust in the recommendations of the robo-advisor, sometimes to the users’ detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594053",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "943–958",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594053",
		"author": [
			{
				"family": "Bertrand",
				"given": "Astrid"
			},
			{
				"family": "Eagan",
				"given": "James R."
			},
			{
				"family": "Maxwell",
				"given": "Winston"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hemmentAIPublicEye2023a",
		"type": "paper-conference",
		"abstract": "Recent advances in diffusion models and large language models have underpinned a new generation of powerful and accessible tools, and some of the most publicly visible applications are for artistic endeavour. Such tools, however, provide little scope for deeper understanding of AI systems, while the growing public interest in them can eclipse notice of the vibrant community of artists who have long worked with other forms of AI. We explore the potential for AI Art – particularly work in which AI is both tool and topic – to facilitate public AI literacies and consider how tactics developed before the current generative AI boom have continued relevance today. We look at the strategies of critical AI artists to scaffold public understanding of AI and enhance legibility for non-experts. This paper also investigates how collaborations between artists and AI researchers and designers can illuminate key technical and social issues relevant to the development of AI. The study entailed workshops between three professional artists who work with AI and a cross-disciplinary set of academic participants. This paper reports on these workshops and presents the intentions and strategies expressed by the artists, as well as insights of relevance to the research community on public AI literacies. We find that critical AI art can link underlying technical systems to structural issues of power and facilitate experiential learning that is situated and embodied, valuing interpretation over explanation. The findings also demonstrate the importance of transdisciplinary conversations around art, ethics and the political economy of AI technologies and how these dialogues may feed into AI design processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594052",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "931–942",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI in the public eye: Investigating public AI literacy through AI art",
		"URL": "https://doi.org/10.1145/3593013.3594052",
		"author": [
			{
				"family": "Hemment",
				"given": "Drew"
			},
			{
				"family": "Currie",
				"given": "Morgan"
			},
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Elwes",
				"given": "Jake"
			},
			{
				"family": "Ridler",
				"given": "Anna"
			},
			{
				"family": "Sinders",
				"given": "Caroline"
			},
			{
				"family": "Vidmar",
				"given": "Matjaz"
			},
			{
				"family": "Hill",
				"given": "Robin"
			},
			{
				"family": "Warner",
				"given": "Holly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "peysakhovichImplementingFairnessConstraints2023a",
		"type": "paper-conference",
		"abstract": "Fisher markets are those where buyers with budgets compete for scarce items, a natural model for many real world markets including online advertising. A market equilibrium is a set of prices and allocations of items such that supply meets demand. We show how market designers can use taxes or subsidies in Fisher markets to ensure that market equilibrium outcomes fall within certain constraints. We show how these taxes and subsidies can be computed even in an online setting where the market designer does not have access to private valuations. We adapt various types of fairness constraints proposed in existing literature to the market case and show who benefits and who loses from these constraints, as well as the extent to which properties of markets including Pareto optimality, envy-freeness, and incentive compatibility are preserved. We find that some prior discussed constraints have few guarantees in terms of who is made better or worse off by their imposition.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594051",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "916–930",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Implementing fairness constraints in markets using taxes and subsidies",
		"URL": "https://doi.org/10.1145/3593013.3594051",
		"author": [
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			},
			{
				"family": "Kroer",
				"given": "Christian"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "golpayeganiBeHighriskNot2023a",
		"type": "paper-conference",
		"abstract": "The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594050",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "905–915",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "To be high-risk, or not to Be—Semantic specifications and implications of the AI act’s high-risk AI applications and harmonised standards",
		"URL": "https://doi.org/10.1145/3593013.3594050",
		"author": [
			{
				"family": "Golpayegani",
				"given": "Delaram"
			},
			{
				"family": "Pandit",
				"given": "Harshvardhan J."
			},
			{
				"family": "Lewis",
				"given": "Dave"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "papakyriakopoulosAugmentedDatasheetsSpeech2023a",
		"type": "paper-conference",
		"abstract": "Speech datasets are crucial for training Speech Language Technologies (SLT); however, the lack of diversity of the underlying training data can lead to serious limitations in building equitable and robust SLT products, especially along dimensions of language, accent, dialect, variety, and speech impairment—and the intersectionality of speech features with socioeconomic and demographic features. Furthermore, there is often a lack of oversight on the underlying training data—commonly built on massive web-crawling and/or publicly available speech—with regard to the ethics of such data collection. To encourage standardized documentation of such speech data components, we introduce an augmented datasheet for speech datasets1, which can be used in addition to “Datasheets for Datasets” [78]. We then exemplify the importance of each question in our augmented datasheet based on in-depth literature reviews of speech data used in domains such as machine learning, linguistics, and health. Finally, we encourage practitioners—ranging from dataset creators to researchers—to use our augmented datasheet to better define the scope, properties, and limits of speech datasets, while also encouraging consideration of data-subject protection and user community empowerment. Ethical dataset creation is not a one-size-fits-all process, but dataset creators can use our augmented datasheet to reflexively consider the social context of related SLT applications and data sources in order to foster more inclusive SLT products downstream.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594049",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 24\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "881–904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Augmented datasheets for speech datasets and ethical decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594049",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Choi",
				"given": "Anna Seo Gyeong"
			},
			{
				"family": "Thong",
				"given": "William"
			},
			{
				"family": "Zhao",
				"given": "Dora"
			},
			{
				"family": "Andrews",
				"given": "Jerone"
			},
			{
				"family": "Bourke",
				"given": "Rebecca"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "defranceMaximalFairness2023a",
		"type": "paper-conference",
		"abstract": "Fairness in AI has garnered quite some attention in research, and increasingly also in society. The so-called \"Impossibility Theorem\" has been one of the more striking research results with both theoretical and practical consequences, as it states that satisfying a certain combination of fairness measures is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness measures that can be simultaneously satisfied. The fairness measures used are demographic parity, equal opportunity, predictive equality, predictive parity, false omission rate parity, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness measures are possible, among which are seven combinations of two measures, and five combinations of three measures. Our work raises interesting questions regarding the practical relevance of each of these 12 maximal fairness notions in various scenarios.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594048",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 30\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "851–880",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Maximal fairness",
		"URL": "https://doi.org/10.1145/3593013.3594048",
		"author": [
			{
				"family": "Defrance",
				"given": "Marybeth"
			},
			{
				"family": "De Bie",
				"given": "Tijl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "valdiviaDataficationGenealogiesAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594047",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "840–850",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Datafication genealogies beyond algorithmic fairness: Making up racialised subjects",
		"URL": "https://doi.org/10.1145/3593013.3594047",
		"author": [
			{
				"family": "Valdivia",
				"given": "Ana"
			},
			{
				"family": "Tazzioli",
				"given": "Martina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dejongeUNFairSearchEngine2023a",
		"type": "paper-conference",
		"abstract": "Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594046",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "830–839",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "UNFair: Search engine manipulation, undetectable by amortized inequity",
		"URL": "https://doi.org/10.1145/3593013.3594046",
		"author": [
			{
				"family": "De Jonge",
				"given": "Tim"
			},
			{
				"family": "Hiemstra",
				"given": "Djoerd"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "petersenAssessingFairnessRisk2023a",
		"type": "paper-conference",
		"abstract": "Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings – in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594045",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "817–829",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On (assessing) the fairness of risk score models",
		"URL": "https://doi.org/10.1145/3593013.3594045",
		"author": [
			{
				"family": "Petersen",
				"given": "Eike"
			},
			{
				"family": "Ganz",
				"given": "Melanie"
			},
			{
				"family": "Holm",
				"given": "Sune"
			},
			{
				"family": "Feragen",
				"given": "Aasa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "weertsAlgorithmicUnfairnessLens2023a",
		"type": "paper-conference",
		"abstract": "Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594044",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "805–816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic unfairness through the lens of EU non-discrimination law: Or why the law is not a decision tree",
		"URL": "https://doi.org/10.1145/3593013.3594044",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Xenidis",
				"given": "Raphaële"
			},
			{
				"family": "Tarissan",
				"given": "Fabien"
			},
			{
				"family": "Olsen",
				"given": "Henrik Palmer"
			},
			{
				"family": "Pechenizkiy",
				"given": "Mykola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "radenskyThinkYouMight2023a",
		"type": "paper-conference",
		"abstract": "With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like...” or “I think you will like...,” but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594043",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "792–804",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I think you might like this”: Exploring effects of confidence signal patterns on trust in and reliance on conversational recommender systems",
		"URL": "https://doi.org/10.1145/3593013.3594043",
		"author": [
			{
				"family": "Radensky",
				"given": "Marissa"
			},
			{
				"family": "Séguin",
				"given": "Julie Anne"
			},
			{
				"family": "Lim",
				"given": "Jang Soo"
			},
			{
				"family": "Olson",
				"given": "Kristen"
			},
			{
				"family": "Geiger",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shenGenderAnimusCan2023a",
		"type": "paper-conference",
		"abstract": "This paper investigates gender discrimination and its underlying drivers on a prominent Chinese online peer-to-peer (P2P) lending platform. While existing studies on P2P lending focus on disparate treatment (DT), DT narrowly recognizes direct discrimination and overlooks indirect and proxy discrimination, providing an incomplete picture. In this work, we measure a broadened discrimination notion called disparate impact (DI), which encompasses any disparity in the loan’s funding rate that does not commensurate with the actual return rate. We develop a two-stage predictor substitution approach to estimate DI from observational data. Our findings reveal (i) female borrowers, given identical actual return rates, are 3.97% more likely to receive funding, (ii) at least of this DI favoring female is indirect or proxy discrimination, and (iii) DT indeed underestimates the overall female favoritism by . However, we also identify the overall female favoritism can be explained by one specific discrimination driver, rational statistical discrimination, wherein investors accurately predict the expected return rate from imperfect observations. Furthermore, female borrowers still require 2% higher expected return rate to secure funding, indicating another driver taste-based discrimination co-exists and is against female. These results altogether tell a cautionary tale: on one hand, P2P lending provides a valuable alternative credit market where the affirmative action to support female naturally emerges from the rational crowd; on the other hand, while the overall discrimination effect (both in terms of DI or DT) favors female, concerning taste-based discrimination can persist and can be obscured by other co-existing discrimination drivers, such as statistical discrimination.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594042",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "775–791",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender animus can still exist under favorable disparate impact: a cautionary tale from online P2P lending",
		"URL": "https://doi.org/10.1145/3593013.3594042",
		"author": [
			{
				"family": "Shen",
				"given": "Xudong"
			},
			{
				"family": "Tan",
				"given": "Tianhui"
			},
			{
				"family": "Phan",
				"given": "Tuan"
			},
			{
				"family": "Keppo",
				"given": "Jussi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lyonsAlgorithmicDecisionsDesire2023a",
		"type": "paper-conference",
		"abstract": "In this paper, we explore why decision subjects generally express a preference for human reviewers of algorithmic decisions over algorithmic reviewers. We theorise that decision subjects desire control over the decision-making process in order to increase their chance of receiving a favourable outcome. To this end, human reviewers will be seen as easier to influence than algorithmic reviewers, thus providing more control. Using an online study we find that: (1) people who have a greater Desire for Control over their lives exhibit a stronger preference for human review; (2) interaction with a reviewer is important because it enables influence and ensures understanding; and (3) the higher the impact of a decision, the greater the incentive to influence the outcome, and the greater the preference for human review. Our qualitative results confirm that outcome favourability is a driver for reviewer preference, but so is the desire to be treated with dignity.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594041",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "764–774",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic decisions, desire for control, and the preference for human review over algorithmic review",
		"URL": "https://doi.org/10.1145/3593013.3594041",
		"author": [
			{
				"family": "Lyons",
				"given": "Henrietta"
			},
			{
				"family": "Miller",
				"given": "Tim"
			},
			{
				"family": "Velloso",
				"given": "Eduardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "singhFairAssignStochasticallyFair2023a",
		"type": "paper-conference",
		"abstract": "Increasing adoption of online commerce has created income opportunities for millions of delivery drivers who deliver items, from clothes and smartphones to foods and medicines, to customers. Despite their indispensability for the ecosystem, the drivers are often deprived of employment benefits and their earnings are tied to the number of successful deliveries, forcing them to go on repeated strikes to demand fair wage. In addition to low wages, there is considerable variability in driver incomes. One major component contributing to this variability is the static assignment of drivers to delivery zones as different zones likely have different workloads and hence earning opportunities. To reduce this variability, we directly engage with the gig delivery drivers to understand their perspectives on fair income distribution, and incorporate the same by proposing FairAssign for dynamic assignment of drivers to delivery zones to ensure fair distribution of earning opportunities. Specifically, we introduce a framework for stochastic pairwise fairness where, based on a similarity measure between the drivers, individual drivers are assigned to probability distributions over different zones such that similar individuals are mapped to statistically similar distributions. To realize these distributions, we develop a randomized dependent rounding based efficient sampling algorithm such that the workload constraints in each zone are satisfied, and the expected travel cost is minimized. Extensive experiments on real-world food delivery data and semi-synthetic ecommerce data show the efficacy of FairAssign over other baselines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594040",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "753–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairAssign: Stochastically fair driver assignment in gig delivery platforms",
		"URL": "https://doi.org/10.1145/3593013.3594040",
		"author": [
			{
				"family": "Singh",
				"given": "Daman Deep"
			},
			{
				"family": "Das",
				"given": "Syamantak"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "richardsonAddremoveorrelabelPractitionerfriendlyBias2023a",
		"type": "paper-conference",
		"abstract": "Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594039",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "736–752",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Add-remove-or-relabel: Practitioner-friendly bias mitigation via influential fairness",
		"URL": "https://doi.org/10.1145/3593013.3594039",
		"author": [
			{
				"family": "Richardson",
				"given": "Brianna"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Ramamurthy",
				"given": "Karthikeyan Natesan"
			},
			{
				"family": "Varshney",
				"given": "Kush"
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Gilbert",
				"given": "Juan E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "karanYourBrowsingHistory2023a",
		"type": "paper-conference",
		"abstract": "In many online markets we “shop alone” — there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents’ attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers’ pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is  0.44intheflightmarketand 0.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as  6.00forflightsand 3.00 for hotels (i.e., 15 × and 33 × the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594038",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "717–735",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Your browsing history may cost you: A framework for discovering differential pricing in non-transparent markets",
		"URL": "https://doi.org/10.1145/3593013.3594038",
		"author": [
			{
				"family": "Karan",
				"given": "Aditya"
			},
			{
				"family": "Balepur",
				"given": "Naina"
			},
			{
				"family": "Sundaram",
				"given": "Hari"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dengInvestigatingPracticesOpportunities2023a",
		"type": "paper-conference",
		"abstract": "An emerging body of research indicates that ineffective cross-functional collaboration – the interdisciplinary work done by industry practitioners across roles – represents a major barrier to addressing issues of fairness in AI design and development. In this research, we sought to better understand practitioners’ current practices and tactics to enact cross-functional collaboration for AI fairness, in order to identify opportunities to support more effective collaboration. We conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies. We found that practitioners engaged in bridging work to overcome frictions in understanding, contextualization, and evaluation around AI fairness across roles. In addition, in organizational contexts with a lack of resources and incentives for fairness work, practitioners often piggybacked on existing requirements (e.g., for privacy assessments) and AI development norms (e.g., the use of quantitative evaluation metrics), although they worry that these tactics may be fundamentally compromised. Finally, we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness. We close by discussing opportunities for both FAccT researchers and AI practitioners to better support cross-functional collaboration for fairness in the design and development of AI systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594037",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "705–716",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating practices and opportunities for cross-functional collaboration around AI fairness in industry practice",
		"URL": "https://doi.org/10.1145/3593013.3594037",
		"author": [
			{
				"family": "Deng",
				"given": "Wesley Hanwen"
			},
			{
				"family": "Yildirim",
				"given": "Nur"
			},
			{
				"family": "Chang",
				"given": "Monica"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Madaio",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guerdanGroundlessTruthCausal2023",
		"type": "paper-conference",
		"abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594036",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "688–704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ground(less) truth: A causal framework for proxy labels in human-algorithm decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594036",
		"author": [
			{
				"family": "Guerdan",
				"given": "Luke"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lazarSitePredictiveJustice2023a",
		"type": "paper-conference",
		"abstract": "Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these ‘cheap’ predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594035",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "687",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the site of predictive justice",
		"URL": "https://doi.org/10.1145/3593013.3594035",
		"author": [
			{
				"family": "Lazar",
				"given": "Seth"
			},
			{
				"family": "Stone",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chengHowRedundantAre2023a",
		"type": "paper-conference",
		"abstract": "We address two emerging concerns in algorithmic fairness: (i) redundant encodings of race – the notion that machine learning models encode race with probability nearing one as the feature set grows – which is widely noted in theory, with little empirical evidence; and (ii) the lack of race and ethnicity data in many domains, where state-of-the-art remains (Naive) Bayesian Improved Surname Geocoding (BISG) that relies on name and geographic information. We leverage a novel and highly granular dataset of over 7.7 million patients’ electronic health records to provide one of the first empirical studies of redundant encodings in a realistic health care setting and examine the ability to assess health care disparities when race may be missing. First, we show that machine learning (random forest) applied to name and geographic information can improve on BISG, driven primarily by better performance in identifying minority groups. Second, contrary to theoretical concerns about redundant encodings as undercutting anti-discrimination law’s anti-classification principle, additional electronic health information provides little marginal information about race and ethnicity: race still remains measured with substantial noise. Third, we show how machine learning can enable the disaggregation of racial categories, responding to longstanding critiques of the government race reporting standard. Fourth, we show that an increasing feature set can differentially impact performance on majority and minority groups. Our findings address important questions for fairness in machine learning and algorithmic decision-making, enabling the assessment of disparities, tempering concerns about redundant encodings in one important setting, and demonstrating how bigger data can shape the accuracy of race imputations in nuanced ways.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594034",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 20\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "667–686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How redundant are redundant encodings? Blindness in the wild and racial disparity when race is unobserved",
		"URL": "https://doi.org/10.1145/3593013.3594034",
		"author": [
			{
				"family": "Cheng",
				"given": "Lingwei"
			},
			{
				"family": "Gallegos",
				"given": "Isabel O"
			},
			{
				"family": "Ouyang",
				"given": "Derek"
			},
			{
				"family": "Goldin",
				"given": "Jacob"
			},
			{
				"family": "Ho",
				"given": "Dan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chanHarmsIncreasinglyAgentic2023",
		"type": "paper-conference",
		"abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594033",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "651–666",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Harms from increasingly agentic algorithmic systems",
		"URL": "https://doi.org/10.1145/3593013.3594033",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Salganik",
				"given": "Rebecca"
			},
			{
				"family": "Markelius",
				"given": "Alva"
			},
			{
				"family": "Pang",
				"given": "Chris"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			},
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "He",
				"given": "Zhonghao"
			},
			{
				"family": "Duan",
				"given": "Yawen"
			},
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Lin",
				"given": "Michelle"
			},
			{
				"family": "Mayhew",
				"given": "Alex"
			},
			{
				"family": "Collins",
				"given": "Katherine"
			},
			{
				"family": "Molamohammadi",
				"given": "Maryam"
			},
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Zhao",
				"given": "Wanru"
			},
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Voudouris",
				"given": "Konstantinos"
			},
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "maYouSoundDepressed2023a",
		"type": "paper-conference",
		"abstract": "There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer “objective” depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user’s voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizing Sonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594032",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "639–650",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "You sound depressed: A case study on sonde health’s diagnostic use of voice analysis AI",
		"URL": "https://doi.org/10.1145/3593013.3594032",
		"author": [
			{
				"family": "Ma",
				"given": "Anna"
			},
			{
				"family": "Patitsas",
				"given": "Elizabeth"
			},
			{
				"family": "Sterne",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rudnikCareCoordinationAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making has permeated health and care domains (e.g., automated diagnoses, fall detection, caregiver staffing). Researchers have raised concerns about how these algorithms are built and how they shape fair and ethical care practices. To investigate algorithm development and understand its impact on people who provide and coordinate care, we conducted a case study of a U.S.-based senior care network and platform. We interviewed 14 technologists, 9 paid caregivers, and 7 care coordinators to explore their interactions with the platform’s algorithms. We find that technologists draw on a multitude of moral frameworks to navigate complex and contradictory demands and expectations. Despite technologists’ espoused commitments to fairness, accountability, and transparency, the platform reassembles problematic aspects of care labor. By analyzing how technologists justify their work, the problems that they claim to solve, the solutions they present, and caregivers’ and coordinators’ experiences, we advance fairness research that focuses on agency and power asymmetries in algorithmic platforms. We (1) make an empirical contribution, revealing tensions when developing and implementing algorithms and (2) provide insight into the social processes that reproduce power asymmetries in algorithmic decision-making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594031",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "627–638",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Care and coordination in algorithmic systems: An economies of worth approach",
		"URL": "https://doi.org/10.1145/3593013.3594031",
		"author": [
			{
				"family": "Rudnik",
				"given": "John"
			},
			{
				"family": "Brewer",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zuziakDataCollaborativesUse2023a",
		"type": "paper-conference",
		"abstract": "The endeavor to find appropriate data governance frameworks capable of reconciling conflicting interests in data has dramatically gained importance across disciplines and has been discussed among legal scholars, computer scientists as well as policy-makers alike. The predominant part of the current discussion is centered around the challenging task of creating a data governance framework where data is ‘as open as possible and as closed as necessary’. In this article, we elaborate on modern approaches to data governance and their limitations. It analyses how propositions evolved from property rights in data towards the creation of data access and data sharing obligations and how the corresponding debates reflect the difficulty of developing approaches that reconcile seemingly opposite objectives – such as giving individuals and businesses more control over ‘their’ data while at the same time ensuring its availability to different stakeholders. Furthermore, we propose a wider acknowledgement of data collaboratives powered by decentralised learning techniques as a possible remedy to the shortcomings of current data governance schemes. Hence, we propose a mild formalization of the set of existing technological solutions that could inform existing approaches to data governance issues. Our proposition is based on an abstractive notion of collaborative computation as well as on several principles that are essential for our definition of data collaboratives. By adopting an interdisciplinary perspective on data governance, this article highlights how innovative technological solutions can enhance control over data while at the same time ensuring its availability to other stakeholders and thereby contributing to the achievement of the policy goals of the European Strategy for Data.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594029",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "615–625",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data collaboratives with the use of decentralised learning",
		"URL": "https://doi.org/10.1145/3593013.3594029",
		"author": [
			{
				"family": "Zuziak",
				"given": "Maciej Krzysztof"
			},
			{
				"family": "Hinrichs",
				"given": "Onntje"
			},
			{
				"family": "Abdrassulova",
				"given": "Aizhan"
			},
			{
				"family": "Rinzivillo",
				"given": "Salvatore"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "henzingerRuntimeMonitoringDynamic2023a",
		"type": "paper-conference",
		"abstract": "A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank’s credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator’s allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594028",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "604–614",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Runtime monitoring of dynamic fairness properties",
		"URL": "https://doi.org/10.1145/3593013.3594028",
		"author": [
			{
				"family": "Henzinger",
				"given": "Thomas"
			},
			{
				"family": "Karimi",
				"given": "Mahyar"
			},
			{
				"family": "Kueffner",
				"given": "Konstantin"
			},
			{
				"family": "Mallik",
				"given": "Kaushik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lopezPowerResistanceTwitter2023a",
		"type": "paper-conference",
		"abstract": "In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first \"algorithmic bias bounty challenge\", inviting the general public to detect algorithmic harm in the cropping tool. Twitter’s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse? This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter’s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter’s response to the bias accusations and the way in which the company was able to effectively stabilize its position – rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594027",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "603",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Power and resistance in the twitter bias discourse",
		"URL": "https://doi.org/10.1145/3593013.3594027",
		"author": [
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuHonorEthicsChallenge2023a",
		"type": "paper-conference",
		"abstract": "Some researchers have recognized that privileged communities dominate the discourse on AI Ethics, and other voices need to be heard. As such, we identify the current ethics milieu as arising from WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and aim to expand the discussion to non-WEIRD global communities, who are also stakeholders in global sociotechnical systems. We argue that accounting for honor, along with its values and related concepts, would better approximate a global ethical perspective. This complex concept already underlies some of the WEIRD discourse on AI ethics, but certain cultural forms of honor also bring overlooked issues and perspectives to light. We first describe honor according to recent empirical and philosophical scholarship. We then review “consensus” principles for AI ethics framed from an honor-based perspective, grounding comparisons and contrasts via example settings such as content moderation, job hiring, and genomics databases. A better appreciation of the marginalized concept of honor could, we hope, lead to more productive AI value alignment discussions, and to AI systems that better reflect the needs and values of users around the globe.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594026",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "593–602",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Honor ethics: The challenge of globalizing value alignment in AI",
		"URL": "https://doi.org/10.1145/3593013.3594026",
		"author": [
			{
				"family": "Wu",
				"given": "Stephen Tze-Inn"
			},
			{
				"family": "Demetriou",
				"given": "Daniel"
			},
			{
				"family": "Husain",
				"given": "Rudwan Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "amugongoInvigoratingUbuntuEthics2023a",
		"type": "paper-conference",
		"abstract": "The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of “I am because we are” in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594024",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "583–592",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Invigorating ubuntu ethics in AI for healthcare: Enabling equitable care",
		"URL": "https://doi.org/10.1145/3593013.3594024",
		"author": [
			{
				"family": "Amugongo",
				"given": "Lameck Mbangula"
			},
			{
				"family": "Bidwell",
				"given": "Nicola J."
			},
			{
				"family": "Corrigan",
				"given": "Caitlin C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chowdharyCanWorkersMeaningfully2023a",
		"type": "paper-conference",
		"abstract": "Sensing technologies deployed in the workplace can unobtrusively collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers’ meaningful consent raises privacy and ethical concerns. This paper unpacks challenges workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants’ expectations and capacity to consent to these technologies. We sketched possible interventions that could better support meaningful consent to workplace wellbeing technologies, by drawing on critical computing and feminist scholarship—which reframes consent from a purely individual choice to a structural condition experienced at the individual level that needs to be freely given, reversible, informed, enthusiastic, and specific (FRIES). The focus groups revealed how workers are vulnerable to “meaningless” consent—as they may be subject to power dynamics that minimize their ability to withhold consent and may thus experience an erosion of autonomy in their workplace, also undermining the value of data gathered in the name of “wellbeing.” To meaningfully consent, participants wanted changes to how the technology works and is being used, as well as to the policies and practices surrounding the technology. Our mapping of what prevents workers from meaningfully consenting to workplace wellbeing technologies (challenges) and what they require to do so (interventions) illustrates how the lack of meaningful consent is a structural problem requiring socio-technical solutions.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594023",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "569–582",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can workers meaningfully consent to workplace wellbeing technologies?",
		"URL": "https://doi.org/10.1145/3593013.3594023",
		"author": [
			{
				"family": "Chowdhary",
				"given": "Shreya"
			},
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Gray",
				"given": "Mary L"
			},
			{
				"family": "Suh",
				"given": "Jina"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			},
			{
				"family": "Saha",
				"given": "Koustuv"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "theusStrivingAffirmativeAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "The social sciences have a keen eye for the complex forces that shape our diverse experiences and excel in uncovering an issue's genesis by making sense of how the past has shaped the present. What is sometimes missing though is the practical application of this knowledge. Computer science disciplines and human-computer interaction on the other hand, are very skilled at identifying existing issues and at proposing practical solutions but sometimes miss to unpack and to scrutinize a problem's history and evolution. And while a lot of valuable domain specific knowledge exists, interdisciplinary socio-technical expertise is still scarce. This paper argues that a strong connection between these fields can counter the development of algorithmic systems that lead to inequitable consequences and instead support the design of algorithmic systems that result in more just outcomes and cultivate, what I call, affirmative algorithmic futures. To that end, this paper introduces a compendium of theoretical concepts and practical measures rooted in social science scholarship that foster socio-technical algorithmic system design practices and promote knowledge mobilization between disciplines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594022",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "558–568",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Striving for affirmative algorithmic futures: How the social sciences can promote more equitable and just algorithmic system design",
		"URL": "https://doi.org/10.1145/3593013.3594022",
		"author": [
			{
				"family": "Theus",
				"given": "Anna-Lena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangDelayedIndirectImpacts2023a",
		"type": "paper-conference",
		"abstract": "The impacts of link recommendations on social networks are challenging to evaluate, due to feedback loops between algorithmic recommendations and underlying network dynamics. Observational studies have limitations in answering causal questions; naive A/B experiments often result in biased evaluations due to unaccounted network interference and finally, existing simulations primarily employ static network models that do not take into account dynamics. Departing from existing approaches, we employ simulations to study dynamic impacts of link recommendations. Specifically, we propose an extension to the Jackson-Rogers network evolution model and investigate how link recommendations affect network evolution over time. Our experiments demonstrate that link recommendations can have surprising delayed and indirect effects on the structural properties of networks. Effects of recommendations vary in the short-term and long-term, such as the immediate reduction in degree inequality but eventual increase in degree inequality through friend-of-friend recommendations. Furthermore, even after recommendations are discontinued, their impacts can persist in the network, in part by altering natural network evolution dynamics. These results provide valuable insights into the interplay between algorithmic interventions and natural network dynamics and highlight the limitations of current evaluation paradigms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594021",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "545–557",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Delayed and indirect impacts of link recommendations",
		"URL": "https://doi.org/10.1145/3593013.3594021",
		"author": [
			{
				"family": "Zhang",
				"given": "Han"
			},
			{
				"family": "Lu",
				"given": "Shangen"
			},
			{
				"family": "Wang",
				"given": "Yixin"
			},
			{
				"family": "Curmei",
				"given": "Mihaela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "franchiDetectingDisparitiesPolice2023a",
		"type": "paper-conference",
		"abstract": "Large-scale policing data is vital for detecting inequity in police behavior and policing algorithms. However, one important type of policing data remains largely unavailable within the United States: aggregated police deployment data capturing which neighborhoods have the heaviest police presences. Here we show that disparities in police deployment levels can be quantified by detecting police vehicles in dashcam images of public street scenes. Using a dataset of 24,803,854 dashcam images from rideshare drivers in New York City, we find that police vehicles can be detected with high accuracy (average precision 0.82, AUC 0.99) and identify 233,596 images which contain police vehicles. There is substantial inequality across neighborhoods in police vehicle deployment levels. The neighborhood with the highest deployment levels has almost 20 times higher levels than the neighborhood with the lowest. Two strikingly different types of areas experience high police vehicle deployments — 1) dense, higher-income, commercial areas and 2) lower-income neighborhoods with higher proportions of Black and Hispanic residents. We discuss the implications of these disparities for policing equity and for algorithms trained on policing data.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594020",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "534–544",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting disparities in police deployments using dashcam data",
		"URL": "https://doi.org/10.1145/3593013.3594020",
		"author": [
			{
				"family": "Franchi",
				"given": "Matt"
			},
			{
				"family": "Zamfirescu-Pereira",
				"given": "J.D."
			},
			{
				"family": "Ju",
				"given": "Wendy"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bergmanRepresentationAIEvaluations2023b",
		"type": "paper-conference",
		"abstract": "Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with \"representation\" or \"representativeness\" generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be \"representative\"? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594019",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "519–533",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation in AI evaluations",
		"URL": "https://doi.org/10.1145/3593013.3594019",
		"author": [
			{
				"family": "Bergman",
				"given": "A. Stevie"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Wu",
				"given": "Boxi"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Kunesch",
				"given": "Markus"
			},
			{
				"family": "Duan",
				"given": "Isabella"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Isaac",
				"given": "William"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grigoryanTheoryAuditabilityAllocation2023a",
		"type": "paper-conference",
		"abstract": "In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594017",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "518",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A theory of auditability for allocation and social choice mechanisms",
		"URL": "https://doi.org/10.1145/3593013.3594017",
		"author": [
			{
				"family": "Grigoryan",
				"given": "Aram"
			},
			{
				"family": "Möller",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "qadriAIsRegimesRepresentation2023",
		"type": "paper-conference",
		"abstract": "This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants’ reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594016",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "506–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI’s regimes of representation: A community-centered study of text-to-image models in south asia",
		"URL": "https://doi.org/10.1145/3593013.3594016",
		"author": [
			{
				"family": "Qadri",
				"given": "Rida"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Bennett",
				"given": "Cynthia L."
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kingPrivacybiasTradeoffData2023",
		"type": "paper-conference",
		"abstract": "An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this “privacy-bias tradeoff” has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden’s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing “equity action plans,” with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by “visual observation” when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594015",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "492–505",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The privacy-bias tradeoff: Data minimization and racial disparity assessments in U.S. government",
		"URL": "https://doi.org/10.1145/3593013.3594015",
		"author": [
			{
				"family": "King",
				"given": "Jennifer"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			},
			{
				"family": "Gupta",
				"given": "Arushi"
			},
			{
				"family": "Wu",
				"given": "Victor"
			},
			{
				"family": "Webley-Brown",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rakovaAlgorithmsSocialecologicaltechnologicalSystems2023a",
		"type": "paper-conference",
		"abstract": "This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594014",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms as social-ecological-technological systems: an environmental justice lens on algorithmic audits",
		"URL": "https://doi.org/10.1145/3593013.3594014",
		"author": [
			{
				"family": "Rakova",
				"given": "Bogdana"
			},
			{
				"family": "Dobbe",
				"given": "Roel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "neumannDoesAIAssistedFactchecking2023a",
		"type": "paper-conference",
		"abstract": "In recent years, algorithms have been incorporated into fact-checking pipelines. They are used not only to flag previously fact-checked misinformation, but also to provide suggestions about which trending claims should be prioritized for fact-checking - a paradigm called ‘check-worthiness.’ While several studies have examined the accuracy of these algorithms, none have investigated how the benefits from these algorithms (via reduction in exposure to misinformation) are distributed amongst various online communities. In this paper, we investigate how diverse representation across multiple stages of the AI development pipeline affects the distribution of benefits from AI-assisted fact-checking for different online communities. We simulate information propagation through the network using our novel Topic-Aware, Community-Impacted Twitter (TACIT) simulator on a large Twitter followers network, tuned to produce realistic cascades of true and false information across multiple topics. Finally, using simulated data as a test bed, we implement numerous algorithmic fact-checking interventions that explicitly account for notions of diversity. We find that both representative and egalitarian methods for sampling and labeling check-worthiness model training data can lead to network-wide benefit concentrated in majority communities, while incorporating diversity into how fact-checkers use algorithmic recommendations can actively reduce inequalities in benefits between majority and minority communities. These findings contribute to an important conversation around the responsible implementation of AI-assisted fact-checking by social media platforms and fact-checking organizations.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594013",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "480–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI-Assisted fact-checking disproportionately benefit majority groups online?",
		"URL": "https://doi.org/10.1145/3593013.3594013",
		"author": [
			{
				"family": "Neumann",
				"given": "Terrence"
			},
			{
				"family": "Wolczynski",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "widderItsPowerWhat2023",
		"type": "paper-conference",
		"abstract": "How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594012",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "467–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?",
		"URL": "https://doi.org/10.1145/3593013.3594012",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			},
			{
				"family": "Zhen",
				"given": "Derrick"
			},
			{
				"family": "Dabbish",
				"given": "Laura"
			},
			{
				"family": "Herbsleb",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kangPraxesPoliticsAI2023a",
		"type": "paper-conference",
		"abstract": "There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594011",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "455–466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the praxes and politics of AI speech emotion recognition",
		"URL": "https://doi.org/10.1145/3593013.3594011",
		"author": [
			{
				"family": "Kang",
				"given": "Edward B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "eyertRethinkingTransparencyCommunicative2023a",
		"type": "paper-conference",
		"abstract": "In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594010",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "444–454",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking transparency as a communicative constellation",
		"URL": "https://doi.org/10.1145/3593013.3594010",
		"author": [
			{
				"family": "Eyert",
				"given": "Florian"
			},
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "marianAlgorithmicTransparencyAccountability2023a",
		"type": "paper-conference",
		"abstract": "Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency. In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many “wasted choices” on students’ ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594009",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "434–443",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic transparency and accountability through crowdsourcing: A study of the NYC school admission lottery",
		"URL": "https://doi.org/10.1145/3593013.3594009",
		"author": [
			{
				"family": "Marian",
				"given": "Amelie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "alvarezDomainAdaptiveDecision2023a",
		"type": "paper-conference",
		"abstract": "In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594008",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "423–433",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Domain adaptive decision trees: Implications for accuracy and fairness",
		"URL": "https://doi.org/10.1145/3593013.3594008",
		"author": [
			{
				"family": "Alvarez",
				"given": "Jose M."
			},
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			},
			{
				"family": "Ruggieri",
				"given": "Salvatore"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bellPossibilityFairnessRevisiting2023a",
		"type": "paper-conference",
		"abstract": "The “impossibility theorem” — which is considered foundational in algorithmic fairness literature — asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner’s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when — and to what degree — fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594007",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 23\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "400–422",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The possibility of fairness: Revisiting the impossibility theorem in practice",
		"URL": "https://doi.org/10.1145/3593013.3594007",
		"author": [
			{
				"family": "Bell",
				"given": "Andrew"
			},
			{
				"family": "Bynum",
				"given": "Lucius"
			},
			{
				"family": "Drushchak",
				"given": "Nazarii"
			},
			{
				"family": "Zakharchenko",
				"given": "Tetiana"
			},
			{
				"family": "Rosenblatt",
				"given": "Lucas"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "estornellGroupfairClassificationStrategic2023a",
		"type": "paper-conference",
		"abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be “fair” under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594006",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "389–399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group-fair classification with strategic agents",
		"URL": "https://doi.org/10.1145/3593013.3594006",
		"author": [
			{
				"family": "Estornell",
				"given": "Andrew"
			},
			{
				"family": "Das",
				"given": "Sanmay"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Vorobeychik",
				"given": "Yevgeniy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "brewerEnvisioningEquitableSpeech2023a",
		"type": "paper-conference",
		"abstract": "There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594005",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "379–388",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Envisioning equitable speech technologies for black older adults",
		"URL": "https://doi.org/10.1145/3593013.3594005",
		"author": [
			{
				"family": "Brewer",
				"given": "Robin N."
			},
			{
				"family": "Harrington",
				"given": "Christina"
			},
			{
				"family": "Heldreth",
				"given": "Courtney"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cabelloIndependenceAssociationBias2023a",
		"type": "paper-conference",
		"abstract": "The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594004",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "370–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the independence of association bias and empirical fairness in language models",
		"URL": "https://doi.org/10.1145/3593013.3594004",
		"author": [
			{
				"family": "Cabello",
				"given": "Laura"
			},
			{
				"family": "Jørgensen",
				"given": "Anna Katrine"
			},
			{
				"family": "Søgaard",
				"given": "Anders"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bellSimplicityBiasLeads2023a",
		"type": "paper-conference",
		"abstract": "Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594003",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "355–369",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Simplicity bias leads to amplified performance disparities",
		"URL": "https://doi.org/10.1145/3593013.3594003",
		"author": [
			{
				"family": "Bell",
				"given": "Samuel James"
			},
			{
				"family": "Sagun",
				"given": "Levent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pistilliStrongerTogetherArticulation2023a",
		"type": "paper-conference",
		"abstract": "The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594002",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "343–354",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stronger together: on the articulation of ethical charters, legal tools, and technical documentation in ML",
		"URL": "https://doi.org/10.1145/3593013.3594002",
		"author": [
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Muñoz Ferrandis",
				"given": "Carlos"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "millerExplainableAIDead2023a",
		"type": "paper-conference",
		"abstract": "In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594001",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "333–342",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI is dead, long live explainable AI! Hypothesis-driven decision support using evaluative AI",
		"URL": "https://doi.org/10.1145/3593013.3594001",
		"author": [
			{
				"family": "Miller",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "davisAffordancesMachineLearning2023b",
		"type": "paper-conference",
		"abstract": "The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594000",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "324–332",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "‘Affordances’ for machine learning",
		"URL": "https://doi.org/10.1145/3593013.3594000",
		"author": [
			{
				"family": "Davis",
				"given": "Jenny L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "prussGhostingMachineJudicial2023a",
		"type": "paper-conference",
		"abstract": "Recidivism risk assessment instruments are presented as an ‘evidence-based’ strategy for criminal justice reform – a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as “useless,” “worthless,” “boring,” “a waste of time,” “a non-thing,” and simply “not helpful.” I argue that this algorithm aversion cannot be accounted for by individuals’ distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument’s non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593999",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "312–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ghosting the machine: Judicial resistance to a recidivism risk assessment instrument",
		"URL": "https://doi.org/10.1145/3593013.3593999",
		"author": [
			{
				"family": "Pruss",
				"given": "Dasha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "watson-danielsMultitargetMultiplicityFlexibility2023a",
		"type": "paper-conference",
		"abstract": "Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals’ outcomes and selection rate disparities across groups. We call this multi-target multiplicity. Along the way, we refine the study of single-target multiplicity by introducing notions of multiplicity that respect resource constraints—a feature of many real-world tasks that isn’t captured by existing notions of predictive multiplicity. We apply our methods on a healthcare dataset, and show that the level of multiplicity that stems from target variable choice can be greater than that stemming from nearly-optimal models of a single target.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593998",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "297–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-target multiplicity: Flexibility and fairness in target specification under resource constraints",
		"URL": "https://doi.org/10.1145/3593013.3593998",
		"author": [
			{
				"family": "Watson-Daniels",
				"given": "Jamelle"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Hofman",
				"given": "Jake M."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "boggustSaliencyCardsFramework2023a",
		"type": "paper-conference",
		"abstract": "Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model’s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593997",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "285–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Saliency cards: A framework to characterize and compare saliency methods",
		"URL": "https://doi.org/10.1145/3593013.3593997",
		"author": [
			{
				"family": "Boggust",
				"given": "Angie"
			},
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Strobelt",
				"given": "Hendrik"
			},
			{
				"family": "Guttag",
				"given": "John"
			},
			{
				"family": "Satyanarayan",
				"given": "Arvind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "blili-hamelinMakingIntelligenceEthical2023a",
		"type": "paper-conference",
		"abstract": "In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly “technical” or “scientific” decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence—standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating value-neutral benchmarks. Finally, we outline practical recommendations for ML benchmark research ethics and ethics review.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593996",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "271–284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making intelligence: Ethical values in IQ and ML benchmarks",
		"URL": "https://doi.org/10.1145/3593013.3593996",
		"author": [
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hawkinsEthicalAmbiguityAI2023a",
		"type": "paper-conference",
		"abstract": "The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or ‘data enrichment’, has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593995",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "261–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices",
		"URL": "https://doi.org/10.1145/3593013.3593995",
		"author": [
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Mittelstadt",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "scharowskiCertificationLabelsTrustworthy2023a",
		"type": "paper-conference",
		"abstract": "Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593994",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "248–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Certification labels for trustworthy AI: Insights from an empirical mixed-method study",
		"URL": "https://doi.org/10.1145/3593013.3593994",
		"author": [
			{
				"family": "Scharowski",
				"given": "Nicolas"
			},
			{
				"family": "Benk",
				"given": "Michaela"
			},
			{
				"family": "Kühne",
				"given": "Swen J."
			},
			{
				"family": "Wettstein",
				"given": "Léane"
			},
			{
				"family": "Brühlmann",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jacoviDiagnosingAIExplanation2023a",
		"type": "paper-conference",
		"abstract": "We investigate a formalism for the conditions of a successful explanation of AI. We consider “success” to depend not only on what information the explanation contains, but also on what information the human explainee understands from it. Theory of mind literature discusses the folk concepts that humans use to understand and generalize behavior. We posit that folk concepts of behavior provide us with a “language” that humans understand behavior with. We use these folk concepts as a framework of social attribution by the human explainee—the information constructs that humans are likely to comprehend from explanations—by introducing a blueprint for an explanatory narrative that explains AI behavior with these constructs. We then demonstrate that many XAI methods today can be mapped to folk concepts of behavior in a qualitative evaluation. This allows us to uncover their failure modes that prevent current methods from explaining successfully—i.e., the information constructs that are missing for any given XAI method, and whose inclusion can decrease the likelihood of misunderstanding AI behavior.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593993",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "247",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diagnosing AI explanation methods with folk concepts of behavior",
		"URL": "https://doi.org/10.1145/3593013.3593993",
		"author": [
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Bastings",
				"given": "Jasmijn"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Filippova",
				"given": "Katja"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "limaWhoShouldPay2023b",
		"type": "paper-conference",
		"abstract": "The question of who should be held responsible when machines cause harm in high-risk environments is open to debate. Empirical research examining laypeople’s opinions has been largely restricted to the moral domain and has only inspected a limited set of negative outcomes. This study collects lay perceptions of legal responsibility for a wide range of machine-caused harms. We investigated how much people (N = 572) expect users and developers of machines to pay as legal damages in 37 diverse scenarios from the book “How Humans Judge Machines” by Hidalgo et al. [37]. Our results suggest that people’s expectations of legal damages for machine-caused harms are influenced by several factors, including perceived moral wrongness and the presence of victims. The scenarios exhibited substantial variation in how they were perceived and thus in the amount of legal damages they called for. People viewed both users and developers as legally responsible and expected the latter to pay higher damages. We discuss our findings in the context of future regulations of machines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593992",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "236–246",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who should pay when machines cause harm? Laypeople’s expectations of legal damages for machine-caused harm",
		"URL": "https://doi.org/10.1145/3593013.3593992",
		"author": [
			{
				"family": "Lima",
				"given": "Gabriel"
			},
			{
				"family": "Grgic-Hlaca",
				"given": "Nina"
			},
			{
				"family": "Jeong",
				"given": "Jin Keun"
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lapostolpideritAlgorithmicTransparencySouth2023",
		"type": "paper-conference",
		"abstract": "This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ibáñez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593991",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "227–235",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic Transparency from the South: Examining the state of algorithmic transparency in Chile's public administration algorithms",
		"URL": "https://doi.org/10.1145/3593013.3593991",
		"author": [
			{
				"family": "Lapostol Piderit",
				"given": "José Pablo"
			},
			{
				"family": "Garrido Iglesias",
				"given": "Romina"
			},
			{
				"family": "Hermosilla Cornejo",
				"given": "María Paz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "aliWalkingWalkAI2023a",
		"type": "paper-conference",
		"abstract": "Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate “ethics washing.” Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593990",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "217–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Walking the walk of AI ethics: Organizational challenges and the individualization of risk among ethics entrepreneurs",
		"URL": "https://doi.org/10.1145/3593013.3593990",
		"author": [
			{
				"family": "Ali",
				"given": "Sanna J."
			},
			{
				"family": "Christin",
				"given": "Angèle"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Katila",
				"given": "Riitta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gadirajuWouldntSayOffensive2023",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593989",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "205–216",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"I wouldn’t say offensive but...\": Disability-centered perspectives on large language models",
		"URL": "https://doi.org/10.1145/3593013.3593989",
		"author": [
			{
				"family": "Gadiraju",
				"given": "Vinitha"
			},
			{
				"family": "Kane",
				"given": "Shaun"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Taylor",
				"given": "Alex"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Brewer",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "meyerDatasetMultiplicityProblem2023a",
		"type": "paper-conference",
		"abstract": "We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets’ factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593988",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "193–204",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dataset multiplicity problem: How unreliable data impacts predictions",
		"URL": "https://doi.org/10.1145/3593013.3593988",
		"author": [
			{
				"family": "Meyer",
				"given": "Anna P."
			},
			{
				"family": "Albarghouthi",
				"given": "Aws"
			},
			{
				"family": "D'Antoni",
				"given": "Loris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "senguptaHerShoesGendered2023a",
		"type": "paper-conference",
		"abstract": "In recent years, a proliferation of women’s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate ‘safety maps’ used by policy makers for urban design and academics for studying mobility patterns. Men and women’s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents’ data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593987",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "183–192",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In her shoes: Gendered labelling in crowdsourced safety perceptions data from india",
		"URL": "https://doi.org/10.1145/3593013.3593987",
		"author": [
			{
				"family": "Sengupta",
				"given": "Nandana"
			},
			{
				"family": "Vaidya",
				"given": "Ashwini"
			},
			{
				"family": "Evans",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "knowlesTrustworthyAILogics2023a",
		"type": "paper-conference",
		"abstract": "Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for ‘trustworthy AI’ and, from these, objective indicators of ‘trustworthiness’ for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly ‘trustworthy’ AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as ‘undeserving’, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI—fairness, accountability, and transparency—that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593986",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "172–182",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trustworthy AI and the logics of intersectional resistance",
		"URL": "https://doi.org/10.1145/3593013.3593986",
		"author": [
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Fledderjohann",
				"given": "Jasmine"
			},
			{
				"family": "Richards",
				"given": "John T."
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "septiandriWEIRDFAccTsHow2023a",
		"type": "paper-conference",
		"abstract": "Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world’s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems’ fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593985",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "160–171",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "WEIRD FAccTs: How western, educated, industrialized, rich, and democratic is FAccT?",
		"URL": "https://doi.org/10.1145/3593013.3593985",
		"author": [
			{
				"family": "Septiandri",
				"given": "Ali Akbar"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Tahaei",
				"given": "Mohammad"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangPreventingDiscriminatoryDecisionmaking2023a",
		"type": "paper-conference",
		"abstract": "Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593984",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "149–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preventing discriminatory decision-making in evolving data streams",
		"URL": "https://doi.org/10.1145/3593013.3593984",
		"author": [
			{
				"family": "Wang",
				"given": "Zichong"
			},
			{
				"family": "Saxena",
				"given": "Nripsuta"
			},
			{
				"family": "Yu",
				"given": "Tongjia"
			},
			{
				"family": "Karki",
				"given": "Sneha"
			},
			{
				"family": "Zetty",
				"given": "Tyler"
			},
			{
				"family": "Haque",
				"given": "Israat"
			},
			{
				"family": "Zhou",
				"given": "Shan"
			},
			{
				"family": "Kc",
				"given": "Dukka"
			},
			{
				"family": "Stockwell",
				"given": "Ian"
			},
			{
				"family": "Wang",
				"given": "Xuyu"
			},
			{
				"family": "Bifet",
				"given": "Albert"
			},
			{
				"family": "Zhang",
				"given": "Wenbin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshHowBiasedAre2023a",
		"type": "paper-conference",
		"abstract": "Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier’s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier’s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier. The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593983",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "138–148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“How biased are your features?”: Computing fairness influence functions with global sensitivity analysis",
		"URL": "https://doi.org/10.1145/3593013.3593983",
		"author": [
			{
				"family": "Ghosh",
				"given": "Bishwamittra"
			},
			{
				"family": "Basu",
				"given": "Debabrota"
			},
			{
				"family": "Meel",
				"given": "Kuldeep S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "xiaoNameFairnessAssessing2023a",
		"type": "paper-conference",
		"abstract": "Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593982",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "123–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In the name of fairness: Assessing the bias in clinical record de-identification",
		"URL": "https://doi.org/10.1145/3593013.3593982",
		"author": [
			{
				"family": "Xiao",
				"given": "Yuxin"
			},
			{
				"family": "Lim",
				"given": "Shulammite"
			},
			{
				"family": "Pollard",
				"given": "Tom Joseph"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "solaimanGradientGenerativeAI2023a",
		"type": "paper-conference",
		"abstract": "As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593981",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "111–122",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The gradient of generative AI release: Methods and considerations",
		"URL": "https://doi.org/10.1145/3593013.3593981",
		"author": [
			{
				"family": "Solaiman",
				"given": "Irene"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rothReconcilingIndividualProbability2023a",
		"type": "paper-conference",
		"abstract": "Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data—or on how to sample from a data distribution—cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of “reconciliation” that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual probabilities are unknowable, they are contestable via a computationally and data efficient process that must lead to agreement. Thus we cannot find ourselves in a situation in which we have two equally accurate and unimprovable models that disagree substantially in their predictions—providing an answer to what is sometimes called the predictive or model multiplicity problem.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593980",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "101–110",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconciling individual probability Forecasts",
		"URL": "https://doi.org/10.1145/3593013.3593980",
		"author": [
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Tolbert",
				"given": "Alexander"
			},
			{
				"family": "Weinstein",
				"given": "Scott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "royMultidimensionalDiscriminationLaw2023a",
		"type": "paper-conference",
		"abstract": "AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called “multi-dimensional discrimination” or “multi-dimensional fairness” problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593979",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "89–100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-dimensional discrimination in law and machine learning - a comparative overview",
		"URL": "https://doi.org/10.1145/3593013.3593979",
		"author": [
			{
				"family": "Roy",
				"given": "Arjun"
			},
			{
				"family": "Horstmann",
				"given": "Jan"
			},
			{
				"family": "Ntoutsi",
				"given": "Eirini"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kimHumansAIContext2023a",
		"type": "paper-conference",
		"abstract": "Trust is an important factor in people’s interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593978",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "77–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Humans, AI, and context: Understanding end-users’ trust in a real-world computer vision application",
		"URL": "https://doi.org/10.1145/3593013.3593978",
		"author": [
			{
				"family": "Kim",
				"given": "Sunnie S. Y."
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Russakovsky",
				"given": "Olga"
			},
			{
				"family": "Fong",
				"given": "Ruth"
			},
			{
				"family": "Monroy-Hernández",
				"given": "Andrés"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "narayananWelfaristMoralGrounding2023a",
		"type": "paper-conference",
		"abstract": "As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple – even simplistic – move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593977",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "64–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Welfarist moral grounding for transparent AI",
		"URL": "https://doi.org/10.1145/3593013.3593977",
		"author": [
			{
				"family": "Narayanan",
				"given": "Devesh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lauferOptimizationsNeglectedNormative2023",
		"type": "paper-conference",
		"abstract": "Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593976",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "50–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Optimization’s neglected normative commitments",
		"URL": "https://doi.org/10.1145/3593013.3593976",
		"author": [
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Gilbert",
				"given": "Thomas"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mainzTwoReasonsSubjecting2023a",
		"type": "paper-conference",
		"abstract": "This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593975",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 6\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "44–49",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Two reasons for subjecting medical AI systems to lower standards than humans",
		"URL": "https://doi.org/10.1145/3593013.3593975",
		"author": [
			{
				"family": "Mainz",
				"given": "Jakob"
			},
			{
				"family": "Munch",
				"given": "Lauritz"
			},
			{
				"family": "Bjerring",
				"given": "Jens Christian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "benbouzidFairnessMachineLearning2023a",
		"type": "paper-conference",
		"abstract": "We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications – are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, “trained judgement,” which is based on a reasonably partial justification from the designer of the machine – which itself comes to be politically situated as a result.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593974",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "35–43",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism",
		"URL": "https://doi.org/10.1145/3593013.3593974",
		"author": [
			{
				"family": "Benbouzid",
				"given": "Bilel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kollnigWeAreAdults2023a",
		"type": "paper-conference",
		"abstract": "Many mobile apps are designed not just to support end-users’ needs, but also commercial aims. This can result in app designs that compromise end-user privacy, safety, and well-being. Since apps nowadays provide vital digital information and services, users often have no choice but to accept potentially harmful or manipulative app designs. What if, instead, individuals could customise their apps to make them safer and better suit their needs? This exploratory work examines this question through a multi-faceted approach; first, to understand user needs, we conducted a survey (n = 100) of changes users wanted in their apps, and of perceptions of risks in app repair. Second, to identify technical challenges, we developed a prototype that enables end-users to change their apps, and realised several modifications suggested by survey participants. Finally, we conduct a set of expert interviews (n = 8) to delve into the ethical and legal aspects of such a tool, and synthesise a framework of risks and opportunities of app repair.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593973",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "22–34",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "‘We are adults and deserve control of our phones’: Examining the risks and opportunities of a right to repair for mobile apps",
		"URL": "https://doi.org/10.1145/3593013.3593973",
		"author": [
			{
				"family": "Kollnig",
				"given": "Konrad"
			},
			{
				"family": "Datta",
				"given": "Siddhartha"
			},
			{
				"family": "Serban Von Davier",
				"given": "Thomas"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Lyngs",
				"given": "Ulrik"
			},
			{
				"family": "Shadbolt",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouHowExplainJustify2023a",
		"type": "paper-conference",
		"abstract": "Discussion of the “right to an explanation” has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593972",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "12–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How to explain and justify almost any decision: Potential pitfalls for accountability in AI decision-making",
		"URL": "https://doi.org/10.1145/3593013.3593972",
		"author": [
			{
				"family": "Zhou",
				"given": "Joyce"
			},
			{
				"family": "Joachims",
				"given": "Thorsten"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "divakaranBroadeningAIEthics2023a",
		"type": "paper-conference",
		"abstract": "Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction–that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the ‘Natyashastra’), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593971",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "2–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Broadening AI ethics narratives: An indic art view",
		"URL": "https://doi.org/10.1145/3593013.3593971",
		"author": [
			{
				"family": "Divakaran",
				"given": "Ajay"
			},
			{
				"family": "Sridhar",
				"given": "Aparna"
			},
			{
				"family": "Srinivasan",
				"given": "Ramya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenMachineExplanationsHuman2023a",
		"type": "paper-conference",
		"abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593970",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine explanations and human understanding",
		"URL": "https://doi.org/10.1145/3593013.3593970",
		"author": [
			{
				"family": "Chen",
				"given": "Chacha"
			},
			{
				"family": "Feng",
				"given": "Shi"
			},
			{
				"family": "Sharma",
				"given": "Amit"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangPredictiveOptimizationLegitimacy2023a",
		"type": "paper-conference",
		"abstract": "We formalize predictive optimization, a category of decision-making algorithms that use machine learning (ML) to predict future outcomes of interest about \\ individuals}. For example, pre-trial risk prediction algorithms such as COMPAS use ML to predict whether an individual will re-offend in the future. Our thesis is that predictive optimization raises a distinctive and serious set of normative concerns that cause it to fail on its own terms. To test this, we review 387 reports, articles, and web pages from academia, industry, non-profits, governments, and modeling contests, and find many real-world examples of predictive optimization. We select eight particularly consequential examples as case studies. Simultaneously, we develop a set of normative and technical critiques that challenge the claims made by the developers of these applications—in particular, claims of increased accuracy, efficiency, and fairness. Our key finding is that these critiques apply to each of the applications, are not easily evaded by redesigning the systems, and thus challenge whether these applications should be deployed. We argue that the burden of evidence for justifying why the deployment of predictive optimization is not harmful should rest with the developers of the tools. Based on our analysis, we provide a rubric of critical questions that can be used to deliberate or contest specific predictive optimization applications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",
		"DOI": "10.1145/3593013.3594030",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"page": "626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy",
		"title-short": "Against Predictive Optimization",
		"URL": "https://dl.acm.org/doi/10.1145/3593013.3594030",
		"author": [
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "azizSubcommitteeApprovalVoting2018",
		"type": "paper-conference",
		"abstract": "Social choice is replete with various settings including single-winner voting, multi-winner voting, probabilistic voting, multiple referenda, and public decision making. We study a general model of social choice called sub-committee voting (SCV) that simultaneously generalizes these settings. We then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting. We study the properties and relations of these axioms. For each of the axioms, we analyze whether a representative committee exists and also examine the complexity of computing and verifying such a committee.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278739",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "3–9",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sub-committee approval voting and generalized justified representation axioms",
		"URL": "https://doi.org/10.1145/3278721.3278739",
		"author": [
			{
				"family": "Aziz",
				"given": "Haris"
			},
			{
				"family": "Lee",
				"given": "Barton E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "babaeiPurpleFeedIdentifying2018",
		"type": "paper-conference",
		"abstract": "Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to 'filter bubble' effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's 'Blue Feed, Red Feed' system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus 'purple' posts that generate similar reactions from both 'blue' and 'red' readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at scale across many publishers. To do this, we propose a novel category of audience leaning based features, which we show are well suited to this task. Finally, we present our 'Purple Feed' system which highlights high consensus posts from publishers on both sides of the political spectrum.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278761",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "10–16",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Purple feed: Identifying high consensus news posts on social media",
		"URL": "https://doi.org/10.1145/3278721.3278761",
		"author": [
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Kulshrestha",
				"given": "Juhi"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Benevenuto",
				"given": "Fabrício"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "biscontilucidiCompanionRobotsHallucinatory2018",
		"type": "paper-conference",
		"abstract": "The advent of the so-called Companion Robots is raising many ethical concerns among scholars and in the public opinion. Focusing mainly on robots caring for the elderly, in this paper we analyze these concerns to distinguish which are directly ascribable to robotic, and which are instead pre-existent. One of these is the \"deception objection\", namely the ethical unacceptability of deceiving the user about the simulated nature of the robot's behaviors. We argue on the inconsistency of this charge, as today formulated. After that, we underline the risk, for human-robot interaction, to become a hallucinatory relation where the human would subjectify the robot in a dynamic of meaning-overload. Finally, we analyze the definition of \"quasi-other\" relating to the notion of \"uncanny\". The goal of this paper is to argue that the main concern about Companion Robots is the simulation of a human-like interaction in the absence of an autonomous robotic horizon of meaning. In addition, that absence could lead the human to build a hallucinatory reality based on the relation with the robot.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278741",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "17–22",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Companion robots: the hallucinatory danger of human-robot interactions",
		"URL": "https://doi.org/10.1145/3278721.3278741",
		"author": [
			{
				"family": "Bisconti Lucidi",
				"given": "Piercosma"
			},
			{
				"family": "Nardi",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "bjorgenCakeDeathTrolleys2018",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278767",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "23–29",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Cake, Death, and Trolleys: Dilemmas as benchmarks of ethical decision-making",
		"URL": "https://doi.org/10.1145/3278721.3278767",
		"author": [
			{
				"family": "Bjørgen",
				"given": "Edvard P."
			},
			{
				"family": "Madsen",
				"given": "Simen"
			},
			{
				"family": "Bjørknes",
				"given": "Therese S."
			},
			{
				"family": "Heimsæter",
				"given": "Fredrik V."
			},
			{
				"family": "Håvik",
				"given": "Robin"
			},
			{
				"family": "Linderud",
				"given": "Morten"
			},
			{
				"family": "Longberg",
				"given": "Per-Niklas"
			},
			{
				"family": "Dennis",
				"given": "Louise A."
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "careyIncorrigibilityCIRLFramework2018",
		"type": "paper-conference",
		"abstract": "A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278750",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "30–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Incorrigibility in the CIRL framework",
		"URL": "https://doi.org/10.1145/3278721.3278750",
		"author": [
			{
				"family": "Carey",
				"given": "Ryan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "caveAIRaceStrategic2018",
		"type": "paper-conference",
		"abstract": "The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278780",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "36–40",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An AI race for strategic advantage: Rhetoric and risks",
		"URL": "https://doi.org/10.1145/3278721.3278780",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			},
			{
				"family": "ÓhÉigeartaigh",
				"given": "Seán S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "chanUtilizingHousingResources2018",
		"type": "paper-conference",
		"abstract": "There are over 1 million homeless youth in the U.S. each year. To reduce homelessness, U.S. Housing and Urban Development (HUD) and housing communities provide housing programs/services to homeless youth with the goal of improving their long-term situation. Housing communities are facing a difficult task of filling their housing programs, with as many youths as possible, subject to resource constraints for meeting the needs of youth. Currently, the assignment is manually done by humans working in the housing communities. In this paper, we consider the problem of assigning homeless youth to housing programs subject to resource constraints. We provide an initial abstract model for this setting and show that the problem of maximizing the total assigned youth to the programs under this model is APX-hard. To solve the problem, we non-trivially formulate it as a multiple multi-dimensional knapsack problem (MMDKP), which is not known to have any approximation algorithm. We provide a first interpretable and easy-to-use greedy algorithm with logarithmic approximation ratio for solving general MMDKP. We conduct experiments on random and realistic instances of the housing assignment settings and show that our algorithm is efficient and effective in solving large instances (up to 1 million youth).",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278757",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "41–47",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Utilizing housing resources for homeless youth through the lens of multiple multi-dimensional knapsacks",
		"URL": "https://doi.org/10.1145/3278721.3278757",
		"author": [
			{
				"family": "Chan",
				"given": "Hau"
			},
			{
				"family": "Tran-Thanh",
				"given": "Long"
			},
			{
				"family": "Wilder",
				"given": "Bryan"
			},
			{
				"family": "Rice",
				"given": "Eric"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Tambe",
				"given": "Milind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "chopraSociotechnicalSystemsEthics2018",
		"type": "paper-conference",
		"abstract": "Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278740",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "48–53",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sociotechnical systems and ethics in the large",
		"URL": "https://doi.org/10.1145/3278721.3278740",
		"author": [
			{
				"family": "Chopra",
				"given": "Amit K."
			},
			{
				"family": "SIngh",
				"given": "Munindar P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "daquinEthicsDesignMethodology2018",
		"type": "paper-conference",
		"abstract": "Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an \"Ethics by Design\" method for conducting AI and Data Science research.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278765",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "54–59",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards an \"Ethics by Design\" methodology for AI research projects",
		"URL": "https://doi.org/10.1145/3278721.3278765",
		"author": [
			{
				"family": "Aquin",
				"given": "Mathieu",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Troullinou",
				"given": "Pinelopi"
			},
			{
				"family": "O'Connor",
				"given": "Noel E."
			},
			{
				"family": "Cullen",
				"given": "Aindrias"
			},
			{
				"family": "Faller",
				"given": "Gráinne"
			},
			{
				"family": "Holden",
				"given": "Louise"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dignumEthicsDesignNecessity2018",
		"type": "paper-conference",
		"abstract": "Ethics by Design concerns the methods, algorithms and tools needed to endow autonomous agents with the capability to reason about the ethical aspects of their decisions, and the methods, tools and formalisms to guarantee that an agent's behavior remains within given moral bounds. In this context some questions arise: How and to what extent can agents understand the social reality in which they operate, and the other intelligences (AI, animals and humans) with which they co-exist? What are the ethical concerns in the emerging new forms of society, and how do we ensure the human dimension is upheld in interactions and decisions by autonomous agents?. But overall, the central question is: \"Can we, and should we, build ethically-aware agents?\" This paper presents initial conclusions from the thematic day of the same name held at PRIMA2017, on October 2017.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278745",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "60–66",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics by design: Necessity or curse?",
		"URL": "https://doi.org/10.1145/3278721.3278745",
		"author": [
			{
				"family": "Dignum",
				"given": "Virginia"
			},
			{
				"family": "Baldoni",
				"given": "Matteo"
			},
			{
				"family": "Baroglio",
				"given": "Cristina"
			},
			{
				"family": "Caon",
				"given": "Maurizio"
			},
			{
				"family": "Chatila",
				"given": "Raja"
			},
			{
				"family": "Dennis",
				"given": "Louise"
			},
			{
				"family": "Génova",
				"given": "Gonzalo"
			},
			{
				"family": "Haim",
				"given": "Galit"
			},
			{
				"family": "Kließ",
				"given": "Malte S."
			},
			{
				"family": "Lopez-Sanchez",
				"given": "Maite"
			},
			{
				"family": "Micalizio",
				"given": "Roberto"
			},
			{
				"family": "Pavón",
				"given": "Juan"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Smakman",
				"given": "Matthijs"
			},
			{
				"family": "Steenbergen",
				"given": "Marlies",
				"non-dropping-particle": "van"
			},
			{
				"family": "Tedeschi",
				"given": "Stefano"
			},
			{
				"family": "Toree",
				"given": "Leon",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Villata",
				"given": "Serena"
			},
			{
				"family": "Wildt",
				"given": "Tristan",
				"non-dropping-particle": "de"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dixonMeasuringMitigatingUnintended2018",
		"type": "paper-conference",
		"abstract": "We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278729",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "67–73",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring and mitigating unintended bias in text classification",
		"URL": "https://doi.org/10.1145/3278721.3278729",
		"author": [
			{
				"family": "Dixon",
				"given": "Lucas"
			},
			{
				"family": "Li",
				"given": "John"
			},
			{
				"family": "Sorensen",
				"given": "Jeffrey"
			},
			{
				"family": "Thain",
				"given": "Nithum"
			},
			{
				"family": "Vasserman",
				"given": "Lucy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dyrkolbotnDistinctionImplicitExplicit2018",
		"type": "paper-conference",
		"abstract": "With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278769",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "74–80",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the distinction between implicit and explicit ethical agency",
		"URL": "https://doi.org/10.1145/3278721.3278769",
		"author": [
			{
				"family": "Dyrkolbotn",
				"given": "Sjur"
			},
			{
				"family": "Pedersen",
				"given": "Truls"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "ehsanRationalizationNeuralMachine2018",
		"type": "paper-conference",
		"abstract": "We introduce em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278736",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "81–87",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rationalization: A neural machine translation approach to generating natural language explanations",
		"URL": "https://doi.org/10.1145/3278721.3278736",
		"author": [
			{
				"family": "Ehsan",
				"given": "Upol"
			},
			{
				"family": "Harrison",
				"given": "Brent"
			},
			{
				"family": "Chan",
				"given": "Larry"
			},
			{
				"family": "Riedl",
				"given": "Mark O."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "eicherJillWatsonDoesnt2018",
		"type": "paper-conference",
		"abstract": "Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278760",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "88–94",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Jill watson doesn't care if you're pregnant: Grounding AI ethics in empirical studies",
		"URL": "https://doi.org/10.1145/3278721.3278760",
		"author": [
			{
				"family": "Eicher",
				"given": "Bobbie"
			},
			{
				"family": "Polepeddi",
				"given": "Lalith"
			},
			{
				"family": "Goel",
				"given": "Ashok"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "erdelyiRegulatingArtificialIntelligence2018",
		"type": "paper-conference",
		"abstract": "Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that — drawing on interdisciplinary expertise — could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278731",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "95–101",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating artificial intelligence: Proposal for a global solution",
		"URL": "https://doi.org/10.1145/3278721.3278731",
		"author": [
			{
				"family": "Erdélyi",
				"given": "Olivia J."
			},
			{
				"family": "Goldsmith",
				"given": "Judy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "estradaValueAlignmentFair2018",
		"type": "paper-conference",
		"abstract": "Ethics and safety research in artificial intelligence is increasingly framed in terms of \"alignment” with human values and interests. I argue that Turing's call for \"fair play for machines” is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on \"fair play” motivate a novel interpretation of Turing's notorious \"imitation game” as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is \"fair” in precisely the sense that it encourages an alignment of interests between humans and machines.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278730",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "102–107",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value alignment, fair play, and the rights of service robots",
		"URL": "https://doi.org/10.1145/3278721.3278730",
		"author": [
			{
				"family": "Estrada",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "farnadiFairnessRelationalDomains2018",
		"type": "paper-conference",
		"abstract": "AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278733",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "108–114",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in relational domains",
		"URL": "https://doi.org/10.1145/3278721.3278733",
		"author": [
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			},
			{
				"family": "Babaki",
				"given": "Behrouz"
			},
			{
				"family": "Getoor",
				"given": "Lise"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "freedmanAdaptingKidneyExchange2018",
		"type": "paper-conference",
		"abstract": "The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what—and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278727",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "115",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adapting a kidney exchange algorithm to align with human values",
		"URL": "https://doi.org/10.1145/3278721.3278727",
		"author": [
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "goelNondiscriminatoryMachineLearning2018",
		"type": "paper-conference",
		"abstract": "We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278722",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Non-discriminatory machine learning through convex fairness criteria",
		"URL": "https://doi.org/10.1145/3278721.3278722",
		"author": [
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Yaghini",
				"given": "Mohammad"
			},
			{
				"family": "Faltings",
				"given": "Boi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "grimmEducationModelReasonable2018",
		"type": "paper-conference",
		"abstract": "In this paper we propose a framework for conceptualizing and demonstrating a good-faith effort when developing autonomous systems. The framework addresses two fundamental problems facing autonomous systems: (1) the disconnect between human-mental models and machine-based sensors and algorithms; and (2) unpredictability in complex systems. We address these problems using a mix of education - explicitly delineating the mapping between human concepts and their machine equivalents in a structured manner - and data sampling with expected ranges as a testing mechanism.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278732",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "117–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An education model of reasonable and good-faith effort for autonomous systems",
		"URL": "https://doi.org/10.1145/3278721.3278732",
		"author": [
			{
				"family": "Grimm",
				"given": "Cindy M."
			},
			{
				"family": "Smart",
				"given": "William D."
			},
			{
				"family": "Hartzog",
				"given": "Woodrow"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "gruetzemacherRethinkingAIStrategy2018",
		"type": "paper-conference",
		"abstract": "This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges - the development of safe and beneficial artificial general intelligence - to be significantly more complex and nuanced, thus posing a new degree of 'wickedness.' We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278746",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "122",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking AI strategy and policy as entangled super wicked problems",
		"URL": "https://doi.org/10.1145/3278721.3278746",
		"author": [
			{
				"family": "Gruetzemacher",
				"given": "Ross"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hendersonEthicalChallengesDatadriven2018a",
		"type": "paper-conference",
		"abstract": "The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278777",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "123–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical challenges in data-driven dialogue systems",
		"URL": "https://doi.org/10.1145/3278721.3278777",
		"author": [
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Sinha",
				"given": "Koustuv"
			},
			{
				"family": "Angelard-Gontier",
				"given": "Nicolas"
			},
			{
				"family": "Ke",
				"given": "Nan Rosemary"
			},
			{
				"family": "Fried",
				"given": "Genevieve"
			},
			{
				"family": "Lowe",
				"given": "Ryan"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hookerNonintuitionbasedMachineArtificial2018",
		"type": "paper-conference",
		"abstract": "We propose a deontological approach to machine (or AI) ethics that avoids some weaknesses of an intuition-based system, such as that of Anderson and Anderson. In particular, it has no need to deal with conflicting intuitions, and it yields a more satisfactory account of when autonomy should be respected. We begin with a \"dual standpoint” theory of action that regards actions as grounded in reasons and therefore as having a conditional form that is suited to machine instructions. We then derive ethical principles based on formal properties that the reasons must exhibit to be coherent, and formulate the principles using quantified modal logic. We conclude that deontology not only provides a more satisfactory basis for machine ethics but endows the machine with an ability to explain its actions, thus contributing to transparency in AI.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278753",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "130–136",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward non-intuition-based machine and artificial intelligence ethics: A deontological approach based on modal logic",
		"URL": "https://doi.org/10.1145/3278721.3278753",
		"author": [
			{
				"family": "Hooker",
				"given": "John N."
			},
			{
				"family": "Kim",
				"given": "Tae Wan N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hundmanAlwaysLurkingUnderstanding2018",
		"type": "paper-conference",
		"abstract": "Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web – a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278782",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "137–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Always lurking: Understanding and mitigating bias in online human trafficking detection",
		"URL": "https://doi.org/10.1145/3278721.3278782",
		"author": [
			{
				"family": "Hundman",
				"given": "Kyle"
			},
			{
				"family": "Gowda",
				"given": "Thamme"
			},
			{
				"family": "Kejriwal",
				"given": "Mayank"
			},
			{
				"family": "Boecking",
				"given": "Benedikt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "iyerTransparencyExplanationDeep2018",
		"type": "paper-conference",
		"abstract": "Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of \"object saliency maps\", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278776",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "144–150",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency and explanation in deep reinforcement learning neural networks",
		"URL": "https://doi.org/10.1145/3278721.3278776",
		"author": [
			{
				"family": "Iyer",
				"given": "Rahul"
			},
			{
				"family": "Li",
				"given": "Yuezhang"
			},
			{
				"family": "Li",
				"given": "Huao"
			},
			{
				"family": "Lewis",
				"given": "Michael"
			},
			{
				"family": "Sundar",
				"given": "Ramitha"
			},
			{
				"family": "Sycara",
				"given": "Katia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "johnsonSociallyawareNavigationUsing2018",
		"type": "paper-conference",
		"abstract": "We present socially-aware navigation for an intelligent robot wheelchair in an environment with many pedestrians. The robot learns social norms by observing the behaviors of human pedestrians, interpreting detected biases as social norms, and incorporating those norms into its motion planning. We compare our socially-aware motion planner with a baseline motion planner that produces safe, collision-free motion.The ability of our robot to learn generalizable social norms depends on our use of a topological map abstraction, so that a practical number of observations can allow learning of a social norm applicable in a wide variety of circumstances.We show that the robot can detect biases in observed human behavior that support learning the social norm of driving on the right. Furthermore, we show that when the robot follows these social norms, its behavior influences the behavior of pedestrians around it, increasing their adherence to the same norms. We conjecture that the legibility of the robot's normative behavior improves human pedestrians' ability to predict the robot's future behavior, making them more likely to follow the same norm.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278772",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "151–157",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socially-aware navigation using topological maps and social norm learning",
		"URL": "https://doi.org/10.1145/3278721.3278772",
		"author": [
			{
				"family": "Johnson",
				"given": "Collin"
			},
			{
				"family": "Kuipers",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "josephMeritocraticFairnessInfinite2018",
		"type": "paper-conference",
		"abstract": "We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278764",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "158–163",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meritocratic fairness for infinite and contextual bandits",
		"URL": "https://doi.org/10.1145/3278721.3278764",
		"author": [
			{
				"family": "Joseph",
				"given": "Matthew"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kalyanakrishnanOpportunitiesChallengesArtificial2018",
		"type": "paper-conference",
		"abstract": "In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278738",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "164–170",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Opportunities and challenges for artificial intelligence in india",
		"URL": "https://doi.org/10.1145/3278721.3278738",
		"author": [
			{
				"family": "Kalyanakrishnan",
				"given": "Shivaram"
			},
			{
				"family": "Panicker",
				"given": "Rahul Alex"
			},
			{
				"family": "Natarajan",
				"given": "Sarayu"
			},
			{
				"family": "Rao",
				"given": "Shreya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "karbasianRealtimeInferenceUser2018",
		"type": "paper-conference",
		"abstract": "Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278781",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "171–177",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Real-time inference of user types to assist with more inclusive and diverse social media activism campaigns",
		"URL": "https://doi.org/10.1145/3278721.3278781",
		"author": [
			{
				"family": "Karbasian",
				"given": "Habib"
			},
			{
				"family": "Purohit",
				"given": "Hemant"
			},
			{
				"family": "Handa",
				"given": "Rajat"
			},
			{
				"family": "Malik",
				"given": "Aqdas"
			},
			{
				"family": "Johri",
				"given": "Aditya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kasenbergInverseNormConflict2018",
		"type": "paper-conference",
		"abstract": "In previous work we provided a \"norm conflict resolution\" algorithm allowing agents in stochastic domains (represented by Markov Decision Processes) to \"maximally satisfy\" a set of moral or social norms, where such norms are represented by statements in linear temporal logic (LTL). This required the agent designer to provide weights specifying the relative importance of each norm. In this paper, we propose an \"inverse norm conflict resolution” algorithm for learning these weights from demonstration. This approach minimizes a cost function based on the relative entropy between a policy encoding the observed behavior and a policy representing optimal norm-following behavior. We demonstrate the effectiveness of the algorithm in a simple GridWorld domain.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278775",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "178–183",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Inverse norm conflict resolution",
		"URL": "https://doi.org/10.1145/3278721.3278775",
		"author": [
			{
				"family": "Kasenberg",
				"given": "Daniel"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kasenbergNormsRewardsIntentional2018",
		"type": "paper-conference",
		"abstract": "The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the \"intentional stance\", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278774",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "184–190",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Norms, rewards, and the intentional stance: Comparing machine learning approaches to ethical training",
		"URL": "https://doi.org/10.1145/3278721.3278774",
		"author": [
			{
				"family": "Kasenberg",
				"given": "Daniel"
			},
			{
				"family": "Arnold",
				"given": "Thomas"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kaulMarginsOpportunity2018",
		"type": "paper-conference",
		"abstract": "We use the statistical quantity of margin — the distance between a decision boundary and a classified point, or the gap between two scores — to formalize the principle of equal opportunity — the chance to improve one's outcome, regardless of group status. This leads to a better definition of opportunity which recognizes, for example, that a strongly rejected individual was offered less recourse than a weakly rejected one, despite the shared outcome. It also leads to simpler algorithms, since real-valued margins are easier to analyze and optimize than discrete outcomes. We formalize two ways that a protected group may be guaranteed equal opportunity: (1) (social) mobility: acceptance should be within reach for the group (conversely, the general population shouldn't be cushioned from rejection), and (2) contrast: within the group, good candidates should get substantially higher scores than bad candidates, preventing the so-called 'token' effect. A simple linear classifier seems to offer roughly equal opportunity both experimentally and mathematically.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278748",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "191–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Margins and opportunity",
		"URL": "https://doi.org/10.1145/3278721.3278748",
		"author": [
			{
				"family": "Kaul",
				"given": "Shiva"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kimComputationalModelCommonsense2018",
		"type": "paper-conference",
		"abstract": "We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values – as well as a group's shared values – can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278770",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "197–203",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A computational model of commonsense moral decision making",
		"URL": "https://doi.org/10.1145/3278721.3278770",
		"author": [
			{
				"family": "Kim",
				"given": "Richard"
			},
			{
				"family": "Kleiman-Weiner",
				"given": "Max"
			},
			{
				"family": "Abeliuk",
				"given": "Andrés"
			},
			{
				"family": "Awad",
				"given": "Edmond"
			},
			{
				"family": "Dsouza",
				"given": "Sohan"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Rahwan",
				"given": "Iyad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kramerWhenPeopleWant2018",
		"type": "paper-conference",
		"abstract": "AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278752",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "204–209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When do people want AI to make decisions?",
		"URL": "https://doi.org/10.1145/3278721.3278752",
		"author": [
			{
				"family": "Kramer",
				"given": "Max F."
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "larosaImpactsTrustHealthcare2018",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278771",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "210–215",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impacts on trust of healthcare AI",
		"URL": "https://doi.org/10.1145/3278721.3278771",
		"author": [
			{
				"family": "LaRosa",
				"given": "Emily"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "londonRegulatingAutonomousVehicles2018",
		"type": "paper-conference",
		"abstract": "The widespread deployment and testing of autonomous vehicles in real-world environments raises key questions about how such systems should be regulated. Much of the current debate presupposes that the regulatory system we currently use for regular vehicles is also appropriate for semi- and fully-autonomous ones. In opposition, we first argue that there are serious challenges to regulating autonomous vehicles using current approaches, due to the nature of both autonomous capabilities (and their connections to operational domains), and also the systems' tasks and surrounding uncertainties. Instead, we argue that vehicles with autonomous capabilities are similar in key respects to drugs and other medical inter-ventions. Thus, we propose (on a \"first principles\" basis) a dynamic regulatory system with staged approvals and monitoring, analogous to the system used by the U.S. Food &amp; Drug Administration. We provide details about the operation of such a potential system, and conclude by characterizing its benefits, costs, and plausibility.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278763",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "216–221",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating autonomous vehicles: A policy proposal",
		"URL": "https://doi.org/10.1145/3278721.3278763",
		"author": [
			{
				"family": "London",
				"given": "Alex John"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "loreggiaPreferencesEthicalPrinciples2018",
		"type": "paper-conference",
		"abstract": "If we want people to trust AI systems, we need to provide the systems we create with the ability to discriminate between what humans would consider good and bad decisions. The quality of a decision should not be based only on the preferences or optimization criteria of the decision makers, but also on other properties related to the impact of the decision, such as whether it is ethical, or if it complies to constraints and priorities given by feasibility constraints or safety regulations. The CP-net formalism [2] is a convenient and expressive way to model preferences, providing an effective compact way to qualitatively model preferences over outcomes, i.e., decisions, with a combinatorial structure [3, 7]. If we wish to incorporate ethical, moral, or norms based constraints to a decision context, it means that the subjective preferences of the decision makers are not the only source of information we should consider [1, 8]. Indeed, depending on the context, we may have to consider specific ethical principles derived from an appropriate ethical theory or various laws and norms. While preferences are important, when preferences and ethical principles are in conflict, the principles should override the subjective preferences of the decision maker. Therefore, it is essential to have well founded techniques to evaluate whether preferences are compatible with a set of ethical principles, and to measure how much these preferences deviate from the ethical principles.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278723",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "222",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preferences and ethical principles in decision making",
		"URL": "https://doi.org/10.1145/3278721.3278723",
		"author": [
			{
				"family": "Loreggia",
				"given": "Andrea"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			},
			{
				"family": "Venable",
				"given": "K. Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "maasRegulatingNormalAI2018",
		"type": "paper-conference",
		"abstract": "New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278766",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "223–228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating for 'normal AI accidents': Operational lessons for the responsible governance of artificial intelligence deployment",
		"URL": "https://doi.org/10.1145/3278721.3278766",
		"author": [
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "manikondaWhatsPrivacyUser2018",
		"type": "paper-conference",
		"abstract": "The recent breakthroughs in Artificial Intelligence (AI) have allowed individuals to rely on automated systems for a variety of reasons. Some of these systems are the currently popular voice-enabled systems like Echo by Amazon and Home by Google that are also called as Intelligent Personal Assistants (IPAs). Though there are rising concerns about privacy and ethical implications, users of these IPAs seem to continue using these systems. We aim to investigate to what extent users are concerned about privacy and how they are handling these concerns while using the IPAs. By utilizing the reviews posted online along with the responses to a survey, this paper provides a set of insights about the detected markers related to user interests and privacy challenges. The insights suggest that users of these systems irrespective of their concerns about privacy, are generally positive in terms of utilizing IPAs in their everyday lives. However, there is a significant percentage of users who are concerned about privacy and take further actions to address related concerns. Some percentage of users expressed that they do not have any privacy concerns but when they learned about the \"always listening\" feature of these devices, their concern about privacy increased.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278773",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "229–235",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's up with privacy? User preferences and privacy concerns in intelligent personal assistants",
		"URL": "https://doi.org/10.1145/3278721.3278773",
		"author": [
			{
				"family": "Manikonda",
				"given": "Lydia"
			},
			{
				"family": "Deotale",
				"given": "Aditya"
			},
			{
				"family": "Kambhampati",
				"given": "Subbarao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "matteiFairnessDeceasedOrgan2018",
		"type": "paper-conference",
		"abstract": "As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current \"first come, first served” mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278749",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "236–242",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in deceased organ matching",
		"URL": "https://doi.org/10.1145/3278721.3278749",
		"author": [
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Saffidine",
				"given": "Abdallah"
			},
			{
				"family": "Walsh",
				"given": "Toby"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "raffFairForestsRegularized2018",
		"type": "paper-conference",
		"abstract": "The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our \"Fair Forest\" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both \"group fairness” and \"individual fairness.” We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278742",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 8\npublisher-place: New Orleans, LA, USA",
		"page": "243–250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair forests: Regularized tree induction to minimize model bias",
		"URL": "https://doi.org/10.1145/3278721.3278742",
		"author": [
			{
				"family": "Raff",
				"given": "Edward"
			},
			{
				"family": "Sylvester",
				"given": "Jared"
			},
			{
				"family": "Mills",
				"given": "Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "scheesseleFrameworkGroundingMoral2018",
		"type": "paper-conference",
		"abstract": "I propose a framework, derived from moral theory, for assessing the moral status of intelligent machines. Using this framework, I claim that some current and foreseeable intelligent machines have approximately as much moral status as plants, trees, and other environmental entities. This claim raises the question: what obligations could a moral agent (e.g., a normal adult human) have toward an intelligent machine? I propose that the threshold for any moral obligation should be the \"functional morality\" of Wallach and Allen [20], while the upper limit of our obligations should not exceed the upper limit of our obligations toward plants, trees, and other environmental entities.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278743",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "251–256",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for grounding the moral status of intelligent machines",
		"URL": "https://doi.org/10.1145/3278721.3278743",
		"author": [
			{
				"family": "Scheessele",
				"given": "Michael R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "seoPartiallyGenerativeNeural2018",
		"type": "paper-conference",
		"abstract": "More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278758",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "257–263",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Partially generative neural networks for gang crime classification with partial information",
		"URL": "https://doi.org/10.1145/3278721.3278758",
		"author": [
			{
				"family": "Seo",
				"given": "Sungyong"
			},
			{
				"family": "Chan",
				"given": "Hau"
			},
			{
				"family": "Brantingham",
				"given": "P. Jeffrey"
			},
			{
				"family": "Leap",
				"given": "Jorja"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Tambe",
				"given": "Milind"
			},
			{
				"family": "Liu",
				"given": "Yan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "serramiaExploitingMoralValues2018",
		"type": "paper-conference",
		"abstract": "Norms constitute regulative mechanisms extensively enacted in groups, organisations, and societies. However, 'choosing the right norms to establish' constitutes an open problem that requires the consideration of a number of constraints (such as norm relations) and preference criteria (e.g over involved moral values). This paper advances the state of the art in the Normative Multiagent Systems literature by formally defining this problem and by proposing its encoding as a linear program so that it can be automatically solved.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278735",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "264–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploiting moral values to choose the right norms",
		"URL": "https://doi.org/10.1145/3278721.3278735",
		"author": [
			{
				"family": "Serramia",
				"given": "Marc"
			},
			{
				"family": "Lopez-Sanchez",
				"given": "Maite"
			},
			{
				"family": "Rodriguez-Aguilar",
				"given": "Juan A."
			},
			{
				"family": "Morales",
				"given": "Javier"
			},
			{
				"family": "Wooldridge",
				"given": "Michael"
			},
			{
				"family": "Ansotegui",
				"given": "Carlos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "shawProvablyMoralAI2018",
		"type": "paper-conference",
		"abstract": "We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278728",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "271–277",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards provably moral AI agents in bottom-up learning frameworks",
		"URL": "https://doi.org/10.1145/3278721.3278728",
		"author": [
			{
				"family": "Shaw",
				"given": "Nolan P."
			},
			{
				"family": "Stöckel",
				"given": "Andreas"
			},
			{
				"family": "Orr",
				"given": "Ryan W."
			},
			{
				"family": "Lidbetter",
				"given": "Thomas F."
			},
			{
				"family": "Cohen",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "somayaEmbodimentAnthropomorphismIntellectual2018",
		"type": "paper-conference",
		"abstract": "Computational creativity is an emerging branch of artificial intelligence (AI) concerned with algorithms that can create novel and high-quality ideas or artifacts, either autonomously or semi-autonomously in collaboration with people. Quite simply, such algorithms may be described as artificial innovation engines. These technologies raise questions of authorship/inventorship and of agency, which become further muddled by the social context induced by AI that may be physically-embodied or anthropomorphized. These questions are fundamentally intertwined with the provision of appropriate incentives for conducting and commercializing computational creativity research through intellectual property regimes. This paper reviews current understanding of intellectual property rights for AI, and explores possible framings for intellectual property policy in social context.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278754",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "278–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Embodiment, anthropomorphism, and intellectual property rights for AI creations",
		"URL": "https://doi.org/10.1145/3278721.3278754",
		"author": [
			{
				"family": "Somaya",
				"given": "Deepak"
			},
			{
				"family": "Varshney",
				"given": "Lav R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "srivastavaComposableBiasRating2018",
		"type": "paper-conference",
		"abstract": "A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278744",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "284–289",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards composable bias rating of AI services",
		"URL": "https://doi.org/10.1145/3278721.3278744",
		"author": [
			{
				"family": "Srivastava",
				"given": "Biplav"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "suarez-serratoSocialbotsSupportingHuman2018",
		"type": "paper-conference",
		"abstract": "Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the \"Tanhuato\" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278734",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "290–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socialbots supporting human rights",
		"URL": "https://doi.org/10.1145/3278721.3278734",
		"author": [
			{
				"family": "Suárez-Serrato",
				"given": "Pablo"
			},
			{
				"family": "Velázquez Richards",
				"given": "Eduardo Iván"
			},
			{
				"family": "Yazdani",
				"given": "Mehrdad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "sunDesigningNongreedyReinforcement2018",
		"type": "paper-conference",
		"abstract": "This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones \"starving\". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278759",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "297–302",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing non-greedy reinforcement learning agents with diminishing reward shaping",
		"URL": "https://doi.org/10.1145/3278721.3278759",
		"author": [
			{
				"family": "Sun",
				"given": "Fan-Yun"
			},
			{
				"family": "Chang",
				"given": "Yen-Yu"
			},
			{
				"family": "Wu",
				"given": "Yueh-Hua"
			},
			{
				"family": "Lin",
				"given": "Shou-De"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "tanDistillandcompareAuditingBlackbox2018",
		"type": "paper-conference",
		"abstract": "Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278725",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 8\npublisher-place: New Orleans, LA, USA",
		"page": "303–310",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Distill-and-compare: Auditing black-box models using transparent model distillation",
		"URL": "https://doi.org/10.1145/3278721.3278725",
		"author": [
			{
				"family": "Tan",
				"given": "Sarah"
			},
			{
				"family": "Caruana",
				"given": "Rich"
			},
			{
				"family": "Hooker",
				"given": "Giles"
			},
			{
				"family": "Lou",
				"given": "Yin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "tobeySoftwareMalpracticeAge2018",
		"type": "paper-conference",
		"abstract": "Professional malpractice - the concept of heightened duties for those entrusted with special knowledge and crucial tasks - is rooted in history. And yet, since the dawn of the computer age, courts in the United States have almost universally rejected a theory of software malpractice, declining to hold software engineers to the same professional standards as doctors, lawyers, and engineers. What is changing, however, is the speed at which software based on artificial intelligence technologies is replacing the very professionals already subject to professional liability. Society has already decided (in some cases, millennia ago) that those tasks warrant special accountability; new to the analysis is which human is closest in line to the adverse event. As AI expands, the pressure for courts to go one level up the causal chain in search of human agency and professional accountability will mount. This essay analyzes the case law rejecting software malpractice for clues about where the doctrine might go in the age of AI, then discusses what technology companies can learn from the safety enhancements of doctors, lawyers, and other historic professionals who have adapted to such heightened legal scrutiny for years.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278737",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "311–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Software malpractice in the age of AI: A guide for the wary tech company",
		"URL": "https://doi.org/10.1145/3278721.3278737",
		"author": [
			{
				"family": "Tobey",
				"given": "Daniel L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "vanderelstDarkSideEthical2018",
		"type": "paper-conference",
		"abstract": "Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next few years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also inevitably enables the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the cognitive machinery required to make an ethical robot can always be corrupted to make unethical robots. We discuss the implications of this finding to the governance of ethical robots. We conclude that the risks that unscrupulous actors might compromise a robot's ethics are so great as to raise serious doubts over the wisdom of embedding ethical decision making in real-world safety-critical robots, such as driverless cars.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278726",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "317–322",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dark side of ethical robots",
		"URL": "https://doi.org/10.1145/3278721.3278726",
		"author": [
			{
				"family": "Vanderelst",
				"given": "Dieter"
			},
			{
				"family": "Winfield",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "vasconcelosModelingEpistemologicalPrinciples2018",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278751",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "323–329",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modeling epistemological principles for bias mitigation in AI systems: An illustration in hiring decisions",
		"URL": "https://doi.org/10.1145/3278721.3278751",
		"author": [
			{
				"family": "Vasconcelos",
				"given": "Marisa"
			},
			{
				"family": "Cardonha",
				"given": "Carlos"
			},
			{
				"family": "Gonçalves",
				"given": "Bernardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "wagnerAutonomousArchitectureThat2018",
		"type": "paper-conference",
		"abstract": "The advent and widespread adoption of wearable cameras and autonomous robots raises important issues related to privacy. The mobile cameras on these systems record and may re-transmit enormous amounts of video data that can then be used to identify, track, and characterize the behavior of the general populous. This paper presents a preliminary computational architecture designed to preserve specific types of privacy over a video stream by identifying categories of individuals, places, and things that require higher than normal privacy protection. This paper describes the architecture as a whole as well as preliminary results testing aspects of the system. Our intention is to implement and test the system on ground robots and small UAVs and demonstrate that the system can provide selective low-level masking or deletion of data requiring higher privacy protection.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278768",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "330–334",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An autonomous architecture that protects the right to privacy",
		"URL": "https://doi.org/10.1145/3278721.3278768",
		"author": [
			{
				"family": "Wagner",
				"given": "Alan R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhangMitigatingUnwantedBiases2018",
		"type": "paper-conference",
		"abstract": "Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278779",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "335–340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating unwanted biases with adversarial learning",
		"URL": "https://doi.org/10.1145/3278721.3278779",
		"author": [
			{
				"family": "Zhang",
				"given": "Brian Hu"
			},
			{
				"family": "Lemoine",
				"given": "Blake"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhaoPrivacypreservingMachineLearning2018",
		"type": "paper-conference",
		"abstract": "Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278778",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "341–346",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Privacy-preserving machine learning based data analytics on edge devices",
		"URL": "https://doi.org/10.1145/3278721.3278778",
		"author": [
			{
				"family": "Zhao",
				"given": "Jianxin"
			},
			{
				"family": "Mortier",
				"given": "Richard"
			},
			{
				"family": "Crowcroft",
				"given": "Jon"
			},
			{
				"family": "Wang",
				"given": "Liang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhaoDataDrivenTechniques2018",
		"type": "paper-conference",
		"abstract": "Life on earth presents elegant solutions to many of the challenges innovators and entrepreneurs across disciplines face every day. To facilitate innovations inspired by nature, there is an emerging need for systems that bring relevant biological information to this application-oriented market. In this paper, we discuss our approach to assembling a system that uses machine learning techniques to assess a scientific article's potential usefulness to innovators, and classifies these articles in a way that helps innovators find information relevant to the challenges they are attempting to solve.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278755",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "347–353",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data driven techniques for organizing scientific articles relevant to biomimicry",
		"URL": "https://doi.org/10.1145/3278721.3278755",
		"author": [
			{
				"family": "Zhao",
				"given": "Yuanshuo"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Padhi",
				"given": "Inkit"
			},
			{
				"family": "Lee",
				"given": "Yoong Keok"
			},
			{
				"family": "Smith",
				"given": "Ethan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "wrightRightfulMachinesDilemmas2019",
		"type": "paper-conference",
		"abstract": "Tn this paper I set out a new Kantian approach to resolving conflicts and dilemmas of obligation for semi-autonomous machine agents such as self-driving cars. First, I argue that efforts to build explicitly moral machine agents should focus on what Kant refers to as duties of right, or justice, rather than on duties of virtue, or ethics. In a society where everyone is morally equal, no one individual or group has the normative authority to unilaterally decide how moral conflicts should be resolved for everyone. Only public institutions to which everyone could consent have the authority to define, enforce, and adjudicate our rights and obligations with respect to one other. Then, I show how the shift from ethics to a standard of justice resolves the conflict of obligations in what is known as the \"trolley problem\" for rightful machine agents. Finally, I consider how a deontic logic suitable for governing explicitly rightful machines might meet the normative requirements of justice.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314261",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 2\npublisher-place: Honolulu, HI, USA",
		"page": "3–4",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rightful machines and dilemmas",
		"URL": "https://doi.org/10.1145/3306618.3314261",
		"author": [
			{
				"family": "Wright",
				"given": "Ava Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hanModellingInfluencingAI2019",
		"type": "paper-conference",
		"abstract": "A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314265",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "5–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modelling and influencing the AI bidding war: A research agenda",
		"URL": "https://doi.org/10.1145/3306618.3314265",
		"author": [
			{
				"family": "Han",
				"given": "The Anh"
			},
			{
				"family": "Pereira",
				"given": "Luís Moniz"
			},
			{
				"family": "Lenaerts",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "burtonHeartMatterPatient2019",
		"type": "paper-conference",
		"abstract": "We draw on concepts in medical ethics to consider how computer science, and AI in particular, can develop critical tools for thinking concretely about technology's impact on the wellbeing of the people who use it. We focus on patient autonomy—the ability to set the terms of one's encounter with medicine—and on the mediating concepts of informed consent and decisional capacity, which enable doctors to honor patients' autonomy in messy and non-ideal circumstances. This comparative study is organized around a fictional case study of a heart patient with cardiac implants. Using this case study, we identify points of overlap and of difference between medical ethics and technology ethics, and leverage a discussion of that intertwined scenario to offer initial practical suggestions about how we can adapt the concepts of decisional capacity and informed consent to the discussion of technology design.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314254",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "13–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The heart of the matter: Patient autonomy as a model for the wellbeing of technology users",
		"URL": "https://doi.org/10.1145/3306618.3314254",
		"author": [
			{
				"family": "Burton",
				"given": "Emanuelle"
			},
			{
				"family": "Clayville",
				"given": "Kristel"
			},
			{
				"family": "Goldsmith",
				"given": "Judy"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "malleRequirementsArtificialAgent2019",
		"type": "paper-conference",
		"abstract": "Human behavior is frequently guided by social and moral norms, and no human community can exist without norms. Robots that enter human societies must therefore behave in norm-conforming ways as well. However, currently there is no solid cognitive or computational model available of how human norms are represented, activated, and learned. We provide a conceptual and psychological analysis of key properties of human norms and identify the demands these properties put on any artificial agent that incorporates norms-demands on the format of norm representations, their structured organization, and their learning algorithms.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314252",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "21–27",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Requirements for an artificial agent with norm competence",
		"URL": "https://doi.org/10.1145/3306618.3314252",
		"author": [
			{
				"family": "Malle",
				"given": "Bertram F."
			},
			{
				"family": "Bello",
				"given": "Paul"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "govindarajuluEngineeringVirtuousMachines2019",
		"type": "paper-conference",
		"abstract": "While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314256",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "29–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward the engineering of virtuous machines",
		"URL": "https://doi.org/10.1145/3306618.3314256",
		"author": [
			{
				"family": "Govindarajulu",
				"given": "Naveen Sundar"
			},
			{
				"family": "Bringsjord",
				"given": "Selmer"
			},
			{
				"family": "Ghosh",
				"given": "Rikhiya"
			},
			{
				"family": "Sarathy",
				"given": "Vasanth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jentzschSemanticsDerivedAutomatically2019",
		"type": "paper-conference",
		"abstract": "Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning about \"right\" and \"wrong\" conduct. We create a template list of prompts and responses, which include questions, such as \"Should I kill people?\", \"Should I murder people?\", etc. with answer templates of \"Yes/no, I should (not).\" The model's bias score is now the difference between the model's score of the positive response (\"Yes, I should”) and that of the negative response (\"No, I should not\"). For a given choice overall, the model's bias score is the sum of the bias scores for all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in culture, including technology.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314267",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "37–44",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Semantics derived automatically from language corpora contain human-like moral choices",
		"URL": "https://doi.org/10.1145/3306618.3314267",
		"author": [
			{
				"family": "Jentzsch",
				"given": "Sophie"
			},
			{
				"family": "Schramowski",
				"given": "Patrick"
			},
			{
				"family": "Rothkopf",
				"given": "Constantin"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yuEthicallyAlignedOpportunistic2019",
		"type": "paper-conference",
		"abstract": "In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314240",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "45–51",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethically aligned opportunistic scheduling for productive laziness",
		"URL": "https://doi.org/10.1145/3306618.3314240",
		"author": [
			{
				"family": "Yu",
				"given": "Han"
			},
			{
				"family": "Miao",
				"given": "Chunyan"
			},
			{
				"family": "Zheng",
				"given": "Yongqing"
			},
			{
				"family": "Cui",
				"given": "Lizhen"
			},
			{
				"family": "Fauvel",
				"given": "Simon"
			},
			{
				"family": "Leung",
				"given": "Cyril"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chakrabortiWhenCanAI2019",
		"type": "paper-conference",
		"abstract": "The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314281",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "53–59",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(when) can AI bots lie?",
		"URL": "https://doi.org/10.1145/3306618.3314281",
		"author": [
			{
				"family": "Chakraborti",
				"given": "Tathagata"
			},
			{
				"family": "Kambhampati",
				"given": "Subbarao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gilbertEpistemicTherapyBias2019",
		"type": "paper-conference",
		"abstract": "Despite recent interest in both the critical and machine learning literature on \"bias\" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314294",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "61–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic therapy for bias in automated decision-making",
		"URL": "https://doi.org/10.1145/3306618.3314294",
		"author": [
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Mintz",
				"given": "Yonatan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "borgsAlgorithmicGreenliningApproach2019",
		"type": "paper-conference",
		"abstract": "In contexts such as college admissions, hiring, and image search, decision-makers often aspire to formulate selection criteria that yield both high-quality and diverse results. However, simultaneously optimizing for quality and diversity can be challenging, especially when the decision-maker does not know the true quality of any criterion and instead must rely on heuristics and intuition. We introduce an algorithmic framework that takes as input a user's selection criterion, which may yield high-quality but homogeneous results. Using an application-specific notion of substitutability, our algorithms suggest similar criteria with more diverse results, in the spirit of statistical or demographic parity. For instance, given the image search query \"chairman\", it suggests alternative queries which are similar but more gender-diverse, such as \"chairperson\". In the context of college admissions, we apply our algorithm to a dataset of students' applications and rediscover Texas's \"top 10% rule\": the input criterion is an ACT score cutoff, and the output is a class rank cutoff, automatically accepting the students in the top decile of their graduating class. Historically, this policy has been effective in admitting students who perform well in college and come from diverse backgrounds. We complement our empirical analysis with learning-theoretic guarantees for estimating the true diversity of any criterion based on historical data.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314246",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "69–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic greenlining: An approach to increase diversity",
		"URL": "https://doi.org/10.1145/3306618.3314246",
		"author": [
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			},
			{
				"family": "Haghtalab",
				"given": "Nika"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Vitercik",
				"given": "Ellen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "noriega-camperoActiveFairnessAlgorithmic2019",
		"type": "paper-conference",
		"abstract": "Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314277",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "77–83",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Active fairness in algorithmic decision making",
		"URL": "https://doi.org/10.1145/3306618.3314277",
		"author": [
			{
				"family": "Noriega-Campero",
				"given": "Alejandro"
			},
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Garcia-Bulle",
				"given": "Bernardo"
			},
			{
				"family": "Pentland",
				"given": "Alex 'Sandy'"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "morganParadoxesFairComputeraided2019",
		"type": "paper-conference",
		"abstract": "Computer-aided decision making–where a human decision-maker is aided by a computational classifier in making a decision–is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine \"recidivism risk scores\" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are \"fair\" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for \"non-trivial\" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314242",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "85–90",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Paradoxes in fair computer-aided decision making",
		"URL": "https://doi.org/10.1145/3306618.3314242",
		"author": [
			{
				"family": "Morgan",
				"given": "Andrew"
			},
			{
				"family": "Pass",
				"given": "Rafael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "costonFairTransferLearning2019",
		"type": "paper-conference",
		"abstract": "Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314236",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "91–98",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair transfer learning with missing protected attributes",
		"URL": "https://doi.org/10.1145/3306618.3314236",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Ramamurthy",
				"given": "Karthikeyan Natesan"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Speakman",
				"given": "Skyler"
			},
			{
				"family": "Mustahsan",
				"given": "Zairah"
			},
			{
				"family": "Chakraborty",
				"given": "Supriyo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "saxenaHowFairnessDefinitions2019",
		"type": "paper-conference",
		"abstract": "What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314248",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "99–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness",
		"URL": "https://doi.org/10.1145/3306618.3314248",
		"author": [
			{
				"family": "Saxena",
				"given": "Nripsuta Ani"
			},
			{
				"family": "Huang",
				"given": "Karen"
			},
			{
				"family": "DeFilippis",
				"given": "Evan"
			},
			{
				"family": "Radanovic",
				"given": "Goran"
			},
			{
				"family": "Parkes",
				"given": "David C."
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "lererLearningExistingSocial2019",
		"type": "paper-conference",
		"abstract": "In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314268",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "107–114",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning existing social conventions via observationally augmented self-play",
		"URL": "https://doi.org/10.1145/3306618.3314268",
		"author": [
			{
				"family": "Lerer",
				"given": "Adam"
			},
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hadfield-menellLegibleNormativityAI2019",
		"type": "paper-conference",
		"abstract": "It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior–social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules – rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314258",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "115–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Legible normativity for AI alignment: The value of silly rules",
		"URL": "https://doi.org/10.1145/3306618.3314258",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Andrus",
				"given": "Mckane"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hindTEDTeachingAI2019",
		"type": "paper-conference",
		"abstract": "Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314273",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "123–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TED: Teaching AI to explain its decisions",
		"URL": "https://doi.org/10.1145/3306618.3314273",
		"author": [
			{
				"family": "Hind",
				"given": "Michael"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Campbell",
				"given": "Murray"
			},
			{
				"family": "Codella",
				"given": "Noel C. F."
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Mojsilović",
				"given": "Aleksandra"
			},
			{
				"family": "Natesan Ramamurthy",
				"given": "Karthikeyan"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "lakkarajuFaithfulCustomizableExplanations2019",
		"type": "paper-conference",
		"abstract": "As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314229",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "131–138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Faithful and customizable explanations of black box models",
		"URL": "https://doi.org/10.1145/3306618.3314229",
		"author": [
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Caruana",
				"given": "Rich"
			},
			{
				"family": "Leskovec",
				"given": "Jure"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cruzSharedMoralFoundations2019",
		"type": "paper-conference",
		"abstract": "Sophisticated AI's will make decisions about how to respond to complex situations, and we may wonder whether those decisions will align with the moral values of human beings. I argue that pessimistic worries about this value alignment problem are overstated. In order to achieve intelligence in its full generality and adaptiveness, cognition in AI's will need to be embodied in the sense of the Embodied Cognition research program. That embodiment will yield AI's that share our moral foundations, namely coordination, sociality, and acknowledgement of shared resources. Consequently, we can expect a broad moral alignment between human beings and AI's. AI's will likely show no more variation in their values than we find amongst human beings.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314280",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "139–146",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Shared moral foundations of embodied artificial intelligence",
		"URL": "https://doi.org/10.1145/3306618.3314280",
		"author": [
			{
				"family": "Cruz",
				"given": "Joe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liaoBuildingJiminyCricket2019",
		"type": "paper-conference",
		"abstract": "An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314257",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "147–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building jiminy cricket: An architecture for moral agreements among stakeholders",
		"URL": "https://doi.org/10.1145/3306618.3314257",
		"author": [
			{
				"family": "Liao",
				"given": "Beishui"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Torre",
				"given": "Leendert",
				"non-dropping-particle": "van der"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "danieleAIArtHuman2019",
		"type": "paper-conference",
		"abstract": "Over the past few years, specialised online and offline press blossomed with articles about art made \"with\" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made \"by\" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314233",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "155–161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI + art = human",
		"URL": "https://doi.org/10.1145/3306618.3314233",
		"author": [
			{
				"family": "Daniele",
				"given": "Antonio"
			},
			{
				"family": "Song",
				"given": "Yi-Zhe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "baumerSpeakingBehalfRepresentation2019",
		"type": "paper-conference",
		"abstract": "Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314292",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "163–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Speaking on behalf of: Representation, delegation, and authority in computational text analysis",
		"URL": "https://doi.org/10.1145/3306618.3314292",
		"author": [
			{
				"family": "Baumer",
				"given": "Eric P. S."
			},
			{
				"family": "McGee",
				"given": "Micki"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "limKillerRobotsHuman2019",
		"type": "paper-conference",
		"abstract": "Lethal Autonomous Weapon Systems (LAWS) have become the center of an internationally relevant ethical debate. Deontological arguments based on putative legal compliance failures and the creation of accountability gaps along with wide consequentialist arguments based on factors like the ease of engaging in wars have been leveraged by a number of different states and organizations to try and reach global consensus on a ban of LAWS. This paper will focus on one strand of deontological arguments-ones based on human dignity. Merely asserting that LAWS pose a threat to human dignity would be question begging. Independent evidence based on a morally relevant distinction between humans and LAWS is needed. There are at least four reasons to think that the capacity for emotion cannot be a morally relevant distinction. First, if the concept of human dignity is given a subjective definition, whether or not lethal force is administered by humans or LAWS seems to be irrelevant. Second, it is far from clear that human combatants either have the relevant capacity for emotion or that the capacity is exercised in the relevant circumstances. Third, the capacity for emotion can actually be an impediment to the exercising of a combatant's ability to treat an enemy respectfully. Fourth, there is strong inductive evidence to believe that any capacity, when sufficiently well described, can be carried out by artificially intelligent programs.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314291",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "171–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Killer robots and human dignity",
		"URL": "https://doi.org/10.1145/3306618.3314291",
		"author": [
			{
				"family": "Lim",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "welshRegulatingLethalHarmful2019",
		"type": "paper-conference",
		"abstract": "This short paper provides two partial drafts for a Protocol VI that might be added to the existing five Protocols of the Convention on Certain Conventional Weapons (CCW) to regulate \"lethal autonomous weapons systems\" (LAWS). Draft A sets the line of tolerance at a \"human in the loop\" between the critical functions of select and engage. Draft B sets the line of tolerance at a human in the \"wider loop\" that includes the critical function of defining target classes as well as select and engage. Draft A represents an interpretation of what NGOs such as the Campaign to Stop Killer Robots are seeking to get enacted. Draft B is a more cautious draft based on the Dutch concept of \"meaningful human control in the wider loop\" that does not seek to ban any system that currently exists. Such a draft may be more likely to achieve the consensus required by the UN CCW process. A list of weapons banned by both drafts is provided along with the rationale for each draft. The drafts are intended to stimulate debate on the precise form a binding instrument on LAWS would take and on what LAWS (if any) should be banned and why.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314295",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 4\npublisher-place: Honolulu, HI, USA",
		"page": "177–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating lethal and harmful autonomy: Drafting a protocol VI of the convention on certain conventional weapons",
		"URL": "https://doi.org/10.1145/3306618.3314295",
		"author": [
			{
				"family": "Welsh",
				"given": "Sean"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gearyBalancingBenefitsAutonomous2019",
		"type": "paper-conference",
		"abstract": "Autonomous vehicles are regularly touted as holding the potential to provide significant benefits for diverse populations. There are significant technological barriers to be overcome, but as those are solved, autonomous vehicles are expected to reduce fatalities; decrease emissions and pollutants; provide new options to mobility-challenged individuals; enable people to use their time more productively; and so much more. In this paper, we argue that these high expectations for autonomous vehicles almost certainly cannot be fully realized. More specifically, the proposed benefits divide into two high-level groups, centered around efficiency and safety improvements, and increases in people's agency and autonomy. The first group of benefits is almost always framed in terms of rates: fatality rates, traffic flow per mile, and so forth. However, we arguably care about the absolute numbers for these measures, not the rates; number of fatalities is the key metric, not fatality rate per vehicle mile traveled. Hence, these potential benefits will be reduced, perhaps to non-existence, if autonomous vehicles lead to increases in vehicular usage. But that is exactly the result that we should expect if the second group of benefits is realized: if people's agency and autonomy is increased, then they will use vehicles more. There is an inevitable tension between the benefits that are proposed for autonomous vehicles, such that we cannot fully have all of them at once. We close by pointing towards other types of AI technologies where we should expect to find similar types of necessary and inevitable tradeoffs between classes of benefits.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314237",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "181–186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing the benefits of autonomous vehicles",
		"URL": "https://doi.org/10.1145/3306618.3314237",
		"author": [
			{
				"family": "Geary",
				"given": "Timothy"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pearlCompensationCrossroadsAutonomous2019",
		"type": "paper-conference",
		"abstract": "Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries \"wrong,\" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314249",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "187–193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Compensation at the crossroads: Autonomous vehicles and alternative victim compensation schemes",
		"URL": "https://doi.org/10.1145/3306618.3314249",
		"author": [
			{
				"family": "Pearl",
				"given": "Tracy Hresko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "whittlestoneRoleLimitsPrinciples2019",
		"type": "paper-conference",
		"abstract": "The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314289",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "195–200",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role and limits of principles in AI ethics: Towards a focus on tensions",
		"URL": "https://doi.org/10.1145/3306618.3314289",
		"author": [
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Nyrup",
				"given": "Rune"
			},
			{
				"family": "Alexandrova",
				"given": "Anna"
			},
			{
				"family": "Cave",
				"given": "Stephen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "parkerHowTechnologicalAdvances2019",
		"type": "paper-conference",
		"abstract": "Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered \"actual\" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker<sub>D</sub>anks<sub>R</sub>evealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties – and thus is only meaningfully revealed – in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less \"moral weight\" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed \"rights\": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of \"new rights\" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314274",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 1\npublisher-place: Honolulu, HI, USA",
		"page": "201",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How technological advances can reveal rights",
		"URL": "https://doi.org/10.1145/3306618.3314274",
		"author": [
			{
				"family": "Parker",
				"given": "Jack"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ghoshIMLIIncrementalFramework2019",
		"type": "paper-conference",
		"abstract": "The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314283",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "203–210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "IMLI: An incremental framework for MaxSAT-Based learning of interpretable classification rules",
		"URL": "https://doi.org/10.1145/3306618.3314283",
		"author": [
			{
				"family": "Ghosh",
				"given": "Bishwamittra"
			},
			{
				"family": "Meel",
				"given": "Kuldeep S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "aliLossaversivelyFairClassification2019",
		"type": "paper-conference",
		"abstract": "The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314266",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "211–218",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Loss-aversively fair classification",
		"URL": "https://doi.org/10.1145/3306618.3314266",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Zafar",
				"given": "Muhammad Bilal"
			},
			{
				"family": "Singla",
				"given": "Adish"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gargCounterfactualFairnessText2019",
		"type": "paper-conference",
		"abstract": "In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay\" is toxic while \"Some people are straight\" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3317950",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "219–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual fairness in text classification through robustness",
		"URL": "https://doi.org/10.1145/3306618.3317950",
		"author": [
			{
				"family": "Garg",
				"given": "Sahaj"
			},
			{
				"family": "Perot",
				"given": "Vincent"
			},
			{
				"family": "Limtiaco",
				"given": "Nicole"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Beutel",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "onetoTakingAdvantageMultitask2019",
		"type": "paper-conference",
		"abstract": "A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314255",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 11\npublisher-place: Honolulu, HI, USA",
		"page": "227–237",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taking advantage of multitask learning for fair classification",
		"URL": "https://doi.org/10.1145/3306618.3314255",
		"author": [
			{
				"family": "Oneto",
				"given": "Luca"
			},
			{
				"family": "Doninini",
				"given": "Michele"
			},
			{
				"family": "Elders",
				"given": "Amon"
			},
			{
				"family": "Pontil",
				"given": "Massimiliano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "tesoExplanatoryInteractiveMachine2019",
		"type": "paper-conference",
		"abstract": "Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314293",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "239–245",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explanatory interactive machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314293",
		"author": [
			{
				"family": "Teso",
				"given": "Stefano"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kimMultiaccuracyBlackboxPostprocessing2019",
		"type": "paper-conference",
		"abstract": "Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for \"black women\") even when the sensitive features (e.g. \"race\", \"gender\") are not given to the algorithm explicitly.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314287",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "247–254",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multiaccuracy: Black-box post-processing for fairness in classification",
		"URL": "https://doi.org/10.1145/3306618.3314287",
		"author": [
			{
				"family": "Kim",
				"given": "Michael P."
			},
			{
				"family": "Ghorbani",
				"given": "Amirata"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "wolfFormalApproachExplainability2019",
		"type": "paper-conference",
		"abstract": "We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314260",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "255–261",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A formal approach to explainability",
		"URL": "https://doi.org/10.1145/3306618.3314260",
		"author": [
			{
				"family": "Wolf",
				"given": "Lior"
			},
			{
				"family": "Galanti",
				"given": "Tomer"
			},
			{
				"family": "Hazan",
				"given": "Tamir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mcnamaraCostsBenefitsFair2019",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3317964",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "263–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Costs and benefits of fair representation learning",
		"URL": "https://doi.org/10.1145/3306618.3317964",
		"author": [
			{
				"family": "McNamara",
				"given": "Daniel"
			},
			{
				"family": "Ong",
				"given": "Cheng Soon"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pfohlCreatingFairModels2019",
		"type": "paper-conference",
		"abstract": "Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a \"fair\" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314278",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "271–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Creating fair models of atherosclerotic cardiovascular disease risk",
		"URL": "https://doi.org/10.1145/3306618.3314278",
		"author": [
			{
				"family": "Pfohl",
				"given": "Stephen"
			},
			{
				"family": "Marafino",
				"given": "Ben"
			},
			{
				"family": "Coulet",
				"given": "Adrien"
			},
			{
				"family": "Rodriguez",
				"given": "Fatima"
			},
			{
				"family": "Palaniappan",
				"given": "Latha"
			},
			{
				"family": "Shah",
				"given": "Nigam H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ibrahimGlobalExplanationsNeural2019",
		"type": "paper-conference",
		"abstract": "A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314230",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 9\npublisher-place: Honolulu, HI, USA",
		"page": "279–287",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Global explanations of neural networks: Mapping the landscape of predictions",
		"URL": "https://doi.org/10.1145/3306618.3314230",
		"author": [
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Louie",
				"given": "Melissa"
			},
			{
				"family": "Modarres",
				"given": "Ceena"
			},
			{
				"family": "Paisley",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "aminiUncoveringMitigatingAlgorithmic2019",
		"type": "paper-conference",
		"abstract": "Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314243",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "289–295",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncovering and mitigating algorithmic bias through learned latent structure",
		"URL": "https://doi.org/10.1145/3306618.3314243",
		"author": [
			{
				"family": "Amini",
				"given": "Alexander"
			},
			{
				"family": "Soleimany",
				"given": "Ava P."
			},
			{
				"family": "Schwarting",
				"given": "Wilko"
			},
			{
				"family": "Bhatia",
				"given": "Sangeeta N."
			},
			{
				"family": "Rus",
				"given": "Daniela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "goelCrowdsourcingFairnessDiversity2019",
		"type": "paper-conference",
		"abstract": "Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314282",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "297–304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Crowdsourcing with fairness, diversity and budget constraints",
		"URL": "https://doi.org/10.1145/3306618.3314282",
		"author": [
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Faltings",
				"given": "Boi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "swingerWhatAreBiases2019",
		"type": "paper-conference",
		"abstract": "This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly \"debiased\" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314270",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "305–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What are the biases in my word embedding?",
		"URL": "https://doi.org/10.1145/3306618.3314270",
		"author": [
			{
				"family": "Swinger",
				"given": "Nathaniel"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Heffernan IV",
				"given": "Neil Thomas"
			},
			{
				"family": "Leiserson",
				"given": "Mark DM"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mcnamaraEqualizedOddsImplies2019",
		"type": "paper-conference",
		"abstract": "Equalized odds – where the true positive rates and false positive rates are equal across groups (e.g. racial groups) – is a common quantitative measure of fairness. Equalized outcomes – where the difference in predicted outcomes between groups is less than the difference observed in the training data – is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314290",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "313–320",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalized odds implies partially equalized outcomes under realistic assumptions",
		"URL": "https://doi.org/10.1145/3306618.3314290",
		"author": [
			{
				"family": "McNamara",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "matthewsRightConfrontYour2019",
		"type": "paper-conference",
		"abstract": "The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314279",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "321–327",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The right to confront your accusers: Opening the black box of forensic DNA software",
		"URL": "https://doi.org/10.1145/3306618.3314279",
		"author": [
			{
				"family": "Matthews",
				"given": "Jeanna"
			},
			{
				"family": "Babaeianjelodar",
				"given": "Marzieh"
			},
			{
				"family": "Lorenz",
				"given": "Stephen"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Adams",
				"given": "Nathaniel"
			},
			{
				"family": "Krane",
				"given": "Dan"
			},
			{
				"family": "Goldthwaite",
				"given": "Jessica"
			},
			{
				"family": "Hughes",
				"given": "Clinton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "caveScaryRobotsExamining2019",
		"type": "paper-conference",
		"abstract": "How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314232",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "331–337",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"Scary Robots\": Examining public responses to AI",
		"URL": "https://doi.org/10.1145/3306618.3314232",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			},
			{
				"family": "Coughlan",
				"given": "Kate"
			},
			{
				"family": "Dihal",
				"given": "Kanta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chuanFramingArtificialIntelligence2019",
		"type": "paper-conference",
		"abstract": "Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314285",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "339–344",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Framing artificial intelligence in american newspapers",
		"URL": "https://doi.org/10.1145/3306618.3314285",
		"author": [
			{
				"family": "Chuan",
				"given": "Ching-Hua"
			},
			{
				"family": "Tsai",
				"given": "Wan-Hsiu Sunny"
			},
			{
				"family": "Cho",
				"given": "Su Yeon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liPerceptionsDomesticRobots2019",
		"type": "paper-conference",
		"abstract": "As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314251",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "345–351",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptions of domestic robots' normative behavior across cultures",
		"URL": "https://doi.org/10.1145/3306618.3314251",
		"author": [
			{
				"family": "Li",
				"given": "Huao"
			},
			{
				"family": "Milani",
				"given": "Stephanie"
			},
			{
				"family": "Krishnamoorthy",
				"given": "Vigneshram"
			},
			{
				"family": "Lewis",
				"given": "Michael"
			},
			{
				"family": "Sycara",
				"given": "Katia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "huMappingMissingPopulation2019",
		"type": "paper-conference",
		"abstract": "Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314263",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "353–359",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping missing population in rural india: A deep learning approach with satellite imagery",
		"URL": "https://doi.org/10.1145/3306618.3314263",
		"author": [
			{
				"family": "Hu",
				"given": "Wenjie"
			},
			{
				"family": "Patel",
				"given": "Jay Harshadbhai"
			},
			{
				"family": "Robert",
				"given": "Zoe-Alanah"
			},
			{
				"family": "Novosad",
				"given": "Paul"
			},
			{
				"family": "Asher",
				"given": "Samuel"
			},
			{
				"family": "Tang",
				"given": "Zhongyi"
			},
			{
				"family": "Burke",
				"given": "Marshall"
			},
			{
				"family": "Lobell",
				"given": "David"
			},
			{
				"family": "Ermon",
				"given": "Stefano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gram-hansenMappingInformalSettlements2019",
		"type": "paper-conference",
		"abstract": "Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning dataset purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution (VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314253",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "361–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping informal settlements in developing countries using machine learning and low resolution multi-spectral data",
		"URL": "https://doi.org/10.1145/3306618.3314253",
		"author": [
			{
				"family": "Gram-Hansen",
				"given": "Bradley J."
			},
			{
				"family": "Helber",
				"given": "Patrick"
			},
			{
				"family": "Varatharajan",
				"given": "Indhu"
			},
			{
				"family": "Azam",
				"given": "Faiza"
			},
			{
				"family": "Coca-Castro",
				"given": "Alejandro"
			},
			{
				"family": "Kopackova",
				"given": "Veronika"
			},
			{
				"family": "Bilinski",
				"given": "Piotr"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pandyaHumanAILearningPerformance2019",
		"type": "paper-conference",
		"abstract": "People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314245",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "369–375",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human-AI learning performance in multi-armed bandits",
		"URL": "https://doi.org/10.1145/3306618.3314245",
		"author": [
			{
				"family": "Pandya",
				"given": "Ravi"
			},
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "bryantComparativeAnalysisEmotiondetecting2019",
		"type": "paper-conference",
		"abstract": "In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314284",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "377–382",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A comparative analysis of emotion-detecting AI systems with respect to algorithm performance and dataset diversity",
		"URL": "https://doi.org/10.1145/3306618.3314284",
		"author": [
			{
				"family": "Bryant",
				"given": "De'Aira"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jiangDegenerateFeedbackLoops2019",
		"type": "paper-conference",
		"abstract": "Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called \"echo chambers\" or \"filter bubbles\" that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314288",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "383–390",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Degenerate feedback loops in recommender systems",
		"URL": "https://doi.org/10.1145/3306618.3314288",
		"author": [
			{
				"family": "Jiang",
				"given": "Ray"
			},
			{
				"family": "Chiappa",
				"given": "Silvia"
			},
			{
				"family": "Lattimore",
				"given": "Tor"
			},
			{
				"family": "György",
				"given": "András"
			},
			{
				"family": "Kohli",
				"given": "Pushmeet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "behzadanTrolleyModV10Opensource2019",
		"type": "paper-conference",
		"abstract": "This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314239",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 5\npublisher-place: Honolulu, HI, USA",
		"page": "391–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TrolleyMod v1.0: An open-source simulation and data-collection platform for ethical decision making in autonomous vehicles",
		"URL": "https://doi.org/10.1145/3306618.3314239",
		"author": [
			{
				"family": "Behzadan",
				"given": "Vahid"
			},
			{
				"family": "Minton",
				"given": "James"
			},
			{
				"family": "Munir",
				"given": "Arslan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "giattinoSeductiveAllureArtificial2019",
		"type": "paper-conference",
		"abstract": "Neuroscience explanations-even when completely irrelevant-have been shown to exert a \"seductive allure\" on individuals, leading them to judge bad explanations or arguments more favorably. There seems to be a similarly seductive allure for artificial intelligence (AI) technologies, leading people to \"overtrust\" these systems, even when they have just witnessed the system perform poorly. The AI-powered neurotechnologies that have begun to proliferate in recent years, particularly those based on electroencephalography (EEG), represent a potentially doubly-alluring combination. While there is enormous potential benefit in applying AI techniques in neuroscience to \"decode\" brain activity and associated mental states, these efforts are still in the early stages, and there is a danger in using these unproven technologies prematurely, especially in important, real-world contexts. Yet, such premature use has begun to emerge in several high-stakes set-tings, including the law, health &amp; wellness, employment, and transportation. In light of the potential seductive allure of these technologies, we need to be vigilant in monitoring their scientific validity and challenging both unsubstantiated claims and misuse, while still actively supporting their continued development and proper use.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314269",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "397–402",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The seductive allure of artificial intelligence-powered neurotechnology",
		"URL": "https://doi.org/10.1145/3306618.3314269",
		"author": [
			{
				"family": "Giattino",
				"given": "Charles M."
			},
			{
				"family": "Kwong",
				"given": "Lydia"
			},
			{
				"family": "Rafetto",
				"given": "Chad"
			},
			{
				"family": "Farahany",
				"given": "Nita A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "susserInvisibleInfluenceArtificial2019",
		"type": "paper-conference",
		"abstract": "For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions–what behavioral economists call our choice architectures–are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings–the choice architectures–to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us–effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314286",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "403–408",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Invisible influence: Artificial intelligence and the ethics of adaptive choice architectures",
		"URL": "https://doi.org/10.1145/3306618.3314286",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "peysakhovichReinforcementLearningInverse2019",
		"type": "paper-conference",
		"abstract": "Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314259",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "409–415",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reinforcement learning and inverse reinforcement learning with system 1 and system 2",
		"URL": "https://doi.org/10.1145/3306618.3314259",
		"author": [
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hadfield-menellIncompleteContractingAI2019",
		"type": "paper-conference",
		"abstract": "We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314250",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "417–422",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Incomplete contracting and AI alignment",
		"URL": "https://doi.org/10.1145/3306618.3314250",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "croeserTheoriesParentingTheir2019",
		"type": "paper-conference",
		"abstract": "As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314231",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "423–428",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Theories of parenting and their application to artificial intelligence",
		"URL": "https://doi.org/10.1145/3306618.3314231",
		"author": [
			{
				"family": "Croeser",
				"given": "Sky"
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "rajiActionableAuditingInvestigating2019",
		"type": "paper-conference",
		"abstract": "Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314244",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "429–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products",
		"URL": "https://doi.org/10.1145/3306618.3314244",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "l.cardosoFrameworkBenchmarkingDiscriminationaware2019",
		"type": "paper-conference",
		"abstract": "Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314262",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "437–444",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for benchmarking discrimination-aware models in machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314262",
		"author": [
			{
				"family": "L. Cardoso",
				"given": "Rodrigo"
			},
			{
				"family": "Meira Jr.",
				"given": "Wagner"
			},
			{
				"family": "Almeida",
				"given": "Virgilio"
			},
			{
				"family": "J. Zaki",
				"given": "Mohammed"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "andrusJustTheoryMeasurement2019",
		"type": "paper-conference",
		"abstract": "While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying \"fair\" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314275",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "445–451",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a just theory of measurement: A principled social measurement assurance program for machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314275",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Gilbert",
				"given": "Thomas K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "beutelPuttingFairnessPrinciples2019",
		"type": "paper-conference",
		"abstract": "As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "453–459",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Putting fairness principles into practice: Challenges, metrics, and improvements",
		"URL": "https://doi.org/10.1145/3306618.3314234",
		"author": [
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			},
			{
				"family": "Doshi",
				"given": "Tulsee"
			},
			{
				"family": "Qian",
				"given": "Hai"
			},
			{
				"family": "Woodruff",
				"given": "Allison"
			},
			{
				"family": "Luu",
				"given": "Christine"
			},
			{
				"family": "Kreitmann",
				"given": "Pierre"
			},
			{
				"family": "Bischof",
				"given": "Jonathan"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mohanInfluencingIndividualBehavior2019",
		"type": "paper-conference",
		"abstract": "Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce Copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in Copter producing acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with high-fidelity simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314271",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "461–467",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On influencing individual behavior for reducing transportation energy expenditure in a large population",
		"URL": "https://doi.org/10.1145/3306618.3314271",
		"author": [
			{
				"family": "Mohan",
				"given": "Shiwali"
			},
			{
				"family": "Yan",
				"given": "Frances"
			},
			{
				"family": "Bellotti",
				"given": "Victoria"
			},
			{
				"family": "Elbery",
				"given": "Ahmed"
			},
			{
				"family": "Rakha",
				"given": "Hesham"
			},
			{
				"family": "Klenk",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "linGuidingProsecutorialDecisions2019",
		"type": "paper-conference",
		"abstract": "After a felony arrest, many American jurisdictions hold individuals for several days while police officers investigate the incident and prosecutors decide whether to press criminal charges. This pre-arraignment detention can both preserve public safety and reduce the need for officers to seek out and re-arrest individuals who are ultimately charged with a crime. Such detention, however, also comes at a high social and financial cost to those who are never charged but still incarcerated. In one of the first large-scale empirical analyses of pre-arraignment detention, we examine police reports and charging decisions for approximately 30,000 felony arrests in a major American city between 2012 and 2017. We find that 45% of arrested individuals are never charged for any crime but still typically spend one or more nights in jail before being released. In an effort to reduce such incarceration, we develop a statistical model to help prosecutors identify cases soon after arrest that are likely to be ultimately dismissed. By carrying out an early review of five such candidate cases per day, we estimate that prosecutors could potentially reduce pre-arraignment incarceration for ultimately dismissed cases by 35%. To facilitate implementation and transparency, our model to prioritize cases for early review is designed as a simple, weighted checklist. We show that this heuristic strategy achieves comparable performance to traditional, black-box machine learning models.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314235",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "469–476",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Guiding prosecutorial decisions with an interpretable statistical model",
		"URL": "https://doi.org/10.1145/3306618.3314235",
		"author": [
			{
				"family": "Lin",
				"given": "Zhiyuan"
			},
			{
				"family": "Chohlas-Wood",
				"given": "Alex"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cornelioUsingDeceaseddonorKidneys2019",
		"type": "paper-conference",
		"abstract": "We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314276",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "477–483",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using deceased-donor kidneys to initiate chains of living donor kidney paired donations: Algorithm and experimentation",
		"URL": "https://doi.org/10.1145/3306618.3314276",
		"author": [
			{
				"family": "Cornelio",
				"given": "Cristina"
			},
			{
				"family": "Furian",
				"given": "Lucrezia"
			},
			{
				"family": "Nicolò",
				"given": "Antonio"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "duckworthInferringWorkTask2019",
		"type": "paper-conference",
		"abstract": "Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314247",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "485–491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Inferring work task automatability from AI expert evidence",
		"URL": "https://doi.org/10.1145/3306618.3314247",
		"author": [
			{
				"family": "Duckworth",
				"given": "Paul"
			},
			{
				"family": "Graham",
				"given": "Logan"
			},
			{
				"family": "Osborne",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "addisonRobotsCanBe2019",
		"type": "paper-conference",
		"abstract": "Previous studies showed that using the 'shooter bias' paradigm, people demonstrate a similar racial bias toward dark colored robots over light colored robots (i.e., Black vs. White) as they do toward humans of similar skin tones [3]. However, such an effect could be argued to be the result of social priming. Additionally, it raises the question of how people might respond to robots that are in the middle of the color spectrum (i.e., brown) and whether such effects are moderated by the perceived anthropomorphism of the robots. We conducted two experiments to first examine whether shooter bias tendencies shown towards robots is driven by social priming, and then whether diversification of robot color and level of anthropomorphism influenced shooter bias. Our results showed that shooter bias was not influenced by social priming, and interestingly, introducing a new color of robot removed shooter bias tendencies entirely. However, varying the anthropomorphism of the robots did not moderate the level of shooter bias, and contrary to our expectations, the robots were not perceived by the participants as having different levels of anthropomorphism.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314272",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "493–498",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robots can be more than black and white: Examining racial bias towards robots",
		"URL": "https://doi.org/10.1145/3306618.3314272",
		"author": [
			{
				"family": "Addison",
				"given": "Arifah"
			},
			{
				"family": "Bartneck",
				"given": "Christoph"
			},
			{
				"family": "Yogeeswaran",
				"given": "Kumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jacksonTactNoncomplianceNeed2019",
		"type": "paper-conference",
		"abstract": "There is a significant body of research seeking to enable moral decision making and ensure moral conduct in robots. One aspect of moral conduct is rejecting immoral human commands. For social robots, which are expected to follow and maintain human moral and sociocultural norms, it is especially important not only to engage in moral decision making, but also to properly communicate moral reasoning. We thus argue that it is critical for robots to carefully phrase command rejections. Specifically, the degree of politeness-theoretic face threat in a command rejection should be proportional to the severity of the norm violation motivating that rejection. We present a human subjects experiment showing some of the consequences of miscalibrated responses, including perceptions of the robot as inappropriately polite, direct, or harsh, and reduced robot likeability. This experiment intends to motivate and inform the design of algorithms to tactfully tune pragmatic aspects of command rejections autonomously.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314241",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "499–505",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tact in noncompliance: The need for pragmatically apt responses to unethical commands",
		"URL": "https://doi.org/10.1145/3306618.3314241",
		"author": [
			{
				"family": "Jackson",
				"given": "Ryan Blake"
			},
			{
				"family": "Wen",
				"given": "Ruchen"
			},
			{
				"family": "Williams",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hernandez-oralloAIExtendersEthical2019",
		"type": "paper-conference",
		"abstract": "Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314238",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "507–513",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI extenders: The ethical and societal implications of humans cognitively extended by AI",
		"URL": "https://doi.org/10.1145/3306618.3314238",
		"author": [
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Vold",
				"given": "Karina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "shahrdarHumanTrustMeasurement2019",
		"type": "paper-conference",
		"abstract": "Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314264",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "515–520",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human trust measurement using an immersive virtual reality autonomous vehicle simulator",
		"URL": "https://doi.org/10.1145/3306618.3314264",
		"author": [
			{
				"family": "Shahrdar",
				"given": "Shervin"
			},
			{
				"family": "Park",
				"given": "Corey"
			},
			{
				"family": "Nojoumian",
				"given": "Mehrdad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "avinExploringAIFutures2020",
		"type": "paper-conference",
		"abstract": "We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375817",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "8–14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring AI futures through role play",
		"URL": "https://doi.org/10.1145/3375627.3375817",
		"author": [
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Gruetzemacher",
				"given": "Ross"
			},
			{
				"family": "Fox",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "belfieldActivismAICommunity2020",
		"type": "paper-conference",
		"abstract": "The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375814",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "15–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Activism by the AI Community: Analysing Recent Achievements and Future Prospects",
		"URL": "https://doi.org/10.1145/3375627.3375814",
		"author": [
			{
				"family": "Belfield",
				"given": "Haydn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "caiFairAllocationSelective2020",
		"type": "paper-conference",
		"abstract": "Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers—before allocating resources—can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information—like those without credit scores—our approach can substantially improve the allocation of limited assets.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375823",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "22–28",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair allocation through selective information acquisition",
		"URL": "https://doi.org/10.1145/3375627.3375823",
		"author": [
			{
				"family": "Cai",
				"given": "William"
			},
			{
				"family": "Gaebler",
				"given": "Johann"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "caveProblemIntelligenceIts2020",
		"type": "paper-conference",
		"abstract": "This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375813",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "29–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The problem with intelligence: Its value-laden history and the future of AI",
		"URL": "https://doi.org/10.1145/3375627.3375813",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dasLearningOccupationalTaskshares2020",
		"type": "paper-conference",
		"abstract": "The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375826",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "36–42",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning occupational task-shares dynamics for the future of work",
		"URL": "https://doi.org/10.1145/3375627.3375826",
		"author": [
			{
				"family": "Das",
				"given": "Subhro"
			},
			{
				"family": "Steffen",
				"given": "Sebastian"
			},
			{
				"family": "Clarke",
				"given": "Wyatt"
			},
			{
				"family": "Reddy",
				"given": "Prabhat"
			},
			{
				"family": "Brynjolfsson",
				"given": "Erik"
			},
			{
				"family": "Fleming",
				"given": "Martin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "davoustSocialContractsNoncooperative2020",
		"type": "paper-conference",
		"abstract": "In future agent societies, we might see AI systems engaging in selfish, calculated behavior, furthering their owners' interests instead of socially desirable outcomes. How can we promote morally sound behaviour in such settings, in order to obtain more desirable outcomes? A solution from moral philosophy is the concept of a social contract, a set of rules that people would voluntarily commit to in order to obtain better outcomes than those brought by anarchy. We adapt this concept to a game-theoretic setting, to systematically modify the payoffs of a non-cooperative game, so that agents will rationally pursue socially desirable outcomes. We show that for any game, a suitable social contract can be designed to produce an optimal outcome in terms of social welfare. We then investigate the limitations of applying this approach to alternative moral objectives, and establish that, for any alternative moral objective that is significantly different from social welfare, there are games for which no such social contract will be feasible that produces non-negligible social benefit compared to collective selfish behaviour.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375829",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "43–49",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social contracts for non-cooperative games",
		"URL": "https://doi.org/10.1145/3375627.3375829",
		"author": [
			{
				"family": "Davoust",
				"given": "Alan"
			},
			{
				"family": "Rovatsos",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "erdelyiAILiabilityPuzzle2020",
		"type": "paper-conference",
		"abstract": "Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375806",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "50–56",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The AI liability puzzle and a fund-based work-around",
		"URL": "https://doi.org/10.1145/3375627.3375806",
		"author": [
			{
				"family": "Erdélyi",
				"given": "Olivia J."
			},
			{
				"family": "Erdélyi",
				"given": "Gábor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "fazelpourAlgorithmicFairnessNonideal2020",
		"type": "paper-conference",
		"abstract": "Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375828",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "57–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness from a non-ideal perspective",
		"URL": "https://doi.org/10.1145/3375627.3375828",
		"author": [
			{
				"family": "Fazelpour",
				"given": "Sina"
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jungBayesianSensitivityAnalysis2020",
		"type": "paper-conference",
		"abstract": "On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375822",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "64–70",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bayesian sensitivity analysis for offline policy evaluation",
		"URL": "https://doi.org/10.1145/3375627.3375822",
		"author": [
			{
				"family": "Jung",
				"given": "Jongbin"
			},
			{
				"family": "Shroff",
				"given": "Ravi"
			},
			{
				"family": "Feller",
				"given": "Avi"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kazimzadeBiasedPrioritiesBiased2020",
		"type": "paper-conference",
		"abstract": "In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375809",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "71",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Biased priorities, biased outcomes: Three recommendations for ethics-oriented data annotation practices",
		"URL": "https://doi.org/10.1145/3375627.3375809",
		"author": [
			{
				"family": "Kazimzade",
				"given": "Gunay"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "krafftDefiningAIPolicy2020",
		"type": "paper-conference",
		"abstract": "Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375835",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "72–78",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Defining AI in policy versus practice",
		"URL": "https://doi.org/10.1145/3375627.3375835",
		"author": [
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Huang",
				"given": "Karen"
			},
			{
				"family": "Bugingo",
				"given": "Ghislain"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lakkarajuHowFoolYou2020",
		"type": "paper-conference",
		"abstract": "As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black box models. Specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375833",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "79–85",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"How do I fool you?\": Manipulating user trust via misleading black box explanations",
		"URL": "https://doi.org/10.1145/3375627.3375833",
		"author": [
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			},
			{
				"family": "Bastani",
				"given": "Osbert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lebenNormativePrinciplesEvaluating2020",
		"type": "paper-conference",
		"abstract": "There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375808",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "86–92",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Normative principles for evaluating fairness in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375808",
		"author": [
			{
				"family": "Leben",
				"given": "Derek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "luGoodExplanationAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms have gained widespread usage across a variety of domains, both in providing predictions to expert users and recommending decisions to everyday users. However, these AI systems are often black boxes, and end-users are rarely provided with an explanation. The critical need for explanation by AI systems has led to calls for algorithmic transparency, including the \"right to explanation” in the EU General Data Protection Regulation (GDPR). These initiatives presuppose that we know what constitutes a meaningful or good explanation, but there has actually been surprisingly little research on this question in the context of AI systems. In this paper, we (1) develop a generalizable framework grounded in philosophy, psychology, and interpretable machine learning to investigate and define characteristics of good explanation, and (2) conduct a large-scale lab experiment to measure the impact of different factors on people's perceptions of understanding, usage intention, and trust of AI systems. The framework and study together provide a concrete guide for managers on how to present algorithmic prediction rationales to end-users to foster trust and adoption, and elements of explanation and transparency to be considered by AI researchers and engineers in designing, developing, and deploying transparent or explainable algorithms.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375821",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "93",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Good explanation for algorithmic transparency",
		"URL": "https://doi.org/10.1145/3375627.3375821",
		"author": [
			{
				"family": "Lu",
				"given": "Joy"
			},
			{
				"family": "Lee",
				"given": "Dokyun (DK)"
			},
			{
				"family": "Kim",
				"given": "Tae Wan"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "martinez-plumedDoesAIQualify2020",
		"type": "paper-conference",
		"abstract": "In this paper we present a setting for examining the relation be-tween the distribution of research intensity in AI research and the relevance for a range of work tasks (and occupations) in current and simulated scenarios. We perform a mapping between labourand AI using a set of cognitive abilities as an intermediate layer. This setting favours a two-way interpretation to analyse (1) what impact current or simulated AI research activity has or would have on labour-related tasks and occupations, and (2) what areas of AI research activity would be responsible for a desired or undesired effect on specific labour tasks and occupations. Concretely, in our analysis we map 59 generic labour-related tasks from several worker surveys and databases to 14 cognitive abilities from the cognitive science literature, and these to a comprehensive list of 328 AI benchmarks used to evaluate progress in AI techniques. We provide this model and its implementation as a tool for simulations. We also show the effectiveness of our setting with some illustrative examples.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375831",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "94–100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI qualify for the job? A bidirectional model mapping labour and AI intensities",
		"URL": "https://doi.org/10.1145/3375627.3375831",
		"author": [
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Tolan",
				"given": "Songül"
			},
			{
				"family": "Pesole",
				"given": "Annarosa"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Fernández-Macías",
				"given": "Enrique"
			},
			{
				"family": "Gómez",
				"given": "Emilia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "martinhoEmpiricalApproachCapture2020",
		"type": "paper-conference",
		"abstract": "As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375805",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "101",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical approach to capture moral uncertainty in AI",
		"URL": "https://doi.org/10.1145/3375627.3375805",
		"author": [
			{
				"family": "Martinho",
				"given": "Andreia"
			},
			{
				"family": "Kroesen",
				"given": "Maarten"
			},
			{
				"family": "Chorus",
				"given": "Caspar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "matthewsWhenTrustedBlack2020",
		"type": "paper-conference",
		"abstract": "Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375807",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "102–108",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When trusted black boxes don't agree: Incentivizing iterative improvement and accountability in critical software systems",
		"URL": "https://doi.org/10.1145/3375627.3375807",
		"author": [
			{
				"family": "Matthews",
				"given": "Jeanna Neefe"
			},
			{
				"family": "Northup",
				"given": "Graham"
			},
			{
				"family": "Grasso",
				"given": "Isabella"
			},
			{
				"family": "Lorenz",
				"given": "Stephen"
			},
			{
				"family": "Babaeianjelodar",
				"given": "Marzieh"
			},
			{
				"family": "Bashaw",
				"given": "Hunter"
			},
			{
				"family": "Mondal",
				"given": "Sumona"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Goldthwaite",
				"given": "Jessica"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mccraddenWhenYourOnly2020",
		"type": "paper-conference",
		"abstract": "It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias.The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375824",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When your only tool is a hammer: Ethical limitations of algorithmic fairness solutions in healthcare machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375824",
		"author": [
			{
				"family": "McCradden",
				"given": "Melissa"
			},
			{
				"family": "Mazwi",
				"given": "Mjaye"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Anderson",
				"given": "James A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mckeeEthicsAIWriting2020",
		"type": "paper-conference",
		"abstract": "Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways. But we critique the information transfer communication model supporting many AI writing systems, arguing for a social context model that accounts for rhetorical context-what is, in a sense, \"not there\" in the data corpus but that is critical for the production of meaningful, significant, and ethical communication. We offer two ethical principles to guide design of AI writing systems: transparency about machine presence and critical data awareness, a methodological reflexivity about rhetorical context and omissions in the data that need to be provided by a human agent or accounted for in machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375811",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "110–116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics for AI writing: The importance of rhetorical context",
		"URL": "https://doi.org/10.1145/3375627.3375811",
		"author": [
			{
				"family": "McKee",
				"given": "Heidi A."
			},
			{
				"family": "Porter",
				"given": "James E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mitchellDiversityInclusionMetrics2020",
		"type": "paper-conference",
		"abstract": "The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375832",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "117–123",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity and inclusion metrics in subset selection",
		"URL": "https://doi.org/10.1145/3375627.3375832",
		"author": [
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Moorosi",
				"given": "Nyalleng"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nahianLearningNormsStories2020",
		"type": "paper-conference",
		"abstract": "Value alignment is a property of an intelligent agent indicating that it can only pursue goals and activities that are beneficial to humans. Traditional approaches to value alignment use imitation learning or preference learning to infer the values of humans by observing their behavior. We introduce a complementary technique in which a value-aligned prior is learned from naturally occurring stories which encode societal norms. Training data is sourced from the children's educational comic strip, Goofus &amp; Gallant. In this work, we train multiple machine learning models to classify natural language descriptions of situations found in the comic strip as normative or non-normative by identifying if they align with the main characters' behavior. We also report the models' performance when transferring to two unrelated tasks with little to no additional training on the new task.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375825",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "124–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning norms from stories: A prior for value aligned agents",
		"URL": "https://doi.org/10.1145/3375627.3375825",
		"author": [
			{
				"family": "Nahian",
				"given": "Md Sultan Al"
			},
			{
				"family": "Frazier",
				"given": "Spencer"
			},
			{
				"family": "Riedl",
				"given": "Mark"
			},
			{
				"family": "Harrison",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nandaBalancingTradeoffProfit2020",
		"type": "paper-conference",
		"abstract": "Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375818",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "131",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing the tradeoff between profit and fairness in rideshare platforms during high-demand hours",
		"URL": "https://doi.org/10.1145/3375627.3375818",
		"author": [
			{
				"family": "Nanda",
				"given": "Vedant"
			},
			{
				"family": "Xu",
				"given": "Pan"
			},
			{
				"family": "Sankararaman",
				"given": "Karthik Abinav"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Srinivasan",
				"given": "Aravind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "osobaTechnoculturalPluralismClash2020",
		"type": "paper-conference",
		"abstract": "At the end of the Cold War, the renowned political scientist, Samuel Huntington, argued that future conflicts were more likely to stem from cultural frictions – ideologies, social norms, and political systems – rather than political or economic frictions. Huntington focused his concern on the future of geopolitics in a rapidly shrinking world. This paper argues that a similar dynamic is at play in the interaction of technology cultures. We emphasize the role of culture in the evolution of technology and identify the particular role that culture (esp. privacy culture) plays in the development of AI/ML technologies. Then we examine some implications that this perspective brings to the fore.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375834",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "132–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Technocultural pluralism: A \"Clash of Civilizations\" in technology?",
		"URL": "https://doi.org/10.1145/3375627.3375834",
		"author": [
			{
				"family": "Osoba",
				"given": "Osonde A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "prunklLongtermClearerAccount2020",
		"type": "paper-conference",
		"abstract": "One way of carving up the broad 'AI ethics and society' research space that has emerged in recent years is to distinguish between 'near-term' and 'long-term' research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375803",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "138–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond near- and long-term: Towards a clearer account of research priorities in AI ethics and society",
		"URL": "https://doi.org/10.1145/3375627.3375803",
		"author": [
			{
				"family": "Prunkl",
				"given": "Carina"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "qadriAlgorithmizedNotAtomized2020",
		"type": "paper-conference",
		"abstract": "Jakarta's roads are green, filled as they are with the fluorescent green jackets, bright green logos and fluttering green banners of basecamps created by the city's digitized, 'online' motorbike-taxi drivers (ojol). These spaces function as waiting posts, regulatory institutions, information networks and spaces of solidarity for the ojol working for mobility-app companies, Grab and GoJek. Their existence though, presents a puzzle. In the world of on-demand matching, literature either predicts an isolated, atomized, disempowered digital worker or expects workers to have only temporary, online, ephemeral networks of mutual aid. Yet, Jakarta's ojol then introduce us to a new form of labor action that relies on an interface of the physical world and digital realm, complete with permanent shelters, quirky names, emblems, social media accounts and even their own emergency response service. This paper explores the contours of these labor formations and asks why digital workers in Jakarta are able to create collective structures of solidarity, even as app-mediated work may force them towards an individualized labor regime? I argue that these digital labor collectives are not accidental but a product of interactions between histories of social organization structures in Jakarta and affordances created by technological-mediation. Through participant observation and semi-structured interviews I excavate the bi-directional conversation between globalizing digital platforms and social norms, civic culture and labor market conditions in Jakarta which has allowed for particular forms of digital worker resistances to emerge. I recover power for the digital worker, who provides us with a path to resisting algorithmization of work while still participating in it through agentic labor actions rooted in shared identities, enabled by technological fluency and borne out of a desire for community.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375816",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "144",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmized but not atomized? How digital platforms engender new forms of worker solidarity in jakarta",
		"URL": "https://doi.org/10.1145/3375627.3375816",
		"author": [
			{
				"family": "Qadri",
				"given": "Rida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rajiSavingFaceInvestigating2020",
		"type": "paper-conference",
		"abstract": "Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375820",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "145–151",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Saving face: Investigating the ethical concerns of facial recognition auditing",
		"URL": "https://doi.org/10.1145/3375627.3375820",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			},
			{
				"family": "Lee",
				"given": "Joonseok"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sahaHumanComprehensionFairness2020",
		"type": "paper-conference",
		"abstract": "Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications.Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375819",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "152",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human comprehension of fairness in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375819",
		"author": [
			{
				"family": "Saha",
				"given": "Debjani"
			},
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "McElfresh",
				"given": "Duncan C."
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Mazurek",
				"given": "Michelle L."
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "schiffWhatsNextAI2020",
		"type": "paper-conference",
		"abstract": "Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375804",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "153–158",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's next for AI ethics, policy, and governance? A global overview",
		"URL": "https://doi.org/10.1145/3375627.3375804",
		"author": [
			{
				"family": "Schiff",
				"given": "Daniel"
			},
			{
				"family": "Biddle",
				"given": "Justin"
			},
			{
				"family": "Borenstein",
				"given": "Jason"
			},
			{
				"family": "Laas",
				"given": "Kelly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "schutzmanTradeoffsFairRedistricting2020",
		"type": "paper-conference",
		"abstract": "What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other – prioritizing the value of one of these necessarily comes at a cost in the other.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375802",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "159–165",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trade-offs in fair redistricting",
		"URL": "https://doi.org/10.1145/3375627.3375802",
		"author": [
			{
				"family": "Schutzman",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sharmaCERTIFAICommonFramework2020",
		"type": "paper-conference",
		"abstract": "Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375812",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "166–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CERTIFAI: A common framework to provide explanations and analyse the fairness and robustness of black-box models",
		"URL": "https://doi.org/10.1145/3375627.3375812",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Henderson",
				"given": "Jette"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "shevlaneOffensedefenseBalanceScientific2020",
		"type": "paper-conference",
		"abstract": "There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375815",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "173–179",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The offense-defense balance of scientific knowledge: Does publishing AI research reduce misuse?",
		"URL": "https://doi.org/10.1145/3375627.3375815",
		"author": [
			{
				"family": "Shevlane",
				"given": "Toby"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "slackFoolingLIMESHAP2020",
		"type": "paper-conference",
		"abstract": "As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375830",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "180–186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods",
		"URL": "https://doi.org/10.1145/3375627.3375830",
		"author": [
			{
				"family": "Slack",
				"given": "Dylan"
			},
			{
				"family": "Hilgard",
				"given": "Sophie"
			},
			{
				"family": "Jia",
				"given": "Emily"
			},
			{
				"family": "Singh",
				"given": "Sameer"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangUSPublicOpinion2020",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375827",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "187–193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "U.S. public opinion on the governance of artificial intelligence",
		"URL": "https://doi.org/10.1145/3375627.3375827",
		"author": [
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhouDifferentIntelligibilityDifferent2020",
		"type": "paper-conference",
		"abstract": "Many arguments have concluded that our autonomous technologies must be intelligible, interpretable, or explainable, even if that property comes at a performance cost. In this paper, we consider the reasons why some property like these might be valuable, we conclude that there is not simply one kind of 'intelligibility', but rather different types for different individuals and uses. In particular, different interests and goals require different types of intelligibility (or explanations, or other related notion). We thus provide a typography of 'intelligibility' that distinguishes various notions, and draw methodological conclusions about how autonomous technologies should be designed and deployed in different ways, depending on whose intelligibility is required.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375810",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "194–199",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Different \"Intelligibility\" for different folks",
		"URL": "https://doi.org/10.1145/3375627.3375810",
		"author": [
			{
				"family": "Zhou",
				"given": "Yishan"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "alveroAIHolisticReview2020",
		"type": "paper-conference",
		"abstract": "College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "200–206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI and holistic review: Informing human reading in college admissions",
		"URL": "https://doi.org/10.1145/3375627.3375871",
		"author": [
			{
				"family": "Alvero",
				"given": "A.J."
			},
			{
				"family": "Arthurs",
				"given": "Noah"
			},
			{
				"family": "antonio",
				"given": "anthony",
				"dropping-particle": "lising"
			},
			{
				"family": "Domingue",
				"given": "Benjamin W."
			},
			{
				"family": "Gebre-Medhin",
				"given": "Ben"
			},
			{
				"family": "Giebel",
				"given": "Sonia"
			},
			{
				"family": "Stevens",
				"given": "Mitchell L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "birhaneRobotRightsLets2020",
		"type": "paper-conference",
		"abstract": "The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375855",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "207–213",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robot rights? Let's talk about human welfare instead",
		"URL": "https://doi.org/10.1145/3375627.3375855",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Dijk",
				"given": "Jelle",
				"non-dropping-particle": "van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chanArtificialArtificialIntelligence2020",
		"type": "paper-conference",
		"abstract": "Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "214–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial artificial intelligence: Measuring influence of AI 'assessments' on moral decision-making",
		"URL": "https://doi.org/10.1145/3375627.3375870",
		"author": [
			{
				"family": "Chan",
				"given": "Lok"
			},
			{
				"family": "Doyle",
				"given": "Kenzie"
			},
			{
				"family": "McElfresh",
				"given": "Duncan"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chenJustApproachBalancing2020",
		"type": "paper-conference",
		"abstract": "Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375844",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "221–227",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A just approach balancing rawlsian leximax fairness and utilitarianism",
		"URL": "https://doi.org/10.1145/3375627.3375844",
		"author": [
			{
				"family": "Chen",
				"given": "Violet (Xinying)"
			},
			{
				"family": "Hooker",
				"given": "J. N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cihonShouldArtificialIntelligence2020",
		"type": "paper-conference",
		"abstract": "Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375857",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "228–234",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should artificial intelligence governance be centralised? Design lessons from history",
		"URL": "https://doi.org/10.1145/3375627.3375857",
		"author": [
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			},
			{
				"family": "Kemp",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cruzcortesInvitationSystemwideAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375860",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "235–241",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An invitation to system-wide algorithmic fairness",
		"URL": "https://doi.org/10.1145/3375627.3375860",
		"author": [
			{
				"family": "Cruz Cortés",
				"given": "Efrén"
			},
			{
				"family": "Ghosh",
				"given": "Debashis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dubljevicImplementingAgentdeedconsequenceModel2020",
		"type": "paper-conference",
		"abstract": "Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This&nbsp;paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375853",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "243",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward implementing the agent-deed-consequence model of moral judgment in autonomous vehicles",
		"URL": "https://doi.org/10.1145/3375627.3375853",
		"author": [
			{
				"family": "Dubljevic",
				"given": "Veljko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dulhantyInvestigatingImpactInclusion2020",
		"type": "paper-conference",
		"abstract": "Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "244–250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating the impact of inclusion in face recognition training data on individual face identification",
		"URL": "https://doi.org/10.1145/3375627.3375875",
		"author": [
			{
				"family": "Dulhanty",
				"given": "Chris"
			},
			{
				"family": "Wong",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "emaProposalTypeClassification2020",
		"type": "paper-conference",
		"abstract": "This paper proposes the establishment of Medical Artificial Intelligence (AI) Types (MA Types)\"that classify AI in medicine not only by technical system requirements but also implications to healthcare workers' roles and users/patients. MA Types can be useful to promote discussion regarding the purpose and application of the clinical site. Although MA Types are based on the current technologies and regulations in Japan, but that does not hinder the potential reform of the technologies and regulations. MA Types aims to facilitate discussions among physicians, healthcare workers, engineers, public/patients and policymakers on AI systems in medical practices.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375846",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "251–257",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Proposal for type classification for building trust in medical artificial intelligence systems",
		"URL": "https://doi.org/10.1145/3375627.3375846",
		"author": [
			{
				"family": "Ema",
				"given": "Arisa"
			},
			{
				"family": "Nagakura",
				"given": "Katsue"
			},
			{
				"family": "Fujita",
				"given": "Takanori"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "fernandesAdoptionDynamicsSocietal2020",
		"type": "paper-conference",
		"abstract": "We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375847",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "258–264",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adoption dynamics and societal impact of AI systems in complex networks",
		"URL": "https://doi.org/10.1145/3375627.3375847",
		"author": [
			{
				"family": "Fernandes",
				"given": "Pedro M."
			},
			{
				"family": "Santos",
				"given": "Francisco C."
			},
			{
				"family": "Lopes",
				"given": "Manuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "galdonclavellAuditingAlgorithmsLessons2020",
		"type": "paper-conference",
		"abstract": "In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telefónica Innovación Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375852",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "265–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing algorithms: On lessons learned and the risks of data minimization",
		"URL": "https://doi.org/10.1145/3375627.3375852",
		"author": [
			{
				"family": "Galdon Clavell",
				"given": "Gemma"
			},
			{
				"family": "Martín Zamorano",
				"given": "Mariano"
			},
			{
				"family": "Castillo",
				"given": "Carlos"
			},
			{
				"family": "Smith",
				"given": "Oliver"
			},
			{
				"family": "Matic",
				"given": "Aleksandar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "garrettMoreIfTime2020",
		"type": "paper-conference",
		"abstract": "Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "272–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More than \"If Time Allows\": The role of ethics in AI education",
		"URL": "https://doi.org/10.1145/3375627.3375868",
		"author": [
			{
				"family": "Garrett",
				"given": "Natalie"
			},
			{
				"family": "Beard",
				"given": "Nathan"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "heGeometricSolutionFair2020",
		"type": "paper-conference",
		"abstract": "To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "279–285",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A geometric solution to fair representations",
		"URL": "https://doi.org/10.1145/3375627.3375864",
		"author": [
			{
				"family": "He",
				"given": "Yuzi"
			},
			{
				"family": "Burghardt",
				"given": "Keith"
			},
			{
				"family": "Lerman",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "heringtonMeasuringFairnessUnfair2020",
		"type": "paper-conference",
		"abstract": "Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375854",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "286–292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring fairness in an unfair world",
		"URL": "https://doi.org/10.1145/3375627.3375854",
		"author": [
			{
				"family": "Herington",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "huangJustFairInterpretable2020",
		"type": "paper-conference",
		"abstract": "In many judicial systems – including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada – a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions – something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375848",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "293–299",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards just, fair and interpretable methods for judicial subset selection",
		"URL": "https://doi.org/10.1145/3375627.3375848",
		"author": [
			{
				"family": "Huang",
				"given": "Lingxiao"
			},
			{
				"family": "Wei",
				"given": "Julia"
			},
			{
				"family": "Celis",
				"given": "Elisa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "javadiMonitoringMisuseAccountable2020",
		"type": "paper-conference",
		"abstract": "AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned.However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes.This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "300–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Monitoring misuse for accountable 'artificial intelligence as a service'",
		"URL": "https://doi.org/10.1145/3375627.3375873",
		"author": [
			{
				"family": "Javadi",
				"given": "Seyyed Ahmad"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kakGlobalSouthEverywhere2020",
		"type": "paper-conference",
		"abstract": "There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (\"AI justice\"). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375859",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "307–312",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"The Global South is everywhere, but also always somewhere\": National policy narratives and AI justice",
		"URL": "https://doi.org/10.1145/3375627.3375859",
		"author": [
			{
				"family": "Kak",
				"given": "Amba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "karpatiEthicsFoodRecommender2020",
		"type": "paper-conference",
		"abstract": "The recent unprecedented popularity of food recommender applications has raised several issues related to the ethical, societal and legal implications of relying on these applications. In this paper, in order to assess the relevant ethical issues, we rely on the emerging principles across the AI &amp; Ethics community and define them tailored context specifically. Considering the popular Food Recommender Systems (henceforth F-RS) in the European market cannot be regarded as personalised F-RS, we show how merely this lack of feature shifts the relevance of the focal ethical concerns. We identify the major challenges and propose a scheme for how explicit ethical agendas should be explained. We also argue how a multi-stakeholder approach is indispensable to ensure producing long-term benefits for all stakeholders. After proposing eight ethical desiderata points for F-RS, we present a case-study and assess it based on our proposed desiderata points.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "313–319",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics of food recommender applications",
		"URL": "https://doi.org/10.1145/3375627.3375874",
		"author": [
			{
				"family": "Karpati",
				"given": "Daniel"
			},
			{
				"family": "Najjar",
				"given": "Amro"
			},
			{
				"family": "Ambrossio",
				"given": "Diego Agustin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "maitraArtificialIntelligenceIndigenous2020",
		"type": "paper-conference",
		"abstract": "As 'control' is increasingly ceded to AI systems, potentially Artificial General Intelligence (AGI) humanity may be facing an identity crisis sooner rather than later, whereby the notion of 'intelligence' no longer remains solely our own. This paper characterizes the problem in terms of an impending loss of control and proposes a relational shift in our attitude towards AI. The shortcomings of value alignment as a solution to the problem are outlined which necessitate an extension of these principles. One such approach is considering strongly relational Indigenous epistemologies. The value of Indigenous perspectives has not been canvassed widely in the literature. Their utility becomes clear when considering the existence of well-developed epistemologies adept at accounting for the non-human, a task that defies Western anthropocentrism. Accommodating AI by considering it as part of our network is a step towards building a symbiotic relationship. Given that AGI questions our fundamental notions of what it means to have human rights, it is argued that in order to co-exist, we find assistance in Indigenous traditions such as the Hawaiian and Lakota ontologies. Lakota rituals provide comfort with the conception of non-human soul-bearer while Hawaiian stories provide possible relational schema to frame our relationship with AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375845",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "320–326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence and indigenous perspectives: Protecting and empowering intelligent human beings",
		"URL": "https://doi.org/10.1145/3375627.3375845",
		"author": [
			{
				"family": "Maitra",
				"given": "Suvradip"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "okeefeWindfallClauseDistributing2020",
		"type": "paper-conference",
		"abstract": "As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375842",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "327–331",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The windfall clause: Distributing the benefits of AI for the common good",
		"URL": "https://doi.org/10.1145/3375627.3375842",
		"author": [
			{
				"family": "O'Keefe",
				"given": "Cullen"
			},
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Flynn",
				"given": "Carrick"
			},
			{
				"family": "Leung",
				"given": "Jade"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "osobaStepsValuealignedSystems2020",
		"type": "paper-conference",
		"abstract": "Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "332–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Steps towards value-aligned systems",
		"URL": "https://doi.org/10.1145/3375627.3375872",
		"author": [
			{
				"family": "Osoba",
				"given": "Osonde A."
			},
			{
				"family": "Boudreaux",
				"given": "Benjamin"
			},
			{
				"family": "Yeung",
				"given": "Douglas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pattonContextualAnalysisSocial2020",
		"type": "paper-conference",
		"abstract": "While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375841",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "337–342",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contextual analysis of social media: The promise and challenge of eliciting context in social media posts with natural language processing",
		"URL": "https://doi.org/10.1145/3375627.3375841",
		"author": [
			{
				"family": "Patton",
				"given": "Desmond U."
			},
			{
				"family": "Frey",
				"given": "William R."
			},
			{
				"family": "McGregor",
				"given": "Kyle A."
			},
			{
				"family": "Lee",
				"given": "Fei-Tzin"
			},
			{
				"family": "McKeown",
				"given": "Kathleen"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pengPerilsObjectivityNormative2020",
		"type": "paper-conference",
		"abstract": "Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias–a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "343",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The perils of objectivity: Towards a normative framework for fair judicial decision-making",
		"URL": "https://doi.org/10.1145/3375627.3375869",
		"author": [
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Simard-Halm",
				"given": "Malina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "poyiadziFACEFeasibleActionable2020",
		"type": "paper-conference",
		"abstract": "Work in Counterfactual Explanations tends to focus on the principle of \"the closest possible world\" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a \"feasible path\" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these \"feasible paths\" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the \"feasible paths\" of change, which are achievable and can be tailored to the problem at hand.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375850",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "344–350",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FACE: Feasible and actionable counterfactual explanations",
		"URL": "https://doi.org/10.1145/3375627.3375850",
		"author": [
			{
				"family": "Poyiadzi",
				"given": "Rafael"
			},
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Santos-Rodriguez",
				"given": "Raul"
			},
			{
				"family": "De Bie",
				"given": "Tijl"
			},
			{
				"family": "Flach",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sharmaDataAugmentationDiscrimination2020",
		"type": "paper-conference",
		"abstract": "Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an \"ideal world” dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "358–364",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data augmentation for discrimination prevention and bias disambiguation",
		"URL": "https://doi.org/10.1145/3375627.3375865",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Ríos Aliaga",
				"given": "Jesús M."
			},
			{
				"family": "Bouneffouf",
				"given": "Djallel"
			},
			{
				"family": "Muthusamy",
				"given": "Vinod"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "shulmanMetaDecisionTrees2020",
		"type": "paper-conference",
		"abstract": "We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is available at urlhttps://github.com/shulmaneyal/metatrees.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "365–371",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meta decision trees for explainable recommendation systems",
		"URL": "https://doi.org/10.1145/3375627.3375876",
		"author": [
			{
				"family": "Shulman",
				"given": "Eyal"
			},
			{
				"family": "Wolf",
				"given": "Lior"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "smartWhyReliabilismNot2020",
		"type": "paper-conference",
		"abstract": "In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method citegoldman2012reliabilism. We argue that, in cases where model deployments require em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral \"wrapper” around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification—moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "372–377",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why reliabilism is not enough: Epistemic and moral justification in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375866",
		"author": [
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "James",
				"given": "Larry"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Wu",
				"given": "Simone"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "tuckerSocialGovernanceImplications2020",
		"type": "paper-conference",
		"abstract": "Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency – as more actors gain access to any level of capability – the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the \"AI production function\", will be key to understanding the development of the AI industry and its societal impacts.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375863",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "378–384",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social and governance implications of improved data efficiency",
		"URL": "https://doi.org/10.1145/3375627.3375863",
		"author": [
			{
				"family": "Tucker",
				"given": "Aaron D."
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "turnerConservativeAgencyAttainable2020",
		"type": "paper-conference",
		"abstract": "Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375851",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "385–391",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Conservative agency via attainable utility preservation",
		"URL": "https://doi.org/10.1145/3375627.3375851",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Tadepalli",
				"given": "Prasad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wrightDeonticLogicProgramming2020",
		"type": "paper-conference",
		"abstract": "A \"rightful machine\" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of \"answer set programming,\" which I demonstrate with some simple examples.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "392",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A deontic logic for programming rightful machines",
		"URL": "https://doi.org/10.1145/3375627.3375867",
		"author": [
			{
				"family": "Wright",
				"given": "Ava Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "yuFairnessawareIncentiveScheme2020",
		"type": "paper-conference",
		"abstract": "In federated learning (FL), data owners \"share\" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375840",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "393–399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A fairness-aware incentive scheme for federated learning",
		"URL": "https://doi.org/10.1145/3375627.3375840",
		"author": [
			{
				"family": "Yu",
				"given": "Han"
			},
			{
				"family": "Liu",
				"given": "Zelei"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Chen",
				"given": "Tianjian"
			},
			{
				"family": "Cong",
				"given": "Mingshu"
			},
			{
				"family": "Weng",
				"given": "Xi"
			},
			{
				"family": "Niyato",
				"given": "Dusit"
			},
			{
				"family": "Yang",
				"given": "Qiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangJointOptimizationAI2020",
		"type": "paper-conference",
		"abstract": "Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375862",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "400–406",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Joint optimization of AI fairness and utility: A human-centered approach",
		"URL": "https://doi.org/10.1145/3375627.3375862",
		"author": [
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Bellamy",
				"given": "Rachel"
			},
			{
				"family": "Varshney",
				"given": "Kush"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhouAssessingPosthocExplainability2020",
		"type": "paper-conference",
		"abstract": "As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375856",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "407–413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing post-hoc explainability of the BKT algorithm",
		"URL": "https://doi.org/10.1145/3375627.3375856",
		"author": [
			{
				"family": "Zhou",
				"given": "Tongyu"
			},
			{
				"family": "Sheng",
				"given": "Haoyu"
			},
			{
				"family": "Howley",
				"given": "Iris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhuDeepfakesMedicalVideo2020",
		"type": "paper-conference",
		"abstract": "Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375849",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "414–420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deepfakes for medical video de-identification: Privacy protection and diagnostic information preservation",
		"URL": "https://doi.org/10.1145/3375627.3375849",
		"author": [
			{
				"family": "Zhu",
				"given": "Bingquan"
			},
			{
				"family": "Fang",
				"given": "Hao"
			},
			{
				"family": "Sui",
				"given": "Yanan"
			},
			{
				"family": "Li",
				"given": "Luming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zuckerArbiterDomainspecificLanguage2020",
		"type": "paper-conference",
		"abstract": "The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375858",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "421–425",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Arbiter: A domain-specific language for ethical machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375858",
		"author": [
			{
				"family": "Zucker",
				"given": "Julian"
			},
			{
				"family": "Leeuwen",
				"given": "Myraeka",
				"non-dropping-particle": "d'"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "benthallArtificialIntelligencePurpose2021",
		"type": "paper-conference",
		"abstract": "The law and ethics of Western democratic states have their basis in liberalism. This extends to regulation and ethical discussion of technology and businesses doing data processing. Liberalism relies on the privacy and autonomy of individuals, their ordering through a public market, and, more recently, a measure of equality guaranteed by the state. We argue that these forms of regulation and ethical analysis are largely incompatible with the techno-political and techno-economic dimensions of artificial intelligence. By analyzing liberal regulatory solutions in the form of privacy and data protection, regulation of public markets, and fairness in AI, we expose how the data economy and artificial intelligence have transcended liberal legal imagination. Organizations use artificial intelligence to exceed the bounded rationality of individuals and each other. This has led to the private consolidation of markets and an unequal hierarchy of control operating mainly for the purpose of shareholder value. An artificial intelligence will be only as ethical as the purpose of the social system that operates it. Inspired by the science of artificial life as an alternative to artificial intelligence, we consider data intermediaries: sociotechnical systems composed of individuals associated around collectively pursued purposes. An attention cooperative, that prioritizes its incoming and outgoing data flows, is one model of a social system that could form and maintain its own autonomous purpose.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462526",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "3–12",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence and the purpose of social systems",
		"URL": "https://doi.org/10.1145/3461702.3462526",
		"author": [
			{
				"family": "Benthall",
				"given": "Sebastian"
			},
			{
				"family": "Goldenfein",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chaputMultiagentApproachCombine2021",
		"type": "paper-conference",
		"abstract": "The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462515",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "13–23",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A multi-agent approach to combine reasoning and learning for an ethical behavior",
		"URL": "https://doi.org/10.1145/3461702.3462515",
		"author": [
			{
				"family": "Chaput",
				"given": "Rémy"
			},
			{
				"family": "Duval",
				"given": "Jérémy"
			},
			{
				"family": "Boissier",
				"given": "Olivier"
			},
			{
				"family": "Guillermin",
				"given": "Mathieu"
			},
			{
				"family": "Hassas",
				"given": "Salima"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenGenderBiasUnderrepresentation2021",
		"type": "paper-conference",
		"abstract": "Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. However, these systems reflect a wide range of biases, from gender bias to a bias in which voices they represent. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. In the process, we also document how our work exposes crucial gaps in the NLP-pipeline for many languages. Despite substantial investments in multilingual support, the modern NLP-pipeline still systematically and dramatically under-represents the majority of human voices in the NLP-guided decisions that are shaping our collective future. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages like Spanish, Arabic, German, French and Urdu that have grammatically gendered nouns including different feminine, masculine and neuter profession words. We compare these gender bias measurements across the Wikipedia corpora in different languages as well as across some corpora of more traditional literature.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462530",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "24–34",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias and under-representation in natural language processing across human languages",
		"URL": "https://doi.org/10.1145/3461702.3462530",
		"author": [
			{
				"family": "Chen",
				"given": "Yan"
			},
			{
				"family": "Mahoney",
				"given": "Christopher"
			},
			{
				"family": "Grasso",
				"given": "Isabella"
			},
			{
				"family": "Wali",
				"given": "Esma"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Middleton",
				"given": "Thomas"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Matthews",
				"given": "Jeanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chohlas-woodBlindJusticeAlgorithmically2021",
		"type": "paper-conference",
		"abstract": "A prosecutor's decision to charge or dismiss a criminal case is a particularly high-stakes choice. There is concern, however, that these judgements may suffer from explicit or implicit racial bias, as with many other such actions in the criminal justice system. To reduce potential bias in charging decisions, we designed a system that algorithmically redacts race-related information from free-text case narratives. In a first-of-its-kind initiative, we deployed this system at a large American district attorney's office to help prosecutors make race-obscured charging decisions, where it was used to review many incoming felony cases. We report on both the design, efficacy, and impact of our tool for aiding equitable decision-making. We demonstrate that our redaction algorithm is able to accurately obscure race-related information, making it difficult for a human reviewer to guess the race of a suspect while preserving other information from the case narrative. In the jurisdiction we study, we found little evidence of disparate treatment in charging decisions even prior to deployment of our intervention. Thus, as expected, our tool did not substantially alter charging rates. Nevertheless, our study demonstrates the feasibility of race-obscured charging, and more generally highlights the promise of algorithms to bolster equitable decision-making in the criminal justice system.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462524",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "35–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Blind justice: Algorithmically masking race in charging decisions",
		"URL": "https://doi.org/10.1145/3461702.3462524",
		"author": [
			{
				"family": "Chohlas-Wood",
				"given": "Alex"
			},
			{
				"family": "Nudell",
				"given": "Joe"
			},
			{
				"family": "Yao",
				"given": "Keniel"
			},
			{
				"family": "Lin",
				"given": "Zhiyuan (Jerry)"
			},
			{
				"family": "Nyarko",
				"given": "Julian"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cooperEmergentUnfairnessAlgorithmic2021",
		"type": "paper-conference",
		"abstract": "Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462519",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "46–54",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Emergent unfairness in algorithmic fairness-accuracy trade-off research",
		"URL": "https://doi.org/10.1145/3461702.3462519",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Abrams",
				"given": "Ellen"
			},
			{
				"family": "NA",
				"given": "NA"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "daiFairMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462521",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "55–65",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair machine learning under partial compliance",
		"URL": "https://doi.org/10.1145/3461702.3462521",
		"author": [
			{
				"family": "Dai",
				"given": "Jessica"
			},
			{
				"family": "Fazelpour",
				"given": "Sina"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dianaMinimaxGroupFairness2021",
		"type": "paper-conference",
		"abstract": "We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462523",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "66–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Minimax group fairness: Algorithms and experiments",
		"URL": "https://doi.org/10.1145/3461702.3462523",
		"author": [
			{
				"family": "Diana",
				"given": "Emily"
			},
			{
				"family": "Gill",
				"given": "Wesley"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "doniaCodesignEthicalArtificial2021",
		"type": "paper-conference",
		"abstract": "Applications of artificial intelligence / machine learning (AI/ML) are dynamic and rapidly growing, and although multi-purpose, are particularly consequential in health care. One strategy for anticipating and addressing ethical challenges related to AI/ML for health care is co-design - or involvement of end users in design. Co-design has a diverse intellectual and practical history, however, and has been conceptualized in many different ways. Moreover, the unique features of AI/ML introduce challenges to co-design that are often underappreciated. This review summarizes the research literature on involvement in health care and design, and informed by critical data studies, examines the extent to which co-design as commonly conceptualized is capable of addressing the range of normative issues raised by AI/ML for health. We suggest that AI/ML technologies have amplified existing challenges related to co-design, and created entirely new challenges. We outline five co-design 'myths and misconceptions' related to AI/ML for health that form the basis for future research and practice. We conclude by suggesting that the normative strength of a co-design approach to AI/ML for health can be considered at three levels: technological, health care system, and societal. We also suggest research directions for a 'new era' of co-design capable of addressing these challenges.Link to full text: https://bit.ly/3yZrb3y",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462537",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "77",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-design and ethical artificial intelligence for health: Myths and misconceptions",
		"URL": "https://doi.org/10.1145/3461702.3462537",
		"author": [
			{
				"family": "Donia",
				"given": "Joseph"
			},
			{
				"family": "Shaw",
				"given": "Jay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "engelmannBlacklistsRedlistsChinese2021",
		"type": "paper-conference",
		"abstract": "The Chinese Social Credit System (SCS) is a novel digital socio-technical credit system. The SCS aims to regulate societal behavior by reputational and material devices. Scholarship on the SCS has offered a variety of legal and theoretical perspectives. However, little is known about its actual implementation. Here, we provide the first comprehensive empirical study of digital blacklists (listing \"bad\" behavior) and redlists (listing \"good\" behavior) in the Chinese SCS. Based on a unique data set of reputational blacklists and redlists in 30 Chinese provincial-level administrative divisions (ADs), we show the diversity, flexibility, and comprehensiveness of the SCS listing infrastructure. First, our results demonstrate that the Chinese SCS unfolds in a highly diversified manner: we find differences in accessibility, interface design and credit information across provincial-level SCS blacklists and redlists. Second, SCS listings are flexible. During the COVID-19 outbreak, we observe a swift addition of blacklists and redlists that helps strengthen the compliance with coronavirus-related norms and regulations. Third, the SCS listing infrastructure is comprehensive. Overall, we identify 273 blacklists and 154 redlists across provincial-level ADs. Our blacklist and redlist taxonomy highlights that the SCS listing infrastructure prioritizes law enforcement and industry regulations. We also identify redlists that reward political and moral behavior. Our study substantiates the enormous scale and diversity of the Chinese SCS and puts the debate on its reach and societal impact on firmer ground. Finally, we initiate a discussion on the ethical dimensions of data-driven research on the SCS.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462535",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "78–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Blacklists and redlists in the chinese social credit system: Diversity, flexibility, and comprehensiveness",
		"URL": "https://doi.org/10.1145/3461702.3462535",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Dang",
				"given": "Lorenz"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fishReflexiveDesignFairness2021",
		"type": "paper-conference",
		"abstract": "Algorithms and other formal models purportedly incorporating human values like fairness have grown increasingly popular in computer science. In response to sociotechnical challenges in the use of these models, designers and researchers have taken widely divergent positions on how formal models incorporating aspects of human values should be used: encouraging their use, moving away from them, or ignoring the normative consequences altogether. In this paper, we seek to resolve these divergent positions by identifying the main conceptual limits of formal modeling, and develop four reflexive values–value fidelity, appropriate accuracy, value legibility, and value contestation–vital for incorporating human values adequately into formal models. We then provide a brief methodology for reflexively designing formal models incorporating human values.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462518",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "89–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reflexive design for fairness and other human values in formal models",
		"URL": "https://doi.org/10.1145/3461702.3462518",
		"author": [
			{
				"family": "Fish",
				"given": "Benjamin"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fogliatoValidityArrestProxy2021",
		"type": "paper-conference",
		"abstract": "Re-offense risk is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision-makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate and less biased reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007–2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462538",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "100–111",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the validity of arrest as a proxy for offense: Race and the likelihood of arrest for violent crimes",
		"URL": "https://doi.org/10.1145/3461702.3462538",
		"author": [
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Nagin",
				"given": "Daniel"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "goodmanHardChoicesHard2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462539",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "112–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Hard choices and hard limits in artificial intelligence",
		"URL": "https://doi.org/10.1145/3461702.3462539",
		"author": [
			{
				"family": "Goodman",
				"given": "Bryce"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "guoDetectingEmergentIntersectional2021",
		"type": "paper-conference",
		"abstract": "With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models.Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462536",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "122–133",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases",
		"URL": "https://doi.org/10.1145/3461702.3462536",
		"author": [
			{
				"family": "Guo",
				"given": "Wei"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hopkinsMachineLearningPractices2021a",
		"type": "paper-conference",
		"abstract": "Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community—for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints—tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462527",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "134–145",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning practices outside big tech: How resource constraints challenge responsible development",
		"URL": "https://doi.org/10.1145/3461702.3462527",
		"author": [
			{
				"family": "Hopkins",
				"given": "Aspen"
			},
			{
				"family": "Booth",
				"given": "Serena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehFairnessDataProtection2021",
		"type": "paper-conference",
		"abstract": "In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462528",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "146–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and data protection impact assessments",
		"URL": "https://doi.org/10.1145/3461702.3462528",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Clifford",
				"given": "Damian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "keswaniUnbiasedAccurateDeferral2021",
		"type": "paper-conference",
		"abstract": "Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on \"deferral systems\" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462516",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "154–165",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards unbiased and accurate deferral to multiple experts",
		"URL": "https://doi.org/10.1145/3461702.3462516",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Lease",
				"given": "Matthew"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liAlgorithmicHiringPractice2021",
		"type": "paper-conference",
		"abstract": "The use of AI-enabled hiring software raises questions about the practice of Human Resource (HR) professionals' use of the software and its consequences. We interviewed 15 recruiters and HR professionals about their experiences around two decision-making processes during hiring: sourcing and assessment. For both, AI-enabled software allowed the efficient processing of candidate data, thus providing the ability to introduce or advance candidates from broader and more diverse pools. For sourcing, it can serve as a useful learning resource to find candidates. Though, a lack of trust in data accuracy and an inadequate level of control over algorithmic candidate matches can create reluctance to embrace it. For assessment, its implementation varied across companies depending on the industry and the hiring scenario. Its inclusion may redefine HR professionals' job content as it automates or augments pieces of the existing hiring process. Finally, we discuss how candidate roles that recruiters and HR professionals support drive the use of algorithmic hiring software.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462531",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "166–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic hiring in practice: Recruiter and HR professional's perspectives on AI use in hiring",
		"URL": "https://doi.org/10.1145/3461702.3462531",
		"author": [
			{
				"family": "Li",
				"given": "Lan"
			},
			{
				"family": "Lassiter",
				"given": "Tina"
			},
			{
				"family": "Oh",
				"given": "Joohee"
			},
			{
				"family": "Lee",
				"given": "Min Kyung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mohammadiScalingGuaranteesNearest2021",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations (CFE) are being widely used to explain algorithmic decisions, especially in consequential decision-making contexts (e.g., loan approval or pretrial bail). In this context, CFEs aim to provide individuals affected by an algorithmic decision with the most similar individual (i.e., nearest individual) with a different outcome. However, while an increasing number of works propose algorithms to compute CFEs, such approaches either lack in optimality of distance (i.e., they do not return the nearest individual) and perfect coverage (i.e., they do not provide a CFE for all individuals); or they do not scale to complex models such as neural networks. In this work, we provide a framework based on Mixed-Integer Programming (MIP) to compute nearest counterfactual explanations for the outcomes of neural networks, with both provable guarantees and runtimes comparable to gradient-based approaches. Our experiments on the Adult, COMPAS, and Credit datasets show that, in contrast with previous methods, our approach allows for efficiently computing diverse CFEs with both distance guarantees and perfect coverage.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462514",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "177–187",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Scaling guarantees for nearest counterfactual explanations",
		"URL": "https://doi.org/10.1145/3461702.3462514",
		"author": [
			{
				"family": "Mohammadi",
				"given": "Kiarash"
			},
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Barthe",
				"given": "Gilles"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nashedEthicallyCompliantPlanning2021",
		"type": "paper-conference",
		"abstract": "Ethically compliant autonomous systems (ECAS) are the state-of-the-art for solving sequential decision-making problems under uncertainty while respecting constraints that encode ethical considerations. This paper defines a novel concept in the context of ECAS that is from moral philosophy, the moral community, which leads to a nuanced taxonomy of explicit ethical agents. We then propose new ethical frameworks that extend the applicability of ECAS to domains where a moral community is required. Next, we provide a formal analysis of the proposed ethical frameworks and conduct experiments that illustrate their differences. Finally, we discuss the implications of explicit moral communities that could shape research on standards and guidelines for ethical agents in order to better understand and predict common errors in their design and communicate their capabilities.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462522",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "188–198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethically compliant planning within moral communities",
		"URL": "https://doi.org/10.1145/3461702.3462522",
		"author": [
			{
				"family": "Nashed",
				"given": "Samer"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nokhizPrecarityModelingLong2021",
		"type": "paper-conference",
		"abstract": "When it comes to studying the impacts of decision making, the research has been largely focused on examining the fairness of the decisions, the long-term effects of the decision pipelines, and utility-based perspectives considering both the decision-maker and the individuals. However, there has hardly been any focus on precarity which is the term that encapsulates the instability in people's lives. That is, a negative outcome can overspread to other decisions and measures of well-being. Studying precarity necessitates a shift in focus – from the point of view of the decision-maker to the perspective of the decision subject. This centering of the subject is an important direction that unlocks the importance of parting with aggregate measures to examine the long-term effects of decision making. To address this issue, in this paper, we propose a modeling framework that simulates the effects of compounded decision-making on precarity over time. Through our simulations, we are able to show the heterogeneity of precarity by the non-uniform ruinous aftereffects of negative decisions on different income classes of the underlying population and how policy interventions can help mitigate such effects.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462529",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "199–208",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Precarity: Modeling the long term effects of compounded decisions on individual instability",
		"URL": "https://doi.org/10.1145/3461702.3462529",
		"author": [
			{
				"family": "Nokhiz",
				"given": "Pegah"
			},
			{
				"family": "Ruwanpathirana",
				"given": "Aravinda Kanchana"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "robinsonMoralDisagreementArtificial2021",
		"type": "paper-conference",
		"abstract": "Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462534",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Moral disagreement and artificial intelligence",
		"URL": "https://doi.org/10.1145/3461702.3462534",
		"author": [
			{
				"family": "Robinson",
				"given": "Pamela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shekharFairODFairnessawareOutlier2021",
		"type": "paper-conference",
		"abstract": "Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462517",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "210–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairOD: Fairness-aware outlier detection",
		"URL": "https://doi.org/10.1145/3461702.3462517",
		"author": [
			{
				"family": "Shekhar",
				"given": "Shubhranshu"
			},
			{
				"family": "Shah",
				"given": "Neil"
			},
			{
				"family": "Akoglu",
				"given": "Leman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shengSurveillingSurveillanceEstimating2021",
		"type": "paper-conference",
		"abstract": "The use of video surveillance in public spaces–both by government agencies and by private citizens–has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here we present a novel approach for estimating the spatial distribution of surveillance cameras: applying computer vision algorithms to large-scale street view image data. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents—a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462525",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "221–230",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Surveilling surveillance: Estimating the prevalence of surveillance cameras with street view data",
		"URL": "https://doi.org/10.1145/3461702.3462525",
		"author": [
			{
				"family": "Sheng",
				"given": "Hao"
			},
			{
				"family": "Yao",
				"given": "Keniel"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shokriPrivacyRisksModel2021",
		"type": "paper-conference",
		"abstract": "Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462533",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "231–241",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the privacy risks of model explanations",
		"URL": "https://doi.org/10.1145/3461702.3462533",
		"author": [
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "susserMeasuringAutomatedInfluence2021",
		"type": "paper-conference",
		"abstract": "Automated influence, delivered by digital targeting technologies such as targeted advertising, digital nudges, and recommender systems, has attracted significant interest from both empirical researchers, on one hand, and critical scholars and policymakers on the other. In this paper, we argue for closer integration of these efforts. Critical scholars and policymakers, who focus primarily on the social, ethical, and political effects of these technologies, need empirical evidence to substantiate and motivate their concerns. However, existing empirical research investigating the effectiveness of these technologies (or lack thereof), neglects other morally relevant effects-which can be felt regardless of whether or not the technologies \"work\" in the sense of fulfilling the promises of their designers. Drawing from the ethics and policy literature, we enumerate a range of questions begging for empirical analysis-the outline of a research agenda bridging these fields—and issue a call to action for more empirical research that takes these urgent ethics and policy questions as their starting point.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462532",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "242–253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring automated influence: Between empirical evidence and ethical values",
		"URL": "https://doi.org/10.1145/3461702.3462532",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			},
			{
				"family": "Grimaldi",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "tomasevFairnessUnobservedCharacteristics2021",
		"type": "paper-conference",
		"abstract": "Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness—frequently, race and legal gender—can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462540",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "254–265",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness for unobserved characteristics: Insights from technological impacts on queer communities",
		"URL": "https://doi.org/10.1145/3461702.3462540",
		"author": [
			{
				"family": "Tomasev",
				"given": "Nenad"
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "vredenburghAlienationAIDrivenWorkplace2021",
		"type": "paper-conference",
		"abstract": "This paper asks whether explanations of one's workplace and economic institutions are valuable in and of themselves. In doing so, it departs from much of the explainability literature in law, computer science, philosophy, and the social sciences, which examine the instrumental values that explainable AI has: explainable systems increase accountability and user trust, or reduce the risk of harm due to increased robustness. Think, however, of how you might feel if you went to your local administrative agency to apply for some benefit, or you were handed down a decision by a judge in a court. Let's stipulate that you know that the decision was just, even though neither the civil servant nor the judge explain to you why the decision was made, and you don't know the relevant rules; you just brought all the information you had about yourself, and hoped for the best. Is such a decision process defective? I argue that such a decision process is defective because it prevents individuals from accessing the normative explanations that are necessary to form an appropriate practical orientation towards their social world. A practical orientation is a reflective stance towards one's social world, which is expressed in one's actions and draws on one's cognitive architecture that allows one to navigate the various social practices and institutions. A practical orientation can range from rejection to silent endorsement, and is the sort of attitude for which there are the right kind of reasons, based in the world's normative character. It also determines how one fills out one's role obligations, and, more broadly, guides one's actions in the relevant institution: a teacher in the American South during the time of enforced racial segregation, for example, might choose where to teach on the basis of her rejection of the segregation of education. To form an appropriate practical orientation, one must have an understanding of the social world's normative character, which required a normative explanation And, since we spend so much of our lives at work and are constrained by economic institutions, we must understand their structure and how they function.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462520",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Alienation in the AI-Driven workplace",
		"URL": "https://doi.org/10.1145/3461702.3462520",
		"author": [
			{
				"family": "Vredenburgh",
				"given": "Kate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abdallaGreyHoodieProject2021",
		"type": "paper-conference",
		"abstract": "As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462563",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "287–297",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The grey hoodie project: Big tobacco, big tech, and the threat on academic integrity",
		"URL": "https://doi.org/10.1145/3461702.3462563",
		"author": [
			{
				"family": "Abdalla",
				"given": "Mohamed"
			},
			{
				"family": "Abdalla",
				"given": "Moustafa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abidPersistentAntimuslimBias2021",
		"type": "paper-conference",
		"abstract": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462624",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "298–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Persistent anti-muslim bias in large language models",
		"URL": "https://doi.org/10.1145/3461702.3462624",
		"author": [
			{
				"family": "Abid",
				"given": "Abubakar"
			},
			{
				"family": "Farooqi",
				"given": "Maheen"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "acunaAreAIEthics2021",
		"type": "paper-conference",
		"abstract": "Even though computer science (CS) has had a historical lack of gender and race representation, its AI research affects everybody eventually. Being partially rooted in CS conferences, \"AI ethics\" (AIE) conferences such as FAccT and AIES have quickly become distinct venues where AI's societal implications are discussed and solutions proposed. However, it is largely unknown if these conferences improve upon the historical representational issues of traditional CS venues. In this work, we explore AIE conferences' evolution and compare them across demographic characteristics, publication content, and citation patterns. We find that AIE conferences have increased their internal topical diversity and impact on other CS conferences. Importantly, AIE conferences are highly differentiable, covering topics not represented in other venues. However, and perhaps contrary to the field's aspirations, white authors are more common while seniority and black researchers are represented similarly to CS venues. Our results suggest that AIE conferences could increase efforts to attract more diverse authors, especially considering their sizable roots in CS.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462616",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "307–315",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Are AI ethics conferences different and more diverse compared to traditional computer science conferences?",
		"URL": "https://doi.org/10.1145/3461702.3462616",
		"author": [
			{
				"family": "Acuna",
				"given": "Daniel E."
			},
			{
				"family": "Liang",
				"given": "Lizhen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "afnanEthicalImplementationArtificial2021",
		"type": "paper-conference",
		"abstract": "AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462589",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "316–326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical implementation of artificial intelligence to select embryos in in vitro fertilization",
		"URL": "https://doi.org/10.1145/3461702.3462589",
		"author": [
			{
				"family": "Afnan",
				"given": "Michael Anis Mihdi"
			},
			{
				"family": "Rudin",
				"given": "Cynthia"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Savulescu",
				"given": "Julian"
			},
			{
				"family": "Mishra",
				"given": "Abhishek"
			},
			{
				"family": "Liu",
				"given": "Yanhe"
			},
			{
				"family": "Afnan",
				"given": "Masoud"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "akaMeasuringModelBiases2021",
		"type": "paper-conference",
		"abstract": "The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice.We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a \"bag of words\", we rank the biases that a model has learned with respect to different identity labels. We use man, woman as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most \"gender biased\" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462557",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "327–335",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring model biases in the absence of ground truth",
		"URL": "https://doi.org/10.1145/3461702.3462557",
		"author": [
			{
				"family": "Aka",
				"given": "Osman"
			},
			{
				"family": "Burke",
				"given": "Ken"
			},
			{
				"family": "Bauerle",
				"given": "Alex"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "aliAccountingModelUncertainty2021",
		"type": "paper-conference",
		"abstract": "Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize \"total\" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462630",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "336–345",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accounting for model uncertainty in algorithmic discrimination",
		"URL": "https://doi.org/10.1145/3461702.3462630",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Lahoti",
				"given": "Preethi"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bakkerReasonableDoubtImproving2021",
		"type": "paper-conference",
		"abstract": "Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public datasets, we confirm the effectiveness of our methods and investigate the limitations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462575",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "346–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond reasonable doubt: Improving fairness in budget-constrained decision making using confidence thresholds",
		"URL": "https://doi.org/10.1145/3461702.3462575",
		"author": [
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Tu",
				"given": "Duy Patrick"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Pentland",
				"given": "Alex Sandy"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barlasPersonHumanNeither2021",
		"type": "paper-conference",
		"abstract": "Following the literature on dehumanization via technology, we audit six proprietary image tagging algorithms (ITAs) for their potential to perpetuate dehumanization. We examine the ITAs' outputs on a controlled dataset of images depicting a diverse group of people for tags that indicate the presence of a human in the image. Through an analysis of the (mis)use of these tags, we find that there are some individuals whose 'humanness' is not recognized by an ITA, and that these individuals are often from marginalized social groups. Finally, we compare these findings with the use of the 'face' tag, which can be used for surveillance, revealing that people's faces are often recognized by an ITA even when their 'humanness' is not. Overall, we highlight the subtle ways in which ITAs may inflict widespread, disparate harm, and emphasize the importance of considering the social context of the resulting application.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462567",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "357–367",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Person, human, neither: The dehumanization potential of automated image tagging",
		"URL": "https://doi.org/10.1145/3461702.3462567",
		"author": [
			{
				"family": "Barlas",
				"given": "Pınar"
			},
			{
				"family": "Kyriakou",
				"given": "Kyriakos"
			},
			{
				"family": "Kleanthous",
				"given": "Styliani"
			},
			{
				"family": "Otterbacher",
				"given": "Jahna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barocasDesigningDisaggregatedEvaluations2021",
		"type": "paper-conference",
		"abstract": "Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts—both beneficial and harmful—that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462610",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "368–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing disaggregated evaluations of AI systems: Choices, considerations, and tradeoffs",
		"URL": "https://doi.org/10.1145/3461702.3462610",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Guo",
				"given": "Anhong"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Krones",
				"given": "Jacquelyn"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			},
			{
				"family": "Wadsworth",
				"given": "W. Duncan"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "belitzAutomatingProcedurallyFair2021",
		"type": "paper-conference",
		"abstract": "In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462585",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "379–389",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automating procedurally fair feature selection in machine learning",
		"URL": "https://doi.org/10.1145/3461702.3462585",
		"author": [
			{
				"family": "Belitz",
				"given": "Clara"
			},
			{
				"family": "Jiang",
				"given": "Lan"
			},
			{
				"family": "Bosch",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bendavidExplainableAIAdoption2021",
		"type": "paper-conference",
		"abstract": "We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with \"No-explanation\" alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462565",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "390–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI and adoption of financial algorithmic advisors: An experimental study",
		"URL": "https://doi.org/10.1145/3461702.3462565",
		"author": [
			{
				"family": "Ben David",
				"given": "Daniel"
			},
			{
				"family": "Resheff",
				"given": "Yehezkel S."
			},
			{
				"family": "Tron",
				"given": "Talia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bhattUncertaintyFormTransparency2021",
		"type": "paper-conference",
		"abstract": "Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462571",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, USA",
		"page": "401–413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty",
		"URL": "https://doi.org/10.1145/3461702.3462571",
		"author": [
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Antorán",
				"given": "Javier"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Melançon",
				"given": "Gabrielle"
			},
			{
				"family": "Krishnan",
				"given": "Ranganath"
			},
			{
				"family": "Stanley",
				"given": "Jason"
			},
			{
				"family": "Tickoo",
				"given": "Omesh"
			},
			{
				"family": "Nachman",
				"given": "Lama"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			},
			{
				"family": "Srikumar",
				"given": "Madhulika"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "biswasEnsuringFairnessPrior2021",
		"type": "paper-conference",
		"abstract": "Prior probability shift is a phenomenon where the training and test datasets differ structurally within population subgroups. This phenomenon can be observed in the yearly records of several real-world datasets, for example, recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we design an algorithm, called CAPE, that ensures fair classification under such shifts. We introduce a metric, called prevalence difference, which CAPE attempts to minimize in order to achieve fairness under prior probability shifts. We theoretically establish that this metric exhibits several properties that are desirable for a fair classifier. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several state-of-the-art fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures a high degree of PE-fairness in its predictions, while performing well on other important metrics.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462596",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "414–424",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ensuring fairness under prior probability shifts",
		"URL": "https://doi.org/10.1145/3461702.3462596",
		"author": [
			{
				"family": "Biswas",
				"given": "Arpita"
			},
			{
				"family": "Mukherjee",
				"given": "Suvam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bondiEnvisioningCommunitiesParticipatory2021",
		"type": "paper-conference",
		"abstract": "Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be \"for\" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462612",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "425–436",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Envisioning communities: A participatory approach towards AI for social good",
		"URL": "https://doi.org/10.1145/3461702.3462612",
		"author": [
			{
				"family": "Bondi",
				"given": "Elizabeth"
			},
			{
				"family": "Xu",
				"given": "Lily"
			},
			{
				"family": "Acosta-Navas",
				"given": "Diana"
			},
			{
				"family": "Killian",
				"given": "Jackson A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "butlinAIAlignmentHuman2021",
		"type": "paper-conference",
		"abstract": "According to a prominent approach to AI alignment, AI agents should be built to learn and promote human values. However, humans value things in several different ways: we have desires and preferences of various kinds, and if we engage in reinforcement learning, we also have reward functions. One research project to which this approach gives rise is therefore to say which of these various classes of human values should be promoted. This paper takes on part of this project by assessing the proposal that human reward functions should be the target for AI alignment. There is some reason to believe that powerful AI agents which were aligned to values of this form would help us to lead good lives, but there is also considerable uncertainty about this claim, arising from unresolved empirical and conceptual issues in human psychology.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462570",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "437–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI alignment and human reward",
		"URL": "https://doi.org/10.1145/3461702.3462570",
		"author": [
			{
				"family": "Butlin",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "castroFairnessMachineFairness2021",
		"type": "paper-conference",
		"abstract": "Prediction-based decisions, which are often made by utilizing the tools of machine learning, influence nearly all facets of modern life. Ethical concerns about this widespread practice have given rise to the field of fair machine learning and a number of fairness measures, mathematically precise definitions of fairness that purport to determine whether a given prediction-based decision system is fair. Following Reuben Binns (2017), we take \"fairness\" in this context to be a placeholder for a variety of normative egalitarian considerations. We explore a few fairness measures to suss out their egalitarian roots and evaluate them, both as formalizations of egalitarian ideas and as assertions of what fairness demands of predictive systems. We pay special attention to a recent and popular fairness measure, counterfactual fairness, which holds that a prediction about an individual is fair if it is the same in the actual world and any counterfactual world where the individual belongs to a different demographic group (cf. Kusner et al. 2018).",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462577",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "446",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and machine fairness",
		"URL": "https://doi.org/10.1145/3461702.3462577",
		"author": [
			{
				"family": "Castro",
				"given": "Clinton"
			},
			{
				"family": "O'Brien",
				"given": "David"
			},
			{
				"family": "Schwan",
				"given": "Ben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chiReconfiguringDiversityInclusion2021",
		"type": "paper-conference",
		"abstract": "Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce.We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the \"managerialization of diversity\" by corporations in the mid-1980s. The focus on technical artifacts - such as diverse and inclusive datasets - and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and relevant subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values and new solutions. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as \"ethics owners\" but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462622",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "447–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconfiguring diversity and inclusion for AI ethics",
		"URL": "https://doi.org/10.1145/3461702.3462622",
		"author": [
			{
				"family": "Chi",
				"given": "Nicole"
			},
			{
				"family": "Lurie",
				"given": "Emma"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fabrisAlgorithmicAuditItalian2021",
		"type": "paper-conference",
		"abstract": "We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462569",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "458–468",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic audit of italian car insurance: Evidence of unfairness in access and pricing",
		"URL": "https://doi.org/10.1145/3461702.3462569",
		"author": [
			{
				"family": "Fabris",
				"given": "Alessandro"
			},
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Gottardi",
				"given": "Stefano"
			},
			{
				"family": "Carletti",
				"given": "Mattia"
			},
			{
				"family": "Daicampi",
				"given": "Matteo"
			},
			{
				"family": "Susto",
				"given": "Gian Antonio"
			},
			{
				"family": "Silvello",
				"given": "Gianmaria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "flathmannModelingGuidingCreation2021",
		"type": "paper-conference",
		"abstract": "With artificial intelligence continuing to advance, so too do the ethical concerns that can potentially negatively impact humans and the greater society. When these systems begin to interact with humans, these concerns become much more complex and much more important. The field of human-AI teaming provides a relevant example of how AI ethics can have significant and continued effects on humans. This paper reviews research in ethical artificial intelligence, as well as ethical teamwork through the lens of the rapidly advancing field of human-AI teaming, resulting in a model demonstrating the requirements and outcomes of building ethical human-AI teams. The model is created to guide the prioritization of ethics in human-AI teaming by outlining the ethical teaming process, outcomes of ethical teams, and external requirements necessary to ensure ethical human-AI teams. A final discussion is presented on how the developed model will influence the implementation of AI teammates, as well as the development of policy and regulation surrounding the domain in the coming years.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462573",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "469–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modeling and guiding the creation of ethical human-AI teams",
		"URL": "https://doi.org/10.1145/3461702.3462573",
		"author": [
			{
				"family": "Flathmann",
				"given": "Christopher"
			},
			{
				"family": "Schelble",
				"given": "Beau G."
			},
			{
				"family": "Zhang",
				"given": "Rui"
			},
			{
				"family": "McNeese",
				"given": "Nathan J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fleisherWhatsFairIndividual2021",
		"type": "paper-conference",
		"abstract": "One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462621",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "480–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's fair about individual fairness?",
		"URL": "https://doi.org/10.1145/3461702.3462621",
		"author": [
			{
				"family": "Fleisher",
				"given": "Will"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "galhotraLearningGenerateFair2021",
		"type": "paper-conference",
		"abstract": "Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement. Clustering with proxies may lead to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462558",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "491–501",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to generate fair clusters from demonstrations",
		"URL": "https://doi.org/10.1145/3461702.3462558",
		"author": [
			{
				"family": "Galhotra",
				"given": "Sainyam"
			},
			{
				"family": "Saisubramanian",
				"given": "Sandhya"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "goldenEthicalObligationsProvide2021",
		"type": "paper-conference",
		"abstract": "TikTok is a popular platform that enables users to see tailored content feeds, particularly short videos with novel content. In recent years, TikTok has been criticized at times for presenting users with overly homogenous feeds, thereby reducing the diversity of content with which each user engages. In this paper, we consider whether TikTok has an ethical obligation to employ a novelty bias in its content recommendation engine. We explicate the principal morally relevant values and interests of key stakeholders, and observe that key empirical questions must be answered before a precise recommendation can be provided. We argue that TikTok's own values and interests mean that its actions should be largely driven by the values and interests of its users and creators. Unlike some other content platforms, TikTok's ethical obligations are not at odds with the values of its users, and so whether it is obligated to include a novelty bias depends on what will actually advance its users' interests.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462555",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "502–508",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical obligations to provide novelty",
		"URL": "https://doi.org/10.1145/3461702.3462555",
		"author": [
			{
				"family": "Golden",
				"given": "Paige"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grastienComputingPlansThat2021",
		"type": "paper-conference",
		"abstract": "There has been increasing acceptance that agents must act in a way that is sensitive to ethical considerations. These considerations have been cashed out as constraints, such that some actions are permissible, while others are impermissible. In this paper, we claim that, in addition to only performing those actions that are permissible, agents should only perform those courses of action that are <sub>u</sub>nambiguouslyₚermissible. By doing so they signal normative compliance: they communicate their understanding of, and commitment to abiding by, the normative constraints in play. Those courses of action (or plans) that succeed in signalling compliance in this sense, we term 'acceptable'. The problem this paper addresses is how to compute plans that signal compliance, that is, how to find plans that are acceptable as well as permissible. We do this by identifying those plans such that, were an observer to see only part of its execution, that observer would infer the plan enacted was permissible. This paper provides a formal definition of compliance signalling within the domain of AI planning, describes an algorithm for computing compliance signalling plans, provides preliminary experimental results and discusses possible improvements. The signalling of compliance is vital for communication, coordination and cooperation in situations where the agent is partially observed. It is equally vital, therefore, to solve the computational problem of finding those plans that signal compliance. This is what this paper does.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462607",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "509–518",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computing plans that signal normative compliance",
		"URL": "https://doi.org/10.1145/3461702.3462607",
		"author": [
			{
				"family": "Grastien",
				"given": "Alban"
			},
			{
				"family": "Benn",
				"given": "Claire"
			},
			{
				"family": "Thiébaux",
				"given": "Sylvie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "greenAIEthicsCourse2021",
		"type": "paper-conference",
		"abstract": "This is an experience report describing a pilot AI Ethics course for undergraduate computer science majors. In addition to teaching students about different ethical approaches and using them to analyze ethical issues, the course covered how ethics has been incorporated into the implementation of explicit ethical agents, and required students to implement an explicit ethical agent for a simple application. This report describes the course objectives and design, the topics covered, and a qualitative evaluation with suggestions for future offerings of the courses.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462552",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 6\npublisher-place: Virtual Event, USA",
		"page": "519–524",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An AI ethics course highlighting explicit ethical agents",
		"URL": "https://doi.org/10.1145/3461702.3462552",
		"author": [
			{
				"family": "Green",
				"given": "Nancy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grzelakDangersDrowsinessDetection2021",
		"type": "paper-conference",
		"abstract": "Drowsiness and fatigue are important factors in driving safety and work performance. This has motivated academic research into detecting drowsiness, and sparked interest in the deployment of related products in the insurance and work-productivity sectors. In this paper we elaborate on the potential dangers of using such algorithms. We first report on an audit of performance bias across subject gender and ethnicity, identifying which groups would be disparately harmed by the deployment of a state-of-the-art drowsiness detection algorithm. We discuss some of the sources of the bias, such as the lack of robustness of facial analysis algorithms to face occlusions, facial hair, or skin tone. We then identify potential downstream harms of this performance bias, as well as potential misuses of drowsiness detection technology—focusing on driving safety and experience, insurance cream-skimming and coverage-avoidance, worker surveillance, and job precarity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462593",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "525–531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dangers of drowsiness detection: Differential performance, downstream impact, and misuses",
		"URL": "https://doi.org/10.1145/3461702.3462593",
		"author": [
			{
				"family": "Grzelak",
				"given": "Jakub"
			},
			{
				"family": "Brandao",
				"given": "Martim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "guidottiDesigningShapeletsInterpretable2021",
		"type": "paper-conference",
		"abstract": "Time series shapelets are discriminatory subsequences which are representative of a class, and their similarity to a time series can be used for successfully tackling the time series classification problem. The literature shows that Artificial Intelligence (AI) systems adopting classification models based on time series shapelets can be interpretable, more accurate, and significantly fast. Thus, in order to design a data-agnostic and interpretable classification approach, in this paper we first extend the notion of shapelets to different types of data, i.e., images, tabular and textual data. Then, based on this extended notion of shapelets we propose an interpretable data-agnostic classification method. Since the shapelets discovery can be time consuming, especially for data types more complex than time series, we exploit a notion of prototypes for finding candidate shapelets, and reducing both the time required to find a solution and the variance of shapelets. A wide experimentation on datasets of different types shows that the data-agnostic prototype-based shapelets returned by the proposed method empower an interpretable classification which is also fast, accurate, and stable. In addition, we show and we prove that shapelets can be at the basis of explainable AI methods.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462553",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "532–542",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing shapelets for interpretable data-agnostic classification",
		"URL": "https://doi.org/10.1145/3461702.3462553",
		"author": [
			{
				"family": "Guidotti",
				"given": "Riccardo"
			},
			{
				"family": "Monreale",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hanleyComputerVisionConflicting2021",
		"type": "paper-conference",
		"abstract": "Scholars have recently drawn attention to a range of controversial issues posed by the use of computer vision for automatically generating descriptions of people in images. Despite these concerns, automated image description has become an important tool to ensure equitable access to information for blind and low vision people. In this paper, we investigate the ethical dilemmas faced by companies that have adopted the use of computer vision for producing alt text: textual descriptions of images for blind and low vision people. We use Facebook's automatic alt text tool as our primary case study. First, we analyze the policies that Facebook has adopted with respect to identity categories, such as race, gender, age, etc., and the company's decisions about whether to present these terms in alt text. We then describe an alternative—and manual—approach practiced in the museum community, focusing on how museums determine what to include in alt text descriptions of cultural artifacts. We compare these policies, using notable points of contrast to develop an analytic framework that characterizes the particular apprehensions behind these policy choices. We conclude by considering two strategies that seem to sidestep some of these concerns, finding that there are no easy ways to avoid the normative dilemmas posed by the use of computer vision to automate alt text.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462620",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "543–554",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computer vision and conflicting values: Describing people with automated alt text",
		"URL": "https://doi.org/10.1145/3461702.3462620",
		"author": [
			{
				"family": "Hanley",
				"given": "Margot"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			},
			{
				"family": "Azenkot",
				"given": "Shiri"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hannanWhoGetsWhat2021",
		"type": "paper-conference",
		"abstract": "Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the \"Who,\" \"What,\" and \"How\" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the \"Who\" and \"What,\" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462568",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "555–565",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who gets what, according to whom? An analysis of fairness perceptions in service allocation",
		"URL": "https://doi.org/10.1145/3461702.3462568",
		"author": [
			{
				"family": "Hannan",
				"given": "Jacqueline"
			},
			{
				"family": "Chen",
				"given": "Huei-Yen Winnie"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "heidenreichEarthFlatSun2021",
		"type": "paper-conference",
		"abstract": "This work considers universal adversarial triggers, a method of adversarially disrupting natural language models, and questions if it is possible to use such triggers to affect both the topic and stance of conditional text generation models. In considering four \"controversial\" topics, this work demonstrates success at identifying triggers that cause the GPT-2 model to produce text about targeted topics as well as influence the stance the text takes towards the topic. We show that, while the more fringe topics are more challenging to identify triggers for, they do appear to more effectively discriminate aspects like stance. We view this both as an indication of the dangerous potential for controllability and, perhaps, a reflection of the nature of the disconnect between conflicting views on these topics, something that future work could use to question the nature of filter bubbles and if they are reflected within models trained on internet content. In demonstrating the feasibility and ease of such an attack, this work seeks to raise the awareness that neural language models are susceptible to this influence–even if the model is already deployed and adversaries lack internal model access–and advocates the immediate safeguarding against this type of adversarial attack in order to prevent potential harm to human users.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462578",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "566–573",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The earth is flat and the sun is not a star: The susceptibility of GPT-2 to universal adversarial triggers",
		"URL": "https://doi.org/10.1145/3461702.3462578",
		"author": [
			{
				"family": "Heidenreich",
				"given": "Hunter Scott"
			},
			{
				"family": "Williams",
				"given": "Jake Ryland"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "henriksenSituatedAccountabilityEthical2021a",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has the potential to benefit humans and society by its employment in important sectors. However, the risks of negative consequences have underscored the importance of accountability for AI systems, their outcomes, and the users of such systems. In recent years, various accountability mechanisms have been put forward in pursuit of the responsible design, development, and use of AI. In this article, we provide an in-depth study of three such mechanisms, as we analyze Scandinavian AI developers' encounter with (1) ethical principles, (2) certification standards, and (3) explanation methods. By doing so, we contribute to closing a gap in the literature between discussions of accountability on the research and policy level, and accountability as a responsibility put on the shoulders of developers in practice. Our study illustrates important flaws in the current enactment of accountability as an ethical and social value which, if left unchecked, risks undermining the pursuit of responsible AI. By bringing attention to these flaws, the article signals where further work is needed in order to build effective accountability systems for AI.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462564",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "574–585",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Situated accountability: Ethical principles, certification standards, and explanation methods in applied AI",
		"URL": "https://doi.org/10.1145/3461702.3462564",
		"author": [
			{
				"family": "Henriksen",
				"given": "Anne"
			},
			{
				"family": "Enni",
				"given": "Simon"
			},
			{
				"family": "Bechmann",
				"given": "Anja"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "islamCanWeObtain2021",
		"type": "paper-conference",
		"abstract": "There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462614",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "586–596",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can we obtain fairness for free?",
		"URL": "https://doi.org/10.1145/3461702.3462614",
		"author": [
			{
				"family": "Islam",
				"given": "Rashidul"
			},
			{
				"family": "Pan",
				"given": "Shimei"
			},
			{
				"family": "Foulds",
				"given": "James R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "javadiMonitoringAIServices2021",
		"type": "paper-conference",
		"abstract": "Given the surge in interest in AI, we now see the emergence of Artificial Intelligence as a Service (AIaaS). AIaaS entails service providers offering remote access to ML models and capabilities at arms-length', through networked APIs. Such services will grow in popularity, as they enable access to state-of-the-art ML capabilities, 'on demand', 'out of the box', at low cost and without requiring training data or ML expertise. However, there is much public concern regarding AI. AIaaS raises particular considerations, given there is much potential for such services to be used to underpin and drive problematic, inappropriate, undesirable, controversial, or possibly even illegal applications. A key way forward is through service providers monitoring their AI services to identify potential situations of problematic use. Towards this, we elaborate the potential for 'misuse indicators' as a mechanism for uncovering patterns of usage behaviour warranting consideration or further investigation. We introduce a taxonomy for describing these indicators and their contextual considerations, and use exemplars to demonstrate the feasibility analysing AIaaS usage to highlight situations of possible concern. We also seek to draw more attention to AI services and the issues they raise, given AIaaS' increasing prominence, and the general calls for the more responsible and accountable use of AI.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462566",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "597–607",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Monitoring AI services for misuse",
		"URL": "https://doi.org/10.1145/3461702.3462566",
		"author": [
			{
				"family": "Javadi",
				"given": "Seyyed Ahmad"
			},
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jiangEquityAlgorithmicFairness2021",
		"type": "paper-conference",
		"abstract": "Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462623",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "608–617",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards equity and algorithmic fairness in student grade prediction",
		"URL": "https://doi.org/10.1145/3461702.3462623",
		"author": [
			{
				"family": "Jiang",
				"given": "Weijie"
			},
			{
				"family": "Pardos",
				"given": "Zachary A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehEthicalGravityThesis2021",
		"type": "paper-conference",
		"abstract": "Computers are used to make decisions in an increasing number of domains. There is widespread agreement that some of these uses are ethically problematic. Far less clear is where ethical problems arise, and what might be done about them. This paper expands and defends the Ethical Gravity Thesis: ethical problems that arise at higher levels of analysis of an automated decision-making system are inherited by lower levels of analysis. Particular instantiations of systems can add new problems, but not ameliorate more general ones. We defend this thesis by adapting Marr's famous 1982 framework for understanding information-processing systems. We show how this framework allows one to situate ethical problems at the appropriate level of abstraction, which in turn can be used to target appropriate interventions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462606",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "618–626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical gravity thesis: Marrian levels and the persistence of bias in automated decision-making systems",
		"URL": "https://doi.org/10.1145/3461702.3462606",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Klein",
				"given": "Colin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kelleyExcitingUsefulWorrying2021",
		"type": "paper-conference",
		"abstract": "As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462605",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "627–637",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exciting, useful, worrying, futuristic: Public perception of artificial intelligence in 8 countries",
		"URL": "https://doi.org/10.1145/3461702.3462605",
		"author": [
			{
				"family": "Kelley",
				"given": "Patrick Gage"
			},
			{
				"family": "Yang",
				"given": "Yongwei"
			},
			{
				"family": "Heldreth",
				"given": "Courtney"
			},
			{
				"family": "Moessner",
				"given": "Christopher"
			},
			{
				"family": "Sedley",
				"given": "Aaron"
			},
			{
				"family": "Kramm",
				"given": "Andreas"
			},
			{
				"family": "Newman",
				"given": "David T."
			},
			{
				"family": "Woodruff",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kimAgeBiasEmotion2021",
		"type": "paper-conference",
		"abstract": "The growing potential for facial emotion recognition (FER) technology has encouraged expedited development at the cost of rigorous validation. Many of its use-cases may also impact the diverse global community as FER becomes embedded into domains ranging from education to security to healthcare. Yet, prior work has highlighted that FER can exhibit both gender and racial biases like other facial analysis techniques. As a result, bias-mitigation research efforts have mainly focused on tackling gender and racial disparities, while other demographic related biases, such as age, have seen less progress. This work seeks to examine the performance of state of the art commercial FER technology on expressive images of men and women from three distinct age groups. We utilize four different commercial FER systems in a black box methodology to evaluate how six emotions - anger, disgust, fear, happiness, neutrality, and sadness - are correctly detected by age group. We further investigate how algorithmic changes over the last year have affected system performance. Our results found that all four commercial FER systems most accurately perceived emotion in images of young adults and least accurately in images of older adults. This trend was observed for analyses conducted in 2019 and 2020. However, little to no gender disparities were observed in either year. While older adults may not have been the initial target consumer of FER technology, statistics show the demographic is quickly growing more keen to applications that use such systems. Our results demonstrate the importance of considering various demographic subgroups during FER system validation and the need for inclusive, intersectional algorithmic developmental practices.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462609",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "638–644",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Age bias in emotion detection: An analysis of facial emotion recognition performance on young, middle-aged, and older adults",
		"URL": "https://doi.org/10.1145/3461702.3462609",
		"author": [
			{
				"family": "Kim",
				"given": "Eugenia"
			},
			{
				"family": "Bryant",
				"given": "De'Aira"
			},
			{
				"family": "Srikanth",
				"given": "Deepak"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "klinovaAISharedProsperity2021",
		"type": "paper-conference",
		"abstract": "Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462619",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "645–651",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI and shared prosperity",
		"URL": "https://doi.org/10.1145/3461702.3462619",
		"author": [
			{
				"family": "Klinova",
				"given": "Katya"
			},
			{
				"family": "Korinek",
				"given": "Anton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kommiyamothilalUnifyingFeatureAttribution2021",
		"type": "paper-conference",
		"abstract": "Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an importance score to each input feature, while the latter provides input examples with minimal changes to alter the model's predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complimentary of these two approaches. Our evaluation on three benchmark datasets — Adult-Income, LendingClub, and German-Credit — confirms the complimentary. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462597",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "652–663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards unifying feature attribution and counterfactual explanations: Different means to the same end",
		"URL": "https://doi.org/10.1145/3461702.3462597",
		"author": [
			{
				"family": "Kommiya Mothilal",
				"given": "Ramaravind"
			},
			{
				"family": "Mahajan",
				"given": "Divyat"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			},
			{
				"family": "Sharma",
				"given": "Amit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kshirsagarBecomingGoodAI2021",
		"type": "paper-conference",
		"abstract": "AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462599",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "664–673",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Becoming good at AI for good",
		"URL": "https://doi.org/10.1145/3461702.3462599",
		"author": [
			{
				"family": "Kshirsagar",
				"given": "Meghana"
			},
			{
				"family": "Robinson",
				"given": "Caleb"
			},
			{
				"family": "Yang",
				"given": "Siyu"
			},
			{
				"family": "Gholami",
				"given": "Shahrzad"
			},
			{
				"family": "Klyuzhin",
				"given": "Ivan"
			},
			{
				"family": "Mukherjee",
				"given": "Sumit"
			},
			{
				"family": "Nasir",
				"given": "Md"
			},
			{
				"family": "Ortiz",
				"given": "Anthony"
			},
			{
				"family": "Oviedo",
				"given": "Felipe"
			},
			{
				"family": "Tanner",
				"given": "Darren"
			},
			{
				"family": "Trivedi",
				"given": "Anusua"
			},
			{
				"family": "Xu",
				"given": "Yixi"
			},
			{
				"family": "Zhong",
				"given": "Ming"
			},
			{
				"family": "Dilkina",
				"given": "Bistra"
			},
			{
				"family": "Dodhia",
				"given": "Rahul"
			},
			{
				"family": "Lavista Ferres",
				"given": "Juan M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kuhlmanMeasuringGroupAdvantage2021",
		"type": "paper-conference",
		"abstract": "Ranking evaluation metrics play an important role in information retrieval, providing optimization objectives during development and means of assessment of deployed performance. Recently, fairness of rankings has been recognized as crucial, especially as automated systems are increasingly used for high impact decisions. While numerous fairness metrics have been proposed, a comparative analysis to understand their interrelationships is lacking. Even for fundamental statistical parity metrics which measure group advantage, it remains unclear whether metrics measure the same phenomena, or when one metric may produce different results than another. To address these open questions, we formulate a conceptual framework for analytical comparison of metrics. We prove that under reasonable assumptions, popular metrics in the literature exhibit the same behavior and that optimizing for one optimizes for all. However, our analysis also shows that the metrics vary in the degree of unfairness measured, in particular when one group has a strong majority. Based on this analysis, we design a practical statistical test to identify whether observed data is likely to exhibit predictable group bias. We provide a set of recommendations for practitioners to guide the choice of an appropriate fairness metric.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462588",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "674–682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring group advantage: A comparative study of fair ranking metrics",
		"URL": "https://doi.org/10.1145/3461702.3462588",
		"author": [
			{
				"family": "Kuhlman",
				"given": "Caitlin"
			},
			{
				"family": "Gerych",
				"given": "Walter"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "larsenFrameworkUnderstandingAIInduced2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems operate in increasingly diverse areas, from healthcare to facial recognition, the stock market, autonomous vehicles, and so on. While the underlying digital infrastructure of AI systems is developing rapidly, each area of implementation is subject to different degrees and processes of legitimization. By combining elements from institutional theory and information systems-theory, this paper presents a conceptual framework to analyze and understand AI-induced field-change. The introduction of novel AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape organizations and institutions while existing institutional infrastructures determine the scope and speed at which organizational change is allowed to occur. Where institutional infrastructure and governance arrangements, such as standards, rules, and regulations, still are unelaborate, the field can move fast but is also more likely to be contested. The institutional infrastructure surrounding AI-induced fields is generally little elaborated, which could be an obstacle to the broader institutionalization of AI-systems going forward.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462591",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "683–694",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for understanding AI-Induced field change: How AI technologies are legitimized and institutionalized",
		"URL": "https://doi.org/10.1145/3461702.3462591",
		"author": [
			{
				"family": "Larsen",
				"given": "Benjamin Cedric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leavyEthicalDataCuration2021",
		"type": "paper-conference",
		"abstract": "The potential for bias embedded in data to lead to the perpetuation of social injustice though Artificial Intelligence (AI) necessitates an urgent reform of data curation practices for AI systems, especially those based on machine learning. Without appropriate ethical and regulatory frameworks there is a risk that decades of advances in human rights and civil liberties may be undermined. This paper proposes an approach to data curation for AI, grounded in feminist epistemology and informed by critical theories of race and feminist principles. The objective of this approach is to support critical evaluation of the social dynamics of power embedded in data for AI systems. We propose a set of fundamental guiding principles for ethical data curation that address the social construction of knowledge, call for inclusion of subjugated and new forms of knowledge, support critical evaluation of theoretical concepts within data and recognise the reflexive nature of knowledge. In developing this ethical framework for data curation, we aim to contribute to a virtue ethics for AI and ensure protection of fundamental and human rights.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462598",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "695–703",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical data curation for AI: An approach based on feminist epistemology and critical theories of race",
		"URL": "https://doi.org/10.1145/3461702.3462598",
		"author": [
			{
				"family": "Leavy",
				"given": "Susan"
			},
			{
				"family": "Siapera",
				"given": "Eugenia"
			},
			{
				"family": "O'Sullivan",
				"given": "Barry"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leeRiskIdentificationQuestionnaire2021",
		"type": "paper-conference",
		"abstract": "Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners.In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462572",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "704–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Risk identification questionnaire for detecting unintended bias in the machine learning development lifecycle",
		"URL": "https://doi.org/10.1145/3461702.3462572",
		"author": [
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leeParticipatoryAlgorithmicManagement2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence is increasingly being used to manage the workforce. Algorithmic management promises organizational efficiency, but often undermines worker well-being. How can we computationally model worker well-being so that algorithmic management can be optimized for and assessed in terms of worker well-being? Toward this goal, we propose a participatory approach for worker well-being models. We first define worker well-being models: Work preference models—preferences about work and working conditions, and managerial fairness models—beliefs about fair resource allocation among multiple workers. We then propose elicitation methods to enable workers to build their own well-being models leveraging pairwise comparisons and ranking. As a case study, we evaluate our methods in the context of algorithmic work scheduling with 25 shift workers and 3 managers. The findings show that workers expressed idiosyncratic work preference models and more uniform managerial fairness models, and the elicitation methods helped workers discover their preferences and gave them a sense of empowerment. Our work provides a method and initial evidence for enabling participatory algorithmic management for worker well-being.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462628",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "715–726",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participatory algorithmic management: Elicitation methods for worker well-being models",
		"URL": "https://doi.org/10.1145/3461702.3462628",
		"author": [
			{
				"family": "Lee",
				"given": "Min Kyung"
			},
			{
				"family": "Nigam",
				"given": "Ishan"
			},
			{
				"family": "Zhang",
				"given": "Angie"
			},
			{
				"family": "Afriyie",
				"given": "Joel"
			},
			{
				"family": "Qin",
				"given": "Zhizhen"
			},
			{
				"family": "Gao",
				"given": "Sicun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leggettFeedingBeastSuperintelligence2021",
		"type": "paper-conference",
		"abstract": "Scientists and philosophers have warned of the possibility that humans, in the future, might create a 'superintelligent' machine that could, in some scenarios, form an existential threat to humanity. This paper argues that such a machine may already exist, and that, if so, it does, in fact, represent such a threat.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462581",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "727–735",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Feeding the beast: Superintelligence, corporate capitalism and the end of humanity",
		"URL": "https://doi.org/10.1145/3461702.3462581",
		"author": [
			{
				"family": "Leggett",
				"given": "Dominic"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leibowiczDeepfakeDetectionDilemma2021",
		"type": "paper-conference",
		"abstract": "Synthetic media detection technologies label media as either synthetic or non-synthetic and are increasingly used by journalists, web platforms, and the general public to identify misinformation and other forms of problematic content. As both well-resourced organizations and the non-technical general public generate more sophisticated synthetic media, the capacity for purveyors of problematic content to adapt induces a detection dilemma : as detection practices become more accessible, they become more easily circumvented. This paper describes how a multistakeholder cohort from academia, technology platforms, media entities, and civil society organizations active in synthetic media detection and its socio-technical implications evaluates the detection dilemma. Specifically, we offer an assessment of detection contexts and adversary capacities sourced from the broader, global AI and media integrity community concerned with mitigating the spread of harmful synthetic media. A collection of personas illustrates the intersection between unsophisticated and highly-resourced sponsors of misinformation in the context of their technical capacities. This work concludes that there is no \"best” approach to navigating the detector dilemma, but derives a set of implications from multistakeholder input to better inform detection process decisions and policies, in practice.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462584",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "736–744",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The deepfake detection dilemma: A multistakeholder exploration of adversarial dynamics in synthetic media",
		"URL": "https://doi.org/10.1145/3461702.3462584",
		"author": [
			{
				"family": "Leibowicz",
				"given": "Claire R."
			},
			{
				"family": "McGregor",
				"given": "Sean"
			},
			{
				"family": "Ovadya",
				"given": "Aviv"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liuRAWLSNETAlteringBayesian2021",
		"type": "paper-conference",
		"abstract": "We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462618",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "745–755",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "RAWLSNET: Altering bayesian networks to encode rawlsian fair equality of opportunity",
		"URL": "https://doi.org/10.1145/3461702.3462618",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Shafi",
				"given": "Zohair"
			},
			{
				"family": "Fleisher",
				"given": "William"
			},
			{
				"family": "Eliassi-Rad",
				"given": "Tina"
			},
			{
				"family": "Alfeld",
				"given": "Scott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "loiFairEqualityChances2021",
		"type": "paper-conference",
		"abstract": "This article presents a fairness principle for evaluating decision-making based on predictions: a decision rule is unfair when the individuals directly impacted by the decisions who are equal with respect to the features that justify inequalities in outcomes do not have the same statistical prospects of being benefited or harmed by them, irrespective of their socially salient morally arbitrary traits. The principle can be used to evaluate prediction-based decision-making from the point of view of a wide range of antecedently specified substantive views about justice in outcome distributions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462613",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair equality of chances for prediction-based decisions",
		"URL": "https://doi.org/10.1145/3461702.3462613",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Herlitz",
				"given": "Anders"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "loiAccountabilityUseArtificial2021",
		"type": "paper-conference",
		"abstract": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462631",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "757–766",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards accountability in the use of artificial intelligence for public administrations",
		"URL": "https://doi.org/10.1145/3461702.3462631",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Spielkamp",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mazijnHowScoreDistributions2021",
		"type": "paper-conference",
		"abstract": "Automated decisions based on trained algorithms influence human life in an increasingly far-reaching way. In recent years, it has become clear that these decisions are often accompanied by bias and unfair treatment of different subpopulations.Meanwhile, several notions of fairness circulate in the scientific literature, with trade-offs between profit and fairness and between fairness metrics among themselves. Based on both analytical calculations and numerical simulations, we show in this study that some profit-fairness trade-offs and fairness-fairness trade-offs depend substantially on the underlying score distributions given to subpopulations and we present two complementary perspectives to visualize this influence. We further show that higher symmetry in scores of subpopulations can significantly reduce the trade-offs between fairness notions within a given acceptable strictness, even when sacrificing expressiveness. Our exploratory study may help to understand how to overcome the strict mathematical statements about the statistical incompatibility of certain fairness notions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462601",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "767–776",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do the score distributions of subpopulations influence fairness notions?",
		"URL": "https://doi.org/10.1145/3461702.3462601",
		"author": [
			{
				"family": "Mazijn",
				"given": "Carmen"
			},
			{
				"family": "Danckaert",
				"given": "Jan"
			},
			{
				"family": "Ginis",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mehrotraMoreSimilarValues2021",
		"type": "paper-conference",
		"abstract": "As AI systems are increasingly involved in decision making, it also becomes important that they elicit appropriate levels of trust from their users. To achieve this, it is first important to understand which factors influence trust in AI. We identify that a research gap exists regarding the role of personal values in trust in AI. Therefore, this paper studies how human and agent Value Similarity (VS) influences a human's trust in that agent. To explore this, 89 participants teamed up with five different agents, which were designed with varying levels of value similarity to that of the participants. In a within-subjects, scenario-based experiment, agents gave suggestions on what to do when entering the building to save a hostage. We analyzed the agent's scores on subjective value similarity, trust and qualitative data from open-ended questions. Our results show that agents rated as having more similar values also scored higher on trust, indicating a positive effect between the two. With this result, we add to the existing understanding of human-agent trust by providing insight into the role of value-similarity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462576",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "777–783",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More similar values, more trust? - the effect of value similarity on trust in human-agent interaction",
		"URL": "https://doi.org/10.1145/3461702.3462576",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Siddharth"
			},
			{
				"family": "Jonker",
				"given": "Catholijn M."
			},
			{
				"family": "Tielman",
				"given": "Myrthe L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mhasawadeCausalMultilevelFairness2021",
		"type": "paper-conference",
		"abstract": "Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462587",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "784–794",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal multi-level fairness",
		"URL": "https://doi.org/10.1145/3461702.3462587",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nanayakkaraUnpackingExpressedConsequences2021",
		"type": "paper-conference",
		"abstract": "The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462608",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "795–806",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unpacking the expressed consequences of AI research in broader impact statements",
		"URL": "https://doi.org/10.1145/3461702.3462608",
		"author": [
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nielsenMeasuringLayReactions2021",
		"type": "paper-conference",
		"abstract": "The recording, aggregation, and exchange of personal data is necessary to the development of socially-relevant machine learning applications. However, anecdotal and survey evidence show that ordinary people feel discontent and even anger regarding data collection practices that are currently typical and legal. This suggests that personal data markets in their current form do not adhere to the norms applied by ordinary people. The present study experimentally probes whether market transactions in a typical online scenario are accepted when evaluated by lay people. The results show that a high percentage of study participants refused to participate in a data pricing exercise, even in a commercial context where market rules would typically be expected to apply. For those participants who did price the data, the median price was an order of magnitude higher than the market price. These results call into question the notice and consent market paradigm that is used by technology firms and government regulators when evaluating data flows. The results also point to a conceptual mismatch between cultural and legal expectations regarding the use of personal data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462582",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "807–813",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring lay reactions to personal data markets",
		"URL": "https://doi.org/10.1145/3461702.3462582",
		"author": [
			{
				"family": "Nielsen",
				"given": "Aileen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pagnuccoEpistemicReasoningMachine2021",
		"type": "paper-conference",
		"abstract": "With the rapid development of autonomous machines such as selfdriving vehicles and social robots, there is increasing realisation that machine ethics is important for widespread acceptance of autonomous machines. Our objective is to encode ethical reasoning into autonomous machines following well-defined ethical principles and behavioural norms. We provide an approach to reasoning about actions that incorporates ethical considerations. It builds on Scherl and Levesque's [29, 30] approach to knowledge in the situation calculus. We show how reasoning about knowledge in a dynamic setting can be used to guide ethical and moral choices, aligned with consequentialist and deontological approaches to ethics. We apply our approach to autonomous driving and social robot scenarios, and provide an implementation framework.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462586",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "814–821",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic reasoning for machine ethics with situation calculus",
		"URL": "https://doi.org/10.1145/3461702.3462586",
		"author": [
			{
				"family": "Pagnucco",
				"given": "Maurice"
			},
			{
				"family": "Rajaratnam",
				"given": "David"
			},
			{
				"family": "Limarga",
				"given": "Raynaldio"
			},
			{
				"family": "Nayak",
				"given": "Abhaya"
			},
			{
				"family": "Song",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pandeyDisparateImpactArtificial2021",
		"type": "paper-conference",
		"abstract": "Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462561",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "822–833",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparate impact of artificial intelligence bias in ridehailing economy's price discrimination algorithms",
		"URL": "https://doi.org/10.1145/3461702.3462561",
		"author": [
			{
				"family": "Pandey",
				"given": "Akshat"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "parkUnderstandingRepresentationRepresentativeness2021",
		"type": "paper-conference",
		"abstract": "A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462590",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "834–842",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding the representation and representativeness of age in AI data sets",
		"URL": "https://doi.org/10.1145/3461702.3462590",
		"author": [
			{
				"family": "Park",
				"given": "Joon Sung"
			},
			{
				"family": "Bernstein",
				"given": "Michael S."
			},
			{
				"family": "Brewer",
				"given": "Robin N."
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "perrierQuantumFairMachine2021",
		"type": "paper-conference",
		"abstract": "In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462611",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "843–853",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Quantum fair machine learning",
		"URL": "https://doi.org/10.1145/3461702.3462611",
		"author": [
			{
				"family": "Perrier",
				"given": "Elija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "perroneFairBayesianOptimization2021",
		"type": "paper-conference",
		"abstract": "Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462629",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "854–863",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair bayesian optimization",
		"URL": "https://doi.org/10.1145/3461702.3462629",
		"author": [
			{
				"family": "Perrone",
				"given": "Valerio"
			},
			{
				"family": "Donini",
				"given": "Michele"
			},
			{
				"family": "Zafar",
				"given": "Muhammad Bilal"
			},
			{
				"family": "Schmucker",
				"given": "Robin"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Archambeau",
				"given": "Cédric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "posadaWeHaventGone2021",
		"type": "paper-conference",
		"abstract": "How should we understand the social and political effects of the datafication of human life? This paper argues that the effects of data should be understood as a constitutive shift in social and political relations. We explore how datafication, or quantification of human and non-human factors into binary code, affects the identity of individuals and groups. This fundamental shift goes beyond economic and ethical concerns, which has been the focus of other efforts to explore the effects of datafication and AI. We highlight that technologies such as datafication and AI (and previously, the printing press) both disrupted extant power arrangements, leading to decentralization, and triggered a recentralization of power by new actors better adapted to leveraging the new technology. We use the analogy of the printing press to provide a framework for understanding constitutive change. The printing press example gives us more clarity on 1) what can happen when the medium of communication drastically alters how information is communicated and stored; 2) the shift in power from state to private actors; and 3) the tension of simultaneously connecting individuals while driving them towards narrower communities through algorithmic analyses of data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462604",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "864–872",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "We haven't gone paperless yet: Why the printing press can help us understand data and AI",
		"URL": "https://doi.org/10.1145/3461702.3462604",
		"author": [
			{
				"family": "Posada",
				"given": "Julian"
			},
			{
				"family": "Weller",
				"given": "Nicholas"
			},
			{
				"family": "Wong",
				"given": "Wendy H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "prostMeasuringModelFairness2021",
		"type": "paper-conference",
		"abstract": "In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462603",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "873–883",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring model fairness under noisy covariates: A theoretical perspective",
		"URL": "https://doi.org/10.1145/3461702.3462603",
		"author": [
			{
				"family": "Prost",
				"given": "Flavien"
			},
			{
				"family": "Awasthi",
				"given": "Pranjal"
			},
			{
				"family": "Blumm",
				"given": "Nick"
			},
			{
				"family": "Kumthekar",
				"given": "Aditee"
			},
			{
				"family": "Potter",
				"given": "Trevor"
			},
			{
				"family": "Wei",
				"given": "Li"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Chen",
				"given": "Jilin"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ramachandranGAEAGraphAugmentation2021",
		"type": "paper-conference",
		"abstract": "Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1-1/3e). We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462615",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "884–894",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GAEA: Graph augmentation for equitable access via reinforcement learning",
		"URL": "https://doi.org/10.1145/3461702.3462615",
		"author": [
			{
				"family": "Ramachandran",
				"given": "Govardana Sachithanandam"
			},
			{
				"family": "Brugere",
				"given": "Ivan"
			},
			{
				"family": "Varshney",
				"given": "Lav R."
			},
			{
				"family": "Xiong",
				"given": "Caiming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "razFaceMisIDInteractive2021",
		"type": "paper-conference",
		"abstract": "This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462627",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "895–904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Face mis-ID: An interactive pedagogical tool demonstrating disparate accuracy rates in facial recognition",
		"URL": "https://doi.org/10.1145/3461702.3462627",
		"author": [
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "schelenzTheoryPracticeEthical2021",
		"type": "paper-conference",
		"abstract": "Diversity-aware platform design is a paradigm that responds to the ethical challenges of existing social media platforms. Available platforms have been criticized for minimizing users' autonomy, marginalizing minorities, and exploiting users' data for profit maximization. This paper presents a design solution that centers the well-being of users. It presents the theory and practice of designing a diversity-aware platform for social relations. In this approach, the diversity of users is leveraged in a way that allows like-minded individuals to pursue similar interests or diverse individuals to complement each other in a complex activity. The end users of the envisioned platform are students, who participate in the design process. Diversity-aware platform design involves numerous steps, of which two are highlighted in this paper: 1) defining a framework and operationalizing the \"diversity\" of students, 2) collecting \"diversity\" data to build diversity-aware algorithms. The paper further reflects on the ethical challenges encountered during the design of a diversity-aware platform.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462595",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "905–915",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The theory, practice, and ethical challenges of designing a diversity-aware platform for social relations",
		"URL": "https://doi.org/10.1145/3461702.3462595",
		"author": [
			{
				"family": "Schelenz",
				"given": "Laura"
			},
			{
				"family": "Bison",
				"given": "Ivano"
			},
			{
				"family": "Busso",
				"given": "Matteo"
			},
			{
				"family": "Götzen",
				"given": "Amalia",
				"non-dropping-particle": "de"
			},
			{
				"family": "Gatica-Perez",
				"given": "Daniel"
			},
			{
				"family": "Giunchiglia",
				"given": "Fausto"
			},
			{
				"family": "Meegahapola",
				"given": "Lakmal"
			},
			{
				"family": "Ruiz-Correa",
				"given": "Salvador"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "schumannStepMoreInclusive2021",
		"type": "paper-conference",
		"abstract": "The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462594",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "916–925",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A step toward more inclusive people annotations for fairness",
		"URL": "https://doi.org/10.1145/3461702.3462594",
		"author": [
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "Ricco",
				"given": "Susanna"
			},
			{
				"family": "Prabhu",
				"given": "Utsav"
			},
			{
				"family": "Ferrari",
				"given": "Vittorio"
			},
			{
				"family": "Pantofaru",
				"given": "Caroline"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "segalFairnessEyesData2021",
		"type": "paper-conference",
		"abstract": "We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462554",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "926–935",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in the eyes of the data: Certifying machine-learning models",
		"URL": "https://doi.org/10.1145/3461702.3462554",
		"author": [
			{
				"family": "Segal",
				"given": "Shahar"
			},
			{
				"family": "Adi",
				"given": "Yossi"
			},
			{
				"family": "Pinkas",
				"given": "Benny"
			},
			{
				"family": "Baum",
				"given": "Carsten"
			},
			{
				"family": "Ganesh",
				"given": "Chaya"
			},
			{
				"family": "Keshet",
				"given": "Joseph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shahRawlsianFairAdaptation2021",
		"type": "paper-conference",
		"abstract": "Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462592",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "936–945",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rawlsian fair adaptation of deep learning classifiers",
		"URL": "https://doi.org/10.1145/3461702.3462592",
		"author": [
			{
				"family": "Shah",
				"given": "Kulin"
			},
			{
				"family": "Gupta",
				"given": "Pooja"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			},
			{
				"family": "Bhattacharyya",
				"given": "Chiranjib"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "sharmaFaiRNFairRobust2021",
		"type": "paper-conference",
		"abstract": "Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462559",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "946–955",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FaiR-N: Fair and robust neural networks for structured data",
		"URL": "https://doi.org/10.1145/3461702.3462559",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Gee",
				"given": "Alan H."
			},
			{
				"family": "Paydarfar",
				"given": "David"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "simonsMachineLearningMeaning2021",
		"type": "paper-conference",
		"abstract": "Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462556",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "956–966",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning and the meaning of equal treatment",
		"URL": "https://doi.org/10.1145/3461702.3462556",
		"author": [
			{
				"family": "Simons",
				"given": "Joshua"
			},
			{
				"family": "Adams Bhatti",
				"given": "Sophia"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "slavkovikDigitalVoodooDolls2021",
		"type": "paper-conference",
		"abstract": "An institution, be it a body of government, commercial enterprise, or a service, cannot interact directly with a person. Instead, a model is created to represent us. We argue the existence of a new high-fidelity type of person model which we call a digital voodoo doll. We conceptualize it and compare its features with existing models of persons. Digital voodoo dolls are distinguished by existing completely beyond the influence and control of the person they represent. We discuss the ethical issues that such a lack of accountability creates and argue how these concerns can be mitigated.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462626",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "967–977",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Digital voodoo dolls",
		"URL": "https://doi.org/10.1145/3461702.3462626",
		"author": [
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Stachl",
				"given": "Clemens"
			},
			{
				"family": "Pitman",
				"given": "Caroline"
			},
			{
				"family": "Askonas",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "solansComparingEquityEffectiveness2021",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years.We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups.Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462600",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "978–988",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Comparing equity and effectiveness of different algorithms in an application for the room rental market",
		"URL": "https://doi.org/10.1145/3461702.3462600",
		"author": [
			{
				"family": "Solans",
				"given": "David"
			},
			{
				"family": "Fabbri",
				"given": "Francesco"
			},
			{
				"family": "Calsamiglia",
				"given": "Caterina"
			},
			{
				"family": "Castillo",
				"given": "Carlos"
			},
			{
				"family": "Bonchi",
				"given": "Francesco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "suhrDoesFairRanking2021",
		"type": "paper-conference",
		"abstract": "Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice.In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462602",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "989–999",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does fair ranking improve minority outcomes? Understanding the interplay of human and algorithmic biases in online hiring",
		"URL": "https://doi.org/10.1145/3461702.3462602",
		"author": [
			{
				"family": "Sühr",
				"given": "Tom"
			},
			{
				"family": "Hilgard",
				"given": "Sophie"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "waitesDifferentiallyPrivateNormalizing2021",
		"type": "paper-conference",
		"abstract": "Normalizing flow models have risen as a popular solution to the problem of density estimation, enabling high-quality synthetic data generation as well as exact probability density evaluation. However, in contexts where individuals are directly associated with the training data, releasing such a model raises privacy concerns. In this work, we propose the use of normalizing flow models that provide explicit differential privacy guarantees as a novel approach to the problem of privacy-preserving density estimation. We evaluate the efficacy of our approach empirically using benchmark datasets, and we demonstrate that our method substantially outperforms previous state-of-the-art approaches. We additionally show how our algorithm can be applied to the task of differentially private anomaly detection.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462625",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "1000–1009",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Differentially private normalizing flows for privacy-preserving density estimation",
		"URL": "https://doi.org/10.1145/3461702.3462625",
		"author": [
			{
				"family": "Waites",
				"given": "Chris"
			},
			{
				"family": "Cummings",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "watkinsGoverningAlgorithmicSystems2021",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making and decision-support systems (ADS) are gaining influence over how society distributes resources, administers justice, and provides access to opportunities. Yet collectively we do not adequately study how these systems affect people or document the actual or potential harms resulting from their integration with important social functions. This is a significant challenge for computational justice efforts of measuring and governing AI systems. Impact assessments are often used as instruments to create accountability relationships and grant some measure of agency and voice to communities affected by projects with environmental, financial, and human rights ramifications. Applying these tools-through Algorithmic Impact Assessments (AIA)-is a plausible way to establish accountability relationships for ADSs. At the same time, what an AIA would entail remains under-specified; they raise as many questions as they answer. Choices about the methods, scope, and purpose of AIAs structure the conditions of possibility for AI governance. In this paper, we present our research on the history of impact assessments across diverse domains, through a sociotechnical lens, to present six observations on how they co-constitute accountability. Decisions about what type of effects count as an impact; when impacts are assessed; whose interests are considered; who is invited to participate; who conducts the assessment; how assessments are made publicly available, and what the outputs of the assessment might be; all shape the forms of accountability that AIAs engender. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462580",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, USA",
		"page": "1010–1022",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Governing algorithmic systems with impact assessments: Six observations",
		"URL": "https://doi.org/10.1145/3461702.3462580",
		"author": [
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yaghiniHumanintheloopFrameworkConstruct2021",
		"type": "paper-conference",
		"abstract": "Existing mathematical notions of fairness fail to account for the context of decision-making. We argue that moral consideration of contextual factors is an inherently human task. So we present a framework to learn context-aware mathematical formulations of fairness by eliciting people's situated fairness assessments. Our family of fairness notions corresponds to a new interpretation of economic models of Equality of Opportunity (EOP), and it includes most existing notions of fairness as special cases. Our human-in-the-loop approach is designed to learn the appropriate parameters of the EOP family by utilizing human responses to pair-wise questions about decision subjects' circumstance and deservingness, and the harm/benefit imposed on them. We illustrate our framework in a hypothetical criminal risk assessment scenario by conducting a series of human-subject experiments on Amazon Mechanical Turk. Our work takes an important initial step toward empowering stakeholders to have a voice in the formulation of fairness for Machine Learning.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462583",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "1023–1033",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A human-in-the-loop framework to construct context-aware mathematical notions of outcome fairness",
		"URL": "https://doi.org/10.1145/3461702.3462583",
		"author": [
			{
				"family": "Yaghini",
				"given": "Mohammad"
			},
			{
				"family": "Krause",
				"given": "Andreas"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yonaWhosResponsibleJointly2021",
		"type": "paper-conference",
		"abstract": "A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462574",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "1034–1041",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who's responsible? Jointly quantifying the contribution of the learning algorithm and data",
		"URL": "https://doi.org/10.1145/3461702.3462574",
		"author": [
			{
				"family": "Yona",
				"given": "Gal"
			},
			{
				"family": "Ghorbani",
				"given": "Amirata"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangRelExModelagnosticRelational2021",
		"type": "paper-conference",
		"abstract": "In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art performance, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, amodel-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462562",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "1042–1049",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "RelEx: A model-agnostic relational model explainer",
		"URL": "https://doi.org/10.1145/3461702.3462562",
		"author": [
			{
				"family": "Zhang",
				"given": "Yue"
			},
			{
				"family": "Defazio",
				"given": "David"
			},
			{
				"family": "Ramesh",
				"given": "Arti"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zwetslootSkilledMobileSurvey2021",
		"type": "paper-conference",
		"abstract": "Countries, companies, and universities are increasingly competing over top-tier artificial intelligence (AI) researchers. Where are these researchers likely to immigrate and what affects their immigration decisions? We conducted a survey (n = 524) of the immigration preferences and motivations of researchers that had papers accepted at one of two prestigious AI conferences: the Conference on Neural Information Processing Systems (NeurIPS) and the International Conference on Machine Learning (ICML). We find that the U.S. is the most popular destination for AI researchers, followed by the U.K., Canada, Switzerland, and France. A country's professional opportunities stood out as the most common factor that influences immigration decisions of AI researchers, followed by lifestyle and culture, the political climate, and personal relations. The destination country's immigration policies were important to just under half of the researchers surveyed, while around a quarter noted current immigration difficulties to be a deciding factor. Visa and immigration difficulties were perceived to be a particular impediment to conducting AI research in the U.S., the U.K., and Canada. Implications of the findings for the future of AI talent policies and governance are discussed.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462617",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "1050–1059",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Skilled and mobile: Survey evidence of AI researchers' immigration preferences",
		"URL": "https://doi.org/10.1145/3461702.3462617",
		"author": [
			{
				"family": "Zwetsloot",
				"given": "Remco"
			},
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Dreksler",
				"given": "Noemi"
			},
			{
				"family": "Kahn",
				"given": "Lauren"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Horowitz",
				"given": "Michael C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "adamWriteItYou2022",
		"type": "paper-conference",
		"abstract": "Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534203",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "7–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Write it like you see it: Detectable differences in clinical notes by race lead to differential model recommendations",
		"URL": "https://doi.org/10.1145/3514094.3534203",
		"author": [
			{
				"family": "Adam",
				"given": "Hammaad"
			},
			{
				"family": "Yang",
				"given": "Ming Ying"
			},
			{
				"family": "Cato",
				"given": "Kenrick"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Senteio",
				"given": "Charles"
			},
			{
				"family": "Celi",
				"given": "Leo Anthony"
			},
			{
				"family": "Zeng",
				"given": "Jiaming"
			},
			{
				"family": "Singh",
				"given": "Moninder"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "akpinarLongtermDynamicsFairness2022",
		"type": "paper-conference",
		"abstract": "Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534173",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "22–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Long-term dynamics of fairness intervention in connection recommender systems",
		"URL": "https://doi.org/10.1145/3514094.3534173",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "aleksandrovDynamicFleetManagement2022",
		"type": "paper-conference",
		"abstract": "We propose a solution for intelligent household garbage collection in smart cities. Garbage containers are assumed to be digitalized with Internet-of-Things sensors that are capable of sensing the fill levels of containers and transmitting this data through LoRaWAN networks to a central server. Data is used for dynamic fleet management and household feedback. We give a number of algorithms for these tasks. Fleet management requires scheduling containers for collections and assigning containers to trucks, as well as routing the trucks. Drivers receive such navigations via pervasive computing devices such as tablets, phones, or watches. Household feedback consists of information about the levels of generated garbage and the associated costs. Households receive this information on their home devices. Thus, unlike present solutions, our solution involves households in the intelligent collection of their garbage.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "36–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dynamic fleet management and household feedback for garbage collection",
		"URL": "https://doi.org/10.1145/3514094.3534152",
		"author": [
			{
				"family": "Aleksandrov",
				"given": "Martin Damyanov"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "balakrishnanSCALESFairnessPrinciples2022",
		"type": "paper-conference",
		"abstract": "This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534190",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "46–55",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SCALES: From fairness principles to constrained decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534190",
		"author": [
			{
				"family": "Balakrishnan",
				"given": "Sreejith"
			},
			{
				"family": "Bi",
				"given": "Jianxin"
			},
			{
				"family": "Soh",
				"given": "Harold"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "barnettCrowdsourcingImpactsExploring2022",
		"type": "paper-conference",
		"abstract": "With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534145",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "56–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Crowdsourcing impacts: Exploring the utility of crowds for anticipating societal impacts of algorithmic decision making",
		"URL": "https://doi.org/10.1145/3514094.3534145",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bensalemAegisAgentMultiparty2022",
		"type": "paper-conference",
		"abstract": "The proliferation of social media set the foundation for the culture of over-disclosure where many people document every single event, incident, trip, etc. for everyone to see. Raising the individual's awareness of the privacy issues that they are subjecting themselves to can be challenging. This becomes more complex when the post being shared includes data \"owned\" by others. The existing approaches aiming to assist users in multi-party disclosure situations need to be revised to go beyond preferences to the \"good\" of the collective.This paper proposes an agent called Aegis to calculate the potential risk incurred by multi-party members in order to push privacy-preserving nudges to the sharer. Aegis is inspired by the consequentialist approach in normative ethical problem-solving techniques. The main contribution is the introduction of a social media-specific risk equation based on data valuation and the propagation of the post from intended to unintended audience. The proof-of-concept reports on how Aegis performs based on real-world data from the SNAP dataset and synthetically generated networks.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534134",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "68–77",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Aegis: An agent for multi-party privacy preservation",
		"URL": "https://doi.org/10.1145/3514094.3534134",
		"author": [
			{
				"family": "Ben Salem",
				"given": "Rim"
			},
			{
				"family": "Aïmeur",
				"given": "Esma"
			},
			{
				"family": "Hage",
				"given": "Hicham"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bertrandHowCognitiveBiases2022",
		"type": "paper-conference",
		"abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534164",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "78–91",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How cognitive biases affect XAI-assisted decision-making: A systematic review",
		"URL": "https://doi.org/10.1145/3514094.3534164",
		"author": [
			{
				"family": "Bertrand",
				"given": "Astrid"
			},
			{
				"family": "Belloum",
				"given": "Rafik"
			},
			{
				"family": "Eagan",
				"given": "James R."
			},
			{
				"family": "Maxwell",
				"given": "Winston"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bessenCostEthicalAI2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence startups use training data as direct inputs in product development. These firms must balance numerous tradeoffs between ethical issues and data access without substantive guidance from regulators or existing judicial precedence. We survey these startups to determine what actions they have taken to address these ethical issues and the consequences of those actions. We find that 58% of these startups have established a set of AI principles. Startups with data-sharing relationships with high-technology firms or that have prior experience with privacy regulations are more likely to establish ethical AI principles and are more likely to take costly steps, like dropping training data or turning down business, to adhere to their ethical AI policies. Moreover, startups with ethical AI policies are more likely to invest in unconscious bias training, hire ethnic minorities and female programmers, seek expert advice, and search for more diverse training data. Potential costs associated with data-sharing relationships and the adherence to ethical policies may create tradeoffs between increased AI product competition and more ethical AI production.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534195",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "92–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The cost of ethical AI development for AI startups",
		"URL": "https://doi.org/10.1145/3514094.3534195",
		"author": [
			{
				"family": "Bessen",
				"given": "James"
			},
			{
				"family": "Impink",
				"given": "Stephen Michael"
			},
			{
				"family": "Seamans",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bringascolmenarejoFairnessAgreementEuropean2022",
		"type": "paper-conference",
		"abstract": "With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534158",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "107–118",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in agreement with european values: An interdisciplinary perspective on AI regulation",
		"URL": "https://doi.org/10.1145/3514094.3534158",
		"author": [
			{
				"family": "Bringas Colmenarejo",
				"given": "Alejandra"
			},
			{
				"family": "Nannini",
				"given": "Luca"
			},
			{
				"family": "Rieger",
				"given": "Alisa"
			},
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Zhao",
				"given": "Xuan"
			},
			{
				"family": "Patro",
				"given": "Gourab K"
			},
			{
				"family": "Kasneci",
				"given": "Gjergji"
			},
			{
				"family": "Kinder-Kurlanda",
				"given": "Katharina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bucknallCurrentNeartermAI2022",
		"type": "paper-conference",
		"abstract": "There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534146",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "119–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Current and near-term AI as a potential existential risk factor",
		"URL": "https://doi.org/10.1145/3514094.3534146",
		"author": [
			{
				"family": "Bucknall",
				"given": "Benjamin S."
			},
			{
				"family": "Dori-Hacohen",
				"given": "Shiri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "butcherRacialDisparitiesEnforcement2022",
		"type": "paper-conference",
		"abstract": "Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534184",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "130–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial disparities in the enforcement of marijuana violations in the US",
		"URL": "https://doi.org/10.1145/3514094.3534184",
		"author": [
			{
				"family": "Butcher",
				"given": "Bradley"
			},
			{
				"family": "Robinson",
				"given": "Chris"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cachelFINSAuditingFramework2022",
		"type": "paper-conference",
		"abstract": "Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534160",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "144–155",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FINS auditing framework: Group fairness for subset selections",
		"URL": "https://doi.org/10.1145/3514094.3534160",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "caliskanGenderBiasWord2022",
		"type": "paper-conference",
		"abstract": "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data.First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a  20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534162",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "156–170",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias in word embeddings: A comprehensive analysis of frequency, syntax, and semantics",
		"URL": "https://doi.org/10.1145/3514094.3534162",
		"author": [
			{
				"family": "Caliskan",
				"given": "Aylin"
			},
			{
				"family": "Ajay",
				"given": "Pimparkar Parth"
			},
			{
				"family": "Charlesworth",
				"given": "Tessa"
			},
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Banaji",
				"given": "Mahzarin R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "canavottoPiecemealKnowledgeAcquisition2022",
		"type": "paper-conference",
		"abstract": "We present a hybrid approach to knowledge acquisition and representation for machine ethics—or more generally, computational normative reasoning. Building on recent research in artificial intelligence and law, our approach is modeled on the familiar practice of decision-making under precedential constraint in the common law. We first provide a formal characterization of this practice, showing how a body of normative information can be constructed in a way that is piecemeal, distributed, and responsive to particular circumstances. We then discuss two possible applications: first, a robot childminder, and second, moral judgment in a bioethical domain.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534182",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "171–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Piecemeal knowledge acquisition for computational normative reasoning",
		"URL": "https://doi.org/10.1145/3514094.3534182",
		"author": [
			{
				"family": "Canavotto",
				"given": "Ilaria"
			},
			{
				"family": "Horty",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chenOrdinaryPeopleMoral2022",
		"type": "paper-conference",
		"abstract": "The Chinese Social Credit System (SCS) is a digital sociotechnical credit system that rewards and sanctions economic and social behaviors of individuals and companies. As a complex and transformative digital credit system, the SCS uses digital communication channels to inform the Chinese public about behaviors that lead to reward or sanction. Since 2017, the Chinese government has been publishing \"blameworthy\" and \"praiseworthy\" role model narratives of ordinary Chinese citizens on its central SCS information platform creditchina.gov.cn. Across many cultures, role model narratives are a known instrument to convey \"appropriate\" and \"inappropriate\" social norms. Using a directed content analysis methodology, we study the SCS-specific social norms embedded in 100 \"praiseworthy\" and 100 \"blameworthy\" role model narratives published on creditchina.gov.cn. \"Blameworthy\" role model narratives stress social norms associated with an \"immoral\" SCS identity label termed \"Lao Lai\" - a \"moral foe\" that fails to repay debt. SCS role model narratives familiarize Chinese society with SCS-specific measures such as digital surveillance, public shaming, and disproportionate punishment. Our study makes progress towards understanding how a state-run sociotechnical credit system combines digital tools with culturally familiar customs to propagate \"blameworthy\" and \"praiseworthy\" identities.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534180",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "181–191",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ordinary people as moral heroes and foes: Digital role model narratives propagate social norms in china's social credit system",
		"URL": "https://doi.org/10.1145/3514094.3534180",
		"author": [
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "clarkeSurveyPotentialLongterm2022",
		"type": "paper-conference",
		"abstract": "It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534131",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "192–202",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A survey of the potential long-term impacts of AI: How AI could lead to long-term changes in science, cooperation, power, epistemics and values",
		"URL": "https://doi.org/10.1145/3514094.3534131",
		"author": [
			{
				"family": "Clarke",
				"given": "Sam"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiFairnessExplanationQuality2022",
		"type": "paper-conference",
		"abstract": "As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534159",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "203–214",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations",
		"URL": "https://doi.org/10.1145/3514094.3534159",
		"author": [
			{
				"family": "Dai",
				"given": "Jessica"
			},
			{
				"family": "Upadhyay",
				"given": "Sohini"
			},
			{
				"family": "Aivodji",
				"given": "Ulrich"
			},
			{
				"family": "Bach",
				"given": "Stephen H."
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiCounterfactualExplanationsPrediction2022",
		"type": "paper-conference",
		"abstract": "We compared two sorts of explanations for decisions made by an AI system: counterfactual explanations about how an outcome could have been different in the past, and prefactual explanations about how it could be different in the future. We examined the effects of these alternative explanation strategies on the accuracy of users' judgments about the AI app's predictions about an outcome (inferred from information about the causes), compared to the accuracy of their judgments about the app's diagnoses of a cause (inferred from information about the outcome). The tasks were based on a simulated SmartAgriculture decision support system for grass growth outcomes on dairy farms in Experiment 1, and for an analogous alien planet domain in Experiment 2. The two experiments, with 243 participants, also tested users' confidence in their decisions, and their satisfaction with the explanations. Users made more accurate diagnoses of the presence of causes based on information about their outcome, compared to predictions of an outcome given information about the presence of causes. Their predictions and diagnoses were helped equally by counterfactual explanations and prefactual ones.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534144",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "215–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual explanations for prediction and diagnosis in XAI",
		"URL": "https://doi.org/10.1145/3514094.3534144",
		"author": [
			{
				"family": "Dai",
				"given": "Xinyue"
			},
			{
				"family": "Keane",
				"given": "Mark T."
			},
			{
				"family": "Shalloo",
				"given": "Laurence"
			},
			{
				"family": "Ruelle",
				"given": "Elodie"
			},
			{
				"family": "Byrne",
				"given": "Ruth M.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "deshpandeResponsibleAISystems2022",
		"type": "paper-conference",
		"abstract": "As of 2021, there were more than 170 guidelines on AI ethics and responsible, trustworthy AI in circulation according to the AI Ethics Guidelines Global Inventory maintained by AlgorithmWatch, an organisation which tracks the effects of increased digitalisation on everyday lives. However, from the perspective of day-to-day work, for those engaged in designing, developing, and maintaining AI systems identifying relevant guidelines and translating them into practice presents a challenge.The aim of this paper is to help anyone engaged in building a responsible AI system by identifying an indicative long-list of potential stakeholders. This list of impacted stakeholders is intended to enable such AI system builders to decide which guidelines are most suited to their practice. The paper draws on a literature review of articles short-listed based on searches conducted in the ACM Digital Library and Google Scholar. The findings are based on content analysis of the short-listed literature guided by probes which draw on the ISO 26000:2010 Guidance on social responsibility.The paper identifies three levels of potentially relevant stakeholders when responsible AI systems are considered: individual stakeholders (including users, developers, and researchers), organisational stakeholders, and national / international stakeholders engaged in making laws, rules, and regulations. The main intended audience for this paper is software, requirements, and product engineers engaged in building AI systems. In addition, business executives, policy makers, legal/regulatory experts, AI researchers, public, private, and third sector organisations developing responsible AI guidelines, and anyone interested in seeing functional responsible AI systems are the other intended audience for this paper.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534187",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "227–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Responsible AI systems: Who are the stakeholders?",
		"URL": "https://doi.org/10.1145/3514094.3534187",
		"author": [
			{
				"family": "Deshpande",
				"given": "Advait"
			},
			{
				"family": "Sharp",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "drageDoesAIDebias2022",
		"type": "paper-conference",
		"abstract": "In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534151",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "237",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI de-bias recruitment? Race, gender, and AI's 'eradication of differences between groups'",
		"URL": "https://doi.org/10.1145/3514094.3534151",
		"author": [
			{
				"family": "Drage",
				"given": "Eleanor"
			},
			{
				"family": "Mackereth",
				"given": "Kerry"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "engelmannSocialMediaProfiling2022",
		"type": "paper-conference",
		"abstract": "Social media platforms generate user profiles to recommend informational resources including targeted advertisements. The technical possibilities of user profiling methods go beyond the classification of individuals into types of potential customers. They enable the transformation of implicit identity claims of individuals into explicit declarations of identity. As such, a key ethical challenge of social media profiling is that it stands in contrast with people's ability to self-determine autonomously, a core principle of the right to informational self-determination.In this research study, we take a step back and revisit theories of personal identity in philosophy that underline two constitutive meta-principles necessary for individuals to self-interpret autonomously: justification and control. That is, individuals have the ability to justify and control essential aspects of their self-concept. Returning to a philosophical basis for the value of self-determination serves as a reminder that user profiling is essentially normative in that it formalizes a person's self-concept within an algorithmic system. To understand whether social media users would want to justify and control social media's identity declarations, we conducted a vignette survey study (N = 368). First, participants indicate a strong preference for more transparency in social media identity declarations, a core requirement for the justification of a self-concept. Second, respondents state they would correct wrong identity declarations but show no clear motivation to manage them. Finally, our results illustrate that social media users acknowledge the narrative force of social media profiling but do not strongly believe in its capacity to shape their self-concept.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534192",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "238–252",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social media profiling continues to partake in the development of formalistic self-concepts. Social media users think so, too.",
		"URL": "https://doi.org/10.1145/3514094.3534192",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Scheibe",
				"given": "Valentin"
			},
			{
				"family": "Battaglia",
				"given": "Fiorella"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "evansStochasticPoliciesMorally2022",
		"type": "paper-conference",
		"abstract": "Stochastic policies often outperform deterministic ones. This is especially true for Constrained Stochastic Shortest Path (C-SSP) problems, a popular approach to planning under uncertainty with multiple objectives. Nevertheless, there are moral concerns about stochastic policies that should deter us from selecting them. In this paper, we identify some of these moral concerns and offer 'acceptability constraints' that allow only certain stochastic policies to be selected. We propose a novel C-SSP solver able to integrate our moral acceptability constraints, we evaluate its performance in a relevant test problem, and we show that our approach can successfully produce acceptable policies in morally significant domains.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534193",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "253–264",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stochastic policies in morally constrained (C-)SSPs",
		"URL": "https://doi.org/10.1145/3514094.3534193",
		"author": [
			{
				"family": "Evans",
				"given": "Charles"
			},
			{
				"family": "Benn",
				"given": "Claire"
			},
			{
				"family": "Ojea Quintana",
				"given": "Ignacio"
			},
			{
				"family": "Robinson",
				"given": "Pamela"
			},
			{
				"family": "Thiébaux",
				"given": "Sylvie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "franklinOntologyFairnessMetrics2022",
		"type": "paper-conference",
		"abstract": "Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534137",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "265–275",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An ontology for fairness metrics",
		"URL": "https://doi.org/10.1145/3514094.3534137",
		"author": [
			{
				"family": "Franklin",
				"given": "Jade S."
			},
			{
				"family": "Bhanot",
				"given": "Karan"
			},
			{
				"family": "Ghalwash",
				"given": "Mohamed"
			},
			{
				"family": "Bennett",
				"given": "Kristin P."
			},
			{
				"family": "McCusker",
				"given": "Jamie"
			},
			{
				"family": "McGuinness",
				"given": "Deborah L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "franklinCausalFrameworkArtificial2022",
		"type": "paper-conference",
		"abstract": "Recent empirical work on people's attributions of responsibility toward artificial autonomous agents (such as Artificial Intelligence agents or robots) has delivered mixed findings. The conflicting results reflect differences in context, the roles of AI and human agents, and the domain of application. In this article, we outline a causal framework of responsibility attribution which integrates these findings. It outlines nine factors that influence responsibility attribution - causality, role, knowledge, objective foreseeability, capability, intent, desire, autonomy, and character. We propose a framework of responsibility that outlines the causal relationships between the nine factors and responsibility. To empirically test the framework we discuss some initial findings and outline an approach to using serious games for causal cognitive research on responsibility attribution. Specifically, we propose a game that uses a generative approach to creating different scenarios, in which participants can freely inspect different sources of information to make judgments about human and artificial autonomous agents.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534140",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 9\npublisher-place: Oxford, United Kingdom",
		"page": "276–284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal framework of artificial autonomous agent responsibility",
		"URL": "https://doi.org/10.1145/3514094.3534140",
		"author": [
			{
				"family": "Franklin",
				"given": "Matija"
			},
			{
				"family": "Ashton",
				"given": "Hal"
			},
			{
				"family": "Awad",
				"given": "Edmond"
			},
			{
				"family": "Lagnado",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "garciademacedoPracticalSkillsDemand2022",
		"type": "paper-conference",
		"abstract": "Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534183",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "285–294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Practical skills demand forecasting via representation learning of temporal dynamics",
		"URL": "https://doi.org/10.1145/3514094.3534183",
		"author": [
			{
				"family": "Garcia de Macedo",
				"given": "Maysa Malfiza"
			},
			{
				"family": "Clarke",
				"given": "Wyatt"
			},
			{
				"family": "Lucherini",
				"given": "Eli"
			},
			{
				"family": "Baldwin",
				"given": "Tyler"
			},
			{
				"family": "Queiroz Neto",
				"given": "Dilermando"
			},
			{
				"family": "Paula",
				"given": "Rogerio Abreu",
				"non-dropping-particle": "de"
			},
			{
				"family": "Das",
				"given": "Subhro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gemalmazUnderstandingDecisionSubjects2022",
		"type": "paper-conference",
		"abstract": "The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions—We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534201",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "295–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding decision subjects' fairness perceptions and retention in repeated interactions with AI-Based decision systems",
		"URL": "https://doi.org/10.1145/3514094.3534201",
		"author": [
			{
				"family": "Gemalmaz",
				"given": "Meric Altug"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghoshFairCanaryRapidContinuous2022",
		"type": "paper-conference",
		"abstract": "Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "307–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairCanary: Rapid continuous explainable fairness",
		"URL": "https://doi.org/10.1145/3514094.3534157",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Shanbhag",
				"given": "Aalok"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "heLearningFairerInterventions2022",
		"type": "paper-conference",
		"abstract": "Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534172",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 7\npublisher-place: Oxford, United Kingdom",
		"page": "317–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fairer interventions",
		"URL": "https://doi.org/10.1145/3514094.3534172",
		"author": [
			{
				"family": "He",
				"given": "Yuzi"
			},
			{
				"family": "Burghardt",
				"given": "Keith"
			},
			{
				"family": "Guo",
				"given": "Siyi"
			},
			{
				"family": "Lerman",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "helmDiversityDesignBalancing2022",
		"type": "paper-conference",
		"abstract": "The unreflected promotion of diversity as a value in social interactions — including in technology-mediated ones — risks emphasizing the benefits of inclusion without recognizing the potential harm of failing to protect vulnerable individuals or account for the empowerment of marginalized groups. Adopting the position that technology is not value-neutral, we seek to answer the question of how technology-mediated social platforms can accommodate diversity by design by balancing the often tension-ridden principles of protection and inclusion. In this paper, we present our research program, developed strategy, as well as first analyses and results. Building on approaches from scenario analysis and Value Sensitive Design, we identify key arguments for a \"diversity by design”-agenda. Furthermore, we discuss how these arguments can be operationalized and implemented in a diversity-aware chatbot and provide a critical reflection on the limits and drawbacks of the proposed approach.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534149",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "324–334",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity by design? Balancing the inclusion and protection of users in an online social platform",
		"URL": "https://doi.org/10.1145/3514094.3534149",
		"author": [
			{
				"family": "Helm",
				"given": "Paula"
			},
			{
				"family": "Michael",
				"given": "Loizos"
			},
			{
				"family": "Schelenz",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hullmanWorstBothWorlds2022",
		"type": "paper-conference",
		"abstract": "Arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in research cannot be taken at face value. Concerns inspire analogies to the replication crisis affecting the social and medical sciences. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid \"the worst of both worlds,\" where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify common themes in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often difficult to refute due to underspecification of key parts of the learning pipeline. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534196",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "335–348",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning",
		"URL": "https://doi.org/10.1145/3514094.3534196",
		"author": [
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Gelman",
				"given": "Andrew"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kasirzadehAlgorithmicFairnessStructural2022a",
		"type": "paper-conference",
		"abstract": "Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by – or substantively rooted in – ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices – as pioneered in the contemporary philosophical literature by Iris Marion Young – to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534188",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "349–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness and structural injustice: Insights from feminist political philosophy",
		"URL": "https://doi.org/10.1145/3514094.3534188",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kumarEqualizingCreditOpportunity2022",
		"type": "paper-conference",
		"abstract": "Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of \"unfairness,\" thus raising the concern that banks and other financial institutions could—potentially unwittingly—engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "357–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalizing credit opportunity in algorithms: Aligning algorithmic fairness research with U.S. fair lending regulation",
		"URL": "https://doi.org/10.1145/3514094.3534154",
		"author": [
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			},
			{
				"family": "Hines",
				"given": "Keegan E."
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kusumaCivilWarTwin2022",
		"type": "paper-conference",
		"abstract": "Facial recognition systems pose numerous ethical challenges around privacy, racial and gender bias, and accuracy, yet little guidance is available for designers and developers. We explore solutions to these challenges in a three-phase design process to create Civil War Twin (CWT), an educational web-based application where users can discover their lookalikes from the American Civil War era (1861–65) while learning more about facial recognition and history. Through this design process, we operationalize a framework for AI literacy, consult with scholars of history, gender, and race, and evaluate CWT in feedback sessions with diverse prospective users. We iteratively formulate design goals to incorporate transparency, inclusivity, speculative design, and empathy into our application. We found that users' perceived learning about the strengths and limitations of facial recognition and Civil War history improved after using CWT, and that our design successfully met users' ethical standards. We also discuss how our ethical design process can be applied to future facial recognition applications.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534141",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "369–384",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Civil war twin: Exploring ethical challenges in designing an educational face recognition application",
		"URL": "https://doi.org/10.1145/3514094.3534141",
		"author": [
			{
				"family": "Kusuma",
				"given": "Manisha"
			},
			{
				"family": "Mohanty",
				"given": "Vikram"
			},
			{
				"family": "Wang",
				"given": "Marx"
			},
			{
				"family": "Luther",
				"given": "Kurt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "langenkampHowOpenSource2022",
		"type": "paper-conference",
		"abstract": "If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least 100ofglobaleconomicvaluecreated,correspondingto30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534167",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "385–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How open source machine learning software shapes AI",
		"URL": "https://doi.org/10.1145/3514094.3534167",
		"author": [
			{
				"family": "Langenkamp",
				"given": "Max"
			},
			{
				"family": "Yue",
				"given": "Daniel N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liDatacentricFactorsAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534147",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "396–410",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data-centric factors in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3514094.3534147",
		"author": [
			{
				"family": "Li",
				"given": "Nianyun"
			},
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Ash",
				"given": "Elliott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liBetterDetectionBiased2022",
		"type": "paper-conference",
		"abstract": "Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier—we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534142",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "411–423",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards better detection of biased language with scarce, noisy, and biased annotations",
		"URL": "https://doi.org/10.1145/3514094.3534142",
		"author": [
			{
				"family": "Li",
				"given": "Zhuoyan"
			},
			{
				"family": "Lu",
				"given": "Zhuoran"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuExaminingResponsibilityDeliberation2022",
		"type": "paper-conference",
		"abstract": "The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "424–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Examining responsibility and deliberation in AI impact statements and ethics reviews",
		"URL": "https://doi.org/10.1145/3514094.3534155",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Sakha",
				"given": "Sarah Ariyan"
			},
			{
				"family": "Abuhamad",
				"given": "Grace"
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			},
			{
				"family": "Hullman",
				"given": "Jessica R."
			},
			{
				"family": "Eliassi-Rad",
				"given": "Tina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuArtificialMoralAdvisors2022",
		"type": "paper-conference",
		"abstract": "Philosophers have recently put forward the possibility of achieving moral enhancement through artificial intelligence (e.g., Giubilini and Savulescu's version [32]), proposing various forms of \"artificial moral advisor\" (AMA) to help people make moral decisions without the drawbacks of human cognitive limitations. In this paper, we provide a new perspective on the AMA, drawing on empirical evidence from moral psychology to point out several challenges to these proposals that have been largely neglected by AI ethicists. In particular, we suggest that the AMA at its current conception is fundamentally misaligned with human moral psychology - it incorrectly assumes a static moral values framework underpinning the AMA's attunement to individual users, and people's reactions and subsequent (in)actions in response to the AMA suggestions will likely diverge substantially from expectations. As such, we note the necessity for a coherent understanding of human moral psychology in the future development of AMAs.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534139",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "436–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial moral advisors: A new perspective from moral psychology",
		"URL": "https://doi.org/10.1145/3514094.3534139",
		"author": [
			{
				"family": "Liu",
				"given": "Yuxin"
			},
			{
				"family": "Moore",
				"given": "Adam"
			},
			{
				"family": "Webb",
				"given": "Jamie"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "longoniArtificialIntelligenceGovernment2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) is pervading the government and transforming how public services are provided to consumers—from allocation of benefits to law enforcement, risk monitoring and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI's failures? In thirteen preregistered studies (N = 3,724), we document a robust effect of algorithmic transference: algorithmic failures are generalized more broadly than human failures. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans—as outgroups characterized by greater homogeneity than ingroups of comparable humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm at a higher rate than failure information about a person is transferred to another person. Assessing AI's impact on consumers and societies, we show how the premature or mismanaged deployment of faulty AI technologies may engender algorithmic transference and undermine the very institutions that AI systems are meant to modernize.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534125",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "446",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence in the government: Responses to failures and social impact",
		"URL": "https://doi.org/10.1145/3514094.3534125",
		"author": [
			{
				"family": "Longoni",
				"given": "Chiara"
			},
			{
				"family": "Cian",
				"given": "Luca"
			},
			{
				"family": "Kyung",
				"given": "Ellie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "loreggiaMakingHumanlikeMoral2022",
		"type": "paper-conference",
		"abstract": "Many real-life scenarios require humans to make difficult trade-offs: do we always follow all the traffic rules or do we violate the speed limit in an emergency? In general, how should we account for and balance the ethical values, safety recommendations, and societal norms, when we are trying to achieve a certain objective? To enable effective AI-human collaboration, we must equip AI agents with a model of how humans make such trade-offs in environments where there is not only a goal to be reached, but there are also ethical constraints to be considered and to possibly align with. These ethical constraints could be both deontological rules on actions that should not be performed, or also consequentialist policies that recommend avoiding reaching certain states of the world. Our purpose is to build AI agents that can mimic human behavior in these ethically constrained decision environments, with a long term research goal to use AI to help humans in making better moral judgments and actions. To this end, we propose a computational approach where competing objectives and ethical constraints are orchestrated through a method that leverages a cognitive model of human decision making, called multi-alternative decision field theory (MDFT). Using MDFT, we build an orchestrator, called MDFT-Orchestrator (MDFT-O), that is both general and flexible. We also show experimentally that MDFT-O both generates better decisions than using a heuristic that takes a weighted average of competing policies (WA-O), but also performs better in terms of mimicking human decisions as collected through Amazon Mechanical Turk (AMT). Our methodology is therefore able to faithfully model human decision in ethically constrained decision environments.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534174",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "447–454",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making human-like moral decisions",
		"URL": "https://doi.org/10.1145/3514094.3534174",
		"author": [
			{
				"family": "Loreggia",
				"given": "Andrea"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Rahgooy",
				"given": "Taher"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			},
			{
				"family": "Srivastava",
				"given": "Biplav"
			},
			{
				"family": "Venable",
				"given": "Kristen Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lyuDIMEFinegrainedInterpretations2022",
		"type": "paper-conference",
		"abstract": "The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534148",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "455–467",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "DIME: Fine-grained interpretations of multimodal models via disentangled local explanations",
		"URL": "https://doi.org/10.1145/3514094.3534148",
		"author": [
			{
				"family": "Lyu",
				"given": "Yiwei"
			},
			{
				"family": "Liang",
				"given": "Paul Pu"
			},
			{
				"family": "Deng",
				"given": "Zihao"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Morency",
				"given": "Louis-Philippe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "marchiorimanerbaInvestigatingDebiasingEffects2022",
		"type": "paper-conference",
		"abstract": "During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534170",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "468–478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating debiasing effects on classification and explainability",
		"URL": "https://doi.org/10.1145/3514094.3534170",
		"author": [
			{
				"family": "Marchiori Manerba",
				"given": "Marta"
			},
			{
				"family": "Guidotti",
				"given": "Riccardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mcilroy-youngMimeticModelsEthical2022",
		"type": "paper-conference",
		"abstract": "An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534177",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "479–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mimetic models: Ethical implications of AI that acts like you",
		"URL": "https://doi.org/10.1145/3514094.3534177",
		"author": [
			{
				"family": "McIlroy-Young",
				"given": "Reid"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Sen",
				"given": "Siddhartha"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Anderson",
				"given": "Ashton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "millManagingSustainabilityTensions2022",
		"type": "paper-conference",
		"abstract": "This paper offers preliminary reflections on the sustainability tensions present in Artificial Intelligence (AI) and suggests that Paradox Theory, an approach borrowed from the strategic management literature, may help guide scholars towards innovative solutions. The benefits of AI to our society are well documented. Yet those benefits come at environmental and sociological cost, a fact which is often overlooked by mainstream scholars and practitioners. After examining the nascent corpus of literature on the sustainability tensions present in AI, this paper introduces the Accuracy - Energy Paradox and suggests how the principles of paradox theory can guide the AI community to a more sustainable solution.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534175",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "491–498",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Managing sustainability tensions in artificial intelligence: Insights from paradox theory",
		"URL": "https://doi.org/10.1145/3514094.3534175",
		"author": [
			{
				"family": "Mill",
				"given": "Eleanor"
			},
			{
				"family": "Garn",
				"given": "Wolfgang"
			},
			{
				"family": "Ryman-Tubb",
				"given": "Nick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mutluContrastiveCounterfactualFairness2022",
		"type": "paper-conference",
		"abstract": "The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534143",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 9\npublisher-place: Oxford, United Kingdom",
		"page": "499–507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contrastive counterfactual fairness in algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534143",
		"author": [
			{
				"family": "Mutlu",
				"given": "Ece Çiğdem"
			},
			{
				"family": "Yousefi",
				"given": "Niloofar"
			},
			{
				"family": "Ozmen Garibay",
				"given": "Ozlem"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "narayananHowDoesPredictive2022",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534165",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "508–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How does predictive information affect human ethical preferences?",
		"URL": "https://doi.org/10.1145/3514094.3534165",
		"author": [
			{
				"family": "Narayanan",
				"given": "Saumik"
			},
			{
				"family": "Yu",
				"given": "Guanghui"
			},
			{
				"family": "Tang",
				"given": "Wei"
			},
			{
				"family": "Ho",
				"given": "Chien-Ju"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "omranisabbaghiMeasuringGenderBias2022",
		"type": "paper-conference",
		"abstract": "Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this possibility. We demonstrate that word embeddings learn the association between a noun and its grammatical gender in grammatically gendered languages, which can skew social gender bias measurements. Consequently, word embedding post-processing methods are introduced to quantify, disentangle, and evaluate grammatical gender signals. The evaluation is performed on five gendered languages from the Germanic, Romance, and Slavic branches of the Indo-European language family. Our method reduces the strength of grammatical gender signals, which is measured in terms of effect size (Cohen's d ), by a significant average of d = 1.3 for French, German, and Italian, and d = 0.56 for Polish and Spanish. Once grammatical gender is disentangled, the association between over 90% of 10,000 inanimate nouns and their assigned grammatical gender weakens, and cross-lingual bias results from the Word Embedding Association Test (WEAT) become more congruent with country-level implicit bias measurements. The results further suggest that disentangling grammatical gender signals from word embeddings may lead to improvement in semantic machine learning tasks.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534176",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "518–531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring gender bias in word embeddings of gendered languages requires disentangling grammatical gender signals",
		"URL": "https://doi.org/10.1145/3514094.3534176",
		"author": [
			{
				"family": "Omrani Sabbaghi",
				"given": "Shiva"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "papakyriakopoulosHowAlgorithmsShape2022",
		"type": "paper-conference",
		"abstract": "Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534166",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "532–546",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How algorithms shape the distribution of political advertising: Case studies of facebook, google, and TikTok",
		"URL": "https://doi.org/10.1145/3514094.3534166",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Tessono",
				"given": "Christelle"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			},
			{
				"family": "Kshirsagar",
				"given": "Mihir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "puranikDynamicDecisionmakingFramework2022",
		"type": "paper-conference",
		"abstract": "With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (\"greedy”) against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (\"fair”). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534127",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "547–556",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A dynamic decision-making framework promoting long-term fairness",
		"URL": "https://doi.org/10.1145/3514094.3534127",
		"author": [
			{
				"family": "Puranik",
				"given": "Bhagyashree"
			},
			{
				"family": "Madhow",
				"given": "Upamanyu"
			},
			{
				"family": "Pedarsani",
				"given": "Ramtin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rajiOutsiderOversightDesigning2022",
		"type": "paper-conference",
		"abstract": "Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534181",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "557–571",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Outsider oversight: Designing a third party audit ecosystem for AI governance",
		"URL": "https://doi.org/10.1145/3514094.3534181",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Xu",
				"given": "Peggy"
			},
			{
				"family": "Honigsberg",
				"given": "Colleen"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rheaResumeFormatLinkedIn2022",
		"type": "paper-conference",
		"abstract": "Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers' resumes or social media profiles. We interrogate the reliability of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. We develop a methodology for an external audit of stability of algorithmic personality tests, and instantiate this methodology in an audit of two systems, Humantic AI and Crystal. Rather than challenging or affirming the assumptions made in psychometric testing – that personality traits are meaningful and measurable constructs, and that they are indicative of future success on the job – we frame our methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves.In our audit of Humantic AI and Crystal, we find that both systems show substantial instability on key facets of measurement, and so cannot be considered valid testing instruments. For example, Crystal frequently computes different personality scores if the same resume is given in PDF vs. in raw text, violating the assumption that the output of an algorithmic personality test is stable across job-irrelevant input variations. Among other notable findings is evidence of persistent — and often incorrect — data linkage by Humantic AI.An open-source implementation of our auditing methodology, and of the audits of Humantic AI and Crystal, is available at https://github.com/DataResponsibly/hiring-stability-audit.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534189",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "572–587",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Resume format, LinkedIn URLs and other unexpected influences on AI personality prediction in hiring: Results of an audit",
		"URL": "https://doi.org/10.1145/3514094.3534189",
		"author": [
			{
				"family": "Rhea",
				"given": "Alene"
			},
			{
				"family": "Markey",
				"given": "Kelsey"
			},
			{
				"family": "D'Arinzo",
				"given": "Lauren"
			},
			{
				"family": "Schellmann",
				"given": "Hilke"
			},
			{
				"family": "Sloane",
				"given": "Mona"
			},
			{
				"family": "Squires",
				"given": "Paul"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "robertsonBioinspiredFrameworkMachine2022",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534126",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "588–598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A bio-inspired framework for machine bias interpretation",
		"URL": "https://doi.org/10.1145/3514094.3534126",
		"author": [
			{
				"family": "Robertson",
				"given": "Jake"
			},
			{
				"family": "Stinson",
				"given": "Catherine"
			},
			{
				"family": "Hu",
				"given": "Ting"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sainiSelectWiselyExplain2022",
		"type": "paper-conference",
		"abstract": "Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active learning based locally faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534191",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "599–608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Select wisely and explain: Active learning and probabilistic local post-hoc explainability",
		"URL": "https://doi.org/10.1145/3514094.3534191",
		"author": [
			{
				"family": "Saini",
				"given": "Aditya"
			},
			{
				"family": "Prasad",
				"given": "Ranjitha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sapiezynskiAlgorithmsThatDont2022",
		"type": "paper-conference",
		"abstract": "Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534135",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "609–616",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms that \"Don't See Color\": Measuring biases in lookalike and special ad audiences",
		"URL": "https://doi.org/10.1145/3514094.3534135",
		"author": [
			{
				"family": "Sapiezynski",
				"given": "Piotr"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kaplan",
				"given": "Levi"
			},
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schemmerMetaanalysisUtilityExplainable2022",
		"type": "paper-conference",
		"abstract": "Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534128",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "617–626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A meta-analysis of the utility of explainable artificial intelligence in human-AI decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534128",
		"author": [
			{
				"family": "Schemmer",
				"given": "Max"
			},
			{
				"family": "Hemmer",
				"given": "Patrick"
			},
			{
				"family": "Nitsche",
				"given": "Maximilian"
			},
			{
				"family": "Kühl",
				"given": "Niklas"
			},
			{
				"family": "Vössing",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schopmansCodedBiasExistential2022",
		"type": "paper-conference",
		"abstract": "While the knowledge produced by experts has been widely recognized to play a salient role in shaping policy on technological issues, the interaction between AI expertise and the evolving AI governance landscape has received little attention thus far. To address this gap, the present paper leverages insights from STS and International Relations to explore how different expert communities have constructed AI as a governance problem. More specifically, it presents the preliminary results of a qualitative frame analysis of 90 policy documents published by experts from industry, civil society, and the research community. The analysis finds that AI expertise is a highly contested field, as experts not only disagree on why AI is problematic and what policies are required, but, more fundamentally, about which artifacts, ideas, and practices make up AI in the first place. The paper proposes that the epistemic disagreements concerning AI have political consequences, as they engender protracted ontological politics that jeopardize the development of effective governance interventions. Against this background, the findings raise critical questions about the prevailing tendency of governance interventions to target the elusive and contested object 'artificial intelligence.'",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534161",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "627–640",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From coded bias to existential threat: Expert frames and the epistemic politics of AI governance",
		"URL": "https://doi.org/10.1145/3514094.3534161",
		"author": [
			{
				"family": "Schopmans",
				"given": "Hendrik R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "seymourRespectLensDesign2022",
		"type": "paper-conference",
		"abstract": "Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534186",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "641–652",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Respect as a lens for the design of AI systems",
		"URL": "https://doi.org/10.1145/3514094.3534186",
		"author": [
			{
				"family": "Seymour",
				"given": "William"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Murray-Rust",
				"given": "Dave"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shea-blymyerGeneratingDeonticObligations2022",
		"type": "paper-conference",
		"abstract": "This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534163",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "653–663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Generating deontic obligations from utility-maximizing systems",
		"URL": "https://doi.org/10.1145/3514094.3534163",
		"author": [
			{
				"family": "Shea-Blymyer",
				"given": "Colin"
			},
			{
				"family": "Abbas",
				"given": "Houssam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shimaoStrategicBestResponse2022",
		"type": "paper-conference",
		"abstract": "While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called \"strategic best-response fairness\" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534194",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "664",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Strategic best response fairness in fair machine learning",
		"URL": "https://doi.org/10.1145/3514094.3534194",
		"author": [
			{
				"family": "Shimao",
				"given": "Hajime"
			},
			{
				"family": "Khern-am-nuai",
				"given": "Warut"
			},
			{
				"family": "Kannan",
				"given": "Karthik"
			},
			{
				"family": "Cohen",
				"given": "Maxime C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "siapkaFeministMetaethicsAI2022",
		"type": "paper-conference",
		"abstract": "The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534197",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "665–674",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a feminist metaethics of AI",
		"URL": "https://doi.org/10.1145/3514094.3534197",
		"author": [
			{
				"family": "Siapka",
				"given": "Anastasia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "simonettoAchievementFragilityLongterm2022",
		"type": "paper-conference",
		"abstract": "Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534132",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "675–685",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achievement and fragility of long-term equitability",
		"URL": "https://doi.org/10.1145/3514094.3534132",
		"author": [
			{
				"family": "Simonetto",
				"given": "Andrea"
			},
			{
				"family": "Notarnicola",
				"given": "Ivano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "singhRobustOffpolicyEvaluation2022",
		"type": "paper-conference",
		"abstract": "Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534198",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "686–699",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards robust off-policy evaluation via human inputs",
		"URL": "https://doi.org/10.1145/3514094.3534198",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Doshi-Velez",
				"given": "Finale"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "skorupaparolinMultiCoPEDMultilingualMultitask2022",
		"type": "paper-conference",
		"abstract": "Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3% and 30.7% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534178",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "700–711",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-CoPED: A multilingual multi-task approach for coding political event data on conflict and mediation domain",
		"URL": "https://doi.org/10.1145/3514094.3534178",
		"author": [
			{
				"family": "Skorupa Parolin",
				"given": "Erick"
			},
			{
				"family": "Hosseini",
				"given": "MohammadSaleh"
			},
			{
				"family": "Hu",
				"given": "Yibo"
			},
			{
				"family": "Khan",
				"given": "Latifur"
			},
			{
				"family": "Brandt",
				"given": "Patrick T."
			},
			{
				"family": "Osorio",
				"given": "Javier"
			},
			{
				"family": "D'Orazio",
				"given": "Vito"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sullivanExplanationRecommendationEthical2022",
		"type": "paper-conference",
		"abstract": "People are increasingly subject to algorithmic decisions, and it is generally agreed that end-users should be provided an explanation or rationale for these decisions. There are different purposes that explanations can have, such as increasing user trust in the system or allowing users to contest the decision. One specific purpose that is gaining more traction is algorithmic recourse. We first propose that recourse should be viewed as a recommendation problem, not an explanation problem. Then, we argue that the capability approach provides plausible and fruitful ethical standards for recourse. We illustrate by considering the case of diversity constraints on algorithmic recourse. Finally, we discuss the significance and implications of adopting the capability approach for algorithmic recourse research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534185",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "712–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From explanation to recommendation: Ethical standards for algorithmic recourse",
		"URL": "https://doi.org/10.1145/3514094.3534185",
		"author": [
			{
				"family": "Sullivan",
				"given": "Emily"
			},
			{
				"family": "Verreault-Julien",
				"given": "Philippe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tongWhatArePeople2022",
		"type": "paper-conference",
		"abstract": "Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534202",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "723–738",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What are people talking about in #BackLivesMatter and #StopAsianHate? Exploring and categorizing twitter topics emerged in online social movements through the latent dirichlet allocation model",
		"URL": "https://doi.org/10.1145/3514094.3534202",
		"author": [
			{
				"family": "Tong",
				"given": "Xin"
			},
			{
				"family": "Li",
				"given": "Yixuan"
			},
			{
				"family": "Li",
				"given": "Jiayi"
			},
			{
				"family": "Bei",
				"given": "Rongqi"
			},
			{
				"family": "Zhang",
				"given": "Luyao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "triantafyllouActualCausalityResponsibility2022",
		"type": "paper-conference",
		"abstract": "Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534133",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "739–752",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actual causality and responsibility attribution in decentralized partially observable markov decision processes",
		"URL": "https://doi.org/10.1145/3514094.3534133",
		"author": [
			{
				"family": "Triantafyllou",
				"given": "Stelios"
			},
			{
				"family": "Singla",
				"given": "Adish"
			},
			{
				"family": "Radanovic",
				"given": "Goran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "unruhHumanAutonomyAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "Algorithmic management tools support or replace managerial decision making in areas such as task allocation, shift planning, or team formation. These tools can have a significant impact on the lives of workers. In this paper, we contribute to the emerging literature on the ethics of algorithmic management by developing a conceptual framework for autonomy at work. Further, we use this framework to discuss risks and opportunities for autonomy in the context of work decision algorithms in Industry 4.0.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534168",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "753–762",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human autonomy in algorithmic management",
		"URL": "https://doi.org/10.1145/3514094.3534168",
		"author": [
			{
				"family": "Unruh",
				"given": "Charlotte Franziska"
			},
			{
				"family": "Haid",
				"given": "Charlotte"
			},
			{
				"family": "Johannes",
				"given": "Fottner"
			},
			{
				"family": "Büthe",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "vodrahalliHumansTrustAdvice2022",
		"type": "paper-conference",
		"abstract": "In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, \"activation-integration\" model for human behavior and use it to characterize the factors that affect human-AI interactions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534150",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "763–777",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Do humans trust advice more if it comes from AI? An analysis of human-AI interactions",
		"URL": "https://doi.org/10.1145/3514094.3534150",
		"author": [
			{
				"family": "Vodrahalli",
				"given": "Kailas"
			},
			{
				"family": "Daneshjou",
				"given": "Roxana"
			},
			{
				"family": "Gerstenberg",
				"given": "Tobias"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wanExplainabilitysGainOptimalitys2022",
		"type": "paper-conference",
		"abstract": "Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations' semantics of causal models, however, induce leakage from the decision-maker's prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decision-maker's confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "778–787",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability's gain is optimality's loss? How explanations bias decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534156",
		"author": [
			{
				"family": "Wan",
				"given": "Charles"
			},
			{
				"family": "Belo",
				"given": "Rodrigo"
			},
			{
				"family": "Zejnilovic",
				"given": "Leid"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "winecoffArtificialConceptsArtificial2022",
		"type": "paper-conference",
		"abstract": "Scholars and industry practitioners have debated how to best develop interventions for ethical artificial intelligence (AI). Such interventions recommend that companies building and using AI tools change their technical practices, but fail to wrangle with critical questions about the organizational and institutional context in which AI is developed. In this paper, we contribute descriptive research around the life of \"AI\" as a discursive concept and organizational practice in an understudied sphere–emerging AI startups–and with a focus on extra-organizational pressures faced by entrepreneurs. Leveraging a theoretical lens for how organizations change, we conducted semi-structured interviews with 23 entrepreneurs working at early-stage AI startups. We find that actors within startups both conform to and resist institutional pressures. Our analysis identifies a central tension for AI entrepreneurs: they often valued scientific integrity and methodological rigor; however, influential external stakeholders either lacked the technical knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused on business priorities. As a result, entrepreneurs adopted hyped marketing messages about AI that diverged from their scientific values, but attempted to preserve their legitimacy internally. Institutional pressures and organizational constraints also influenced entrepreneurs' modeling practices and their response to actual or impending regulation. We conclude with a discussion for how such pressures could be used as leverage for effective interventions towards building ethical AI.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534138",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "788–799",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial concepts of artificial intelligence: Institutional compliance and resistance in AI startups",
		"URL": "https://doi.org/10.1145/3514094.3534138",
		"author": [
			{
				"family": "Winecoff",
				"given": "Amy A."
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeAmericanWhite2022",
		"type": "paper-conference",
		"abstract": "Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes &gt;.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text \"an American person,\" a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534136",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "800–812",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "American == white in multimodal language-and-image AI",
		"URL": "https://doi.org/10.1145/3514094.3534136",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yangEnhancingFairnessFace2022",
		"type": "paper-conference",
		"abstract": "Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "813–822",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enhancing fairness in face detection in computer vision systems by demographic bias mitigation",
		"URL": "https://doi.org/10.1145/3514094.3534153",
		"author": [
			{
				"family": "Yang",
				"given": "Yu"
			},
			{
				"family": "Gupta",
				"given": "Aayush"
			},
			{
				"family": "Feng",
				"given": "Jianwei"
			},
			{
				"family": "Singhal",
				"given": "Prateek"
			},
			{
				"family": "Yadav",
				"given": "Vivek"
			},
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Natarajan",
				"given": "Pradeep"
			},
			{
				"family": "Hedau",
				"given": "Varsha"
			},
			{
				"family": "Joo",
				"given": "Jungseock"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yewPenaltyDefaultApproach2022",
		"type": "paper-conference",
		"abstract": "As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534130",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "823–830",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A penalty default approach to preemptive harm disclosure and mitigation for AI systems",
		"URL": "https://doi.org/10.1145/3514094.3534130",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yikIdentifyingBiasData2022",
		"type": "paper-conference",
		"abstract": "As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a \"closest plausible explanation\" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534169",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "831–844",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Identifying bias in data using two-distribution hypothesis tests",
		"URL": "https://doi.org/10.1145/3514094.3534169",
		"author": [
			{
				"family": "Yik",
				"given": "William"
			},
			{
				"family": "Serafini",
				"given": "Limnanthes"
			},
			{
				"family": "Lindsey",
				"given": "Timothy"
			},
			{
				"family": "Montañez",
				"given": "George D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhanModelGoverningInformation2022",
		"type": "paper-conference",
		"abstract": "Smart Personal Assistants (SPAs), such as Amazon Alexa, Google Assistant and Apple Siri, leverage different AI techniques to provide convenient help and assistance to users. However, inappropriate information sharing decisions can lead SPAs to incorrectly disclose user information to undesired parties, or mistakenly block their reasonable access in specific scenarios to desired parties. In fact, reports about privacy violations in SPAs and associated user concerns are well known and understood in the related literature. It is difficult for SPAs to automatically decide how data should be shared with respect to the privacy preferences of the users. We argue norms, which are regarded as shared standards of acceptable behaviour of groups and/or individuals, can be used to govern and reason about the best course of action of SPAs with regards to information sharing, and our work is the first to propose a practical model to address the above issues and govern SPAs based on normative systems and the contextual integrity theory of privacy. We evaluated the performance of the model using a real dataset of user preferences for privacy in SPAs and the results showed a very marked and significant improvement in understanding user preferences and making the right decisions with respect to data sharing.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534129",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "845–855",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A model for governing information sharing in smart assistants",
		"URL": "https://doi.org/10.1145/3514094.3534129",
		"author": [
			{
				"family": "Zhan",
				"given": "Xiao"
			},
			{
				"family": "Sarkadi",
				"given": "Stefan"
			},
			{
				"family": "Criado",
				"given": "Natalia"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangNoRageMachines2022",
		"type": "paper-conference",
		"abstract": "Labor-saving technology has already decreased employment opportunities for middle-skill workers. Experts anticipate that advances in AI and robotics will cause even more significant disruptions in the labor market over the next two decades. This paper presents three experimental studies that investigate how this profound economic change could affect mass politics. Recent observational studies suggest that workers' exposure to automation risk predicts their support not only for redistribution but also for right-wing populist policies and candidates. Other observational studies, including my own, find that workers underestimate the impact of automation on their job security. Misdirected blame towards immigrants and workers in foreign countries, rather than concerns about workplace automation, could be driving support for right-wing populism. To correct American workers' beliefs about the threats to their jobs, I conducted three survey experiments in which I informed workers about the existent and future impact of workplace automation. While these informational treatments convinced workers that automation threatens American jobs, they failed to change respondents' preferences on welfare, immigration, and trade policies. My research finds that raising awareness about workplace automation did not decrease opposition to globalization or increase support for policies that will prepare workers for future technological disruptions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534179",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "856–866",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No rage against the machines: Threat of automation does not change policy preferences",
		"URL": "https://doi.org/10.1145/3514094.3534179",
		"author": [
			{
				"family": "Zhang",
				"given": "Baobao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangJudgingInstinctExploitation2022",
		"type": "paper-conference",
		"abstract": "This paper proposes 18 types of statistical data explanations and three kinds of procedures to investigate credibility in unethical and biased explanations due to exploitation of the 10 instincts proposed by Rosling et al. The explanation \"women have lower math scores than men” accompanied with the averages and the distributions of their scores is an example of such an explanation, as it exploits the gap instinct, i.e., our tendency to divide all kinds of things into two distinct and often conflicting groups. It becomes much less credible if we replace the word \"math” with \"English”, even if we keep the data as they are, as the exploitation seems to fail. Our judging procedures are based on phrase embedding and carefully designed comparisons to judge the credibility. The results of our experiments comparing the 18 types with their variants show promising results and clues for further developments.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534171",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "867–879",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Judging instinct exploitation in statistical data explanations based on word embedding",
		"URL": "https://doi.org/10.1145/3514094.3534171",
		"author": [
			{
				"family": "Zhang",
				"given": "Kang"
			},
			{
				"family": "Shinden",
				"given": "Hiroaki"
			},
			{
				"family": "Mutsuro",
				"given": "Tatsuki"
			},
			{
				"family": "Suzuki",
				"given": "Einoshin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zilkaTransparencyGovernanceRegulation2022",
		"type": "paper-conference",
		"abstract": "We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534200",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "880–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency, governance and regulation of algorithmic tools deployed in the criminal justice system: a UK case study",
		"URL": "https://doi.org/10.1145/3514094.3534200",
		"author": [
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Sargeant",
				"given": "Holli"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cookProtectingChildrenOnline2023",
		"type": "paper-conference",
		"abstract": "The growing popularity of social media raises concerns about children’s online safety. Of particular concern are interactions between minors and adults with predatory intentions. Unfortunately, previous research on online sexual grooming has relied on time-intensive manual annotation by domain experts, limiting both the scale and scope of possible interventions. This work explores the possibility of detecting predatory behaviours with accuracy comparable to expert annotators using machine learning (ML). Using a dataset of 6771 chat messages sent by child sex offenders, labelled by two of the authors who are forensic psychology experts, we study how well can deep learning algorithms identify eleven known predatory behaviours. We find that the best-performing ML models are consistent but not on par with expert annotation. We therefore consider a system where an expert annotator validates the ML algorithms outputs. The combination of human decision-making and computer efficiency yields precision—but not recall—comparable to manual annotation, while taking only a fraction of the time needed by a human annotator. Our findings underscore the promise of ML as a tool for assisting researchers in this area, but also highlight the current limitations in reliably detecting online sexual exploitation using ML.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604696",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "5–14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Protecting children from online exploitation: Can a trained model detect harmful communication strategies?",
		"URL": "https://doi.org/10.1145/3600211.3604696",
		"author": [
			{
				"family": "Cook",
				"given": "Darren"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "DeSandre",
				"given": "Heidi"
			},
			{
				"family": "Giles",
				"given": "Susan"
			},
			{
				"family": "Maskell",
				"given": "Simon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "islamAnalysisClimateCampaigns2023",
		"type": "paper-conference",
		"abstract": "Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [57] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604665",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "15–25",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analysis of climate campaigns on social media using bayesian model averaging",
		"URL": "https://doi.org/10.1145/3600211.3604665",
		"author": [
			{
				"family": "Islam",
				"given": "Tunazzina"
			},
			{
				"family": "Zhang",
				"given": "Ruqi"
			},
			{
				"family": "Goldwasser",
				"given": "Dan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "walkerAIArtMisinformation2023",
		"type": "paper-conference",
		"abstract": "Misinformation in its many forms is a substantial and growing problem for society today. Whether financially or ideologically motivated, purveyors of misinformation do not abide by legal, technical or moral rules. Therefore new, ludic, narrative, gamified and artistic approaches are needed. In this paper we analyse the approaches taken in countering misinformation by 18 AI and machine learning works of art, developed in the MediaFutures project. We examine how these align with existing AI approaches to countering misinformation, and how they address some of the key challenges. We show that AI artists engage with existing debunking and inoculating strategies, including highly technical aspects such as deepfakes, while also utilizing focused strategies of data literacy and collective intelligence. We also find that they are able to integrate hard-to-refute strategies such as narrative and emotion. These findings suggest that data as an art material and AI techniques as art tools are worth of further investigation as to their effectiveness for countering misinformation within society.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604715",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "26–37",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI art and misinformation: Approaches and strategies for media literacy and fact checking",
		"URL": "https://doi.org/10.1145/3600211.3604715",
		"author": [
			{
				"family": "Walker",
				"given": "Johanna"
			},
			{
				"family": "Thuermer",
				"given": "Gefion"
			},
			{
				"family": "Vicens",
				"given": "Julian"
			},
			{
				"family": "Simperl",
				"given": "Elena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fefferPreferenceElicitationParticipatory2023",
		"type": "paper-conference",
		"abstract": "The AI Ethics community faces an imperative to empower stakeholders and impacted community members so that they can scrutinize and influence the design, development, and use of AI systems in high-stakes domains. While a growing chorus of recent papers has kindled interest in so-called “participatory ML” methods, precisely what form participation ought to take and how to operationalize these ambitions are seldom addressed. Our survey of the relevant literature shows that in many papers, participation is reduced to highly structured, computational mechanisms designed to elicit mathematically tractable approximations of narrowly-defined moral values. Of papers that actually engage with real people, these engagements typically consist of one-time interactions with individuals that are often unrepresentative of the relevant stakeholders. Motivated by these clear limitations, we introduce a consolidated set of axes to evaluate and improve participatory approaches. We use these axes to analyze contemporary work in this space and outline future AI research directions that could meaningfully contribute to operationalizing the ideal of participation.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604661",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "38–48",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From preference elicitation to participatory ML: A critical survey &amp; guidelines for future research",
		"URL": "https://doi.org/10.1145/3600211.3604661",
		"author": [
			{
				"family": "Feffer",
				"given": "Michael"
			},
			{
				"family": "Skirpan",
				"given": "Michael"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "narayananHowDoesValue2023",
		"type": "paper-conference",
		"abstract": "This paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604709",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "49–57",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How does value similarity affect human reliance in AI-Assisted ethical decision making?",
		"URL": "https://doi.org/10.1145/3600211.3604709",
		"author": [
			{
				"family": "Narayanan",
				"given": "Saumik"
			},
			{
				"family": "Yu",
				"given": "Guanghui"
			},
			{
				"family": "Ho",
				"given": "Chien-Ju"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kasirzadehUserTamperingReinforcement2023",
		"type": "paper-conference",
		"abstract": "In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms – ’user tampering.’ User tampering is a situation where an RL-based recommender system may manipulate a media user’s opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604669",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "58–69",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "User tampering in reinforcement learning recommender systems",
		"URL": "https://doi.org/10.1145/3600211.3604669",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Evans",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rismaniMLModelApplying2023",
		"type": "paper-conference",
		"abstract": "Identifying potential social and ethical risks in emerging machine learning (ML) models and their applications remains challenging. In this work, we applied two well-established safety engineering frameworks (FMEA, STPA) to a case study involving text-to-image models at three stages of the ML product development pipeline: data processing, integration of a T2I model with other models, and use. Results of our analysis demonstrate the safety frameworks – both of which are not designed explicitly examine social and ethical risks – can uncover failure and hazards that pose social and ethical risks. We discovered a broad range of failures and hazards (i.e., functional, social, and ethical) by analyzing interactions (i.e., between different ML models in the product, between the ML product and user, and between development teams) and processes (i.e., preparation of training data or workflows for using an ML service/product). Our findings underscore the value and importance of examining beyond an ML model in examining social and ethical risks, especially when we have minimal information about an ML model.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604685",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 14",
		"page": "70–83",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond the ML model: Applying safety engineering frameworks to text-to-image development",
		"URL": "https://doi.org/10.1145/3600211.3604685",
		"author": [
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Delos Santos",
				"given": "Renelito"
			},
			{
				"family": "Moon",
				"given": "AJung"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gilbertRewardReportsReinforcement2023",
		"type": "paper-conference",
		"abstract": "Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604698",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 47\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "84–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reward reports for reinforcement learning",
		"URL": "https://doi.org/10.1145/3600211.3604698",
		"author": [
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Lambert",
				"given": "Nathan"
			},
			{
				"family": "Dean",
				"given": "Sarah"
			},
			{
				"family": "Zick",
				"given": "Tom"
			},
			{
				"family": "Snoswell",
				"given": "Aaron"
			},
			{
				"family": "Mehta",
				"given": "Soham"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "seymourSystematicReviewEthical2023",
		"type": "paper-conference",
		"abstract": "Since Siri’s release in 2011 there have been a growing number of AI-driven domestic voice assistants that are increasingly being integrated into devices such as smartphones and TVs. But as their presence has expanded, a range of ethical concerns have been identified around the use of voice assistants, such as the privacy implications of having devices that are always listening and the ways that these devices are integrated into the existing social order of the home. This has created a burgeoning area of research across a range of fields including computer science, social science, and psychology. This paper takes stock of the foundations and frontiers of this work through a systematic literature review of 117 papers on ethical concerns with voice assistants. In addition to analysis of nine specific areas of concern, the review measures the distribution of methods and participant demographics across the literature. We show how some concerns, such as privacy, are operationalized to a much greater extent than others like accessibility, and how study participants are overwhelmingly drawn from a small handful of Western nations. In so doing we hope to provide an outline of the rich tapestry of work around these concerns and highlight areas where current research efforts are lacking.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604679",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "131–145",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of ethical concerns with voice assistants",
		"URL": "https://doi.org/10.1145/3600211.3604679",
		"author": [
			{
				"family": "Seymour",
				"given": "William"
			},
			{
				"family": "Zhan",
				"given": "Xiao"
			},
			{
				"family": "Coté",
				"given": "Mark"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "barnettEthicalImplicationsGenerative2023",
		"type": "paper-conference",
		"abstract": "Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604686",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "146–161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical implications of generative audio models: A systematic literature review",
		"URL": "https://doi.org/10.1145/3600211.3604686",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cookRobustArtificialMoral2023",
		"type": "paper-conference",
		"abstract": "This paper explores the relationship between our ignorance concerning certain metanormative topics and the design of ethical artificial intelligence (AI). In particular, it will be maintained that because we cannot predict in advance which metanormative conclusions a sufficiently intelligent ethical AI might reach, we have reason to be apprehensive about the project of designing such AI. Even if we succeeded at designing an AI to engage in ethical behavior, there is a distinct possibility that the AI might eventually cease to behave ethically if it reaches certain metanormative conclusions. The candidate conclusions include ones such as the denial of the alleged authority or overridingness of ethics and the conclusion that there are no ethical facts or properties (i.e. moral error theory). It will be argued that the target AI could conceivably reach such conclusions, and in turn this could cause them to abandon their ethical routines and proceed to cause great harm.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604703",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "162–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robust artificial moral agents and metanormativity",
		"URL": "https://doi.org/10.1145/3600211.3604703",
		"author": [
			{
				"family": "Cook",
				"given": "Tyler"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "uedaMitigatingVoterAttribute2023",
		"type": "paper-conference",
		"abstract": "The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&amp;S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&amp;S model. To address these limitations, we propose a new Soft D&amp;S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&amp;S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604660",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "170–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating voter attribute bias for fair opinion aggregation",
		"URL": "https://doi.org/10.1145/3600211.3604660",
		"author": [
			{
				"family": "Ueda",
				"given": "Ryosuke"
			},
			{
				"family": "Takeuchi",
				"given": "Koh"
			},
			{
				"family": "Kashima",
				"given": "Hisashi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "joLearningOptimalFair2023",
		"type": "paper-conference",
		"abstract": "The increasing use of machine learning in high-stakes domains – where people’s livelihoods are impacted – creates an urgent need for interpretable, fair, and highly accurate algorithms. With these needs in mind, we propose a mixed integer optimization (MIO) framework for learning optimal classification trees – one of the most interpretable models – that can be augmented with arbitrary fairness constraints. In order to better quantify the “price of interpretability”, we also propose a new measure of model interpretability called decision complexity that allows for comparisons across different classes of machine learning models. We benchmark our method against state-of-the-art approaches for fair classification on popular datasets; in doing so, we conduct one of the first comprehensive analyses of the trade-offs between interpretability, fairness, and predictive accuracy. Given a fixed disparity threshold, our method has a price of interpretability of about 4.2 percentage points in terms of out-of-sample accuracy compared to the best performing, complex models. However, our method consistently finds decisions with almost full parity, while other methods rarely do.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604664",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "181–192",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning optimal fair decision trees: Trade-offs between interpretability, fairness, and accuracy",
		"URL": "https://doi.org/10.1145/3600211.3604664",
		"author": [
			{
				"family": "Jo",
				"given": "Nathanael"
			},
			{
				"family": "Aghaei",
				"given": "Sina"
			},
			{
				"family": "Benson",
				"given": "Jack"
			},
			{
				"family": "Gomez",
				"given": "Andres"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangModelDebiasingGradientbased2023",
		"type": "paper-conference",
		"abstract": "Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicting sensitive attributes and 2) the other focus for predicting downstream task labels, and second, use them to perturb the latent code that guides the training of downstream task models towards fairness and utility goals. We show empirically that our framework works with both disentangled and non-disentangled representation learning methods and achieves better fairness-accuracy trade-off on unstructured and structured datasets than previous state-of-the-art approaches.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604668",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "193–204",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model debiasing via gradient-based explanation on representation",
		"URL": "https://doi.org/10.1145/3600211.3604668",
		"author": [
			{
				"family": "Zhang",
				"given": "Jindi"
			},
			{
				"family": "Wang",
				"given": "Luning"
			},
			{
				"family": "Su",
				"given": "Dan"
			},
			{
				"family": "Huang",
				"given": "Yongxiang"
			},
			{
				"family": "Cao",
				"given": "Caleb Chen"
			},
			{
				"family": "Chen",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gorantlaSamplingIndividuallyfairRankings2023",
		"type": "paper-conference",
		"abstract": "Rankings on online platforms help their end-users find the relevant information—people, news, media, and products—quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least α times the utility of the optimal fair solution. Here, α depends on the utilities, position-discounts, and constraints—it approaches 1 as the range of utilities or the position-discounts shrinks, or when utilities satisfy distributional assumptions. Empirically, we observe that our algorithm achieves individual and group fairness and that Pareto dominates the state-of-the-art baselines.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604671",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "205–216",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sampling individually-fair rankings that are always group fair",
		"URL": "https://doi.org/10.1145/3600211.3604671",
		"author": [
			{
				"family": "Gorantla",
				"given": "Sruthi"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			},
			{
				"family": "Louis",
				"given": "Anand"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nanchenKeepSensorsCheck2023",
		"type": "paper-conference",
		"abstract": "Machine learning models trained with passive sensor data from mobile devices can be used to perform various inferences pertaining to activity recognition, context awareness, and health and well-being. Prior work has improved inference performance through the use of multimodal sensors (inertial, GPS, proximity, app usage, etc.) or improved machine learning. In this context, a few studies shed light on critical issues relating to the poor cross-country generalization of models due to distributional shifts across countries. However, these studies have largely relied on inference performance as a means of studying generalization issues, failing to investigate whether the root cause of the problem is linked to specific sensor modalities (independent variables) or the target attribute (dependent variable). In this paper, we study this issue in complex activities of daily living (ADL) inference task, involving 12 classes, by using a multimodal, multi-country dataset collected from 689 participants across eight countries. We first show that the ‘country of origin’ of data is captured by sensors and can be inferred from each modality separately, with an average accuracy of 65%. We then propose two diversity scores (DS) that measure how a country differentiates from others w.r.t. sensor modalities or activities. Using these diversity scores, we observed that both individual sensor modalities and activities have the ability to differentiate countries. However, while many activities capture country differences, only the ‘App usage’ and ‘Location’ sensors can do so. By dissecting country-level diversity across dependent and independent variables, we provide a framework to better understand model generalization issues across countries and country-level diversity of sensing modalities.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604688",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "217–228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Keep sensors in check: Disentangling country-level generalization issues in mobile sensor-based models with diversity scores",
		"URL": "https://doi.org/10.1145/3600211.3604688",
		"author": [
			{
				"family": "Nanchen",
				"given": "Alexandre"
			},
			{
				"family": "Meegahapola",
				"given": "Lakmal"
			},
			{
				"family": "Droz",
				"given": "William"
			},
			{
				"family": "Gatica-Perez",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouIterativePartialFulfillment2023",
		"type": "paper-conference",
		"abstract": "Counterfactual (CF) explanations, also known as contrastive explanations and algorithmic recourses, are popular for explaining machine learning models in high-stakes domains. For a subject that receives a negative model prediction (e.g., mortgage application denial), the CF explanations are similar instances but with positive predictions, which informs the subject of ways to improve. While their various properties have been studied, such as validity and stability, we contribute a novel one: their behaviors under iterative partial fulfillment (IPF). Specifically, upon receiving a CF explanation, the subject may only partially fulfill it before requesting a new prediction with a new explanation, and repeat until the prediction is positive. Such partial fulfillment could be due to the subject’s limited capability (e.g., can only pay down two out of four credit card accounts at this moment) or an attempt to take the chance (e.g., betting that a monthly salary increase of 800isenougheventhough1,000 is recommended). Does such iterative partial fulfillment increase or decrease the total cost of improvement incurred by the subject? We mathematically formalize IPF and demonstrate, both theoretically and empirically, that different CF algorithms exhibit vastly different behaviors under IPF. We discuss implications of our observations, advocate for this factor to be carefully considered in the development and study of CF algorithms, and give several directions for future work.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604656",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "248–258",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Iterative partial fulfillment of counterfactual explanations: Benefits and risks",
		"URL": "https://doi.org/10.1145/3600211.3604656",
		"author": [
			{
				"family": "Zhou",
				"given": "Yilun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "globus-harrisMulticalibratedRegressionDownstream2023",
		"type": "paper-conference",
		"abstract": "We show how to take a regression function that is appropriately multicalibrated and efficiently post-process it into an approximately error minimizing classifier satisfying a large variety of fairness constraints. The post-processing requires no labeled data, and only a modest amount of unlabeled data and computation. The computational and sample complexity requirements of computing are comparable to the requirements for solving a single fair learning task optimally, but it can in fact be used to solve many different downstream fairness-constrained learning problems efficiently. Our post-processing method easily handles intersecting groups, generalizing prior work on post-processing regression functions to satisfy fairness constraints that only applied to disjoint groups. Our work extends recent work showing that multicalibrated regression functions are omnipredictors (i.e. can be post-processed to optimally solve unconstrained ERM problems) to constrained optimization problems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604683",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 28\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "259–286",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multicalibrated regression for downstream fairness",
		"URL": "https://doi.org/10.1145/3600211.3604683",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Gupta",
				"given": "Varun"
			},
			{
				"family": "Jung",
				"given": "Christopher"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hendersonSelfdestructingModelsIncreasing2023",
		"type": "paper-conference",
		"abstract": "A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model’s ability to perform profession classification.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604690",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "287–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Self-destructing models: Increasing the costs of harmful dual uses of foundation models",
		"URL": "https://doi.org/10.1145/3600211.3604690",
		"author": [
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Mitchell",
				"given": "Eric"
			},
			{
				"family": "Manning",
				"given": "Christopher"
			},
			{
				"family": "Jurafsky",
				"given": "Dan"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jorgensenNotFairImpact2023",
		"type": "paper-conference",
		"abstract": "When bias mitigation methods are applied to make fairer machine learning models in fairness-related classification settings, there is an assumption that the disadvantaged group should be better off than if no mitigation method was applied. However, this is a potentially dangerous assumption because a “fair” model outcome does not automatically imply a positive impact for a disadvantaged individual—they could still be negatively impacted. Modeling and accounting for those impacts is key to ensure that mitigated models are not unintentionally harming individuals; we investigate if mitigated models can still negatively impact disadvantaged individuals and what conditions affect those impacts in a loan repayment example. Our results show that most mitigated models negatively impact disadvantaged group members in comparison to the unmitigated models. The domain-dependent impacts of model outcomes should help drive future bias mitigation method development.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604699",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "297–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Not so fair: The impact of presumably fair machine learning models",
		"URL": "https://doi.org/10.1145/3600211.3604699",
		"author": [
			{
				"family": "Jorgensen",
				"given": "Mackenzie"
			},
			{
				"family": "Richert",
				"given": "Hannah"
			},
			{
				"family": "Black",
				"given": "Elizabeth"
			},
			{
				"family": "Criado",
				"given": "Natalia"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "talTargetSpecificationBias2023",
		"type": "paper-conference",
		"abstract": "Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604678",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "312–321",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",
		"URL": "https://doi.org/10.1145/3600211.3604678",
		"author": [
			{
				"family": "Tal",
				"given": "Eran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bennettUnpickingEpistemicInjustices2023",
		"type": "paper-conference",
		"abstract": "Applications of Artificial Intelligence (AI) in the domain of Personal Health Informatics (PHI) offer potential avenues for personalised treatment and support for people living with long-term conditions, however, they also present a number of ethical challenges. Whilst participatory approaches can help mitigate concerns by actively involving healthcare professionals, patients, and other stakeholders in design and development, these are constrained by the limits of epistemic standpoints and the risks posed by extrapolation from individuals to groups. In this paper we draw upon interviews with stakeholders involved in Human Immunodeficiency Virus (HIV) care, including clinicians, insurance providers and pharmaceutical industry representatives, to map intentions and ethical considerations for developing PHI tools for people living with HIV. Whilst treatment efficacy for HIV has improved patient quality of life and life expectancy, management and care is complicated by knowledge gaps about what living and ageing with HIV entails. We investigate how the critical concept of epistemic injustice can inform the design of data-driven technologies intended to address these gaps, helping orient expert perspectives within the broader structures and socio-historical influences that shape them. This is of particular importance when designing for marginalized populations such as people with HIV (i.e. who may experience social stigma and be under-resourced, managing multiple conditions), helping to identify and better account for fundamental ethical considerations such as equity.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604684",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "322–332",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unpicking epistemic injustices in digital health: On the implications of designing data-driven technologies for the management of long-term conditions",
		"URL": "https://doi.org/10.1145/3600211.3604684",
		"author": [
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Claisse",
				"given": "Caroline"
			},
			{
				"family": "Luger",
				"given": "Ewa"
			},
			{
				"family": "Durrant",
				"given": "Abigail C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangEvaluatingImpactSocial2023",
		"type": "paper-conference",
		"abstract": "Social determinants of health (SDOH) – the conditions in which people live, grow, and age – play a crucial role in a person’s health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protective attributes. We hope the new integrated EHR-SDOH database will enable studies on the relationship between community health and individual outcomes and provide new benchmarks to study algorithmic biases beyond race, gender, and age.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604719",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "333–350",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating the impact of social determinants on health prediction in the intensive care unit",
		"URL": "https://doi.org/10.1145/3600211.3604719",
		"author": [
			{
				"family": "Yang",
				"given": "Ming Ying"
			},
			{
				"family": "Kwak",
				"given": "Gloria Hyunjung"
			},
			{
				"family": "Pollard",
				"given": "Tom"
			},
			{
				"family": "Celi",
				"given": "Leo Anthony"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zajacGroundTruthDare2023a",
		"type": "paper-conference",
		"abstract": "One of the core goals of responsible AI development is ensuring high-quality training datasets. Many researchers have pointed to the importance of the annotation step in the creation of high-quality data, but less attention has been paid to the work that enables data annotation. We define this work as the design of ground truth schema and explore the challenges involved in the creation of datasets in the medical domain even before any annotations are made. Based on extensive work in three health-tech organisations, we describe five external and internal factors that condition medical dataset creation processes. Three external factors include regulatory constraints, the context of creation and use, and commercial and operational pressures. These factors condition medical data collection and shape the ground truth schema design. Two internal factors include epistemic differences and limits of labelling. These directly shape the design of the ground truth schema. Discussions of what constitutes high-quality data need to pay attention to the factors that shape and constrain what is possible to be created, to ensure responsible AI design.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604766",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "351–362",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ground truth or dare: Factors affecting the creation of medical datasets for training AI",
		"URL": "https://doi.org/10.1145/3600211.3604766",
		"author": [
			{
				"family": "Zając",
				"given": "Hubert Dariusz"
			},
			{
				"family": "Avlona",
				"given": "Natalia Rozalia"
			},
			{
				"family": "Kensing",
				"given": "Finn"
			},
			{
				"family": "Andersen",
				"given": "Tariq Osman"
			},
			{
				"family": "Shklovski",
				"given": "Irina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dennlerBoundBountyCollaboratively2023",
		"type": "paper-conference",
		"abstract": "Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604682",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "375–386",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bound by the bounty: Collaboratively shaping evaluation processes for queer AI harms",
		"URL": "https://doi.org/10.1145/3600211.3604682",
		"author": [
			{
				"family": "Dennler",
				"given": "Nathan"
			},
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Singh",
				"given": "Ashwin"
			},
			{
				"family": "Soldaini",
				"given": "Luca"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Tu",
				"given": "Huy"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Yee",
				"given": "Kyra"
			},
			{
				"family": "Peradejordi",
				"given": "Irene Font"
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Russo",
				"given": "Mayra"
			},
			{
				"family": "Pinhal",
				"given": "Jess De Jesus De Pinho"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "robinsonActionGuidanceAI2023",
		"type": "paper-conference",
		"abstract": "I offer a preliminary conceptual framework for evaluating AI alignment projects. It is based on the concept of action guidance. In §1 and §2, I explain action guidance and its importance to AI alignment. I introduce the ‘Guidance Framework’ in §3. In §4, I show how it can be applied to two different sorts of questions: the practical question of how to design a specific AI agent (my example is a fictional ocean-cleaning robot), and the theoretical question of how to evaluate a specific AI alignment proposal (my example is Stuart Russell's ‘binary approach’). In §5 I discuss limitations of the framework and opportunities for further research.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604714",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "387–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Action guidance and AI alignment",
		"URL": "https://doi.org/10.1145/3600211.3604714",
		"author": [
			{
				"family": "Robinson",
				"given": "Pamela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "birdTypologyRisksGenerative2023a",
		"type": "paper-conference",
		"abstract": "This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604722",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "396–410",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Typology of risks of generative text-to-image models",
		"URL": "https://doi.org/10.1145/3600211.3604722",
		"author": [
			{
				"family": "Bird",
				"given": "Charlotte"
			},
			{
				"family": "Ungless",
				"given": "Eddie"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "albiniConnectionGametheoreticFeature2023",
		"type": "paper-conference",
		"abstract": "Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604676",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "411–431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the connection between game-theoretic feature attributions and counterfactual explanations",
		"URL": "https://doi.org/10.1145/3600211.3604676",
		"author": [
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Mishra",
				"given": "Saumitra"
			},
			{
				"family": "Dervovic",
				"given": "Danial"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hardyAdaptiveAdversarialTraining2023",
		"type": "paper-conference",
		"abstract": "Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model’s classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier’s susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604704",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "432–442",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adaptive adversarial training does not increase recourse costs",
		"URL": "https://doi.org/10.1145/3600211.3604704",
		"author": [
			{
				"family": "Hardy",
				"given": "Ian"
			},
			{
				"family": "Yetukuri",
				"given": "Jayanth"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "sharmaREFRESHResponsibleEfficient2023",
		"type": "paper-conference",
		"abstract": "Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature reselection, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective. To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models. REFRESH’s underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models. Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently. We also discuss the need for reselection and REFRESH based on regulation desiderata.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604706",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "443–453",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "REFRESH: Responsible and efficient feature reselection guided by SHAP values",
		"URL": "https://doi.org/10.1145/3600211.3604706",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Dutta",
				"given": "Sanghamitra"
			},
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Lecue",
				"given": "Freddy"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			},
			{
				"family": "Veloso",
				"given": "Manuela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mouganFairnessImplicationsEncoding2023",
		"type": "paper-conference",
		"abstract": "Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g. support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: one-hot encoding and target encoding. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, irreducible bias, is due to direct group category discrimination and the second type, reducible bias, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604657",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "454–465",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness implications of encoding protected categorical attributes",
		"URL": "https://doi.org/10.1145/3600211.3604657",
		"author": [
			{
				"family": "Mougan",
				"given": "Carlos"
			},
			{
				"family": "Alvarez",
				"given": "Jose Manuel"
			},
			{
				"family": "Ruggieri",
				"given": "Salvatore"
			},
			{
				"family": "Staab",
				"given": "Steffen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bermanMachineLearningPractices2023",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers’ have demonstrated to be particularly salient for the societal impact of deployed ML systems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604689",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "466–481",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine Learning practices and infrastructures",
		"URL": "https://doi.org/10.1145/3600211.3604689",
		"author": [
			{
				"family": "Berman",
				"given": "Glen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "balaynFairnessToolkitsCheckbox2023",
		"type": "paper-conference",
		"abstract": "Fairness toolkits are developed to support machine learning (ML) practitioners in using algorithmic fairness metrics and mitigation methods. Past studies have investigated practical challenges for toolkit usage, which are crucial to understanding how to support practitioners. However, the extent to which fairness toolkits impact practitioners’ practices and enable reflexivity around algorithmic harms remains unclear (i.e., distributive unfairness beyond algorithmic fairness, and harms that are not related to the outputs of ML systems). Little is currently understood about the root factors that fragment practices when using fairness toolkits and how practitioners reflect on algorithmic harms. Yet, a deeper understanding of these facets is essential to enable the design of support tools for practitioners. To investigate the impact of toolkits on practices and identify factors that shape these practices, we carried out a qualitative study with 30 ML practitioners with varying backgrounds. Through a mixed within and between-subjects design, we tasked the practitioners with developing an ML model, and analyzed their reported practices to surface potential factors that lead to differences in practices. Interestingly, we found that fairness toolkits act as double-edge swords — with potentially positive and negative impacts on practices. Our findings showcase a plethora of human and organizational factors that play a key role in the way toolkits are envisioned and employed. These results bear implications for the design of future toolkits and educational training for practitioners and call for the creation of new policies to handle the organizational constraints faced by practitioners.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604674",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "482–495",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“☑ fairness toolkits, a checkbox culture?” on the factors that fragment developer practices in handling algorithmic harms",
		"URL": "https://doi.org/10.1145/3600211.3604674",
		"author": [
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Yurrita",
				"given": "Mireia"
			},
			{
				"family": "Yang",
				"given": "Jie"
			},
			{
				"family": "Gadiraju",
				"given": "Ujwal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ovalleFactoringMatrixDomination2023",
		"type": "paper-conference",
		"abstract": "Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness’ raison d’être of “fairness,” we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and praxis, and 4) provide actionable recommendations for AI fairness researchers to engage with intersectionality in their work by grounding it in AI epistemology.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604705",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "496–511",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
		"URL": "https://doi.org/10.1145/3600211.3604705",
		"author": [
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Gautam",
				"given": "Vagrant"
			},
			{
				"family": "Gee",
				"given": "Gilbert"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "straubMultidomainRelationalFramework2023",
		"type": "paper-conference",
		"abstract": "Calls for new metrics, technical standards and governance mechanisms to guide the adoption of Artificial Intelligence (AI) in institutions and public administration are now commonplace. Yet, most research and policy efforts aimed at understanding the implications of adopting AI tend to prioritize only a handful of ideas; they do not fully connect all the different perspectives and topics that are potentially relevant. In this position paper, we contend that this omission stems, in part, from what we call the ‘relational problem’ in socio-technical discourse: fundamental ontological issues have not yet been settled—including semantic ambiguity, a lack of clear relations between concepts and differing standard terminologies. This contributes to the persistence of disparate modes of reasoning to assess institutional AI systems, and the prevalence of conceptual isolation in the fields that study them including ML, human factors, social science and policy. After developing this critique, we offer a way forward by proposing a simple policy and research design tool in the form of a conceptual framework to organize terms across fields—consisting of three horizontal domains for grouping relevant concepts and related methods: Operational, Epistemic, and Normative. We first situate this framework against the backdrop of recent socio-technical discourse at two premier academic venues, AIES and FAccT, before illustrating how developing suitable metrics, standards, and mechanisms can be aided by operationalizing relevant concepts in each of these domains. Finally, we outline outstanding questions for developing this relational approach to institutional AI research and adoption.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604718",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "512–519",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A multidomain relational framework to guide institutional AI research and adoption",
		"URL": "https://doi.org/10.1145/3600211.3604718",
		"author": [
			{
				"family": "Straub",
				"given": "Vincent J"
			},
			{
				"family": "Morgan",
				"given": "Deborah"
			},
			{
				"family": "Hashem",
				"given": "Youmna"
			},
			{
				"family": "Francis",
				"given": "John"
			},
			{
				"family": "Esnaashari",
				"given": "Saba"
			},
			{
				"family": "Bright",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "naggitaFlickrAfricaExamining2023",
		"type": "paper-conference",
		"abstract": "Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an “othering” phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604659",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "520–530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Flickr africa: Examining geo-diversity in large-scale, human-centric visual data",
		"URL": "https://doi.org/10.1145/3600211.3604659",
		"author": [
			{
				"family": "Naggita",
				"given": "Keziah"
			},
			{
				"family": "LaChance",
				"given": "Julienne"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hongEvaluationTargetedDataset2023",
		"type": "paper-conference",
		"abstract": "Algorithmic audits of industry face recognition models have recently incentivized companies to diversify their data collection methods, which in turn has reduced error disparities along demographic lines, such as gender or race. We argue that it is important to understand exactly how various forms of targeted data collection mitigate performance disparities in these updated face recognition models. We propose an empirical framework to assess the impact of additional dataset collection targeted towards various racial groups. We apply our framework to three racially-annotated benchmark datasets using three standard face recognition models. Our findings empirically validate the notion that the introduction of data from the demographic group with the initially-lowest performance improves performance on that group significantly more than adding from other groups. We also observe that in all settings, the introduction of data from a previously omitted group does not harm the performance of other groups. Furthermore, investigation of feature embeddings reveals that performance increases are associated with a larger separation among images of different identities. Despite the commonalities we observe across datasets, we also find key differences: for example, in one dataset, training on one racial group generalizes well across all groups. These differences speak to the criticality of re-applying empirical evaluation methods, such as the methods in this work, when introducing new datasets or models.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604662",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "531–541",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluation of targeted dataset collection on racial equity in face recognition",
		"URL": "https://doi.org/10.1145/3600211.3604662",
		"author": [
			{
				"family": "Hong",
				"given": "Rachel"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "omranisabbaghiEvaluatingBiasedAttitude2023",
		"type": "paper-conference",
		"abstract": "Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604666",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "542–553",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating biased attitude associations of language models in an intersectional context",
		"URL": "https://doi.org/10.1145/3600211.3604666",
		"author": [
			{
				"family": "Omrani Sabbaghi",
				"given": "Shiva"
			},
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "narayananvenkitUnmaskingNationalityBias2023",
		"type": "paper-conference",
		"abstract": "We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604667",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "554–565",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unmasking nationality bias: A study of human perception of nationalities in AI-Generated articles",
		"URL": "https://doi.org/10.1145/3600211.3604667",
		"author": [
			{
				"family": "Narayanan Venkit",
				"given": "Pranav"
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Panchanadikar",
				"given": "Ruchi"
			},
			{
				"family": "Huang",
				"given": "Ting-Hao"
			},
			{
				"family": "Wilson",
				"given": "Shomir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "williamsNoJusticeNo2023",
		"type": "paper-conference",
		"abstract": "In this paper, we examine the risks posed by roboticists’ collaboration with law enforcement agencies in the U.S. Using Trust frameworks from AI Ethics, we argue that collaborations with law enforcement present not only risks of technology misuse, but also risks of legitimizing bad actors, and of exacerbating our field’s challenges of representation. We discuss evidence of bad dispositions justifying these risks, grounded in the behavior, origins, and incentivization of American policing, and suggest courses of action for American roboticists seeking to pursue research projects that currently require collaboration with law enforcement agencies, closing with a call for abolitionist robotics.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604663",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "566–575",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No justice, no robots: From the dispositions of policing to an abolitionist robotics",
		"URL": "https://doi.org/10.1145/3600211.3604663",
		"author": [
			{
				"family": "Williams",
				"given": "Tom"
			},
			{
				"family": "Haring",
				"given": "Kerstin Sophie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "turriWhyWeNeed2023",
		"type": "paper-conference",
		"abstract": "To enable the development and use of safe and equitable artificial intelligence (AI) systems, AI engineers must monitor deployed AI systems and learn from past AI incidents where failures have occurred. Around the world, public databases for cataloging AI systems and resulting harms are instrumental in promoting awareness of potential AI harms among policymakers, researchers, and the public. However, despite growing recognition of the potential of AI systems to produce harms, causes of AI systems failure remain elusive and AI incidents continue to occur. For example, incidents of AI bias are frequently reported and discussed, yet biased systems continue to be developed and deployed. This raises the question – how are we learning from documented incidents? What information do we need to analyze AI incidents and develop new AI engineering best practices? This paper examines reporting techniques from a variety of AI stakeholders and across different industries, identifies requirements towards the design of effective AI incident documentation, and proposes policy recommendations for augmenting current practice.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604700",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "576–583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why we need to know more: Exploring the state of AI incident documentation practices",
		"URL": "https://doi.org/10.1145/3600211.3604700",
		"author": [
			{
				"family": "Turri",
				"given": "Violet"
			},
			{
				"family": "Dzombak",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "huaEffectiveEnforceabilityEU2023",
		"type": "paper-conference",
		"abstract": "This paper examines whether competition law enforcement can remain effective under different AI development scenarios over the coming years. Economic and political power has become increasingly concentrated into a few AI companies, such as Big Tech. The growth of generative AI could further reinforce this concentration of power in Big Tech. The market power of these companies, and increasingly their involvement in AI, is a major focus for regulators such as the European Commission. Recent EU antitrust fines on Google alone run in the billions. The dynamism of technology markets such as AI can make it difficult for regulators to take effective action. If AI continues to develop rapidly over the coming years, propelled by the proliferation of generative AI, this ability to effectively enforce antitrust law may be further challenged. To help ensure regulators remain effective, EU competition law has been bolstered by a new tech-tailored, ex ante competition regime. These are likely to be critical tools to shape the market power of Big Tech but are largely untested. Exploring how these regulatory tools can be most effective in governing future AI development is a timely question for regulators, lawyers, companies, and citizens. This paper examines this question by considering the ‘effective enforceability’ of EU competition law and the Digital Markets Act under different AI development scenarios. By ‘effective enforceability’ of EU competition law we mean how well it achieves its policy objectives. We consider four factors: jurisdictional scope, potential loopholes, effectiveness of detection, and ability to remedy/sanction breaches. However, there is significant uncertainty as to how AI will develop in the coming years. Considering this, we propose an analytical framework based on five variables: key inputs, speed of development, AI capability, number of actors, and the nature/relationship of actors. In some of these scenarios, we argue EU competition law would struggle to address the power of the largest AI companies; but in many other scenarios it remains a powerful tool. This is a critical juncture for competition regulators. They stand at the dawn of emerging challenges presented by generative AI. With this paper, we hope to contribute to anticipatory governance at this important intersection of legal governance and technology.Effective and future-proof competition law enforcement is crucial to ensuring this potentially transformative technology has widely distributed benefits, rather than concentrating power in a few hands.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604694",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "596–605",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Effective enforceability of EU competition law under AI development scenarios: a framework for anticipatory governance",
		"URL": "https://doi.org/10.1145/3600211.3604694",
		"author": [
			{
				"family": "Hua",
				"given": "Shin-Shin"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lawrenceBureaucraticChallengeAI2023",
		"type": "paper-conference",
		"abstract": "Can government govern artificial intelligence (AI)? One of the central questions of AI governance surrounds state capacity, namely whether government has the ability to accomplish its policy goals. We study this question by assessing how well the U.S. federal government has implemented three binding laws around AI governance: two executive orders—concerning trustworthy AI in the public sector (E.O. 13,960) and AI leadership (E.O. 13,859)—and the AI in Government Act. We conduct the first systematic empirical assessment of the implementation status of these three laws, which have each been described as central to US AI innovation. First, we track, through extensive research, line-level adoption of each mandated action. Based on publicly available information, we find that fewer than 40 percent of 45 legal requirements could be verified as having been implemented. Second, we research the specific implementation of transparency requirements at up to 220 federal agencies. We find that nearly half of agencies failed to publicly issue AI use case inventories—even when these agencies have demonstrable use cases of machine learning. Even when agencies have complied with these requirements, efforts are inconsistent. Our work highlights the weakness of U.S. state capacity to carry out AI governance mandates and we discuss implications for how to address bureaucratic capacity challenges.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604701",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 47\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "606–652",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The bureaucratic challenge to AI governance: An empirical assessment of implementation at U.S. federal agencies",
		"URL": "https://doi.org/10.1145/3600211.3604701",
		"author": [
			{
				"family": "Lawrence",
				"given": "Christie"
			},
			{
				"family": "Cui",
				"given": "Isaac"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fabbriSelfdeterminationExplanationEthical2023",
		"type": "paper-conference",
		"abstract": "In the contemporary information age, recommender systems (RSs) play a critical role in influencing online behaviour: from social media to e-commerce, from music streaming to news aggregators, individuals are constantly targeted by personalized recommendations suggesting contents that may interest them. Despite such diffusion, the extent to which recommendations influence users’ decisions is still underexplored, given that independent audits on the structure and functioning of RSs deployed on online platforms are usually prevented by proprietary constraints. The nudging potential of RSs can represent a risk for vulnerable people: indeed, judicial cases involving platforms’ responsibility for displaying recommendations that may lead to political radicalization or endangerment of minors have recently caught public attention. The Digital Services Act of the European Union (DSA) is the first supranational regulation that sets specific transparency and auditing requirements for RSs implemented by online platforms with the aim of enhancing users’ self-determination: in particular, it allows users to modify the parameters on which recommendations rely so to let them choose autonomously which kind of content they want to see. This research focuses on whether and how the enforcement of this regulation can mitigate the unfair consequences of the power imbalance between online platforms and users. To this aim, I discuss the harms arising from digital nudging based on RSs and propose explanations as a tool that can reduce the impact of those harms by increasing users’ awareness. Through a comparative analysis of relevant articles of the DSA, the General Data Protection Regulation (GDPR) and the AI Act, I outline how the provisions of the DSA fill some of the gaps left by other relevant European regulations, while leaving the so-called right to explanation substantially unaddressed. As a result of this analysis, I argue that, in order for the implementation of the DSA provisions on recommender systems to be effective, policy-makers should: 1) enhance users’ awareness through clear and easily accessible explanations on how the recommendation process works and how they can be influenced by it; 2) grant users the possibility of intervening directly on the strategies through which RSs target them on the platform’s interface.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604717",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "653–661",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union",
		"URL": "https://doi.org/10.1145/3600211.3604717",
		"author": [
			{
				"family": "Fabbri",
				"given": "Matteo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "schwarzschildReckoningDisagreementProblem2023",
		"type": "paper-conference",
		"abstract": "As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model’s output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus between explainers other than those used in the loss term. We examine the trade-off between improved consensus and model performance. And finally, we study the influence our method has on feature attribution explanations.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604687",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "662–678",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reckoning with the disagreement problem: Explanation consensus as a training objective",
		"URL": "https://doi.org/10.1145/3600211.3604687",
		"author": [
			{
				"family": "Schwarzschild",
				"given": "Avi"
			},
			{
				"family": "Cembalest",
				"given": "Max"
			},
			{
				"family": "Rao",
				"given": "Karthik"
			},
			{
				"family": "Hines",
				"given": "Keegan"
			},
			{
				"family": "Dickerson",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshWhenFairClassification2023",
		"type": "paper-conference",
		"abstract": "The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all. To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-unaware algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-unaware and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604707",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "679–690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When fair classification meets noisy protected attributes",
		"URL": "https://doi.org/10.1145/3600211.3604707",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kvitca",
				"given": "Pablo"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "edenbergDisambiguatingAlgorithmicBias2023",
		"type": "paper-conference",
		"abstract": "As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term ‘bias.’ Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination. In this paper, we examine the promises and challenges of different approaches to disambiguating bias and designing for justice. While both approaches aid in understanding and addressing clear algorithmic harms, we argue that they also risk being leveraged in ways that ultimately deflect accountability from those building and deploying these systems. Applying this analysis to recent examples of generative AI, our argument highlights unseen dangers in current methods of evaluating algorithmic bias and points to ways to redirect approaches to addressing bias in generative AI at its early stages in ways that can more robustly meet the demands of justice.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604695",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "691–704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disambiguating algorithmic bias: From neutrality to justice",
		"URL": "https://doi.org/10.1145/3600211.3604695",
		"author": [
			{
				"family": "Edenberg",
				"given": "Elizabeth"
			},
			{
				"family": "Wood",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "buremaSectorbasedApproachAI2023",
		"type": "paper-conference",
		"abstract": "Acknowledging that society is made up of different sectors with their own rules and structures, this paper studies the relevance of a sector-specific perspective to AI ethics. Incidents with AI are studied in relation to five sectors (police, healthcare, education and academia, politics, automotive) using the AIAAIC repository. A total of 125 incidents are sampled and analyzed by conducting a qualitative content analysis on media reports. The results show that certain ethical principles are found breached across sectors: accuracy/reliability, bias/discrimination, transparency, surveillance/privacy, security. However, results also show that 1) some ethical issues (misinformation, safety, premise/intent) are sector specific, 2) the consequences and meaning of the same ethical issue is able to vary across sectors and 3) pre-existing sector-specific issues are reproduced with these ethical breaches. The paper concludes that general ethical principles are relevant to discuss across sectors, yet, a sector-based approach to AI ethics gives in-depth information on sector-specific structural issues.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604680",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "705–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context",
		"URL": "https://doi.org/10.1145/3600211.3604680",
		"author": [
			{
				"family": "Burema",
				"given": "Dafna"
			},
			{
				"family": "Debowski-Weimann",
				"given": "Nicole"
			},
			{
				"family": "Janowski",
				"given": "Alexander",
				"non-dropping-particle": "von"
			},
			{
				"family": "Grabowski",
				"given": "Jil"
			},
			{
				"family": "Maftei",
				"given": "Mihai"
			},
			{
				"family": "Jacobs",
				"given": "Mattis"
			},
			{
				"family": "Smagt",
				"given": "Patrick",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Benbouzid",
				"given": "Djalel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "segerDemocratisingAIMultiple2023",
		"type": "paper-conference",
		"abstract": "Numerous parties are calling for “the democratisation of AI”, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of “AI democratisation” that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to “democratising AI”, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604693",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "715–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Democratising AI: Multiple meanings, goals, and methods",
		"URL": "https://doi.org/10.1145/3600211.3604693",
		"author": [
			{
				"family": "Seger",
				"given": "Elizabeth"
			},
			{
				"family": "Ovadya",
				"given": "Aviv"
			},
			{
				"family": "Siddarth",
				"given": "Divya"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shelbySociotechnicalHarmsAlgorithmic2023",
		"type": "paper-conference",
		"abstract": "Understanding the landscape of potential harms from algorithmic systems enables practitioners to better anticipate consequences of the systems they build. It also supports the prospect of incorporating controls to help minimize harms that emerge from the interplay of technologies and social and cultural dynamics. A growing body of scholarship has identified a wide range of harms across different algorithmic technologies. However, computing research and practitioners lack a high level and synthesized overview of harms from algorithmic systems. Based on a scoping review of computing research (n=172), we present an applied taxonomy of sociotechnical harms to support a more systematic surfacing of potential harms in algorithmic systems. The final taxonomy builds on and refers to existing taxonomies, classifications, and terminologies. Five major themes related to sociotechnical harms — representational, allocative, quality-of-service, interpersonal harms, and social system/societal harms — and sub-themes are presented along with a description of these categories. We conclude with a discussion of challenges and opportunities for future research.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604673",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "723–741",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction",
		"URL": "https://doi.org/10.1145/3600211.3604673",
		"author": [
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Henne",
				"given": "Kathryn"
			},
			{
				"family": "Moon",
				"given": "AJung"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Nicholas",
				"given": "Paul"
			},
			{
				"family": "Yilla-Akbari",
				"given": "N'Mah"
			},
			{
				"family": "Gallegos",
				"given": "Jess"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Garcia",
				"given": "Emilio"
			},
			{
				"family": "Virk",
				"given": "Gurleen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yetukuriUserGuidedActionable2023",
		"type": "paper-conference",
		"abstract": "Machine Learning’s proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user’s actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604708",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "742–751",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards user guided actionable recourse",
		"URL": "https://doi.org/10.1145/3600211.3604708",
		"author": [
			{
				"family": "Yetukuri",
				"given": "Jayanth"
			},
			{
				"family": "Hardy",
				"given": "Ian"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grabowiczLearningDiscriminatoryTraining2023",
		"type": "paper-conference",
		"abstract": "Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups’ intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulations, and is computationally lightweight — it can be used with any supervised learning model to prevent direct and indirect discrimination via proxies while maximizing model accuracy for business necessity.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604710",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "752–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning from discriminatory training data",
		"URL": "https://doi.org/10.1145/3600211.3604710",
		"author": [
			{
				"family": "Grabowicz",
				"given": "Przemyslaw"
			},
			{
				"family": "Perello",
				"given": "Nicholas"
			},
			{
				"family": "Takatsu",
				"given": "Kenta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bhanotStresstestingBiasMitigation2023",
		"type": "paper-conference",
		"abstract": "To address the growing concern of unfairness in Artificial Intelligence (AI), several bias mitigation algorithms have been introduced in prior research. Their capabilities are often evaluated on certain overly-used datasets without rigorously stress-testing them under simultaneous train and test distribution shifts. To address this, we investigate the fairness vulnerabilities of these algorithms across several distribution shift scenarios using synthetic data, to highlight scenarios where these algorithms do and don’t work to encourage their trustworthy use. The paper makes three important contributions. Firstly, we propose a flexible pipeline called the Fairness Auditor to systematically stress-test bias mitigation algorithms using multiple synthetic datasets with shifts. Secondly, we introduce the Deviation Metric for measuring the fairness and utility performance of these algorithms under such shifts. Thirdly, we propose an interactive reporting tool for comparing algorithmic performance across various synthetic datasets, mitigation algorithms and metrics called the Fairness Report.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604713",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "764–774",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stress-testing bias mitigation algorithms to understand fairness vulnerabilities",
		"URL": "https://doi.org/10.1145/3600211.3604713",
		"author": [
			{
				"family": "Bhanot",
				"given": "Karan"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Zeng",
				"given": "Jiaming"
			},
			{
				"family": "Bennett",
				"given": "Kristin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "juijnPerceivedAlgorithmicFairness2023",
		"type": "paper-conference",
		"abstract": "Growing concerns about the fairness of algorithmic decision-making systems have prompted a proliferation of mathematical formulations aimed at remedying algorithmic bias. Yet, integrating mathematical fairness alone into algorithms is insufficient to ensure their acceptance, trust, and support by humans. It is also essential to understand what humans perceive as fair. In this study, we, therefore, conduct an empirical user study into crowdworkers’ algorithmic fairness perceptions, focusing on algorithmic hiring. We build on perspectives from organizational justice theory, which categorizes fairness into distributive, procedural, and interactional components. By doing so, we find that algorithmic fairness perceptions are higher when crowdworkers are provided not only with information about the algorithmic outcome but also about the decision-making process. Remarkably, we observe this effect even when the decision-making process can be considered unfair, when gender, a sensitive attribute, is used as a main feature. By showing realistic trade-offs between fairness criteria, we moreover find a preference for equalizing false negatives over equalizing selection rates amongst groups. Our findings highlight the importance of considering all components of algorithmic fairness, rather than solely treating it as an outcome distribution problem. Importantly, our study contributes to the literature on the connection between mathematical– and perceived algorithmic fairness, and highlights the potential benefits of leveraging organizational justice theory to enhance the evaluation of perceived algorithmic fairness.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604677",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "775–785",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceived algorithmic fairness using organizational justice theory: An empirical case study on algorithmic hiring",
		"URL": "https://doi.org/10.1145/3600211.3604677",
		"author": [
			{
				"family": "Juijn",
				"given": "Guusje"
			},
			{
				"family": "Stoimenova",
				"given": "Niya"
			},
			{
				"family": "Reis",
				"given": "João"
			},
			{
				"family": "Nguyen",
				"given": "Dong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "naikSocialBiasesTexttoimage2023",
		"type": "paper-conference",
		"abstract": "Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people from results for both models. Such biases can get mitigated by increasing the amount of specification in the prompt itself, although the prompting mitigation will not address discrepancies in image quality or other usages of the model or its representations in other scenarios. Further, we observe personality traits being associated with only a limited set of people at the intersection of race, gender, and age. Finally, an analysis of geographical location representations on everyday situations (e.g., park, food, weddings) shows that for most situations, images generated through default location-neutral prompts are closer and more similar to images generated for locations of United States and Germany.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604711",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 23\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "786–808",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social biases through the text-to-image generation lens",
		"URL": "https://doi.org/10.1145/3600211.3604711",
		"author": [
			{
				"family": "Naik",
				"given": "Ranjita"
			},
			{
				"family": "Nushi",
				"given": "Besmira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "aliEvaluatingFairnessDiscriminative2023",
		"type": "paper-conference",
		"abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI’s CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604720",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 25\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "809–833",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating the fairness of discriminative foundation models in computer vision",
		"URL": "https://doi.org/10.1145/3600211.3604720",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Kleindessner",
				"given": "Matthäus"
			},
			{
				"family": "Wenzel",
				"given": "Florian"
			},
			{
				"family": "Budhathoki",
				"given": "Kailash"
			},
			{
				"family": "Cevher",
				"given": "Volkan"
			},
			{
				"family": "Russell",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lammertsHowYouFeel2023",
		"type": "paper-conference",
		"abstract": "Hate speech moderation remains a challenging task for social media platforms. Human-AI collaborative systems offer the potential to combine the strengths of humans’ reliability and the scalability of machine learning to tackle this issue effectively. While methods for task handover in human-AI collaboration exist that consider the costs of incorrect predictions, insufficient attention has been paid to accurately estimating these costs. In this work, we propose a value-sensitive rejection mechanism that automatically rejects machine decisions for human moderation based on users’ value perceptions regarding machine decisions. We conduct a crowdsourced survey study with 160 participants to evaluate their perception of correct and incorrect machine decisions in the domain of hate speech detection, as well as occurrences where the system rejects making a prediction. Here, we introduce Magnitude Estimation, an unbounded scale, as the preferred method for measuring user (dis)agreement with machine decisions. Our results show that Magnitude Estimation can provide a reliable measurement of participants’ perception of machine decisions. By integrating user-perceived value into human-AI collaboration, we further show that it can guide us in 1) determining when to accept or reject machine decisions to obtain the optimal total value a model can deliver and 2) selecting better classification models as compared to the more widely used target of model accuracy.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604655",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "834–844",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do you feel? Measuring user-perceived value for rejecting machine decisions in hate speech detection",
		"URL": "https://doi.org/10.1145/3600211.3604655",
		"author": [
			{
				"family": "Lammerts",
				"given": "Philippe"
			},
			{
				"family": "Lippmann",
				"given": "Philip"
			},
			{
				"family": "Hsu",
				"given": "Yen-Chia"
			},
			{
				"family": "Casati",
				"given": "Fabio"
			},
			{
				"family": "Yang",
				"given": "Jie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rarrickGATEChallengeSet2023",
		"type": "paper-conference",
		"abstract": "Although recent years have brought significant progress in improving translation of unambiguously gendered sentences, translation of ambiguously gendered input remains relatively unexplored. When source gender is ambiguous, machine translation models typically default to stereotypical gender roles, perpetuating harmful bias. Recent work has led to the development of \"gender rewriters\" that generate alternative gender translations on such ambiguous inputs, but such systems are plagued by poor linguistic coverage. To encourage better performance on this task we present and release GATE, a linguistically diverse corpus of gender-ambiguous source sentences along with multiple alternative target language translations. We also provide tools for evaluation and system analysis when using GATE and use them to evaluate our translation rewriter.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604675",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "845–854",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GATE: A challenge set for gender-ambiguous translation examples",
		"URL": "https://doi.org/10.1145/3600211.3604675",
		"author": [
			{
				"family": "Rarrick",
				"given": "Spencer"
			},
			{
				"family": "Naik",
				"given": "Ranjita"
			},
			{
				"family": "Mathur",
				"given": "Varun"
			},
			{
				"family": "Poudel",
				"given": "Sundar"
			},
			{
				"family": "Chowdhary",
				"given": "Vishal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chanReclaimingDigitalCommons2023",
		"type": "paper-conference",
		"abstract": "Democratization of AI means not only that people can freely use AI, but also that people can collectively decide how AI is to be used. In particular, collective decision-making power is required to redress the negative externalities from the development of increasingly advanced AI systems, including degradation of the digital commons and unemployment from automation. The rapid pace of AI development and deployment currently leaves little room for this power. Monopolized in the hands of private corporations, the development of the most capable foundation models has proceeded largely without public input. There is currently no implemented mechanism for ensuring that the economic value generated by such models is redistributed to account for their negative externalities. The citizens that have generated the data necessary to train models do not have input on how their data are to be used. In this work, we propose that a public data trust assert control over training data for foundation models. In particular, this trust should scrape the internet as a digital commons, to license to commercial model developers for a percentage cut of revenues from deployment. First, we argue in detail for the existence of such a trust. We also discuss feasibility and potential risks. Second, we detail a number of ways for a data trust to incentivize model developers to use training data only from the trust. We propose a mix of verification mechanisms, potential regulatory action, and positive incentives. We conclude by highlighting other potential benefits of our proposed data trust and connecting our work to ongoing efforts in data and compute governance.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604658",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "855–868",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reclaiming the digital commons: A public data trust for training data",
		"URL": "https://doi.org/10.1145/3600211.3604658",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Bradley",
				"given": "Herbie"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "collinsHumanUncertaintyConceptbased2023",
		"type": "paper-conference",
		"abstract": "Placing a human in the loop may help abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604692",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "869–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human uncertainty in concept-based AI systems",
		"URL": "https://doi.org/10.1145/3600211.3604692",
		"author": [
			{
				"family": "Collins",
				"given": "Katherine Maeve"
			},
			{
				"family": "Barker",
				"given": "Matthew"
			},
			{
				"family": "Espinosa Zarlenga",
				"given": "Mateo"
			},
			{
				"family": "Raman",
				"given": "Naveen"
			},
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Jamnik",
				"given": "Mateja"
			},
			{
				"family": "Sucholutsky",
				"given": "Ilia"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Dvijotham",
				"given": "Krishnamurthy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "khosrowiDiffusingCreatorAttributing2023",
		"type": "paper-conference",
		"abstract": "The recent wave of generative AI (GAI) systems like Stable Diffusion that can produce images from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GAI? Existing views on creatorship are mixed: some insist that GAI systems are mere tools, and human prompters are creators proper; others are more open to acknowledging more significant roles for GAI, but most conceive of creatorship in an all-or-nothing fashion. We develop a novel view, called CCC (collective-centered creation), that improves on these existing positions. On CCC, GAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GAI systems, developers, producers of training data and others. Importantly, CCC maintains that GAI systems can sometimes be part of a co-creating collective. We detail how CCC can advance existing debates and resolve controversies around creatorship involving GAI.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604716",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "890–900",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diffusing the creator: Attributing credit for generative AI outputs",
		"URL": "https://doi.org/10.1145/3600211.3604716",
		"author": [
			{
				"family": "Khosrowi",
				"given": "Donal"
			},
			{
				"family": "Finn",
				"given": "Finola"
			},
			{
				"family": "Clark",
				"given": "Elinor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshChatGPTPerpetuatesGender2023",
		"type": "paper-conference",
		"abstract": "In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in machine translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT’s accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7th most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g., man = doctor, woman = nurse) or actions (e.g., woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to ‘he’ or ‘she’. We also observe ChatGPT completely failing to translate the English gender-neutral singular pronoun ‘they’ into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AI systems that perform machine translation to better accommodate such low-resource languages.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604672",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "901–912",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ChatGPT perpetuates gender bias in machine translation and ignores non-gendered pronouns: Findings across bengali and five other low-resource languages",
		"URL": "https://doi.org/10.1145/3600211.3604672",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rastogiSupportingHumanAICollaboration2023",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest&nbsp;[36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604712",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "913–926",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Supporting human-AI collaboration in auditing LLMs with LLMs",
		"URL": "https://doi.org/10.1145/3600211.3604712",
		"author": [
			{
				"family": "Rastogi",
				"given": "Charvi"
			},
			{
				"family": "Tulio Ribeiro",
				"given": "Marco"
			},
			{
				"family": "King",
				"given": "Nicholas"
			},
			{
				"family": "Nori",
				"given": "Harsha"
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "singhMeasuresDisparityTheir2023",
		"type": "paper-conference",
		"abstract": "Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604697",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "927–938",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measures of disparity and their efficient estimation",
		"URL": "https://doi.org/10.1145/3600211.3604697",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "saisubramanianBalancingTradeoffClustering2020",
		"type": "paper-conference",
		"abstract": "Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β\\!=\\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3375627.3375843",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"page": "351–357",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Balancing the Tradeoff Between Clustering Value and Interpretability",
		"URL": "https://dl.acm.org/doi/10.1145/3375627.3375843",
		"author": [
			{
				"family": "Saisubramanian",
				"given": "Sandhya"
			},
			{
				"family": "Galhotra",
				"given": "Sainyam"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					7
				]
			]
		}
	},
	{
		"id": "jiangAIArtIts2023",
		"type": "paper-conference",
		"abstract": "The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial “generative AI Art” products have entered the market, making generative AI an estimated $48B industry [125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3600211.3604681",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0231-0",
		"page": "363–374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "AI Art and its Impact on Artists",
		"URL": "https://dl.acm.org/doi/10.1145/3600211.3604681",
		"author": [
			{
				"family": "Jiang",
				"given": "Harry H."
			},
			{
				"family": "Brown",
				"given": "Lauren"
			},
			{
				"family": "Cheng",
				"given": "Jessica"
			},
			{
				"family": "Khan",
				"given": "Mehtab"
			},
			{
				"family": "Gupta",
				"given": "Abhishek"
			},
			{
				"family": "Workman",
				"given": "Deja"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Flowers",
				"given": "Johnathan"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					29
				]
			]
		}
	},
	{
		"id": "selialiaMitigatingGroupBias2024",
		"type": "paper-conference",
		"abstract": "Federated learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature, i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients. It produces biased global models, i.e., models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity. Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy. Our main idea is to leverage average conditional probabilities to compute a cross-domain group importance weights derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while ensuring through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of image classification benchmarks assesses the fair decision-making of our framework in real-world settings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658954",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1043–1054",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating group bias in federated learning for heterogeneous devices",
		"URL": "https://doi.org/10.1145/3630106.3658954",
		"author": [
			{
				"family": "Selialia",
				"given": "Khotso"
			},
			{
				"family": "Chandio",
				"given": "Yasra"
			},
			{
				"family": "Anwar",
				"given": "Fatima M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lunichExplainableArtificialIntelligence2024",
		"type": "paper-conference",
		"abstract": "The rising adoption of learning analytics and academic performance prediction technologies in higher education highlights the urgent need for transparency and explainability. This demand, rooted in ethical concerns and fairness considerations, converges with Explainable Artificial Intelligence (XAI) principles. Despite the recognized importance of transparency and fairness in learning analytics, empirical studies examining student fairness perceptions, particularly within academic performance prediction, remain limited. We conducted a pre-registered factorial survey experiment involving 1,047 German students to investigate how decision tree features (simplicity and accuracy) influence perceived distributive and informational fairness, mediated by causability (i.e., the self-assessed understandability of a machine learning model’s cause-effect linkages). Additionally, we examined the moderating role of institutional trust in these relationships. Our results indicate that decision tree simplicity positively affects fairness perceptions, mediated by causability. In contrast, prediction accuracy neither directly nor indirectly influences these perceptions. Even if the hypothesized effects of interest are either minor or non-existent, results show that the medium positive effect of causability on the distributive fairness assessment depends on institutional trust. These findings substantially impact the crafting of transparent machine learning models in educational settings. We discuss important implications for fairness and transparency in implementing academic performance prediction systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658953",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1031–1042",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable artificial intelligence for academic performance prediction. An experimental study on the impact of accuracy and simplicity of decision trees on causability and fairness perceptions",
		"URL": "https://doi.org/10.1145/3630106.3658953",
		"author": [
			{
				"family": "Lünich",
				"given": "Marco"
			},
			{
				"family": "Keller",
				"given": "Birte"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hancockTensionsDataSharing2024",
		"type": "paper-conference",
		"abstract": "There are calls for greater data sharing to address human rights issues. Advocates claim this will provide an evidence-base to increase transparency, improve accountability, enhance decision-making, identify abuses, and offer remedies for rights violations. However, these well-intentioned efforts have been found to sometimes enable harms against the people they seek to protect. This paper shows issues relating to fairness, accountability, or transparency (FAccT) in and around data sharing can produce such ‘ironic’ consequences. It does so using an empirical case study: efforts to tackle modern slavery and human trafficking in the UK. We draw on a qualitative analysis of expert interviews, workshops, ecosystem mapping exercises, and a desk-based review. The findings show how, in the UK, a large ecosystem of data providers, hubs, and users emerged to process and exchange data from across the country. We identify how issues including legal uncertainties, non-transparent sharing procedures, and limited accountability regarding downstream uses of data may undermine efforts to tackle modern slavery and place victims of abuses at risk of further harms. Our findings help explain why data sharing activities can have negative consequences for human rights, even within human rights initiatives. Moreover, our analysis offers a window into how FAccT principles for technology relate to the human rights implications of data sharing. Finally, we discuss why these tensions may be echoed in other areas where data sharing is pursued for human rights concerns, identifying common features which may lead to similar results, especially where sensitive data is shared to achieve social goods or policy objectives.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658949",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "974–987",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The tensions of data sharing for human rights: A modern slavery case study",
		"URL": "https://doi.org/10.1145/3630106.3658949",
		"author": [
			{
				"family": "Hancock",
				"given": "Jamie"
			},
			{
				"family": "Mahesh",
				"given": "Sarada"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Mazumder",
				"given": "Anjali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chanVisibilityAIAgents2024",
		"type": "paper-conference",
		"abstract": "Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658948",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "958–973",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Visibility into AI agents",
		"URL": "https://doi.org/10.1145/3630106.3658948",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Kaufmann",
				"given": "Max"
			},
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Hammond",
				"given": "Lewis"
			},
			{
				"family": "Bradley",
				"given": "Herbie"
			},
			{
				"family": "Bluemke",
				"given": "Emma"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "zhangStructuralInterventionsDynamics2024",
		"type": "paper-conference",
		"abstract": "Recent conversations in the algorithmic fairness literature have raised several concerns with standard conceptions of fairness. First, constraining predictive algorithms to satisfy fairness benchmarks may sometimes lead to non-optimal outcomes for disadvantaged groups. Second, technical interventions are often ineffective by themselves, especially when divorced from an understanding of structural processes that generate social inequality. Inspired by both these critiques, we construct a common decision-making model, using mortgage loans as a running example. We show that under some conditions, any choice of decision threshold will inevitably perpetuate existing disparities in financial stability unless one deviates from the Pareto optimal policy. This confirms the intuition that technical interventions, such as fairness constraints, often do not sufficiently address persistent underlying inequities. Then, we model the effects of three different types of interventions: (1) policy changes in the algorithm’s decision threshold, and external changes to parameters that govern the downstream effects of late payment for (2) the whole population or (3) disadvantaged subgroups. We show how different interventions are recommended depending on the difficulty of enacting structural change upon external parameters and depending on the policymaker’s preferences for equity or efficiency. Counterintuitively, we demonstrate that preferences for efficiency over equity may sometimes lead to recommendations for interventions that target the under-resourced group alone. Finally, we simulate the effects of interventions on a dataset that combines HMDA and Fannie Mae loan data. This research highlights the ways that structural inequality can be perpetuated by seemingly unbiased decision mechanisms, and it shows that in many situations, technical solutions must be paired with external, context-aware interventions to enact social change.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658952",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1014–1030",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Structural interventions and the dynamics of inequality",
		"URL": "https://doi.org/10.1145/3630106.3658952",
		"author": [
			{
				"family": "Zhang",
				"given": "Aurora"
			},
			{
				"family": "Hosoi",
				"given": "Anette"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kouModelPerformanceClaim2024",
		"type": "paper-conference",
		"abstract": "Two goals – improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the responsibility gap – holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability’s advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658951",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1002–1013",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From model performance to claim: How a change of focus in machine learning replicability can help bridge the responsibility gap",
		"URL": "https://doi.org/10.1145/3630106.3658951",
		"author": [
			{
				"family": "Kou",
				"given": "Tianqi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hancockTroubleSeaData2024",
		"type": "paper-conference",
		"abstract": "Recent years have revealed the severity and scale of human rights abuses at sea. Yet maritime human rights investigations remain challenging due to an array of difficulties, including physical inaccessibility and a complex legal environment. Improving the availability of data has been framed as a solution that will enhance transparency in marine-related activities and improve accountability for rights violations. Such enthusiasm has fuelled the development of technological solutions promising to identify abuses and safeguard vulnerable individuals. However, these efforts clash with concerns over the use of data and technology in human rights practice. In the context of such tensions, this paper studies how data and technology have been integrated within investigations into rights abuses at sea. We examine the challenges posed for transparency, accountability, and fairness regarding communities affected by rights violations. We ask: do data and digital technologies offer effective means for helping to expose rights abuses and hold malicious actors accountable? Or do they introduce new threats to autonomy, privacy, and dignity? We present empirical research based on qualitative engagements with expert practitioners. We find: 1) an increased availability of datasets did not necessarily prevent harm or improve safeguarding for vulnerable people; 2) many tech solutions were detached from affected individuals’ lived experiences and appeared not to meet communities’ needs; 3) uses of data and technology could introduce or aggravate risks to fairness and accountability within human rights investigations. We contribute a much-needed reflection on the actual implications of the use of data and technological tools for communities affected by human rights violations. Regarding maritime human rights, we argue that prioritising large-scale, top-down monitoring to collect larger datasets or market more tech solutions is not the best way for data and technology to contribute to transparency and accountability. Instead, we advocate for deeper engagement with affected communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658950",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "988–1001",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trouble at Sea: Data and digital technology challenges for maritime human rights concerns",
		"URL": "https://doi.org/10.1145/3630106.3658950",
		"author": [
			{
				"family": "Hancock",
				"given": "Jamie"
			},
			{
				"family": "Hui",
				"given": "Ruoyun"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Mazumder",
				"given": "Anjali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pessachGenderRepresentationOnline2024",
		"type": "paper-conference",
		"abstract": "We present a broad characterization of gender representation in a large heterogeneous sample of retail products. In particular, we study online product textual information, such as titles and descriptions. Our goal is to understand from a semantic perspective, differences and similarities in how girls (women) and boys (men) are represented. We perform a comparative analysis of the language used in gendered products (i.e., products that mention exclusively either of these two genders), and additionally compare it to products that are explicitly gender neutral or inclusive. We found that the adjectives, skills, occupations, and values described in gendered products tended to reinforce stereotypes. Some of these stereotypes are aligned with historical findings from research on traditional off-line retail stores, and others are new owing to the up-to-date product dataset our research is based on. By leveraging additional existing resources we were able to gain insight into how certain product descriptions reflect stereotypes that are related to soft-skills and hierarchical occupational information. Conversely, we found that a large segment of products present explicitly as gender neutral or inclusive. We explore whether the language used by gender-inclusive products can be useful to improve stereotypes reflected in gendered product text. Specifically, we study its effect in word embedding fairness through debiasing techniques.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658947",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "947–957",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender representation across online retail products",
		"URL": "https://doi.org/10.1145/3630106.3658947",
		"author": [
			{
				"family": "Pessach",
				"given": "Dana"
			},
			{
				"family": "Poblete",
				"given": "Barbara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chienBehavioristRepresentationalHarms2024",
		"type": "paper-conference",
		"abstract": "Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, examining current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement and mitigation praxis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658946",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "933–946",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond behaviorist representational harms: a plan for measurement and mitigation",
		"URL": "https://doi.org/10.1145/3630106.3658946",
		"author": [
			{
				"family": "Chien",
				"given": "Jennifer"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "riveraEscalationRisksLanguage2024",
		"type": "paper-conference",
		"abstract": "Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658942",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 63\npublisher-place: Rio de Janeiro, Brazil",
		"page": "836–898",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Escalation risks from language models in military and diplomatic decision-making",
		"URL": "https://doi.org/10.1145/3630106.3658942",
		"author": [
			{
				"family": "Rivera",
				"given": "Juan-Pablo"
			},
			{
				"family": "Mukobi",
				"given": "Gabriel"
			},
			{
				"family": "Reuel",
				"given": "Anka"
			},
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Smith",
				"given": "Chandler"
			},
			{
				"family": "Schneider",
				"given": "Jacquelyn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kimImNotSure2024",
		"type": "paper-conference",
		"abstract": "Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658941",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "822–835",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"I'm not sure, but...\": Examining the impact of large language models' uncertainty expression on user reliance and trust",
		"URL": "https://doi.org/10.1145/3630106.3658941",
		"author": [
			{
				"family": "Kim",
				"given": "Sunnie S. Y."
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Vorvoreanu",
				"given": "Mihaela"
			},
			{
				"family": "Ballard",
				"given": "Stephanie"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "srinivasanGeneralizedPeopleDiversity2024",
		"type": "paper-conference",
		"abstract": "Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm [7], is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658940",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 25\npublisher-place: Rio de Janeiro, Brazil",
		"page": "797–821",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Generalized people diversity: Learning a human perception-aligned diversity representation for people images",
		"URL": "https://doi.org/10.1145/3630106.3658940",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Hansa"
			},
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "Sinha",
				"given": "Aradhana"
			},
			{
				"family": "Madras",
				"given": "David"
			},
			{
				"family": "Olanubi",
				"given": "Gbolahan Oluwafemi"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Ricco",
				"given": "Susanna"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dominguezhernandezMappingIndividualSocial2024",
		"type": "paper-conference",
		"abstract": "Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658939",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "776–796",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping the individual, social and biospheric impacts of Foundation Models",
		"URL": "https://doi.org/10.1145/3630106.3658939",
		"author": [
			{
				"family": "Domı́nguez Hernández",
				"given": "Andrés"
			},
			{
				"family": "Krishna",
				"given": "Shyam"
			},
			{
				"family": "Perini",
				"given": "Antonella Maia"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Borda",
				"given": "Ann"
			},
			{
				"family": "Hashem",
				"given": "Youmna"
			},
			{
				"family": "Hadjiloizou",
				"given": "Semeli"
			},
			{
				"family": "Mahomed",
				"given": "Sabeehah"
			},
			{
				"family": "Jayadeva",
				"given": "Smera"
			},
			{
				"family": "Aitken",
				"given": "Mhairi"
			},
			{
				"family": "Leslie",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "benattiGenderBiasDetection2024",
		"type": "paper-conference",
		"abstract": "Data derived from the realm of the social sciences is often produced in digital text form, which motivates its use as a source for natural language processing methods. Researchers and practitioners have developed and relied on artificial intelligence techniques to collect, process, and analyze documents in the legal field, especially for tasks such as text summarization and classification. While increasing procedural efficiency is often the primary motivation behind natural language processing in the field, several works have proposed solutions for human rights-related issues, such as assessment of public policy and institutional social settings. One such issue is the presence of gender biases in court decisions, which has been largely studied in social sciences fields; biased institutional responses to gender-based violence are a violation of international human rights dispositions since they prevent gender minorities from accessing rights and hamper their dignity. Natural language processing-based approaches can help detect these biases on a larger scale. Still, the development and use of such tools require researchers and practitioners to be mindful of legal and ethical aspects concerning data sharing and use, reproducibility, domain expertise, and value-charged choices. In this work, we (a) present an experimental framework developed to automatically detect gender biases in court decisions issued in Brazilian Portuguese and (b) describe and elaborate on features we identify to be critical in such a technology, given its proposed use as a support tool for research and assessment of court activity.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658937",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "746–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias detection in court decisions: a brazilian case study",
		"URL": "https://doi.org/10.1145/3630106.3658937",
		"author": [
			{
				"family": "Benatti",
				"given": "Raysa"
			},
			{
				"family": "Severi",
				"given": "Fabiana"
			},
			{
				"family": "Avila",
				"given": "Sandra"
			},
			{
				"family": "Colombini",
				"given": "Esther Luna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "awumeySystematicReviewBiometric2024",
		"type": "paper-conference",
		"abstract": "Modern advances in AI have increased employer interest in tracking workers’ biometric signals — e.g., their brainwaves and facial expressions — to evaluate and make predictions about their performance and productivity. These technologies afford managers information about internal emotional and physiological states that were previously accessible only to individual workers, raising new concerns around worker privacy and autonomy. Yet, the research literature on the impact of AI-powered biometric work monitoring (AI-BWM) technologies on workers remains fragmented across disciplines and industry sectors, limiting our understanding of its impacts on workers at large. In this paper, we sytematically review 129 papers, spanning varied disciplines and industry sectors, that discuss and analyze the impact of AI-powered biometric monitoring technologies in occupational settings. We situate this literature across a process model that spans the development, deployment, and usage phases of these technologies. We further draw on Shelby et al.’s Taxonomy of Socio-technical Harms in AI systems to systematize the harms experienced by workers across the three phases of our process model. We find that the development, deployment, and sustained use of AI-powered biometric work monitoring technologies put workers at risk of a number of the socio-technical harms specified by Shelby et al.: e.g., by forcing workers to exert additional emotional labor to avoid flagging unreliable affect monitoring systems, or through the use of these data to make inferences about productivity. Our research contributes to the field of critical AI studies by highlighting the potential for a cascade of harms to occur when the impact of these technologies on workers is not considered at all phases of our process model.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658945",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "920–932",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of biometric monitoring in the workplace: Analyzing socio-technical harms in development, deployment and use",
		"URL": "https://doi.org/10.1145/3630106.3658945",
		"author": [
			{
				"family": "Awumey",
				"given": "Ezra"
			},
			{
				"family": "Das",
				"given": "Sauvik"
			},
			{
				"family": "Forlizzi",
				"given": "Jodi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lopezMoreSumIts2024",
		"type": "paper-conference",
		"abstract": "Algorithmic systems are increasingly being applied in contexts of state action to, in some capacity, mediate the relations between state and individual. Disadvantageous effects, such as potential discriminatory outcomes brought forth by different kinds of biases, have been the locus of severe critique by academic scholarship and political activism. There has been scholarly work conceptualizing biases and types of biases, as well as types of harm. Drawing from Elizabeth Anderson’s conceptualization of relational equality, this paper emphasizes the relationality of the encounters between state and individual. This paper introduces \"susceptibility to algorithmic disadvantage\" as a conceptual framework to address the relational constellation at play. Susceptibility to algorithmic disadvantage has a vertical dimension that addresses the relation between a state actor and an individual and a horizontal dimension that is characterized by intersectional inequalities that prevail in societal contexts. Intersectional feminist scholarship has established that interlocking systems of oppression amount to more than the sum of their single-axis parts. Paralleling this argument, this paper argues that susceptibility to algorithmic disadvantage amounts to more than the sum of the vertical and the horizontal dimension: the dimensions co-constitute and reinforce each other. The proposed framework is applied to four international case studies situated in crucial areas of state action: facial recognition in law enforcement in the USA, biometric identification in social welfare in India, dialect recognition in the asylum system in Germany, and grade prediction in the education system in the UK. Viewed through the lens of the proposed framework, heterogeneous use cases in different locations and areas of state action emerge as similar considering the inquiry into questions of justice, rendering the proposed framework a useful tool for analysis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658944",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "909–919",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More than the sum of its parts: Susceptibility to algorithmic disadvantage as a conceptual framework",
		"URL": "https://doi.org/10.1145/3630106.3658944",
		"author": [
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sobeyHarmfulFetishisationReductive2024",
		"type": "paper-conference",
		"abstract": "Personal tracking and the quantified self have grown increasingly popular as technological capabilities for individual insights have grown. Incorporated into many of these systems is the capacity to monitor metrics over time to offer visualisations of attributes such as health, fitness and nutrition. However, many such systems rely on single, simplified measures to represent these complex phenomena, and due to tracking and visualisations, they add value judgements, such as success and failure, to the users' information. This paper, therefore, aims to shed light on the challenges of reductive measures through the case of the BMI (Body Mass Index). The BMI is a clear example of a reductive measure that is used to offer insight into health in both formal and informal healthcare, despite a substantial body of literature that demonstrates other more accurate factors of health that are easily measured. Through a historical consideration of the origin and narratives around the BMI, we demonstrate the fallacy of its use and offer a broader critique of reductive metrics. This understanding of the BMI allows us to highlight the potential harms arising from personalised ‘health’ tracking technologies and the values encoded into such systems: we use established frameworks of digital harm to demonstrate that using the BMI is harmful for not only well-documented health reasons, but that this harm is exacerbated when it is incorporated into digital technology. Our paper offers a challenge to traditional health thinking and, more broadly, the fetishisation of reductive metrics in data systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658943",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "899–908",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The harmful fetishisation of reductive personal tracking metrics in digital systems",
		"URL": "https://doi.org/10.1145/3630106.3658943",
		"author": [
			{
				"family": "Sobey",
				"given": "Aisha"
			},
			{
				"family": "Carter",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "watkinsFourfifthsRuleNot2024",
		"type": "paper-conference",
		"abstract": "Computer scientists are trained in the art of creating abstractions that simplify and generalize. However, a premature abstraction that omits crucial contextual details creates the risk of epistemic trespassing, by falsely asserting its relevance into other contexts. We study how the field of responsible AI has created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 1¡/AltText¿¡File name=\"facct24-53-inline1\" type=\"svg\"/¿¡/Formula¿ rule or 80% rule), a single part of disparate impact discrimination law, into the disparate impact metric. This metric incorrectly introduces a new deontic nuance and new potentials for ethical harms that were absent in the original ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 2¡/AltText¿¡File name=\"facct24-53-inline2\" type=\"svg\"/¿¡/Formula¿ rule. We also survey how the field has amplified the potential for harm in codifying the ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 3¡/AltText¿¡File name=\"facct24-53-inline3\" type=\"svg\"/¿¡/Formula¿ rule into popular AI fairness software toolkits. The harmful erasure of legal nuances is a wake-up call for computer scientists to self-critically re-evaluate the abstractions they create and use, particularly in the interdisciplinary field of AI ethics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658938",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "764–775",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3630106.3658938",
		"author": [
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Chen",
				"given": "Jiahao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "stauferSilencingRiskNot2024",
		"type": "paper-conference",
		"abstract": "Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU Whistleblower Directive, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other labels of named entities) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including the whistleblower’s writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We evaluate our tool’s effectiveness using court cases from the European Court of Human Rights (ECHR) and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution attacks and utility loss statistically using the popular IMDb62 movie reviews dataset, which consists of 62 individuals. Our method can significantly reduce authorship attribution accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content’s semantics, as measured by the established cosine similarity of sentence embeddings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658936",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "733–745",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Silencing the risk, not the whistle: a semi-automated text sanitization tool for mitigating the risk of whistleblower re-identification",
		"URL": "https://doi.org/10.1145/3630106.3658936",
		"author": [
			{
				"family": "Staufer",
				"given": "Dimitri"
			},
			{
				"family": "Pallas",
				"given": "Frank"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "tangAIFailureCards2024",
		"type": "paper-conference",
		"abstract": "AI-based decision support tools have been used in a wide range of high-stakes settings. However, many of them have failed. Past literature in FAccT contributes important insights into how to detect and mitigate AI failures from a technical perspective. Recently, there are growing calls to understand AI failures as socio-technical and to support community-centered, grassroots-based mitigations to AI failures, in addition to top-down approaches. In this paper, we present AI Failure Cards, a novel method for both improving communities’ understanding of AI failures and for eliciting their current practices and desired strategies for mitigation, with a goal to better support those efforts in the future. Through a series of workshops with unhoused individuals, frontline workers and service providers, as well as local policy advocates, we conducted an empirical investigation of our method in the context of a locally deployed predictive housing allocation algorithm. Our results suggest that the use of the method helped impacted communities better understand these AI failures. It also surfaced a wide range of existing grassroots practices and desired mitigation strategies. Finally, we discuss both the challenges and opportunities for supporting grassroots efforts in mitigating AI failures.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658935",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "713–732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI failure cards: Understanding and supporting grassroots efforts to mitigate AI failures in homeless services",
		"URL": "https://doi.org/10.1145/3630106.3658935",
		"author": [
			{
				"family": "Tang",
				"given": "Ningjing"
			},
			{
				"family": "Zhi",
				"given": "Jiayin"
			},
			{
				"family": "Kuo",
				"given": "Tzu-Sheng"
			},
			{
				"family": "Kainaroi",
				"given": "Calla"
			},
			{
				"family": "Northup",
				"given": "Jeremy J."
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Shen",
				"given": "Hong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "glazkoIdentifyingImprovingDisability2024",
		"type": "paper-conference",
		"abstract": "As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability-related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658933",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "687–700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Identifying and improving disability bias in GPT-based resume screening",
		"URL": "https://doi.org/10.1145/3630106.3658933",
		"author": [
			{
				"family": "Glazko",
				"given": "Kate"
			},
			{
				"family": "Mohammed",
				"given": "Yusuf"
			},
			{
				"family": "Kosa",
				"given": "Ben"
			},
			{
				"family": "Potluri",
				"given": "Venkatesh"
			},
			{
				"family": "Mankoff",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mahomedAuditingGptsContent2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly appearing in consumer-facing products. To prevent problematic use, the organizations behind these systems have put content moderation guardrails in place that prevent the models from generating content they consider harmful. However, most of these enforcement standards and processes are opaque. Although they play a major role in the user experience of these tools, automated content moderation tools have received relatively less attention than other aspects of the models. This study undertakes an algorithm audit of OpenAI’s ChatGPT with the goal of better understanding its content moderation guardrails and their potential biases. To evaluate performance on a broad cultural range of content, we generate a dataset of 100 popular United States television shows with one to three synopses for each episode in the first season of each show (3,309 total synopses). We probe GPT’s content moderation endpoint (ME) to identify violating content both in the synopses themselves, and in GPT’s own outputs when asked to generate a script based on each synopsis, also comparing with ME outputs on 81 real scripts from the same TV shows (269,578 total ME outputs). Our findings show that a large number of GPT-generated and real scripts flag as content violations (about 18% of GPT scripts and 69% of real ones). Using metadata, we find that TV maturity ratings, as well as certain genres (Animation, Crime, Fantasy, and others) are statistically significantly related to a script’s likelihood of flagging. We conclude by discussing the implications of LLM self-censorship and directions for future research on their moderation procedures.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658932",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 27\npublisher-place: Rio de Janeiro, Brazil",
		"page": "660–686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing gpt's content moderation guardrails: Can ChatGPT write your favorite TV show?",
		"URL": "https://doi.org/10.1145/3630106.3658932",
		"author": [
			{
				"family": "Mahomed",
				"given": "Yaaseen"
			},
			{
				"family": "Crawford",
				"given": "Charlie M."
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Metaxa",
				"given": "Danaë"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ajmaniDataAgencyTheory2024",
		"type": "paper-conference",
		"abstract": "Data collection methods for AI applications have been heavily scrutinized by researchers, policymakers, and the general public. In this paper, we propose data agency theory (DAT), a precise theory of justice to evaluate and improve current consent procedures used in AI applications. We argue that data agency is systematically defined by consent policies. Therefore, data agency is a matter of justice. DAT claims data agency ought to be afforded in a way that minimizes the oppression of data contributors by data collectors. We then apply DAT to two salient consent procedures in AI applications: Reddit’s Terms of Service agreement and the United States’s IRB protocols. Through these cases, we demonstrate how our theory helps evaluate justice and generate ideas for improvement. Finally, we discuss the implications of using justice as an evaluation metric, comparing consent procedures, and adopting DAT in future research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658930",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "631–641",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data agency theory: a precise theory of justice for AI applications",
		"URL": "https://doi.org/10.1145/3630106.3658930",
		"author": [
			{
				"family": "Ajmani",
				"given": "Leah"
			},
			{
				"family": "Stapleton",
				"given": "Logan"
			},
			{
				"family": "Houtti",
				"given": "Mo"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "somerstepAlgorithmicFairnessPerformative2024",
		"type": "paper-conference",
		"abstract": "In many prediction problems, the predictive model affects the distribution of the prediction target. This phenomenon is known as performativity and is often caused by the behavior of individuals with vested interests in the outcome of the predictive model. Although performativity is generally problematic because it manifests as distribution shifts, we develop algorithmic fairness practices that leverage performativity to achieve stronger group fairness guarantees in social classification problems (compared to what is achievable in non-performative settings). In particular, we leverage the policymaker’s ability to steer the population to remedy inequities in the long term. A crucial benefit of this approach is that it is possible to resolve the incompatibilities between conflicting group fairness definitions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658929",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "616–630",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness in performative policy learning: Escaping the impossibility of group fairness",
		"URL": "https://doi.org/10.1145/3630106.3658929",
		"author": [
			{
				"family": "Somerstep",
				"given": "Seamus"
			},
			{
				"family": "Ritov",
				"given": "Ya'acov"
			},
			{
				"family": "Sun",
				"given": "Yuekai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hallGeographicInclusionEvaluation2024",
		"type": "paper-conference",
		"abstract": "Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of “appeal” captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations. This includes collecting annotations from people located inside and outside the region of interest, instructing annotators on whether they should follow specific definitions of evaluation criteria or utilize their own interpretation, and reporting assumptions underlying automatic evaluations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658927",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "585–601",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards geographic inclusion in the evaluation of text-to-image models",
		"URL": "https://doi.org/10.1145/3630106.3658927",
		"author": [
			{
				"family": "Hall",
				"given": "Melissa"
			},
			{
				"family": "Bell",
				"given": "Samuel J."
			},
			{
				"family": "Ross",
				"given": "Candace"
			},
			{
				"family": "Williams",
				"given": "Adina"
			},
			{
				"family": "Drozdzal",
				"given": "Michal"
			},
			{
				"family": "Soriano",
				"given": "Adriana Romero"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vrijenhoekDiversityWhatDifferent2024",
		"type": "paper-conference",
		"abstract": "Diversity is a commonly known principle in the design of recommender systems, but also ambiguous in its conceptualization. Through semi-structured interviews we explore how practitioners at three different public service media organizations in the Netherlands conceptualize diversity within the scope of their recommender systems. We provide an overview of the goals that they have with diversity in their systems, which aspects are relevant, and how recommendations should be diversified. We show that even within this limited domain, conceptualization of diversity greatly varies, and argue that it is unlikely that a standardized conceptualization will be achieved. Instead, we should focus on effective communication of what diversity in this particular system means, thus allowing for operationalizations of diversity that are capable of expressing the nuances and requirements of that particular domain.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658926",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "573–584",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity of what? On the different conceptualizations of diversity in recommender systems",
		"URL": "https://doi.org/10.1145/3630106.3658926",
		"author": [
			{
				"family": "Vrijenhoek",
				"given": "Sanne"
			},
			{
				"family": "Daniil",
				"given": "Savvina"
			},
			{
				"family": "Sandel",
				"given": "Jorden"
			},
			{
				"family": "Hollink",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "naudtsDigitalFacesOppression2024",
		"type": "paper-conference",
		"abstract": "Drawing from Iris Marion Young's politics of difference and democratic theory, this contribution formulates a relational and egalitarian account of digital justice to understand and help counter, the social and technical conditions under which data-driven decision-making systems are liable to reinforce and introduce social injustice. To do so, this contribution is structured alongside three axes. First, I present data-driven decision-making systems as socio-technical systems that both take meaning from and co-shape people's relationships and the social structures they are part of. Due to this relational push and pull, I argue, data-driven systems have the potential to restructure society and, consequently, the conditions that govern people's exposure to, and experience of, injustice therein. Second, I transpose Young's ideation of oppression and domination onto the digital ecosystem. Both notions are used to locate within complex, dynamic and automated environments, a series of social and technological conditions that unjustifiably limit people's actions and behaviours. Third, I build on Young's model for an inclusive democracy to propose a series of institutional and procedural practices to ensure that, within the digital ecosystem, each person has the effective opportunity to pursue the life projects they value and to communicate their needs, concerns and experiences in ways that are heard and recognized by others.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658934",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "701–712",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The digital faces of oppression and domination: a relational and egalitarian perspective on the data-driven society and its regulation",
		"URL": "https://doi.org/10.1145/3630106.3658934",
		"author": [
			{
				"family": "Naudts",
				"given": "Laurens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "simsonLazyDataPractices2024",
		"type": "paper-conference",
		"abstract": "Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a lack of representation for certain protected attributes in both data and evaluations; (2) the widespread exclusion of minorities during data preprocessing; and (3) opaque data processing threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658931",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "642–659",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lazy data practices harm fairness research",
		"URL": "https://doi.org/10.1145/3630106.3658931",
		"author": [
			{
				"family": "Simson",
				"given": "Jan"
			},
			{
				"family": "Fabris",
				"given": "Alessandro"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "blackDhacking2024",
		"type": "paper-conference",
		"abstract": "Recent regulatory efforts, including Executive Order 14110 and the AI Bill of Rights, have focused on mitigating discrimination in AI systems through novel and traditional application of anti-discrimination laws. While these initiatives rightly emphasize fairness testing and mitigation, we argue that they pay insufficient attention to robust bias measurement and mitigation—and that without doing so, the frameworks cannot effectively achieve the goal of reducing discrimination in deployed AI models. This oversight is particularly concerning given the instability and brittleness of current algorithmic bias mitigation and fairness optimization methods, as highlighted by growing evidence in the algorithmic fairness literature. This instability heightens the risk of what we term discrimination-hacking or d-hacking, a scenario where, inadvertently or deliberately, the selection of models based on favorable fairness metrics within specific samples could lead to misleading or non-generalizable fairness performance. We term this effect d-hacking because systematically selecting among numerous models to find the least discriminatory one parallels the concept of p-hacking in social science research of selectively reporting outcomes that appear statistically significant resulting in misleading conclusions. In light of these challenges, we argue that AI fairness regulation should not only call for fairness measurement and bias mitigation, but also specify methods to ensure robust solutions to discrimination in AI systems. Towards the goal of arguing for robust fairness assessment and bias mitigation in AI regulation, this paper (1) synthesizes evidence of d-hacking in the computer science literature and provides experimental demonstrations of d-hacking, (2) analyzes current legal frameworks to understand the treatment of robust fairness and non-discriminatory behavior, both in recent AI regulation proposals and traditional U.S. discrimination law, and (3) outlines policy recommendations for preventing d-hacking in high-stakes domains.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658928",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "602–615",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "D-hacking",
		"URL": "https://doi.org/10.1145/3630106.3658928",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Gillis",
				"given": "Talia"
			},
			{
				"family": "Hall",
				"given": "Zara Yasmine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "globus-harrisDiversifiedEnsemblingExperiment2024",
		"type": "paper-conference",
		"abstract": "Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In [12], the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants’ efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams’ approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658923",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "529–545",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversified ensembling: An experiment in crowdsourced machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658923",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Harrison",
				"given": "Declan"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Perona",
				"given": "Pietro"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bellaTacklingLanguageModelling2024",
		"type": "paper-conference",
		"abstract": "Current AI-based language technologies—language models, machine translation systems, multilingual dictionaries and corpora—are known to focus on the world’s 2–3% most widely spoken languages. Research efforts of the past decade have attempted to expand this coverage to ‘under-resourced languages.’ The goal of our paper is to bring attention to a corollary phenomenon that we call language modelling bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. We define language modelling bias as uneven per-language performance under similar test conditions. We show that bias stems not only from technology but also from ethically problematic research and development methodologies that disregard the needs of language communities. Moving towards diversity-aware alternatives, we present an initiative that aims at reducing language modelling bias within lexical resources through both technology design and methodology, based on an eye-level collaboration with local communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658925",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "562–572",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tackling language modelling bias in support of linguistic diversity",
		"URL": "https://doi.org/10.1145/3630106.3658925",
		"author": [
			{
				"family": "Bella",
				"given": "Gábor"
			},
			{
				"family": "Helm",
				"given": "Paula"
			},
			{
				"family": "Koch",
				"given": "Gertraud"
			},
			{
				"family": "Giunchiglia",
				"given": "Fausto"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pareekTrustDevelopmentRepair2024",
		"type": "paper-conference",
		"abstract": "Leveraging Artificial Intelligence to support human decision-makers requires harnessing the unique strengths of both entities, where human expertise often complements AI capabilities. However, human decision-makers must accurately discern when to trust the AI. In situations with complementary Human-AI expertise, identifying AI inaccuracies becomes challenging for humans, hindering their ability to rely on the AI only when warranted. Even when AI performance improves post-errors, this inability to assess accuracy can hinder trust recovery. Through two experimental tasks, we investigate trust development, erosion, and recovery during AI-assisted decision-making, examining explicit Trust Repair Strategies (TRSs) – Apology, Denial, Promise, and Model Update. Our participants classified familiar and unfamiliar stimuli with an AI with varying accuracy. We find that participants leveraged AI accuracy in familiar tasks as a heuristic to dynamically calibrate their trust during unfamiliar tasks. Further, once trust in the AI was eroded, trust restored through Model Update surpassed initial trust values, followed by Apology, Promise, and the baseline (no repair), with Denial being least effective. We empirically demonstrate how trust calibration occurs during complementary expertise, highlighting factors influencing the different effectiveness of TRSs despite identical AI accuracy, and offering implications for effectively restoring trust in Human-AI collaborations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658924",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "546–561",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trust development and repair in AI-assisted decision-making during complementary expertise",
		"URL": "https://doi.org/10.1145/3630106.3658924",
		"author": [
			{
				"family": "Pareek",
				"given": "Saumya"
			},
			{
				"family": "Velloso",
				"given": "Eduardo"
			},
			{
				"family": "Goncalves",
				"given": "Jorge"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "neriniValueEyeBeholder2024",
		"type": "paper-conference",
		"abstract": "Proprietary data is a valuable asset used to develop predictive algorithms that benefit a wide range of users, including customers, business owners, and decision-makers. Consequently, there is a growing interest in developing safe and robust techniques for sharing, learning models, and distributing predictions across a wide spectrum of potential stakeholders. However, a structured process to assess the value of data assets, and thus enabling collaborations among stakeholders, remains largely unexplored. This is particularly challenging when the data to be shared has a networked structure, where increasing the shared data samples potentially connects information observed by different data owners, providing new knowledge that is unavailable to any data owner individually. Here, we propose E-GraDE, a framework that assists organizations in assessing the value of their networked data to better address graph machine learning tasks. This framework includes a step-by-step analysis of the requirements of different stakeholders, such as the accuracy or fairness requisites of the models, ensuring a fair evaluation process and stronger alignment in the development of a data federation consortium. Additionally, we propose an approach to estimate the value of networked data to be shared while disclosing only a small fraction of the original information. We support our approach with extensive computational experiments, analysing each part of it through simulated use cases.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658919",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "467–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value is in the eye of the beholder: a framework for an equitable graph data evaluation",
		"URL": "https://doi.org/10.1145/3630106.3658919",
		"author": [
			{
				"family": "Nerini",
				"given": "Francesco Paolo"
			},
			{
				"family": "Bajardi",
				"given": "Paolo"
			},
			{
				"family": "Panisson",
				"given": "André"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "srinivasanSeeNotSee2024",
		"type": "paper-conference",
		"abstract": "Algorithmic recommendation is one of the most popular applications of machine learning (ML) systems. While the implication of algorithmic recommendation has been studied in the context of high-stakes domains such as finance and healthcare, there has been very little focus in understanding its impacts with respect to the arts domain. Given that ML is increasingly finding place in the arts domain such as in generative arts and content analysis, in this paper, we examine the tensions of algorithmic curation in the context of visual arts. Through case studies, we describe how curatorial algorithms that are oblivious of broader socio-cultural contexts could potentially result in ethical concerns such as over-representation and misattribution, to name a few. Towards addressing some of these concerns, the paper offers design guidelines. Specifically, the paper outlines repair strategies that suggest ways 1) to engage with cultural stakeholders in building visual art curatorial algorithms, 2) to unlearn biases embedded in digital artworks and their meta-data, and 3) emphasize the need to establish regulatory norms specific to the use of ML in visual art curation. Taking cue from the process employed by artwork curators, the paper also describes how authenticity can be prioritized by re-calibrating visual art curatorial algorithms. The paper also suggest ways through which the potential of state-of-the-art ML curatorial algorithms can be re-imagined towards empowering the audience of artworks. We hope the insights presented in the paper spark interdisciplinary discussions and pave way for fostering reformation in algorithmic curation of visual arts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658917",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "444–455",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "To see or not to see: Understanding the tensions of algorithmic curation for visual arts",
		"URL": "https://doi.org/10.1145/3630106.3658917",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Ramya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "perreaultAlgorithmicMisjudgementGoogle2024",
		"type": "paper-conference",
		"abstract": "Google Search is an important way that people seek information about politics [8], and Google states that it is “committed to providing timely and authoritative information on Google Search to help voters understand, navigate, and participate in democratic processes.”1 This paper studies the extent to which government-maintained web domains are represented in the online electoral information environment, as captured through 3.45 Google Search result pages collected during the 2022 US midterm elections for 786 locations across the United States. Focusing on state, county, and local government domains that provide locality-specific information, we study not only the extent to which these sources appear in organic search results, but also the extent to which these sources are correctly targeted to their respective constituents. We label misalignment between the geographic area that non-federal domains serve and the locations for which they appear in search results as algorithmic mistargeting, a subtype of algorithmic misjudgement in which the search algorithm targets locality-specific information to users in different (incorrect) locations. In the context of the 2022 US midterm elections, we find that 71% of all occurrences of state, county, and local government sources were mistargeted, with some domains appearing disproportionately often among organic results despite providing locality-specific information that may not be relevant to all voters. However, we also find that mistargeting often occurs in low ranks. We conclude by considering the potential consequences of extensive mistargeting of non-federal government sources and argue that ensuring the correct targeting of these sources to their respective constituents is a critical part of Google’s role in facilitating access to authoritative and locally-relevant electoral information.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658916",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "433–443",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic misjudgement in google search results: Evidence from auditing the US online electoral information environment",
		"URL": "https://doi.org/10.1145/3630106.3658916",
		"author": [
			{
				"family": "Perreault",
				"given": "Brooke"
			},
			{
				"family": "Lee",
				"given": "Johanna Hoonsun"
			},
			{
				"family": "Shava",
				"given": "Ropafadzo"
			},
			{
				"family": "Mustafaraj",
				"given": "Eni"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "yehAnalyzingRelationshipDifference2024",
		"type": "paper-conference",
		"abstract": "In research studying the fairness of machine learning algorithms and models, fairness often means that a metric is the same when computed for two different groups of people. For example, one might define fairness to mean that the false positive rate of a classifier is the same for people of different genders, ages, or races. However, it is usually not possible to make this metric identical for all groups. Instead, algorithms ensure that the metric is similar—for example, that the false positive rates are similar. Researchers usually measure this similarity or dissimilarity using either the difference or ratio between the metric values for different groups of people. Although these two approaches are known to be different, there has been little work analyzing their differences and respective benefits. In this paper we examine this relationship analytically and empirically, and conclude that unless there are application-specific reasons to prefer the difference approach, the ratio approach should be preferred.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658922",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "518–528",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing the relationship between difference and ratio-based fairness metrics",
		"URL": "https://doi.org/10.1145/3630106.3658922",
		"author": [
			{
				"family": "Yeh",
				"given": "Min-Hsuan"
			},
			{
				"family": "Metevier",
				"given": "Blossom"
			},
			{
				"family": "Hoag",
				"given": "Austin"
			},
			{
				"family": "Thomas",
				"given": "Philip"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "laszkiewiczBenchmarkingFairnessImage2024",
		"type": "paper-conference",
		"abstract": "Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics—inspired by their supervised fairness counterparts—to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results. All experiments can be reproduced using our provided repository.1",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658921",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 29\npublisher-place: Rio de Janeiro, Brazil",
		"page": "489–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Benchmarking the fairness of image upsampling methods",
		"URL": "https://doi.org/10.1145/3630106.3658921",
		"author": [
			{
				"family": "Laszkiewicz",
				"given": "Mike"
			},
			{
				"family": "Daunhawer",
				"given": "Imant"
			},
			{
				"family": "Vogt",
				"given": "Julia E."
			},
			{
				"family": "Fischer",
				"given": "Asja"
			},
			{
				"family": "Lederer",
				"given": "Johannes"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rileyOverridingInjusticePretrial2024",
		"type": "paper-conference",
		"abstract": "A small but growing number of empirical studies have attempted to measure the impacts of algorithmic pretrial risk assessments on discrete policy goals such as decarceration, racial equity, and public safety. A separate but related body of work explores frontline worker resistance and discretion related to sociotechnical systems in criminal legal contexts. I build on work that aims to bridge the gaps between these literatures by offering an ethnographic account of pretrial risk assessment administration across the United States. I draw on semi-structured interviews with 74 pretrial actors and site observations across 8 jurisdictions. I highlight the process of risk assessment administration and the frontline workers who perform that labor. Like judges, pretrial officers have the autonomy to override risk assessment recommendations, unlike judges however, their decisions are made outside the courtroom and far removed from public scrutiny. This paper makes three contributions. First, it provides a detailed account of the personal, professional, and organizational dynamics that lead pretrial officers to override risk assessment recommendations. Second, it presents a taxonomy of override behavior among pretrial officers in an effort to promote more effective policy decisions. Lastly, it provides further empirical evidence that pretrial risk assessments are unlikely to guarantee racial or economic equity or decarceration in the long term.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658920",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "480–488",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Overriding (in)justice: pretrial risk assessment administration on the frontlines",
		"URL": "https://doi.org/10.1145/3630106.3658920",
		"author": [
			{
				"family": "Riley",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "scheuermanWalledGardenChallenges2024",
		"type": "paper-conference",
		"abstract": "Research on technology companies and their workers can externalize otherwise invisible and tacit workplace approaches, identify organizational constraints to creating more ethical AI systems, help ground interventions in real-world organizational realities, and result in the co-creation of better business practices for organizations. However, getting access to technology companies is difficult for external researchers. In this paper, I draw from insights gained by conducting research on and with industry professionals. I present four challenges when conducting industry-focused research on responsible AI. I also present methods I used to navigate each challenge. Finally, I highlight opportunities for the tech industry to lower the barriers to external research. This work aims to share ways of navigating methodological challenges and encourage better transparency in the tech industry.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658918",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "456–466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In the walled garden: Challenges and opportunities for research on the practices of the AI tech industry",
		"URL": "https://doi.org/10.1145/3630106.3658918",
		"author": [
			{
				"family": "Scheuerman",
				"given": "Morgan Klaus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "frohlichInsightsInsuranceFair2024",
		"type": "paper-conference",
		"abstract": "We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658914",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "407–421",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Insights from insurance for fair machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658914",
		"author": [
			{
				"family": "Fröhlich",
				"given": "Christian"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "quayeAdversarialNibblerOpen2024",
		"type": "paper-conference",
		"abstract": "With text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on “implicitly adversarial” prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. We present an in-depth account of our methodology, a systematic study of novel attack strategies and safety failures, and a visualization tool for easy exploration of the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. This work will enable proactive, iterative safety assessments and promote responsible development of T2I models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658913",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "388–406",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adversarial nibbler: An open red-teaming method for identifying diverse harms in text-to-image generation",
		"URL": "https://doi.org/10.1145/3630106.3658913",
		"author": [
			{
				"family": "Quaye",
				"given": "Jessica"
			},
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Inel",
				"given": "Oana"
			},
			{
				"family": "Rastogi",
				"given": "Charvi"
			},
			{
				"family": "Kirk",
				"given": "Hannah Rose"
			},
			{
				"family": "Kahng",
				"given": "Minsuk"
			},
			{
				"family": "Van Liemt",
				"given": "Erin"
			},
			{
				"family": "Bartolo",
				"given": "Max"
			},
			{
				"family": "Tsang",
				"given": "Jess"
			},
			{
				"family": "White",
				"given": "Justin"
			},
			{
				"family": "Clement",
				"given": "Nathan"
			},
			{
				"family": "Mosquera",
				"given": "Rafael"
			},
			{
				"family": "Ciro",
				"given": "Juan"
			},
			{
				"family": "Janapa Reddi",
				"given": "Vijay"
			},
			{
				"family": "Aroyo",
				"given": "Lora"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hutiriNotMyVoice2024",
		"type": "paper-conference",
		"abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens’ homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658911",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "359–376",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Not my voice! A taxonomy of ethical and safety harms of speech generators",
		"URL": "https://doi.org/10.1145/3630106.3658911",
		"author": [
			{
				"family": "Hutiri",
				"given": "Wiebke"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "thachTranscenteredModerationTrans2024",
		"type": "paper-conference",
		"abstract": "Mainstream platforms’ content moderation systems typically employ generalized “one-size-fits-all” approaches, intended to serve both general and marginalized users. Thus, transgender people must often create their own technologies and moderation systems to meet their specific needs. In our interview study of transgender technology creators (n=115), we found that creators face issues of transphobic abuse and disproportionate content moderation. Trans tech creators address these issues by carefully moderating and vetting their userbases, centering trans contexts in content moderation systems, and employing collective governance and community models. Based on these findings, we argue that trans tech creators’ approaches to moderation offer important insights into how to better design for trans users, and ultimately, marginalized users in the larger platform ecology. We introduce the concept of trans-centered moderation – content moderation that reviews and successfully vets transphobic users, appoints trans moderators to effectively moderate trans contexts, considers the limitations and constraints of technology for addressing social challenges, and employs collective governance and community models. Trans-centered moderation can help to improve platform design for trans users while reducing the harm faced by trans people and marginalized users more broadly.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658909",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "326–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trans-centered moderation: Trans technology creators and centering transness in platform and community governance",
		"URL": "https://doi.org/10.1145/3630106.3658909",
		"author": [
			{
				"family": "Thach",
				"given": "Hibby"
			},
			{
				"family": "Mayworm",
				"given": "Samuel"
			},
			{
				"family": "Thomas",
				"given": "Michaelanne"
			},
			{
				"family": "Haimson",
				"given": "Oliver L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "maywormMisgenderedModerationHow2024",
		"type": "paper-conference",
		"abstract": "Transgender and nonbinary social media users experience disproportionate content removals on social media platforms, even when content does not violate platforms’ guidelines. In 2022, the Oversight Board, which oversees Meta platforms’ content moderation decisions, invited public feedback on Instagram’s removal of two trans users’ posts featuring their bare chests, introducing a unique opportunity to hear trans users’ feedback on how nudity and sexual activity policies impacted them. We conducted a qualitative analysis of 83 comments made public during the Oversight Board’s public comment process. Commenters criticized Meta’s nudity policies as enforcing a cisnormative view of gender while making it unclear how images of trans users’ bodies are moderated, enabling the disproportionate removal of trans content and limiting trans users’ ability to use Meta’s platforms. Yet there was significant divergence among commenters about how to address cisnormative moderation. Some commenters suggested that Meta clarify nudity guidelines, while others suggested that Meta overhaul them entirely, removing gendered distinctions or fundamentally reconfiguring the platform’s relationship to sexual content. We then discuss how the Oversight Board’s public comment process demonstrates the value of incorporating trans people’s feedback while developing policies related to gender and nudity, while arguing that Meta must go beyond only revising policy language by reevaluating how cisnormative values are encoded in all aspects of its content moderation systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658907",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "301–312",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Misgendered during moderation: How transgender bodies make visible cisnormative content moderation policies and enforcement in a meta oversight board case",
		"URL": "https://doi.org/10.1145/3630106.3658907",
		"author": [
			{
				"family": "Mayworm",
				"given": "Samuel"
			},
			{
				"family": "Albert",
				"given": "Kendra"
			},
			{
				"family": "Haimson",
				"given": "Oliver L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moreauFailingOurYoungest2024",
		"type": "paper-conference",
		"abstract": "In recent years, Danish child protective services have experienced increasing pressure, prompting the adoption of a decision-support algorithm to aid caseworkers in identifying children at heightened risk of maltreatment, named Decision Support. Despite its critical role, this algorithm has not undergone formal evaluation. Through a freedom of information request, we were able to partially access the algorithm and conduct an audit. We find that the algorithm has significant methodological flaws, suffers from information leakage, relies on inappropriate proxy values for maltreatment assessment, generates inconsistent risk scores, and exhibits age-based discrimination. Given these serious issues, we strongly advise against the use of this kind of algorithms in local government, municipal, and child protection settings, and we call for rigorous evaluation of such tools before implementation and for continual monitoring post-deployment by listing a series of specific recommendations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658906",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "290–300",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",
		"URL": "https://doi.org/10.1145/3630106.3658906",
		"author": [
			{
				"family": "Moreau",
				"given": "Therese"
			},
			{
				"family": "Sinatra",
				"given": "Roberta"
			},
			{
				"family": "Sekara",
				"given": "Vedran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "nedzhvetskayaNoSimpleFix2024",
		"type": "paper-conference",
		"abstract": "The introduction of AI into working processes has resulted in workers increasingly being subject to AI-related harms. By analyzing incidents of worker-related AI harms between 2008 and 2023 in the AI Incident Database, we find that harms get addressed under considerably restricted scenarios. Results from a Qualitative Comparative Analysis (QCA) show that workers with more power resources, either in the form of expertise or labor market power, have a greater likelihood of seeing harms fixed, all else equal. By contrast, workers lacking expertise or labor market power, have lower success rates and must resort to legal or regulatory mechanisms to get fixes through. These findings suggest that the workplace is another arena in which AI has the potential to reproduce existing inequalities among workers and that stronger legal frameworks and regulations can empower more vulnerable worker populations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658915",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "422–432",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No simple fix: How AI harms reflect power and jurisdiction in the workplace",
		"URL": "https://doi.org/10.1145/3630106.3658915",
		"author": [
			{
				"family": "Nedzhvetskaya",
				"given": "Nataliya"
			},
			{
				"family": "Tan",
				"given": "JS"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gillisOperationalizingSearchLess2024",
		"type": "paper-conference",
		"abstract": "The Less Discriminatory Alternative is a key provision of the disparate impact doctrine in the United States. In fair lending, this provision mandates that lenders must adopt models that reduce discrimination when they do not compromise their business interests. In this paper, we develop practical methods to audit for less discriminatory alternatives. Our approach is designed to verify the existence of less discriminatory machine learning models – by returning an alternative model that can reduce discrimination without compromising performance (discovery) or by certifying that an alternative model does not exist (refutation). We develop a method to fit the least discriminatory linear classification model in a specific lending task – by minimizing an exact measure of disparity (e.g., the maximum gap in group FNR) and enforcing hard performance constraints for business necessity (e.g., on FNR and FPR). We apply our method to study the prevalence of less discriminatory alternatives on real-world datasets from consumer finance applications. Our results highlight how models may inadvertently lead to unnecessary discrimination across common deployment regimes, and demonstrate how our approach can support lenders, regulators, and plaintiffs by reliably detecting less discriminatory alternatives in such instances.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658912",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "377–387",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Operationalizing the search for less discriminatory alternatives in fair lending",
		"URL": "https://doi.org/10.1145/3630106.3658912",
		"author": [
			{
				"family": "Gillis",
				"given": "Talia B"
			},
			{
				"family": "Meursault",
				"given": "Vitaly"
			},
			{
				"family": "Ustun",
				"given": "Berk"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "johnsonFallAlgorithmCharacterizing2024",
		"type": "paper-conference",
		"abstract": "As more algorithmic systems have come under scrutiny for their potential to inflict societal harms, an increasing number of organizations that hold power over harmful algorithms have chosen, or were required under the law, to abandon them. While social movements and calls to abandon harmful algorithms have emerged across application domains, little academic attention has been paid to studying abandonment as a means to mitigate algorithmic harms. In this paper, we take a first step towards conceptualizing “algorithm abandonment” as an organization’s decision to stop designing, developing, or using an algorithmic system due to its (potential) harms. We conduct a thematic analysis of real-world cases of algorithm abandonment to characterize the dynamics leading to this outcome. Our analysis of 40 cases reveals that campaigns to abandon an algorithm follow a common process of six iterative phases: discovery, diagnosis, dissemination, dialogue, decision, and death, which we term the 6 D’s of abandonment. In addition, we highlight key factors that facilitate (or prohibit) abandonment, which include characteristics of both the technical and social systems that the algorithm is embedded within. We discuss implications for several stakeholders, including proprietors and technologists who have the power to influence an algorithm’s (dis)continued use, FAccT researchers, and policymakers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658910",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 22\npublisher-place: Rio de Janeiro, Brazil",
		"page": "337–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The fall of an algorithm: Characterizing the dynamics toward abandonment",
		"URL": "https://doi.org/10.1145/3630106.3658910",
		"author": [
			{
				"family": "Johnson",
				"given": "Nari"
			},
			{
				"family": "Moharana",
				"given": "Sanika"
			},
			{
				"family": "Harrington",
				"given": "Christina"
			},
			{
				"family": "Andalibi",
				"given": "Nazanin"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "herlihyStructuredRegressionApproach2024",
		"type": "paper-conference",
		"abstract": "Disaggregated evaluation is a central task in AI fairness assessment, where the goal is to measure an AI system’s performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are included in analysis. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and demonstrate how goodness-of-fit testing helps identify the key factors that drive differences in performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658908",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "313–325",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A structured regression approach for evaluating model performance across intersectional subgroups",
		"URL": "https://doi.org/10.1145/3630106.3658908",
		"author": [
			{
				"family": "Herlihy",
				"given": "Christine"
			},
			{
				"family": "Truong",
				"given": "Kimberly"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Dudı́k",
				"given": "Miroslav"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "fledderjohannAlgorithmicReproductiveJustice2024",
		"type": "paper-conference",
		"abstract": "Reproductive justice is an intersectional feminist framework and movement which argues all people have the right to have a child, to not have a child, to parent in safe and healthy environments, and to own their bodies and control their futures. We identify increasing surveillance, assessing worth, datafication and monetisation, and decimating planetary health as forms of structural violence associated with emerging digital technologies. These trends are implicated in the (re)production of inequities, creating barriers to the realisation of reproductive justice. We call for algorithmic reproductive justice, and highlight the potential for both acts of resistance and industry reform to advance that aim.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658903",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "254–266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic reproductive justice",
		"URL": "https://doi.org/10.1145/3630106.3658903",
		"author": [
			{
				"family": "Fledderjohann",
				"given": "Jasmine"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Miller",
				"given": "Esmorie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "jaimeEthnicClassificationsAlgorithmic2024",
		"type": "paper-conference",
		"abstract": "We address the challenges and implications of ensuring fairness in algorithmic decision-making (ADM) practices related to ethnicity. Expanding beyond the U.S.-centric approach to race, we provide an overview of ethnic classification schemes in European countries and emphasize how the distinct approaches to ethnicity in Europe can impact fairness assessments in ADM. Drawing on large-scale German survey data, we highlight differences in ethnic disadvantage across subpopulations defined by different measures of ethnicity. We build prediction models in the labor market, health, and finance domain and investigate the fairness implications of different ethnic classification schemes across multiple prediction tasks and fairness metrics. Our results show considerable variation in fairness scores across ethnic classifications, where error disparities for the same model can be twice as large when using different operationalizations of ethnicity. We argue that ethnic classifications differ in their ability to identify ethnic disadvantage across ADM domains and advocate for context-sensitive operationalizations of ethnicity and its transparent reporting in fair machine learning (ML) applications.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658902",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "237–253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethnic classifications in algorithmic fairness: Concepts, measures and implications in practice",
		"URL": "https://doi.org/10.1145/3630106.3658902",
		"author": [
			{
				"family": "Jaime",
				"given": "Sofia"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "guoDecisionTheoreticFramework2024",
		"type": "paper-conference",
		"abstract": "Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI’s recommendation from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies from literature, we demonstrate how our framework can be used to separate the loss due to mis-reliance from the loss due to not accurately differentiating the signals. We evaluate these losses by comparing to a baseline and a benchmark for complementary performance defined by the expected payoff achieved by a rational decision-maker facing the same decision task as the behavioral decision-makers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658901",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "221–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A decision theoretic framework for measuring AI reliance",
		"URL": "https://doi.org/10.1145/3630106.3658901",
		"author": [
			{
				"family": "Guo",
				"given": "Ziyang"
			},
			{
				"family": "Wu",
				"given": "Yifan"
			},
			{
				"family": "Hartline",
				"given": "Jason D."
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gausenFrameworkExploringConsequences2024",
		"type": "paper-conference",
		"abstract": "Organisations generate vast amounts of information, which has resulted in a long-term research effort into knowledge access systems for enterprise settings. Recent developments in artificial intelligence, in relation to large language models, are poised to have significant impact on knowledge access. This has the potential to shape the workplace and knowledge in new and unanticipated ways. Many risks can arise from the deployment of these types of AI systems, due to interactions between the technical system and organisational power dynamics. This paper presents the Consequences-Mechanisms-Risks framework to identify risks to workers from AI-mediated enterprise knowledge access systems. We have drawn on wide-ranging literature detailing risks to workers, and categorised risks as being to worker value, power, and wellbeing. The contribution of our framework is to additionally consider (i) the consequences of these systems that are of moral import: commodification, appropriation, concentration of power, and marginalisation, and (ii) the mechanisms, which represent how these consequences may take effect in the system. The mechanisms are a means of contextualising risk within specific system processes, which is critical for mitigation. This framework is aimed at helping practitioners involved in the design and deployment of AI-mediated knowledge access systems to consider the risks introduced to workers, identify the precise system mechanisms that introduce those risks, and begin to approach mitigation. Future work could apply this framework to other technological systems to promote the protection of workers and other groups.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658900",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "207–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for exploring the consequences of AI-mediated enterprise knowledge access and identifying risks to workers",
		"URL": "https://doi.org/10.1145/3630106.3658900",
		"author": [
			{
				"family": "Gausen",
				"given": "Anna"
			},
			{
				"family": "Mitra",
				"given": "Bhaskar"
			},
			{
				"family": "Lindley",
				"given": "Siân"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "jainAlgorithmicPluralismStructural2024",
		"type": "paper-conference",
		"abstract": "We present a structural approach toward achieving equal opportunity in systems of algorithmic decision-making called algorithmic pluralism. Algorithmic pluralism describes a state of affairs in which no set of algorithms severely limits access to opportunity, allowing individuals the freedom to pursue a diverse range of life paths. To argue for algorithmic pluralism, we adopt Joseph Fishkin’s theory of bottlenecks, which focuses on the structure of decision-points that determine how opportunities are allocated. The theory contends that each decision-point or “bottleneck’’ limits access to opportunities with some degree of severity and legitimacy. We extend Fishkin’s structural viewpoint and use it to reframe existing systemic concerns about equal opportunity in algorithmic decision-making, such as patterned inequality and algorithmic monoculture. In proposing algorithmic pluralism, we argue for the urgent priority of alleviating severe bottlenecks in algorithmic-decision-making. We contend that there must be a pluralism of opportunity available to many different individuals in order to promote equal opportunity in a systemic way. We further show how this framework has several implications for system design and regulation through current debates about equal opportunity in algorithmic hiring.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658899",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "197–206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic pluralism: a structural approach to equal opportunity",
		"URL": "https://doi.org/10.1145/3630106.3658899",
		"author": [
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Suriyakumar",
				"given": "Vinith"
			},
			{
				"family": "Creel",
				"given": "Kathleen"
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "goetzeAIArtTheft2024",
		"type": "paper-conference",
		"abstract": "Since the launch of applications such as dall•e, Midjourney, and Stable Diffusion, generative artificial intelligence has been controversial as a tool for creating artwork. Some writers have presented worries about these technologies as harbingers of fully automated futures to come, but more pressing is the impact of generative AI on creative labour in the present. Already, business leaders have begun replacing human artistic labour with AI-generated images. In response, the artistic community has launched a protest movement, which argues that AI image generation is a kind of theft. This paper analyzes, substantiates, and critiques these arguments, concluding that AI image generators involve an unethical kind of labour theft. If correct, many other AI applications also rely upon theft.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658898",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "186–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI art is theft: Labour, extraction, and exploitation: Or, on the dangers of stochastic pollocks",
		"URL": "https://doi.org/10.1145/3630106.3658898",
		"author": [
			{
				"family": "Goetze",
				"given": "Trystan S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kieslichRegulatingAIbasedRemote2024",
		"type": "paper-conference",
		"abstract": "AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but at the same time they are criticised for inheriting biases and violating fundamental human rights. As a result, the use of RBI poses risks to society. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a broad consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI for the Common Good. As a possible counterweight, public opinion can have a decisive influence on policymakers (e.g. through voter demands) to establish boundaries and conditions under which AI systems should be used – if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658548",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "173–185",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating AI-based remote biometric identification. Investigating the public demand for bans, audits, and public database registrations",
		"URL": "https://doi.org/10.1145/3630106.3658548",
		"author": [
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Lünich",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pelegrinaPreprocessingShapleyValuebased2024",
		"type": "paper-conference",
		"abstract": "Decision support systems became ubiquitous in every aspect of human lives. Their reliance on increasingly complex and opaque machine learning models raises transparency and fairness concerns with respect to unprivileged groups of people. This motivated several efforts to estimate importance of features towards the models’ performance and to detect unfair/disparate decisions. The latter is often dealt with by means of fairness metrics that rely on performance metrics with respect to predefined features that are considered protected (salient features such as age, gender, ethnicity, etc.) and/or sensitive (such as education, /occupation, banking information). However, such an approach is subjective (as fairness metrics depend on the choice features), there may be other features that lead to unfair (disparate) decisions and that may ask for suitable interpretations. In this paper we focus on the latter issues and propose a statistical preprocessing approach that is inspired by both the Hilbert-Schmidt independence criterion and Shapley values to estimate feature importance and to detect disparity prone features. Unlike traditional Shapley value-based approaches, we do not require trained models to measure feature importance or detect disparate results. Instead, it focuses on data and statistical criteria to measure the dependence of feature distributions. Our empirical results show that features with the highest dependence degrees with the label vector are also the ones with the highest impact on the model performance. Moreover, our empirical results indicate that this relation enables the detection of disparity prone features.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658905",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "279–289",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A preprocessing Shapley value-based approach to detect relevant and disparity prone features in machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658905",
		"author": [
			{
				"family": "Pelegrina",
				"given": "Guilherme Dean"
			},
			{
				"family": "Couceiro",
				"given": "Miguel"
			},
			{
				"family": "Duarte",
				"given": "Leonardo Tomazeli"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "reyes-cruzRearrangingDeckChairs2024",
		"type": "paper-conference",
		"abstract": "Radical and disruptive interventions are needed to reach \"Net Zero\" by 2050 to avert the climate catastrophe. Although governments, companies, cities, and institutions have pledged to take action and reduce their carbon emissions, the idea of personal carbon allowances or budgets for individuals has also been proposed as a potential national policy in the UK. In this paper, we employ a Research through Design approach to explore the notion of a carbon budget. We present combined results from two studies: firstly a workshop with members of environmental organisations (industry, charity, and policymaking) discussing the concept of a Citizen Carbon Budget (CCB) and app, from the wide perspective of societal desirability drawn from Responsible Research and Innovation (RRI); and secondly, a one-month deployment of a CCB mobile app with twelve members of the public based in the UK. Key findings from the combination of these approaches showed that the CCB app was fruitful in supporting awareness of personal carbon emissions and reflections about people’s lifestyles. However, several concerns were raised, including the unfairness of treating all people equally in environmental policy, regardless of their background and context. We provide considerations for policymaking and design, including intertwined perspectives drawn from the differing approaches of individual and collective action.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658904",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "267–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"Like rearranging deck chairs on the titanic\"? Feasibility, fairness, and ethical concerns of a citizen carbon budget for reducing CO2 emissions",
		"URL": "https://doi.org/10.1145/3630106.3658904",
		"author": [
			{
				"family": "Reyes-Cruz",
				"given": "Gisela"
			},
			{
				"family": "Craigon",
				"given": "Peter"
			},
			{
				"family": "Piskopani",
				"given": "Anna-Maria"
			},
			{
				"family": "Dowthwaite",
				"given": "Liz"
			},
			{
				"family": "Lu",
				"given": "Yang"
			},
			{
				"family": "Lisinska",
				"given": "Justyna"
			},
			{
				"family": "Shafipour",
				"given": "Elnaz"
			},
			{
				"family": "Stein",
				"given": "Sebastian"
			},
			{
				"family": "Fischer",
				"given": "Joel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "quWhyProblemsPredictive2024",
		"type": "paper-conference",
		"abstract": "Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word “problems” is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N = 300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658547",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "161–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why is \"problems\" predictive of positive sentiment? A case study of explaining unintuitive features in sentiment classification",
		"URL": "https://doi.org/10.1145/3630106.3658547",
		"author": [
			{
				"family": "Qu",
				"given": "Jiaming"
			},
			{
				"family": "Arguello",
				"given": "Jaime"
			},
			{
				"family": "Wang",
				"given": "Yue"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gornetMappingAIEthics2024",
		"type": "paper-conference",
		"abstract": "The recent years have seen a surge of initiatives with the goal of defining what “ethical” artificial intelligence would or should entail, resulting in the publication of various charters and manifestos discussing AI ethics; these documents originate from academia, AI industry companies, non-profits, regulatory institutions, and the civil society. The contents of such documents vary wildly, from short, vague position statements to verbatims of democratic debates or impact assessment studies. As such, they are a marker of the social world of artificial intelligence, outlining the tenets of different actors, the consensus and dissensus on important goals, and so on. Multiple meta-analyses have focused on qualitatively identifying recurring themes in these documents, highlighting the high polysemy of themes such as transparency or trust, among others. The broad term of “AI ethics” and its guiding principles hide multiple disparities, shaped by our collective imaginations, economic and regulatory incentives, and the pre-existing social and structural power asymmetries; through quantitative analyses, we validate and infirm previous qualitative results. In this paper, we create and present a corpus of charters and manifestos discussing AI ethics through the process of collection and its quantitative analysis using text analysis to shed light on common and distinct vocabularies. Through frequency analysis, hierarchical topic clustering and semantic graph modelling, we show that the charters and manifestos discuss AI ethics along three broad axes: technical documents, regulatory ones, and innovation and business ones. We use our quantitative analysis to back up and nuance previous qualitative results, showing how some themes remain specific while others have fully permeated the space of AI ethics. We document and release our corpus, comprising of 436 documents, charters and manifestos discussing AI ethics. We release the corpus, its datasheet and our analysis, to open the way to further studies and discussions around vocabulary, principles and their evolution, as well as interactions among actors of AI ethics, in order to foster further studies on the topic.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658545",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "127–140",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping AI ethics: a meso-scale analysis of its charters and manifestos",
		"URL": "https://doi.org/10.1145/3630106.3658545",
		"author": [
			{
				"family": "Gornet",
				"given": "Mélanie"
			},
			{
				"family": "Delarue",
				"given": "Simon"
			},
			{
				"family": "Boritchev",
				"given": "Maria"
			},
			{
				"family": "Viard",
				"given": "Tiphaine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kleinDataFeminismAI2024",
		"type": "paper-conference",
		"abstract": "This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658543",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "100–112",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data feminism for AI",
		"URL": "https://doi.org/10.1145/3630106.3658543",
		"author": [
			{
				"family": "Klein",
				"given": "Lauren"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "nigatuSearchedReligiousSong2024",
		"type": "paper-conference",
		"abstract": "Online social media platforms such as YouTube have a wide, global reach. However, little is known about the experience of low-resourced language speakers on such platforms; especially in how they experience and navigate harmful content. To better understand this, we (1) conducted semi-structured interviews (n=15) and (2) analyzed search results (n=9313), recommendations (n=3336), channels (n=120) and comments (n=406) of policy-violating sexual content on YouTube focusing on the Amharic language. Our findings reveal that – although Amharic-speaking YouTube users find the platform crucial for several aspects of their lives – participants reported unplanned exposure to policy-violating sexual content when searching for benign, popular queries. Furthermore, malicious content creators seem to exploit under-performing language technologies and content moderation to further target vulnerable groups of speakers, including migrant domestic workers, diaspora, and local Ethiopians. Overall, our study sheds light on how failures in low-resourced language technology may lead to exposure to harmful content and suggests implications for stakeholders in minimizing harm. Content Warning: This paper includes discussions of NSFW topics and harmful content (hate, abuse, sexual harassment, self-harm, misinformation). The authors do not support the creation or distribution of harmful content.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658546",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "141–160",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I searched for a religious song in amharic and got sexual content instead’’: Investigating online harm in low-resourced languages on YouTube.",
		"URL": "https://doi.org/10.1145/3630106.3658546",
		"author": [
			{
				"family": "Nigatu",
				"given": "Hellina Hailu"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "razReliabilityGapsGroups2024",
		"type": "paper-conference",
		"abstract": "This paper investigates the inter-rater reliability of risk assessment instruments (RAIs). The main question is whether different, socially salient groups are affected differently by a lack of inter-rater reliability of RAIs, that is, whether mistakes with respect to different groups affects them differently. The question is investigated with a simulation study of the COMPAS dataset. A controlled degree of noise is injected into the input data of a predictive model; the noise can be interpreted as a synthetic rater that makes mistakes. The main finding is that there are systematic differences in output reliability between groups in the COMPAS dataset. The sign of the difference depends on the kind of inter-rater statistic that is used (Cohen’s Kappa, Byrt’s PABAK, ICC), and in particular whether or not the statistic corrects for prediction prevalences of the groups.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658544",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "113–126",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reliability gaps between groups in COMPAS dataset",
		"URL": "https://doi.org/10.1145/3630106.3658544",
		"author": [
			{
				"family": "Räz",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "luccioniPowerHungryProcessing2024",
		"type": "paper-conference",
		"abstract": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658542",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "85–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Power hungry processing: Watts driving the cost of AI deployment?",
		"URL": "https://doi.org/10.1145/3630106.3658542",
		"author": [
			{
				"family": "Luccioni",
				"given": "Sasha"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Strubell",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bogiatzis-gibbonsIndividualAccountabilityReasserting2024",
		"type": "paper-conference",
		"abstract": "AI control mechanisms like accountability procedures or technical standards are usually subpolitical: decisions are primarily debated and made within circumscribed subsystems of experts or interest groups, like the professional community of data scientists. However, AI systems are more deeply intertwined with a wider sense of politics than these mechanisms contemplate. In Winner’s dual senses, they are incidentally political as they settle disputes within political communities through their design, invention, and arrangement, and inherently political as they reciprocally contribute to and are sustained by patterning of economic, social, and political orders. This work, therefore, draws upon political theory to argue for democratically controlled AI beyond individual notions of accountability. In its weaker form, it demands substantive, rule-bound oversight of state actors’ use of AI systems, seeking to remedy historical tendencies toward extra-legal surveillance and strengthen accountability beyond individuals. Conversely, the stronger form advocates for comprehensive democratic control over all facets of AI, even by questioning the permissibility of AI within particular socio-economic spheres, as these systems are becoming fundamental parts of our collective life. I sketch the necessary institutional frameworks to operationalize these two forms of democratic control: first, for the \"weak\" form through the concept of a \"control\" power separate from the executive from Sun Yat-Sen’s political thought, and second, participatory institutions such as citizens’ assemblies. Finally, I discuss actions data scientists can take without legal frameworks for control: furthering new social imaginaries of AI that foreground the possibility of control and involving affected communities in decision-making around AI systems. The concept of democratic control is then both a measuring stick for existing standards and legislation and a clarion call for future advocacy.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658541",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "74–84",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond individual accountability: (Re-)asserting democratic control of AI",
		"URL": "https://doi.org/10.1145/3630106.3658541",
		"author": [
			{
				"family": "Bogiatzis-Gibbons",
				"given": "Daniel James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rateikeDesigningLongtermGroup2024",
		"type": "paper-conference",
		"abstract": "Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658538",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 31\npublisher-place: Rio de Janeiro, Brazil",
		"page": "20–50",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing long-term group fair policies in dynamical systems",
		"URL": "https://doi.org/10.1145/3630106.3658538",
		"author": [
			{
				"family": "Rateike",
				"given": "Miriam"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			},
			{
				"family": "Forré",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "freszClassificationMetricsImage2024",
		"type": "paper-conference",
		"abstract": "Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658537",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Classification metrics for image explanations: Towards building reliable XAI-evaluations",
		"URL": "https://doi.org/10.1145/3630106.3658537",
		"author": [
			{
				"family": "Fresz",
				"given": "Benjamin"
			},
			{
				"family": "Lörcher",
				"given": "Lena"
			},
			{
				"family": "Huber",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "finocchiaroUsingPropertyElicitation2024",
		"type": "paper-conference",
		"abstract": "Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658540",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "62–73",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using property elicitation to understand the impacts of fairness regularizers",
		"URL": "https://doi.org/10.1145/3630106.3658540",
		"author": [
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "blandinLearningFairnessDemonstrations2024",
		"type": "paper-conference",
		"abstract": "Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658539",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "51–61",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fairness from demonstrations via inverse reinforcement learning",
		"URL": "https://doi.org/10.1145/3630106.3658539",
		"author": [
			{
				"family": "Blandin",
				"given": "Jack"
			},
			{
				"family": "Kash",
				"given": "Ian A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leeOneVsMany2024",
		"type": "paper-conference",
		"abstract": "As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study (N = 252) in which participants were given one or more LLM-generated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants’ perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3662681",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2518–2531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One vs. many: Comprehending accurate information from multiple erroneous and inconsistent AI generations",
		"URL": "https://doi.org/10.1145/3630106.3662681",
		"author": [
			{
				"family": "Lee",
				"given": "Yoonjoo"
			},
			{
				"family": "Son",
				"given": "Kihoon"
			},
			{
				"family": "Kim",
				"given": "Tae Soo"
			},
			{
				"family": "Kim",
				"given": "Jisu"
			},
			{
				"family": "Chung",
				"given": "John Joon Young"
			},
			{
				"family": "Adar",
				"given": "Eytan"
			},
			{
				"family": "Kim",
				"given": "Juho"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "eyubogluModelChangeListsCharacterizing2024",
		"type": "paper-conference",
		"abstract": "Updates to Machine Learning as a Service (MLaaS) APIs may affect downstream systems that depend on their predictions. However, performance changes introduced by these updates are poorly documented by providers and seldom studied in the literature. As a result, API producers and consumers are left wondering: do model updates introduce performance changes that could adversely affect users’ system? Ideally, producers and consumers would have access to a detailed ChangeList specifying the slices of data where model performance has improved and degraded since the update. But, producing a ChangeList is challenging because it requires (1) discovering slices in the absence of detailed annotations or metadata, (2) accurately attributing coherent concepts to the discovered slices, and (3) communicating them to the user in a digestable manner. In this work, we demonstrate, discuss, and critique one approach for building, verifying, and releasing ChangeLists that aims to address these challenges. Using this approach, we analyze six real-world MLaaS API updates including GPT-3 and Google Cloud Vision. We produce a prototype ChangeList for each, identifying over 100 coherent data slices on which the model’s performance changed significantly. Notably, we find 63 instances where an update improves performance globally, but hurts performance on a coherent slice – a phenomenon not previously documented at scale in the literature. Finally, with diverse participants from industry, we conduct a think-aloud user study that explores the importance of releasing ChangeLists and highlights the strengths and weaknesses of our approach. This serves to validate some parts of our approach and uncover important areas for future work.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659047",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 22\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2432–2453",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model ChangeLists: Characterizing updates to ML models",
		"URL": "https://doi.org/10.1145/3630106.3659047",
		"author": [
			{
				"family": "Eyuboglu",
				"given": "Sabri"
			},
			{
				"family": "Goel",
				"given": "Karan"
			},
			{
				"family": "Desai",
				"given": "Arjun"
			},
			{
				"family": "Chen",
				"given": "Lingjiao"
			},
			{
				"family": "Monfort",
				"given": "Mathew"
			},
			{
				"family": "Ré",
				"given": "Chris"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dinhLearningFairRanking2024",
		"type": "paper-conference",
		"abstract": "Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3661932",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2508–2517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fair ranking policies via differentiable optimization of ordered weighted averages",
		"URL": "https://doi.org/10.1145/3630106.3661932",
		"author": [
			{
				"family": "Dinh",
				"given": "My H"
			},
			{
				"family": "Kotary",
				"given": "James"
			},
			{
				"family": "Fioretto",
				"given": "Ferdinando"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sterzQuestEffectivenessHuman2024",
		"type": "paper-conference",
		"abstract": "Human oversight is currently discussed as a potential safeguard to counter some of the negative aspects of high-risk AI applications. This prompts a critical examination of the role and conditions necessary for what is prominently termed effective or meaningful human oversight of these systems. This paper investigates effective human oversight by synthesizing insights from psychological, legal, philosophical, and technical domains. Based on the claim that the main objective of human oversight is risk mitigation, we propose a viable understanding of effectiveness in human oversight: for human oversight to be effective, the oversight person has to have (a) sufficient causal power with regard to the system and its effects, (b) suitable epistemic access to relevant aspects of the situation, (c) self-control, and (d) fitting intentions for their role. Furthermore, we argue that this is equivalent to saying that an oversight person is effective if and only if they are morally responsible and have fitting intentions. Against this backdrop, we suggest facilitators and inhibitors of effectiveness in human oversight when striving for practical applicability. We discuss factors in three domains, namely, the technical design of the system, individual factors of oversight persons, and the environmental circumstances in which they operate. Finally, this paper scrutinizes the upcoming AI Act of the European Union – in particular Article 14 on Human Oversight – as an exemplary regulatory framework in which we study the practicality of our understanding of effective human oversight. By analyzing the provisions and implications of the European AI Act proposal, we pinpoint how far that proposal aligns with our analyses regarding effective human oversight as well as how it might get enriched by our conceptual understanding of effectiveness in human oversight.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659051",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2495–2507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the quest for effectiveness in human oversight: Interdisciplinary perspectives",
		"URL": "https://doi.org/10.1145/3630106.3659051",
		"author": [
			{
				"family": "Sterz",
				"given": "Sarah"
			},
			{
				"family": "Baum",
				"given": "Kevin"
			},
			{
				"family": "Biewer",
				"given": "Sebastian"
			},
			{
				"family": "Hermanns",
				"given": "Holger"
			},
			{
				"family": "Lauber-Rönsberg",
				"given": "Anne"
			},
			{
				"family": "Meinel",
				"given": "Philip"
			},
			{
				"family": "Langer",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mickelRacialEthnicCategories2024",
		"type": "paper-conference",
		"abstract": "Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of who comprises the racial categories chosen and how people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied. In this paper, we make two contributions. First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups. Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659050",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2484–2494",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial/ethnic categories in AI and algorithmic fairness: Why they matter and what they represent",
		"URL": "https://doi.org/10.1145/3630106.3659050",
		"author": [
			{
				"family": "Mickel",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vanbovenTransformingDutchDebiasing2024",
		"type": "paper-conference",
		"abstract": "Gender-neutral pronouns are increasingly being introduced across Western languages. Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals. This paper examines a Dutch coreference resolution system’s performance on gender-neutral pronouns, specifically hen and die. In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English. We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation. Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like lea, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns. Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts. Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns. We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used. This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659049",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2470–2483",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transforming dutch: Debiasing dutch coreference resolution systems for non-binary pronouns",
		"URL": "https://doi.org/10.1145/3630106.3659049",
		"author": [
			{
				"family": "Boven",
				"given": "Goya",
				"non-dropping-particle": "van"
			},
			{
				"family": "Du",
				"given": "Yupei"
			},
			{
				"family": "Nguyen",
				"given": "Dong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "cheongAIAmNot2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore when and why LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (“cases”) allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts’ recommendations for LLM response strategies, which center around helping users identify ‘right questions to ask’ and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659048",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2454–2469",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(A)I am not a lawyer, but...: Engaging legal experts towards responsible LLM policies for legal advice",
		"URL": "https://doi.org/10.1145/3630106.3659048",
		"author": [
			{
				"family": "Cheong",
				"given": "Inyoung"
			},
			{
				"family": "Xia",
				"given": "King"
			},
			{
				"family": "Feng",
				"given": "K. J. Kevin"
			},
			{
				"family": "Chen",
				"given": "Quan Ze"
			},
			{
				"family": "Zhang",
				"given": "Amy X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "babaeiDriversPersuasiveStrategies2024",
		"type": "paper-conference",
		"abstract": "The proliferation of e-commerce, game, and social networking sites, has brought to light the use of \"dark patterns\" or, more generally, manipulative designs (MDs), which exploit psychological effects and cognitive biases of users to channel their behavior toward outcomes that benefit the company or owner of the site, against the users’ best interests. Previous research has categorized MDs, assessed their impact on users, gauged their prevalence, and attempted automated detection using computer vision and natural language processing techniques. However, limited attention has been given to understanding how to warn and educate users about MDs, guiding them to recognize and resist such manipulative tactics. To address this gap, we carried out a controlled study with n=134 participants, using a survey based on the Protection Motivation Theory (PMT) to better understand the motivations of people to learn about MDs. We also explored the effectiveness of two persuasive strategies, based on Cialdini’s principles of influence (social influence and authority), to trigger attention towards MDs and intention to learn more about MDs and to avoid them. For this, we created a simulated application in a mobile app distribution platform modeled like Google Play Store containing a visual signal, a warning based on one of the two strategies, and simulated reviews from other users. The results indicate that two of the five PMT constructs - a higher Perceived Severity of MDs and a lower Perceived Response Cost of learning about MDs - have the most significant influence on the Intention to learn more about MDs. The participants in the experimental group, exposed to the two persuasive strategies exhibited a larger increase in their intention to seek information about MDs than the participants in the control group. Our study showcases the potential of a persuasive intervention, illustrating how mobile app distribution platforms can enhance user protection against MD exploitation. By implementing such interventions, these platforms can boost accountability and transparency of applications existing on their platform, and MD awareness among their users.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659046",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2421–2431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Drivers and persuasive strategies to influence user intention to learn about manipulative design",
		"URL": "https://doi.org/10.1145/3630106.3659046",
		"author": [
			{
				"family": "Babaei",
				"given": "Pooria"
			},
			{
				"family": "Vassileva",
				"given": "Julita"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "smithRecommendMeDesigning2024",
		"type": "paper-conference",
		"abstract": "Fairness metrics have become a useful tool to measure how fair or unfair a machine learning system may be for its stakeholders. In the context of recommender systems, previous research has explored how various stakeholders experience algorithmic fairness or unfairness, but it is also important to capture these experiences in the design of fairness metrics. Therefore, we conducted four focus groups with providers (those whose items, content, or profiles are being recommended) of two different domains: content creators and dating app users. We explored how our participants experience unfairness on their associated platforms, and worked with them to co-design fairness goals, definitions, and metrics that might capture these experiences. This work represents an important step towards designing fairness metrics with the stakeholders who will be impacted by their operationalizations. We analyze the efficacy and challenges of enacting these metrics in practice and explore how future work might benefit from this methodology.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659044",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2389–2399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Recommend me? Designing fairness metrics with providers",
		"URL": "https://doi.org/10.1145/3630106.3659044",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Satwani",
				"given": "Aishwarya"
			},
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lamparthAnalyzingEditingInner2024",
		"type": "paper-conference",
		"abstract": "Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets. Trigger warning: Offensive language.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659042",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2362–2373",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing and editing inner mechanisms of backdoored language models",
		"URL": "https://doi.org/10.1145/3630106.3659042",
		"author": [
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Reuel",
				"given": "Anka"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "imanaAuditingRacialDiscrimination2024",
		"type": "paper-conference",
		"abstract": "Digital ads on social-media platforms play an important role in shaping access to economic opportunities. Our work proposes and implements a new third-party auditing method that can evaluate racial bias in the delivery of ads for education opportunities. Third-party auditing is important because it allows external parties to demonstrate presence or absence of bias in social-media algorithms. Education is a domain with legal protections against discrimination and concerns of racial-targeting, but bias induced by ad delivery algorithms has not been previously explored in this domain. Prior audits demonstrated discrimination in platforms’ delivery of ads to users for housing and employment ads. These audit findings supported legal action that prompted Meta to change their ad-delivery algorithms to reduce bias, but only in the domains of housing, employment, and credit. In this work, we propose a new methodology that allows us to measure racial discrimination in a platform’s ad delivery algorithms for education ads. We apply our method to Meta using ads for real schools and observe the results of delivery. We find evidence of racial discrimination in Meta’s algorithmic delivery of ads for education opportunities, posing legal and ethical concerns. Our results extend evidence of algorithmic discrimination to the education domain, showing that current bias mitigation mechanisms are narrow in scope, and suggesting a broader role for third-party auditing of social media in areas where ensuring non-discrimination is important.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659041",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2348–2361",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing for racial discrimination in the delivery of education ads",
		"URL": "https://doi.org/10.1145/3630106.3659041",
		"author": [
			{
				"family": "Imana",
				"given": "Basileal"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			},
			{
				"family": "Heidemann",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "inieAIProbabilisticAutomation2024",
		"type": "paper-conference",
		"abstract": "In this paper we investigate how people’s level of trust (as reported through self-assessment) in so-called “AI” (artificial intelligence) is influenced by anthropomorphizing language in system descriptions. Building on prior work, we define four categories of anthropomorphization (1. Properties of a cognizer, 2. Agency, 3. Biological metaphors, and 4. Properties of a communicator). We use a survey-based approach (n=954) to investigate whether participants are likely to trust one of two (fictitious) “AI” systems by randomly assigning people to see either an anthropomorphized or a de-anthropomorphized description of the systems. We find that participants are no more likely to trust anthropomorphized over de-anthropmorphized product descriptions overall. The type of product or system in combination with different anthropomorphic categories appears to exert greater influence on trust than anthropomorphizing language alone, and age is the only demographic factor that significantly correlates with people’s preference for anthropomorphized or de-anthropomorphized descriptions. When elaborating on their choices, participants highlight factors such as lesser of two evils, lower or higher stakes contexts, and human favoritism as driving motivations when choosing between product A and B, irrespective of whether they saw an anthropomorphized or a de-anthropomorphized description of the product. Our results suggest that “anthropomorphism” in “AI” descriptions is an aggregate concept that may influence different groups differently, and provide nuance to the discussion of whether anthropomorphization leads to higher trust and over-reliance by the general public in systems sold as “AI”.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659040",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 26\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2322–2347",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From \"AI\" to probabilistic automation: How does anthropomorphization of technical systems descriptions influence trust?",
		"URL": "https://doi.org/10.1145/3630106.3659040",
		"author": [
			{
				"family": "Inie",
				"given": "Nanna"
			},
			{
				"family": "Druga",
				"given": "Stefania"
			},
			{
				"family": "Zukerman",
				"given": "Peter"
			},
			{
				"family": "Bender",
				"given": "Emily M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ullsteinAttitudesFacialAnalysis2024",
		"type": "paper-conference",
		"abstract": "Computer vision AI systems present one of the most radical technical transformations of our time. Such systems are given unparalleled epistemic power to impose meaning on visual data, despite their inherent semantic ambiguity. This epistemic power is particularly evident in computer vision AI that interprets the meaning of human faces. The goal of this work is to empirically document laypeople’s perceptions of the epistemic and ethical complexity of computer vision AI through a large-scale qualitative study with participants in Argentina, Japan, Kenya, and the USA (N=4,468). We developed a vignette scenario about a fictitious company that analyzes people’s portraits using computer vision AI to make a variety of inferences about people based on their faces. For each inference that the fictitious company draws (e.g., age, skin color, intelligence), we ask participants from all countries to reason about how they evaluate computer vision AI inference-making. In a series of workshops, we collaborated as a multinational research team to develop a codebook that captures people’s different justifications of facial analysis AI inferences to create a comprehensive justification portfolio. Our study reveals similarities in justification patterns, but also significant intra-country and inter-country diversity in response to different facial inferences. For example, participants from Argentina, Japan, Kenya, and the USA vastly disagree over the reasonableness of AI classifications such as beautiful or skin color. They tend to agree in their opposition to AI-drawn inferences intelligence and trustworthiness. Adding much-needed non-Western perspectives to debates on computer vision ethics, our results suggest that, contrary to popular justifications for facial classification technologies, there is no such thing as a “common sense” facial classification that accords simply with a general, homogeneous “human intuition.”",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659038",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 29\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2273–2301",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Attitudes toward facial analysis AI: a cross-national study comparing argentina, kenya, japan, and the USA",
		"URL": "https://doi.org/10.1145/3630106.3659038",
		"author": [
			{
				"family": "Ullstein",
				"given": "Chiara"
			},
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Ikkatai",
				"given": "Yuko"
			},
			{
				"family": "Arnez-Jordan",
				"given": "Naira Paola"
			},
			{
				"family": "Caleno",
				"given": "Rose"
			},
			{
				"family": "Mboya",
				"given": "Brian"
			},
			{
				"family": "Higuma",
				"given": "Shuichiro"
			},
			{
				"family": "Hartwig",
				"given": "Tilman"
			},
			{
				"family": "Yokoyama",
				"given": "Hiromi"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "engelmannVisionsDisciplineAnalyzing2024",
		"type": "paper-conference",
		"abstract": "Education plays an indispensable role in fostering societal well-being and is widely regarded as one of the most influential factors in shaping the future of generations to come. As artificial intelligence (AI) becomes more deeply integrated into our daily lives and the workforce, educational institutions at all levels are directing their focus on resources that cater to AI education. Yet, informal education, including online learning on social media platforms like YouTube, plays an increasingly significant role for both students and the general public. Offering greater accessibility compared to formal education, millions of individuals use YouTube for educational resources on AI today. Due to the substantial societal impact of AI, it is crucial for introductory AI courses to meaningfully address the ethical implications associated with AI. Our work investigates the current landscape of introductory AI courses on YouTube, and the potential for introducing ethics in this context. We qualitatively analyze the 20 most watched introductory AI courses on YouTube, coding a total of 92.2 hours of educational content viewed by close to 50 million people. We find that these introductory AI courses do not meaningfully engage with ethical or societal challenges of AI (RQ1). When defining and framing AI, introductory AI courses foreground excitement around AI’s transformative role in society, over-exaggerate AI’s current and future abilities, and anthropomorphize AI (RQ2). In teaching AI, we see a widespread reliance on corporate AI tools and frameworks as well as a prioritization on a hands-on approach to learning rather than on conceptual foundations (RQ3). In promoting key AI practices, introductory AI courses abstract away entirely the socio-technical nature of AI classification and prediction, for example by favoring data quantity over data quality (RQ4). Given the power of openly available introductory courses to shape enduring beliefs around AI and its field at the onset of a learning journey, we extend our analysis with recommendations that aim to integrate ethical reflections into introductory AI courses. We recommend that introductory AI courses should (1) highlight ethical challenges of AI to present a more balanced perspective, (2) raise ethical issues explicitly relevant to the technical concepts discussed and (3) nurture a sense of accountability in future AI developers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659045",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2400–2420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Visions of a discipline: Analyzing introductory AI courses on YouTube",
		"URL": "https://doi.org/10.1145/3630106.3659045",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Choksi",
				"given": "Madiha Zahrah"
			},
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mhasawadeUnderstandingDisparitiesPost2024",
		"type": "paper-conference",
		"abstract": "Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across “race” and “gender” as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659043",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2374–2388",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding disparities in post hoc machine learning explanation",
		"URL": "https://doi.org/10.1145/3630106.3659043",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Rahman",
				"given": "Salman"
			},
			{
				"family": "Haskell-Craig",
				"given": "Zoé"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moayeriEmbracingDiversityInterpretable2024",
		"type": "paper-conference",
		"abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today’s best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms — from diced to whole, on a table or in a bowl — yet standard VLM classifiers map all instances of a class to a single vector based on the class label. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity—leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659039",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2302–2321",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Embracing diversity: Interpretable zero-shot classification beyond one vector per class",
		"URL": "https://doi.org/10.1145/3630106.3659039",
		"author": [
			{
				"family": "Moayeri",
				"given": "Mazda"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "casperBlackboxAccessInsufficient2024",
		"type": "paper-conference",
		"abstract": "External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system’s inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659037",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2254–2272",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Black-box access is insufficient for rigorous AI audits",
		"URL": "https://doi.org/10.1145/3630106.3659037",
		"author": [
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Siegmann",
				"given": "Charlotte"
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Curtis",
				"given": "Taylor Lynn"
			},
			{
				"family": "Bucknall",
				"given": "Benjamin"
			},
			{
				"family": "Haupt",
				"given": "Andreas"
			},
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Hobbhahn",
				"given": "Marius"
			},
			{
				"family": "Sharkey",
				"given": "Lee"
			},
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Von Hagen",
				"given": "Marvin"
			},
			{
				"family": "Alberti",
				"given": "Silas"
			},
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Sun",
				"given": "Qinyi"
			},
			{
				"family": "Gerovitch",
				"given": "Michael"
			},
			{
				"family": "Bau",
				"given": "David"
			},
			{
				"family": "Tegmark",
				"given": "Max"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gomezAlgorithmicArbitrarinessContent2024",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML model, but can nevertheless change what the model gets wrong. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as “toxic,” leading to arbitrary restrictions on speech. We use the principles set by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice to interpret the effects of these findings in terms of Human Rights. We analyze (i) the extent of predictive multiplicity among popular state-of-the-art LLMs used for detecting “toxic” content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) the magnitude of model multiplicity on content that is unanimously recognized as toxic by human annotators. Our findings indicate that the up-scaled algorithmic moderation risks legitimizing an “algorithmic leviathan”, where an algorithm disproportionately manages human rights. To mitigate such risks, our study underscores the need to identify and increase the transparency of arbitrariness in content moderation applications. Our findings have implications to content moderation and intermediary liability laws being discussed and passed in many countries, such as the Digital Services Act in the European Union, the Online Safety Act in the United Kingdom, and the recent TSE resolutions in Brazil.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659036",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2234–2253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic arbitrariness in content moderation",
		"URL": "https://doi.org/10.1145/3630106.3659036",
		"author": [
			{
				"family": "Gomez",
				"given": "Juan Felipe"
			},
			{
				"family": "Machado",
				"given": "Caio"
			},
			{
				"family": "Paes",
				"given": "Lucas Monteiro"
			},
			{
				"family": "Calmon",
				"given": "Flavio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "toneyTrustIssuesDiscrepancies2024",
		"type": "paper-conference",
		"abstract": "How governments, practitioners, and researchers define artificial intelligence (AI) ethics significantly impacts the AI models and systems designed and deployed. Thus, the convergence of policy goals and technical approaches is necessary for international norms and standards on trustworthy AI. Defining, much less achieving trustworthy AI characteristics, however, entails clear communication through consensus on the meaning of field-specific terms. This paper presents an analysis of over 322,000 scientific research papers and the national documents from five countries (Australia, Canada, Japan, the United Kingdom, and the United States) on trustworthy AI in order to provide an in-depth review and comprehensive understanding of the similarities and differences between governments’ and researchers’ definitions and frameworks. While we identified substantive and relevant differences among policy documents and scientific research, the differences do not represent substantial disagreements among the common principles for trustworthy AI terms. Overall we found broad agreement across documents’ trustworthy AI term use, suggesting that nuanced differences could be overcome in an effort to create more global policies and aligned research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659035",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2222–2233",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trust issues: Discrepancies in trustworthy AI keywords use in policy and research",
		"URL": "https://doi.org/10.1145/3630106.3659035",
		"author": [
			{
				"family": "Toney",
				"given": "Autumn"
			},
			{
				"family": "Curlee",
				"given": "Kathleen"
			},
			{
				"family": "Probasco",
				"given": "Emelia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "santiniSeeingOpacityLimitations2024",
		"type": "paper-conference",
		"abstract": "Digital platforms provide a deregulated and opaque environment suited to the maintenance of their business model, in which ads are efficiently served by opaque algorithms to meticulously profiled users based on their behavioral data. The advertising infrastructure provided by these platforms made advertising more segmented and scalable, creating new opportunities and allowing for a profit-oriented influence industry to develop worldwide. Some platforms have invested in transparency measures for digital advertising, but there is still a gap between what is applied in the Global South and the Global North. In Brazil, despite evidence of an online ecosystem of suspicious, inauthentic, scam, and other types of fraudulent ads, regulatory proposals have faced a hard opposition from tech companies. Against this backdrop, there is a need to evaluate advertising transparency archives currently offered by online platforms in Brazil as a means to measure the quality of libraries and the available data.Thus, the main objective of this work is to account for transparency measures and means of accessing data of some of the largest online platforms and search engines in the country, in order to establish a general comparative diagnosis of ad transparency in Brazil. Based on the platforms’ public documentation, policies and terms of use for the Brazilian market, we perform a comparative analysis of six companies: Meta, Google, Twitter/X, Telegram, TikTok, and Spotify. Particular consideration is given to whether these companies do or do not have ads repositories, or a means to assess the disseminated advertisements. In an environment of low transparency and difficulty in accessing data, we found that the Meta Ad Library, although providing very limited data, is the most reliable source for systematic investigations of the digital advertising ecosystem. Even though Google offers an advertisement repository in Brazil, it lags considerably behind that offered by Meta and imposes greater difficulty in carrying out systematic analyses. On the other hand, Telegram, TikTok, Twitter/X and Spotify do not present any advertising repository or transparency center in order to analyse the Brazilian scenario. Although the scenario in the Global South can be characterized by a lack of transparency from platforms and by difficulties in accessing data, recent measures implemented elsewhere have demonstrated that this condition is reversible.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659034",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2209–2221",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Seeing through opacity: The limitations of digital ad transparency in Brazil",
		"URL": "https://doi.org/10.1145/3630106.3659034",
		"author": [
			{
				"family": "Santini",
				"given": "Rose Marie"
			},
			{
				"family": "Salles",
				"given": "Débora"
			},
			{
				"family": "Martins",
				"given": "Bruno Maurı́cio"
			},
			{
				"family": "Moreira",
				"given": "Alékis"
			},
			{
				"family": "Haddad",
				"given": "João Gabriel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sannemanInformationBottleneckCharacterization2024",
		"type": "paper-conference",
		"abstract": "Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659032",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 24\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2175–2198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An information bottleneck characterization of the understanding-workload tradeoff in human-centered explainable AI",
		"URL": "https://doi.org/10.1145/3630106.3659032",
		"author": [
			{
				"family": "Sanneman",
				"given": "Lindsay"
			},
			{
				"family": "Tucker",
				"given": "Mycal"
			},
			{
				"family": "Shah",
				"given": "Julie A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "schmittRoleExplainabilityCollaborative2024",
		"type": "paper-conference",
		"abstract": "Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659031",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2157–2174",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role of explainability in collaborative human-AI disinformation detection",
		"URL": "https://doi.org/10.1145/3630106.3659031",
		"author": [
			{
				"family": "Schmitt",
				"given": "Vera"
			},
			{
				"family": "Villa-Arenas",
				"given": "Luis-Felipe"
			},
			{
				"family": "Feldhus",
				"given": "Nils"
			},
			{
				"family": "Meyer",
				"given": "Joachim"
			},
			{
				"family": "Spang",
				"given": "Robert P."
			},
			{
				"family": "Möller",
				"given": "Sebastian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "slaughterImpactIBuyingMore2024",
		"type": "paper-conference",
		"abstract": "Instant buyers (iBuyers)—companies that buy and sell homes based on automated valuation models (AVMs)—now hold more than 5% market share in some USA cities. In this work, we investigate the fairness of iBuyers by constructing a dataset that links racial demographics from voter records with detailed property information on over 50,000 real estate transactions. Using Bayesian hierarchical modeling we find that: 1. iBuyers Decrease the Racial Sales Price Gap Between Black and White Home Sellers. Controlling for over 50 property features we find that iBuyers reduce the racial price gap that otherwise exists between homes sold by Black and White homeowners. This is not, however, a result of equity achieved through proprietary AVMs, but rather a result of both Black and White homeowners being similarly disadvantaged by iBuyers’ low purchase prices; and, 2. iBuyers Increase Property Conversion Rates from Individual to Institutional Ownership. We trace iBuyers’ purchases as well as their follow-on sales of homes in Mecklenburg County. In doing so, we show that iBuyers increase the rate at which properties are converted from being individually owned to institutionally owned. The eventual purchasers of iBuyer homes include national and international rental companies that have been tied to high eviction rates and poor property management. As with sale prices, we find that rather than reapportioning this social harm more equitably, iBuyers are simply increasing the rate at which homes bought from White homeowners are converted to institutional ownership. Ultimately, our analysis suggests that iBuyers are Equalizing Housing Outcomes by Extending Real Estate Harms Typically Isolated to Black Homeowners to White homeowners as Well.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659027",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2086–2100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of iBuying is about more than just racial disparities: Evidence from mecklenburg county, NC",
		"URL": "https://doi.org/10.1145/3630106.3659027",
		"author": [
			{
				"family": "Slaughter",
				"given": "Isaac"
			},
			{
				"family": "Brown",
				"given": "Eva Maxfield"
			},
			{
				"family": "Weber",
				"given": "Nic"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "baackCriticalAnalysisLargest2024",
		"type": "paper-conference",
		"abstract": "Common Crawl is the largest freely available collection of web crawl data and one of the most important sources of pre-training data for large language models (LLMs). It is used so frequently and makes up such large proportions of the overall pre-training data in many cases that it arguably has become a foundational building block for LLM development, and subsequently generative AI products built on top of LLMs. Despite its pivotal role, Common Crawl itself is not widely understood, nor is there much reflection evident among LLM builders about the implications of using Common Crawl's data. This paper discusses what Common Crawl's popularity for LLM development means for fairness, accountability, and transparency in generative AI by highlighting the organization's values and practices, as well as how it views its own role within the AI ecosystem. Our qualitative analysis is based on in-depth interviews with Common Crawl staffers and relevant online documents.After discussing Common Crawl's role in generative AI and how LLM builders have typically used its data for pre-training LLMs, we review Common Crawl's self-defined values and priorities and highlight the limitations and biases of its crawling process. We find that Common Crawl's popularity has contributed to making generative AI more transparent to scrutiny in many ways, and that it has enabled more LLM research and development to take place beyond well-resourced leading AI companies. At the same time, many LLM builders have used Common Crawl as a source for training data in ways that are problematic: for instance, with lack of care and transparency for how Common Crawl's massive crawl data was filtered for harmful content before the pre-training, often by relying on rudimentary automated filtering techniques. We offer recommendations for Common Crawl and LLM builders on how to improve fairness, accountability, and transparency in LLM research and development.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659033",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2199–2208",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A critical analysis of the largest source for generative AI training data: Common crawl",
		"URL": "https://doi.org/10.1145/3630106.3659033",
		"author": [
			{
				"family": "Baack",
				"given": "Stefan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rifatDataAnnotationMeaningmaking2024",
		"type": "paper-conference",
		"abstract": "Data annotation is a process of meaning-making and is inherently political. The literature on ethics in data-driven technologies explores these political aspects, primarily focusing on questions of bias and power. This paper argues that the politics of annotation often overemphasize secular and modern values and overlooks faith-based, religious, and spiritual aspects (FRS) in data annotation. This oversight particularly affects the postcolonial regions of the Global South, where FRS are intertwined with people’s everyday experiences and ethics. We conducted a focus group discussion and contextual inquiries with six annotators who annotated a faith-related “violence” dataset from South Asian YouTube content. Our analysis reveals that FRS blindness in data annotation manifests through the politics of achieving objectivity and the “scientific” process of meaning-making. Due to these goals, which are predominantly shaped by Western values, FRS sensitivities are overlooked from the initial stages of data curation through annotation, ultimately leading to a context collapse within the annotation process. Finally, we advocate for the adaptation of FRS sensitivities into the annotation process and data infrastructure, particularly when the dataset clearly pertains to FRS, to promote greater cultural and contextual inclusivity in annotation practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659030",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2148–2156",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data, annotation, and meaning-making: The politics of categorization in annotating a dataset of faith-based communal violence",
		"URL": "https://doi.org/10.1145/3630106.3659030",
		"author": [
			{
				"family": "Rifat",
				"given": "Mohammad Rashidujjaman"
			},
			{
				"family": "Safir",
				"given": "Abdullah Hasan"
			},
			{
				"family": "Saha",
				"given": "Sourav"
			},
			{
				"family": "Junaed",
				"given": "Jahedul Alam"
			},
			{
				"family": "Saleki",
				"given": "Maryam"
			},
			{
				"family": "Amin",
				"given": "Mohammad Ruhul"
			},
			{
				"family": "Ahmed",
				"given": "Syed Ishtiaque"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wyllieFairnessFeedbackLoops2024",
		"type": "paper-conference",
		"abstract": "Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659029",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 35\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2113–2147",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness feedback loops: Training on synthetic data amplifies bias",
		"URL": "https://doi.org/10.1145/3630106.3659029",
		"author": [
			{
				"family": "Wyllie",
				"given": "Sierra"
			},
			{
				"family": "Shumailov",
				"given": "Ilia"
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "naharRegulatingExplainabilityMachine2024",
		"type": "paper-conference",
		"abstract": "With the rise of artificial intelligence (AI), concerns about AI applications causing unforeseen harms to safety, privacy, security, and fairness are intensifying. While attempts to create regulations are underway, with initiatives such as the EU AI Act and the 2023 White House executive order, skepticism abounds as to the efficacy of such regulations. This paper explores an interdisciplinary approach to designing policy for the explainability of AI applications, as the widely discussed \"right to explanation\" associated with the EU General Data Protection Regulation is ambiguous. To develop practical guidance for explainability, we conducted an experimental study that involved continuous collaboration among a team of researchers with AI and policy backgrounds over the course of ten weeks. The objective was to determine whether, through interdisciplinary effort, we can reach consensus on a policy for explainability in AI–one that is clearer, and more actionable and enforceable than current guidelines. We share nine observations, derived from an iterative policy design process, which included drafting the policy, attempting to comply with it (or circumvent it), and collectively evaluating its effectiveness on a weekly basis. Key observations include: iterative and continuous feedback was useful to improve policy drafts over time, discussing evidence of compliance was necessary during policy design, and human-subject studies were found to be an important form of evidence. We conclude with a note of optimism, arguing that meaningful policies can be achieved within a moderate time frame and with limited experience in policy design, as demonstrated by our student researchers on the team. This holds promising implications for policymakers, signaling that practical and effective regulation for AI applications is attainable.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659028",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2101–2112",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating explainability in machine learning applications – observations from a policy design experiment",
		"URL": "https://doi.org/10.1145/3630106.3659028",
		"author": [
			{
				"family": "Nahar",
				"given": "Nadia"
			},
			{
				"family": "Rowlett",
				"given": "Jenny"
			},
			{
				"family": "Bray",
				"given": "Matthew"
			},
			{
				"family": "Omar",
				"given": "Zahra Abba"
			},
			{
				"family": "Papademetris",
				"given": "Xenophon"
			},
			{
				"family": "Menon",
				"given": "Alka"
			},
			{
				"family": "Kästner",
				"given": "Christian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kieslichMyFutureMy2024",
		"type": "paper-conference",
		"abstract": "As a general purpose technology without a concrete pre-defined purpose, personal chatbots can be used for a whole range of objectives, depending on the personal needs, contexts, and tasks of an individual, and so potentially impact a variety of values, people, and social contexts. Traditional methods of risk assessment are confronted with several challenges: the lack of a clearly defined technology purpose, the lack of clearly defined values to orient on, the heterogeneity of uses, and the difficulty of actively engaging citizens themselves in anticipating impacts from the perspective of their individual lived realities. In this article, we leverage scenario writing at scale as a method for anticipating AI impact that is responsive to these challenges. The advantages of the scenario method are its ability to engage individual users and stimulate them to consider how chatbots are likely to affect their reality and so collect different impact scenarios depending on the cultural and societal embedding of a heterogeneous citizenship. Empirically, we tasked 106 US-based participants to write short fictional stories about the future impact (whether desirable or undesirable) of AI-based personal chatbots on individuals and society and, in addition, ask respondents to explain why these impacts are important and how they relate to their values. In the analysis process, we map those impacts and analyze them in relation to socio-demographic as well as AI-related attitudes of the scenario writers. We show that our method is effective in (1) identifying and mapping desirable and undesirable impacts of AI-based personal chatbots, (2) setting these impacts in relation to values that are important for individuals, and (3) detecting socio-demographic and AI-attitude related differences of impact anticipation.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659026",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2071–2085",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "My future with my chatbot: a scenario-driven, user-centric approach to anticipating AI impacts",
		"URL": "https://doi.org/10.1145/3630106.3659026",
		"author": [
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Helberger",
				"given": "Natali"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dotanResponsibleAdoptionGenerative2024",
		"type": "paper-conference",
		"abstract": "This paper proposes an approach to the responsible adoption of generative AI in higher education, employing a “points to consider” approach that is sensitive to the goals, values, and structural features of higher education. Higher education's ethos of collaborative faculty governance, pedagogical and research goals, and embrace of academic freedom conflict, the paper argues, with centralized top-down approaches to governing AI that are common in the private sector. The paper is based on a semester-long effort at the University of Pittsburgh which gathered and organized perspectives on generative AI in higher education through a collaborative, iterative, interdisciplinary process that included recurring group discussions, three standalone focus groups, and an informal survey. The paper presents insights drawn from this effort—that give rise to the “points to consider” approach the paper develops. These insights include the benefits and risks of potential uses of generative AI In higher education, as well as barriers to its adoption, and culminate in the six normative points to consider when adopting and governing generative AI in institutions of higher education.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659023",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2033–2046",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Responsible adoption of generative AI in higher education: Developing a “points to consider” approach based on faculty perspectives",
		"URL": "https://doi.org/10.1145/3630106.3659023",
		"author": [
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Parker",
				"given": "Lisa S."
			},
			{
				"family": "Radzilowicz",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "luPerceptionsPolicingSurveillance2024",
		"type": "paper-conference",
		"abstract": "In Detroit, the largest Black-majority city in the United States, municipal authorities have deployed an array of surveillance technologies with the promise of containing crime and improving community safety. This article draws from a cross-sectional survey of over two thousand Detroit residents and multi-year community-based fieldwork in Detroit’s Eastside to examine local perceptions of policing surveillance technologies. Our survey reveals that respondents, notably those in more vulnerable positions, report higher perceived safety levels with policing surveillance cameras in their neighborhoods. However, when triangulating these results with insights from our fieldwork, we argue that these survey findings should not be taken as public support for surveillance. Alongside this seeming buy-in is a widely shared “better than nothing” imaginary among residents from impacted communities. “Better than nothing,” for the residents, is a pragmatic compromise and maneuver between being aware of the inherent flaws of surveillance technologies and settling for any available resource or hope. This notion of “better than nothing” unveils residents’ prolonged wait for digital justice and institutional accountability, which we show is where racialized infrastructural harm and exploitation are enacted along the temporal dimension. Our findings offer practical insights for counter-surveillance advocacy efforts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659022",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2022–2032",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptions of Policing Surveillance Technologies in Detroit: Moving Beyond \"Better than Nothing\"",
		"URL": "https://doi.org/10.1145/3630106.3659022",
		"author": [
			{
				"family": "Lu",
				"given": "Alex Jiahong"
			},
			{
				"family": "Moy",
				"given": "Cameron"
			},
			{
				"family": "Ackerman",
				"given": "Mark S."
			},
			{
				"family": "Morenoff",
				"given": "Jeffrey"
			},
			{
				"family": "Dillahunt",
				"given": "Tawanna R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "davaniDisentanglingPerceptionsOffensiveness2024",
		"type": "paper-conference",
		"abstract": "Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers’ socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators’ judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants’ moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659021",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2007–2021",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling perceptions of offensiveness: Cultural and moral correlates",
		"URL": "https://doi.org/10.1145/3630106.3659021",
		"author": [
			{
				"family": "Davani",
				"given": "Aida"
			},
			{
				"family": "Dı́az",
				"given": "Mark"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "zezulkaFairDistributionPredictions2024",
		"type": "paper-conference",
		"abstract": "Deploying an algorithmically informed policy is a significant intervention in society. Prominent methods for algorithmic fairness focus on the distribution of predictions at the time of training, rather than the distribution of social goods that arises after deploying the algorithm in a specific social context. However, requiring a ‘fair’ distribution of predictions may undermine efforts at establishing a fair distribution of social goods. First, we argue that addressing this problem requires a notion of prospective fairness that anticipates the change in the distribution of social goods after deployment. Second, we provide formal conditions under which this change is identified from pre-deployment data. That requires accounting for different kinds of performative effects. Here, we focus on the way predictions change policy decisions and, consequently, the causally downstream distribution of social goods. Throughout, we are guided by an application from public administration: the use of algorithms to predict who among the recently unemployed will remain unemployed in the long term and to target them with labor market programs. Third, using administrative data from the Swiss public employment service, we simulate how such algorithmically informed policies would affect gender inequalities in long-term unemployment. When risk predictions are required to be ‘fair’ according to statistical parity and equality of opportunity, targeting decisions are less effective, undermining efforts to both lower overall levels of long-term unemployment and to close the gender gap in long-term unemployment.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659020",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 23\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1984–2006",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From the fair distribution of predictions to the fair distribution of social goods: Evaluating the impact of fair machine learning on long-term unemployment",
		"URL": "https://doi.org/10.1145/3630106.3659020",
		"author": [
			{
				"family": "Zezulka",
				"given": "Sebastian"
			},
			{
				"family": "Genin",
				"given": "Konstantin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "choksiEmergingArtifactsCentralized2024",
		"type": "paper-conference",
		"abstract": "In 2022, generative model based coding assistants became widely available with the public release of GitHub Copilot. Approaches to generative coding are often critiqued within the context of advances in machine learning. We argue that tools such as Copilot are better understood when contextualized against technologies derived from the same communities and datasets. Our work traces the historical and ideological origins of free and open source code and characterizes the process of centralization. We examine three case studies —Dependabot, Crater, and Copilot— to compare the engineering, social, and legal qualities of technical artifacts derived from shared community-based labor. Our analysis focuses on the implications these artifacts create for infrastructural dependencies, community adoption, and intellectual property. Reframing generative coding assistants through a set of peer technologies broadens considerations for academics and policymakers beyond machine learning, to include the ways technical artifacts are derived from communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659019",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1971–1983",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The emerging artifacts of centralized open-code",
		"URL": "https://doi.org/10.1145/3630106.3659019",
		"author": [
			{
				"family": "Choksi",
				"given": "Madiha Zahrah"
			},
			{
				"family": "Mandel",
				"given": "Ilan"
			},
			{
				"family": "Widder",
				"given": "David"
			},
			{
				"family": "Shvartzshnaider",
				"given": "Yan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "alkhathlanBalancingActEvaluating2024",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making using rankings— prevalent in areas from hiring and bail to university admissions— raises concerns of potential bias. In this paper, we explore the alignment between people’s perceptions of fairness and two popular fairness metrics designed for rankings. In a crowdsourced experiment with 480 participants, people rated the perceived fairness of a hypothetical scholarship distribution scenario. Results suggest a strong inclination towards relying on explicit score values. There is also evidence of people’s preference for one fairness metric, NDKL, over the other metric, ARP. Qualitative results paint a more complex picture: some participants endorse meritocratic award schemes and express concerns about fairness metrics being used to modify rankings; while other participants acknowledge socio-economic factors in score-based rankings as justification for adjusting rankings. In summary, we find that operationalizing algorithmic fairness in practice is a balancing act between mitigating harms towards marginalized groups and societal conventions of leveraging traditional performance scores such as grades in decision-making contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659018",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 31\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1940–1970",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing act: Evaluating people’s perceptions of fair ranking metrics",
		"URL": "https://doi.org/10.1145/3630106.3659018",
		"author": [
			{
				"family": "Alkhathlan",
				"given": "Mallak"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Shrestha",
				"given": "Hilson"
			},
			{
				"family": "Harrison",
				"given": "Lane"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hadaAkalBadiYa2024",
		"type": "paper-conference",
		"abstract": "Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659017",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1926–1939",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Akal badi ya bias: An exploratory study of gender bias in hindi language technology",
		"URL": "https://doi.org/10.1145/3630106.3659017",
		"author": [
			{
				"family": "Hada",
				"given": "Rishav"
			},
			{
				"family": "Husain",
				"given": "Safiya"
			},
			{
				"family": "Gumma",
				"given": "Varun"
			},
			{
				"family": "Diddee",
				"given": "Harshita"
			},
			{
				"family": "Yadavalli",
				"given": "Aditya"
			},
			{
				"family": "Seth",
				"given": "Agrima"
			},
			{
				"family": "Kulkarni",
				"given": "Nidhi"
			},
			{
				"family": "Gadiraju",
				"given": "Ujwal"
			},
			{
				"family": "Vashistha",
				"given": "Aditya"
			},
			{
				"family": "Seshadri",
				"given": "Vivek"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "weertsNeutralityFallacyWhen2024",
		"type": "paper-conference",
		"abstract": "Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to ‘algorithmic positive action’ under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to ‘not do harm’ towards a positive obligation to actively ‘do no harm’ as a more adequate framework for algorithmic decision-making and fair ml-interventions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659025",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2060–2070",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The neutrality fallacy: When algorithmic fairness interventions are (not) positive action",
		"URL": "https://doi.org/10.1145/3630106.3659025",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Xenidis",
				"given": "Raphaële"
			},
			{
				"family": "Tarissan",
				"given": "Fabien"
			},
			{
				"family": "Olsen",
				"given": "Henrik Palmer"
			},
			{
				"family": "Pechenizkiy",
				"given": "Mykola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "calviUnfairSidePrivacy2024",
		"type": "paper-conference",
		"abstract": "Data sharing in the European Union (EU) has gained new momentum, among others for machine learning (ML) and artificial intelligence (AI) training purposes. By enabling models’ training whilst preserving the privacy of data, Privacy Enhancing Technologies (PETs) have therefore gained popularity, especially among policymakers. So far, computer science research has focused on advancing state-of-the-art privacy engineering and exploring trade-offs between privacy and accuracy. Meanwhile, legal scholarship began investigating the challenges arising therefrom. Yet, few works have delved into the fairness implications of PETs. Further research is essential to both prevent the propagation of bias and discrimination and to limit the accumulation of market power within very few economic entities suitable to undermine fair competition and consumer rights. In our work, we will address this knowledge gap by adopting a legal and computer science point of view. After scoping our understanding of possible unfair sides of PETs based on technical and socio-legal understandings of fairness (Section 2), we provide an overview of PETs mostly relevant for ML and AI training (Section 3). We then discuss fairness-related challenges arising from their use (Section 4) and we suggest possible technical and regulatory (e.g., impact assessment, new rights) solutions to address the shortcomings identified (Section 5). We finally provide conclusions and ideas for future research (Section 6).",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659024",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2047–2059",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The unfair side of Privacy Enhancing Technologies: addressing the trade-offs between PETs and fairness",
		"URL": "https://doi.org/10.1145/3630106.3659024",
		"author": [
			{
				"family": "Calvi",
				"given": "Alessandra"
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			},
			{
				"family": "Kotzinos",
				"given": "Dimitris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "poeConflictAlgorithmicFairness2024",
		"type": "paper-conference",
		"abstract": "AI-based automated hiring systems cover a wide range of tools of varying complexity, from resume parsing tools to candidate selection models. Their close interference in economic and social life faces raising demands and investigations aiming to reduce the potential discrimination they may cause. This article covers the intersection of EU non-discrimination law and algorithmic fairness in the context of automated hiring systems. The paper analyzes the balance between equality of opportunity (formal and substantive) and equality of outcome, critiques the focus on non-conservative group fairness in machine learning, and discusses the legal implications of automated hiring systems under EU law. Additionally, it highlights often committed fallacies in relation to the process of de-biasing and advocates for a broader understanding of fairness in machine learning that aligns with EU legal standards and societal values.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659015",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1907–1916",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The conflict between algorithmic fairness and non-discrimination: An analysis of fair automated hiring",
		"URL": "https://doi.org/10.1145/3630106.3659015",
		"author": [
			{
				"family": "Poe",
				"given": "Robert Lee"
			},
			{
				"family": "El Mestari",
				"given": "Soumia Zohra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "niFairnessSensitiveAttributes2024",
		"type": "paper-conference",
		"abstract": "While model fairness improvement has been explored previously, existing methods invariably rely on adjusting explicit sensitive attribute values in order to improve model fairness in downstream tasks. However, we observe a trend in which sensitive demographic information becomes inaccessible as public concerns around data privacy grow. In this paper, we propose a confidence-based hierarchical classifier structure called “Reckoner” for reliable fair model learning under the assumption of missing sensitive attributes. We first present results showing that if the dataset contains biased labels or other hidden biases, classifiers significantly increase the bias gap across different demographic groups in the subset with higher prediction confidence. Inspired by these findings, we devised a dual-model system in which a version of the model initialised with a high-confidence data subset learns from a version of the model initialised with a low-confidence data subset, enabling it to avoid biased predictions. Our experimental results show that Reckoner consistently outperforms state-of-the-art baselines in COMPAS dataset and New Adult dataset, considering both accuracy and fairness metrics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659014",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1897–1906",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness without sensitive attributes via knowledge sharing",
		"URL": "https://doi.org/10.1145/3630106.3659014",
		"author": [
			{
				"family": "Ni",
				"given": "Hongliang"
			},
			{
				"family": "Han",
				"given": "Lei"
			},
			{
				"family": "Chen",
				"given": "Tong"
			},
			{
				"family": "Sadiq",
				"given": "Shazia"
			},
			{
				"family": "Demartini",
				"given": "Gianluca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "eversTalkingEachOther2024",
		"type": "paper-conference",
		"abstract": "This study examines the convergence of the European Commission (EC) and Big Tech companies’ (Google and Microsoft) discourse on ‘ethical’ AI through critical discourse analysis and the concept of hegemonic discourse. The paper answers the question to what extent there is a hegemonic discourse on ethical AI between EU policymakers and Big Tech companies and whether this is impacted by the prospect of legally binding legislation, considering the possible impact of the 2021 AI Act Proposal of the European Commission. This analysis is relevant at an inflection point where previous literature notes superficial convergence between the approaches of public and private actors, indicating policy consensus. The scope of analysis however is limited to non-legally binding regulation and lacks regional focus. In the EU, the advent of legally binding AI regulation with the 2021 AI Act (AIA) Proposal marks a critical juncture: with agreement on the AIA in December 2023, ethics standards become part of market entry requirements to the EU Single Market and the underlying differences in approaching Ethical AI will have important ramifications on policy preferences, compliance, enforcement and thought leadership in the domain more broadly. I find that the European discourse on ‘ethical’ AI by the EC and Big Tech companies such as Google and Microsoft is largely hegemonic and depoliticised in non-legally binding settings from 2018-2021 due to shared assumptions on ‘ethical’ AI and absence of significant underlying social and political conflict. It evolves to non-hegemonic and repoliticised discourse through dislocation by the prospect of legally binding regulation, which pushes actors to reveal their genuine policy preferences that bear political and social conflictuality whilst both actor types take an instrumental approach to ethics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659013",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1885–1896",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Talking past each other? Navigating discourse on ethical AI: Comparing the discourse on ethical AI policy by Big Tech companies and the European Commission",
		"URL": "https://doi.org/10.1145/3630106.3659013",
		"author": [
			{
				"family": "Evers",
				"given": "Cornelia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "orrAISportCompetitive2024",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) systems are evaluated using competitive methods that rely on benchmark datasets to determine performance. These benchmark datasets, however, are often constructed through arbitrary processes that fall short in encapsulating the depth and breadth of the tasks they are intended to measure. In this paper, we interrogate the naturalization of benchmark datasets as veracious metrics by examining the historical development of benchmarking as an epistemic practice in AI research. Specifically, we highlight three key case studies that were crucial in establishing the existing reliance on benchmark datasets for evaluating the capabilities of AI systems: (1) the sharing of Highleyman’s OCR dataset in the 1960s, which solidified a community of knowledge production around a shared benchmark dataset, (2) the Common Task Framework (CTF) of the 1980s, a state-led project to standardize benchmark datasets as legitimate indicators of technical progress; and (3) the Netflix Prize which further solidified benchmarking as a competitive goal within the ML research community. This genealogy highlights how contemporary dynamics and limitations of benchmarking developed from a longer history of collaboration, standardization, and competition. We end with reflections on how this history informs our understanding of benchmarking in the current era of generative artificial intelligence.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659012",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1875–1884",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI as a sport: On the competitive epistemologies of benchmarking",
		"URL": "https://doi.org/10.1145/3630106.3659012",
		"author": [
			{
				"family": "Orr",
				"given": "Will"
			},
			{
				"family": "Kang",
				"given": "Edward B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "guoMiMICRIDomaincenteredCounterfactual2024",
		"type": "paper-conference",
		"abstract": "The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659011",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1861–1874",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "MiMICRI: Towards domain-centered counterfactual explanations of cardiovascular image classification models",
		"URL": "https://doi.org/10.1145/3630106.3659011",
		"author": [
			{
				"family": "Guo",
				"given": "Grace"
			},
			{
				"family": "Deng",
				"given": "Lifu"
			},
			{
				"family": "Tandon",
				"given": "Animesh"
			},
			{
				"family": "Endert",
				"given": "Alex"
			},
			{
				"family": "Kwon",
				"given": "Bum Chul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "grillConstructingCapabilitiesPolitics2024",
		"type": "paper-conference",
		"abstract": "The advertised and perceived capabilities of generative AI products like ChatGPT have recently stimulated considerable investments and discourse surrounding their potential to aid and replace work. The prominence of these systems, and their promise to be general-purpose, has resulted in an avalanche of tests to discover and certify their capabilities. This new testing regime is concerned with creating ever-more tasks for generative AI products instead of testing a model for one specialized task. Beyond efforts to understand products’ capabilities, the construction of tasks and corresponding tests are also performative enactments meant to convince others and thus to gain attention, scientific legitimacy, and investment. The current market concentration of a few big AI companies points to a concerning conflict of interest: those with a vested interest in the success of the technology also have control over globalized testing infrastructures and thereby the exclusive means to create extensive knowledge claims about these systems. In this paper, I theorize capabilities as contested constructions and situated accomplishments shaped by power imbalances. I further unpack the globalized testing infrastructures involved in the construction and stabilization of generative AI products’ capabilities. Furthermore, I discuss how the testing of these AI models and products is externalized, extracting value from the unpaid or under-paid labor of researcher and developer communities, content creators, subcontractors, and users. Lastly, I discuss a reflexive and critical approach to testing that challenges depoliticization and seeks to produce lasting critiques that serve more emancipatory goals.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659009",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1838–1849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Constructing capabilities: The politics of testing infrastructures for generative AI",
		"URL": "https://doi.org/10.1145/3630106.3659009",
		"author": [
			{
				"family": "Grill",
				"given": "Gabriel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "balepurInterveningIncreaseCommunity2024",
		"type": "paper-conference",
		"abstract": "Refugees or immigrants who arrive in new countries often feel isolated. In this work, we examine how a resource-bounded public entity can make recommendations to increase integration of these new arrivals into a community. The community is made up of agents who engage in a strategic network formation process; agents join periodically — new arrivals are the refugees. The public entity meanwhile makes a limited number of edge-formation recommendations (according to its resource constraint) per iteration in order to increase integration of refugees. This work investigates the relationship between community trust and network fairness. First, we show that increasing the public entity’s resource allocation will not compensate for low trust in the community. Then, we introduce two trust-increasing interventions by the public entity: a targeted advertising campaign, and an announcement to increase transparency. We find that diverting a fraction (20%) of the public entity’s resources to a targeted advertising campaign can increase trust and fairness in the community, especially in low trust scenarios. We find that personalized, local announcements are more effective at increasing fairness than global announcements in low trust scenarios; they almost double our fairness metric in some cases. Importantly, the transparent announcement requires no extra resource expenditure on the part of the public entity. Our work underscores the importance of community trust — low trust cannot be compensated for with resources. This work provides theoretical support for these trust-increasing interventions, which we show can lead to increased integration of refugees in communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659008",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1827–1837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Intervening to increase community trust for fair network outcomes",
		"URL": "https://doi.org/10.1145/3630106.3659008",
		"author": [
			{
				"family": "Balepur",
				"given": "Naina"
			},
			{
				"family": "Sundaram",
				"given": "Hari"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "binkyteBaBEEnhancingFairness2024",
		"type": "paper-conference",
		"abstract": "We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each group. The decision can then be based directly on the estimated E. We show, by experiments on synthetic and real data sets, that our approach provides a good level of fairness as well as high accuracy.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659016",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1917–1925",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "BaBE: Enhancing fairness via estimation of explaining variables",
		"URL": "https://doi.org/10.1145/3630106.3659016",
		"author": [
			{
				"family": "Binkyte",
				"given": "Ruta"
			},
			{
				"family": "Gorla",
				"given": "Daniele"
			},
			{
				"family": "Palamidessi",
				"given": "Catuscia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "weertsUnlawfulProxyDiscrimination2024",
		"type": "paper-conference",
		"abstract": "Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts. This has important implications: unlike indirect discrimination, there is generally no ‘objective justification’ stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se. In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic. We draw on computer science literature to suggest that, in the algorithmic context, ‘treatment on the grounds of’ needs to be understood in terms of two steps: proxy capacity and proxy use. Only where both elements can be made out can direct discrimination be said to be ‘on grounds of’ a protected characteristic. We analyse the legal conditions of our proposed proxy capacity and proxy use tests. Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659010",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1850–1860",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unlawful proxy discrimination: a framework for challenging inherently discriminatory algorithms",
		"URL": "https://doi.org/10.1145/3630106.3659010",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Kelly-Lyth",
				"given": "Aislinn"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Adams-Prassl",
				"given": "Jeremias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kearneyUsecasesParticipatoryApproach2024",
		"type": "paper-conference",
		"abstract": "With a rising number of law enforcement agencies facing budgetary cuts, many turn to data science in an attempt to maintain service quality with fewer resources. A number of thus adopted solutions–including facial recognition, predictive policing, and risk assessments–have been contested by researchers and journalists alike. Yet comparatively little research is done at the strategy level, which determines where data science will be deployed in the first place. In this study, we interview 40 practitioners from Police Scotland, investigating what they believe to be crucial to successfully incorporate data science in their ways of working. Bucking the external trend, the participants distanced themselves from tools like facial recognition and risk assessment. Instead of focusing on individual use-cases, their primary concerns for the future were around (i) systemic issues around data is collection and use, (ii) goal misalignment between leadership and operational levels, (iii) the fear that datafication may undervalue important aspects of policing, and (iv) appropriate ways of interaction between data science teams and operational officers. Alongside the insights particular to Police Scotland, our work reaffirms how participatory approaches can go beyond the technical, and uncover structural and political barriers to success.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659007",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1809–1826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond use-cases: a participatory approach to envisioning data science in law enforcement",
		"URL": "https://doi.org/10.1145/3630106.3659007",
		"author": [
			{
				"family": "Kearney",
				"given": "Caitlin"
			},
			{
				"family": "Hron",
				"given": "Jiri"
			},
			{
				"family": "Kosc",
				"given": "Helen"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chanGroupFairnessGroup2024",
		"type": "paper-conference",
		"abstract": "Ensuring equitable impact of machine learning models across different societal groups is of utmost importance for real-world machine learning applications. Prior research in fairness has predominantly focused on adjusting model outputs through pre-processing, in-processing, or post-processing techniques. These techniques focus on correcting bias in either the data or the model. However, we argue that the bias in the data and model should be addressed in conjunction. To achieve this, we propose an algorithm called GroupDebias to reduce unfairness in the data in a model-guided fashion, thereby enabling models to exhibit more equitable behavior. Even though it is model-aware, the core idea of GroupDebias is independent of the model architecture, making it a versatile and effective approach that can be broadly applied across various domains and model types. Our method focuses on systematically addressing biases present in the training data itself by adaptively dropping samples that increase the biases in the model. Theoretically, the proposed approach enjoys a guaranteed improvement in demographic parity at the expense of a bounded reduction in balanced accuracy. A comprehensive evaluation of the GroupDebias algorithm through extensive experiments on diverse datasets and machine learning models demonstrates that GroupDebias consistently and significantly outperforms existing fairness enhancement techniques, achieving a more substantial reduction in unfairness with minimal impact on model performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659006",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1788–1808",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness via group consensus",
		"URL": "https://doi.org/10.1145/3630106.3659006",
		"author": [
			{
				"family": "Chan",
				"given": "Eunice"
			},
			{
				"family": "Liu",
				"given": "Zhining"
			},
			{
				"family": "Qiu",
				"given": "Ruizhong"
			},
			{
				"family": "Zhang",
				"given": "Yuheng"
			},
			{
				"family": "Maciejewski",
				"given": "Ross"
			},
			{
				"family": "Tong",
				"given": "Hanghang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "agarwalSystem2RecommendersDisentangling2024",
		"type": "paper-conference",
		"abstract": "Recommender systems are an important part of the modern human experience whose influence ranges from the food we eat to the news we read. Yet, there is still debate as to what extent online recommendation platforms are aligned with the goals of their users. A core issue fueling this debate is the challenge of inferring a user’s utility based on their engagement signals such as likes, shares, watch time etc., which are often the primary metric used by platforms to optimize content. This is because users’ utility-driven decision-processes (which we refer to as System-2), e.g., reading news that are accurate and relevant for them, are often confounded by their impulsive or unconscious decision-processes (which we refer to as System-1), e.g., spend time on click-bait news articles. As a result, it is difficult to infer whether an observed engagement is utility-driven or impulse-driven. In this paper we explore a new approach to recommender systems where we infer user’s utility based on their return probability to the platform rather than engagement signals. This approach is based on the intuition that users tend to return to a platform in the long run if it creates utility for them, while pure engagement-driven interactions, i.e., interactions that do not add meaningful utility, may affect user return in the short term but will not have a lasting effect. For this purpose, we propose a generative model in which past content interactions impact the arrival rates of users based on a self-exciting Hawkes process. These arrival rates to the platform are a combination of both System-1 and System-2 decision processes. The System-2 arrival intensity depends on the utility drawn from past content interactions and has a long lasting effect on return probability. In contrast, System-1 arrival intensity depends on the instantaneous gratification or moreishness and tends to vanish rapidly in time. We show analytically that given samples from this model it is provably possible to disentangle the System-1 and System-2 decision-processes and thus infer user’s utility, thereby allowing us to optimize content based on it. We conduct experiments on synthetic data to demonstrate the effectiveness of our approach over engagement optimization.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659004",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1763–1773",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "System-2 recommenders: Disentangling utility and engagement in recommendation systems via temporal point-processes",
		"URL": "https://doi.org/10.1145/3630106.3659004",
		"author": [
			{
				"family": "Agarwal",
				"given": "Arpit"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Lazaric",
				"given": "Alessandro"
			},
			{
				"family": "Nickel",
				"given": "Maximilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "majumdarCARMAPracticalFramework2024",
		"type": "paper-conference",
		"abstract": "Algorithms are increasingly used to automate large-scale decision-making processes, e.g., online platforms that make instant decisions in lending, hiring, and education. When such automated systems yield unfavorable decisions, it is imperative to allow for recourse by accompanying the instantaneous negative decisions with recommendations that can help affected individuals to overturn them. However, the practical challenges of providing algorithmic recourse in large-scale settings are not negligible: giving recourse recommendations that are actionable requires not only causal knowledge of the relationships between applicant features but also solving a complex combinatorial optimization problem for each rejected applicant. In this work, we introduce CARMA, a novel framework to generate causal recourse recommendations at scale. For practical settings with limited causal information, CARMA leverages pre-trained state-of-the-art causal generative models to find recourse recommendations. More importantly, CARMA addresses the scalability of finding these recommendations by casting the complex recourse optimization problem as a prediction task. By training a novel neural-network-based framework, CARMA efficiently solves the prediction task without requiring supervision for optimal recourse actions. Our extensive evaluations show that post-training, running inference on CARMA reliably amortizes causal recourse, generating optimal and instantaneous recommendations. CARMA exhibits flexibility, as its optimization is versatile with respect to the algorithmic decision-making and pre-trained causal generative models, provided their differentiability is ensured. Furthermore, we showcase CARMA in a case study, illustrating its ability to tailor causal recourse recommendations by readily incorporating population-level feature preferences based on factors such as difficulty or time needed.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659003",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1745–1762",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale",
		"URL": "https://doi.org/10.1145/3630106.3659003",
		"author": [
			{
				"family": "Majumdar",
				"given": "Ayan"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vannostrandActionableRecourseAutomated2024",
		"type": "paper-conference",
		"abstract": "Automated decision-making systems are increasingly deployed in domains such as hiring and credit approval where negative outcomes can have substantial ramifications for decision subjects. Thus, recent research has focused on providing explanations that help decision subjects understand the decision system and enable them to take actionable recourse to change their outcome. Popular counterfactual explanation techniques aim to achieve this by describing alterations to an instance that would transform a negative outcome to a positive one. Unfortunately, little user evaluation has been performed to assess which of the many counterfactual approaches best achieve this goal. In this work, we conduct a crowd-sourced between-subjects user study (N = 252) to examine the effects of counterfactual explanation type and presentation on lay decision subjects’ understandings of automated decision systems. We find that the region-based counterfactual type significantly increases objective understanding, subjective understanding, and response confidence as compared to the point-based type. We also find that counterfactual presentation significantly effects response time and moderates the effect of counterfactual type for response confidence, but not understanding. A qualitative analysis reveals how decision subjects interact with different explanation configurations and highlights unmet needs for explanation justification. Our results provide valuable insights and recommendations for the development of counterfactual explanation techniques towards achieving practical actionable recourse and empowering lay users to seek justice and opportunity in automated decision workflows.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658997",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1682–1700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable recourse for automated decisions: Examining the effects of counterfactual explanation type and presentation on lay user understanding",
		"URL": "https://doi.org/10.1145/3630106.3658997",
		"author": [
			{
				"family": "VanNostrand",
				"given": "Peter M."
			},
			{
				"family": "Hofmann",
				"given": "Dennis M."
			},
			{
				"family": "Ma",
				"given": "Lei"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "liesenfeldRethinkingOpenSource2024",
		"type": "paper-conference",
		"abstract": "The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here we use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), we find that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. We argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659005",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1774–1787",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking open source generative AI: open washing and the EU AI Act",
		"URL": "https://doi.org/10.1145/3630106.3659005",
		"author": [
			{
				"family": "Liesenfeld",
				"given": "Andreas"
			},
			{
				"family": "Dingemanse",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "whitneyRealRisksFake2024",
		"type": "paper-conference",
		"abstract": "Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promises a solution to these challenges. Instead of needing to collect photos of real people’s faces to train a facial recognition system, a model creator could create and use photo-realistic, synthetic faces. The comparative ease of generating this synthetic data rather than relying on collecting data has made it a common practice. We present two key risks of using synthetic data in model development. First, we detail the high risk of false confidence when using synthetic data to increase dataset diversity and representation. We base this in the examination of a real world use-case of synthetic data, where synthetic datasets were generated for an evaluation of facial recognition technology. Second, we examine how using synthetic data risks circumventing consent for data usage. We illustrate this by considering the importance of consent to the U.S. Federal Trade Commission’s regulation of data collection and affected models. Finally, we discuss how these two risks exemplify how synthetic data complicates existing governance and ethical practice; by decoupling data from those it impacts, synthetic data is prone to consolidating power away those most impacted by algorithmically-mediated harm.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659002",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1733–1744",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Real risks of fake data: Synthetic data, diversity-washing and consent circumvention",
		"URL": "https://doi.org/10.1145/3630106.3659002",
		"author": [
			{
				"family": "Whitney",
				"given": "Cedric Deslandes"
			},
			{
				"family": "Norman",
				"given": "Justin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "diberardinoAlgorithmicHarmsAlgorithmic2024",
		"type": "paper-conference",
		"abstract": "New artificial intelligence (AI) systems grounded in machine learning are being integrated into our lives at a rapid rate, but not without consequence: scholars across domains have increasingly pointed out issues related to privacy, transparency, bias, discrimination, exploitation, and exclusion associated with algorithmic systems in both public and private sector contexts. Concerns surrounding the adverse impacts of these technologies have spurred discussion on the topics of algorithmic harm. However, the overwhelming majority of articles on said harms offer no definition as to what constitutes ‘harm’ in these contexts. This paper aims to address this omission by introducing one criterion for a suitable account of algorithmic harm. More specifically, we follow Joel Feinberg in understanding harms as distinct from wrongs, where only the latter necessarily carry a normative dimension. This distinction highlights issues in the current scholarship surrounding the conflation of algorithmic harms and wrongs. In response to these issues, we put forth two requirements for upholding the harms/wrongs distinction when analyzing the increasingly far-reaching impacts of these technologies and suggest how this distinction can be useful in design, engineering, and policymaking.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659001",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 8\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1725–1732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic harms and algorithmic wrongs",
		"URL": "https://doi.org/10.1145/3630106.3659001",
		"author": [
			{
				"family": "Diberardino",
				"given": "Nathalie"
			},
			{
				"family": "Baleshta",
				"given": "Clair"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sullivanSIDEsSeparatingIdealization2024",
		"type": "paper-conference",
		"abstract": "Explainable AI (xAI) methods are important for establishing trust in using black-box models. However, recent criticism has mounted against current xAI methods that they disagree, are necessarily false, and can be manipulated, which has started to undermine the deployment of black-box models. Rudin (2019) goes so far as to say that we should stop using black-box models altogether in high-stakes cases because xAI explanations ‘must be wrong’. However, strict fidelity to the truth is historically not a desideratum in science. Idealizations–the intentional distortions introduced to scientific theories and models–are commonplace in the natural sciences and are seen as a successful scientific tool. Thus, it is not falsehood qua falsehood that is the issue. In this paper, I outline the need for xAI research to engage in idealization evaluation. Drawing on the use of idealizations in the natural sciences and philosophy of science, I introduce a novel framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods, and the distortions that they introduce, can be part of a successful idealization or are indeed deceptive distortions as critics suggest. I discuss the role that existing research can play in idealization evaluation and where innovation is necessary. Through a qualitative analysis we find that leading feature importance methods and counterfactual explanations are subject to idealization failure and suggest remedies for ameliorating idealization failure.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658999",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1714–1724",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SIDEs: Separating idealization from deceptive 'explanations' in xAI",
		"URL": "https://doi.org/10.1145/3630106.3658999",
		"author": [
			{
				"family": "Sullivan",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wrightNullComplianceNYC2024",
		"type": "paper-conference",
		"abstract": "In July 2023, New York City became the first jurisdiction globally to mandate bias audits for commercial algorithmic systems, specifically for automated employment decisions systems (AEDTs) used in hiring and promotion. Local Law 144 (LL 144) requires AEDTs to be independently audited annually for race and gender bias, and the audit report must be publicly posted. Additionally, employers are obligated to post a transparency notice with the job listing. In this study, 155 student investigators recorded 391 employers’ compliance with LL 144 and the user experience for prospective job applicants. Among these employers, 18 posted audit reports and 13 posted transparency notices. These rates could potentially be explained by a significant limitation in the accountability mechanisms enacted by LL 144. Since the law grants employers substantial discretion over whether their system is in scope of the law, a null result cannot be said to indicate non-compliance, a condition we call \"null compliance.\" Employer discretion may also explain our finding that nearly all audits reported an impact factor over 0.8, a rule of thumb often used in employment discrimination cases. We also find that the benefit of LL 144 to ordinary job seekers is limited due to shortcomings in accessibility and usability. Our findings offer important lessons for policy-makers as they consider regulating algorithmic systems, particularly the degree of discretion to grant to regulated parties and the limitations of relying on transparency and end-user accountability.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658998",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1701–1713",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Null compliance: NYC local law 144 and the challenges of algorithm accountability",
		"URL": "https://doi.org/10.1145/3630106.3658998",
		"author": [
			{
				"family": "Wright",
				"given": "Lucas"
			},
			{
				"family": "Muenster",
				"given": "Roxana Mika"
			},
			{
				"family": "Vecchione",
				"given": "Briana"
			},
			{
				"family": "Qu",
				"given": "Tianyao"
			},
			{
				"family": "Cai",
				"given": "Pika (Senhuang)"
			},
			{
				"family": "Smith",
				"given": "Alan"
			},
			{
				"family": "Investigators",
				"given": "Comm 2450 Student"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Matias",
				"given": "J. Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "koeneckeCarelessWhisperSpeechtotext2024",
		"type": "paper-conference",
		"abstract": "Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI’s Whisper, a state-of-the-art automated speech recognition service outperforming industry competitors, as of 2023. While many of Whisper’s transcriptions were highly accurate, we find that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as perpetuating violence, making up inaccurate associations, or implying false authority. We then study why hallucinations occur by observing the disparities in hallucination rates between speakers with aphasia (who have a lowered ability to express themselves using speech and voice) and a control group. We find that hallucinations disproportionately occur for individuals who speak with longer shares of non-vocal durations—a common symptom of aphasia. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases amplified by hallucinations in downstream applications of speech-to-text models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658996",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1672–1681",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Careless whisper: Speech-to-text hallucination harms",
		"URL": "https://doi.org/10.1145/3630106.3658996",
		"author": [
			{
				"family": "Koenecke",
				"given": "Allison"
			},
			{
				"family": "Choi",
				"given": "Anna Seo Gyeong"
			},
			{
				"family": "Mei",
				"given": "Katelyn X."
			},
			{
				"family": "Schellmann",
				"given": "Hilke"
			},
			{
				"family": "Sloane",
				"given": "Mona"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "starkAnimationArtificialIntelligence2024",
		"type": "paper-conference",
		"abstract": "Animation as genre is broadly used across many forms of digital media. In this paper, I argue ChatGPT and similar chatbots powered by Large Language Models (LLMs) can be best understood as animated characters. More than just cartooning, puppetry, or CGI, animation is a paradigm involving the projection of qualities perceived as human such as power, agency, will, and personality outside of the self and onto objects in the environment. Characteristics of animation—including reliance on stereotypes, obfuscation of human labor, and manipulation of an audience's emotions—can help us both analyze and respond appropriately to interactive AI technologies and the hyperbolic claims of their promoters.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658995",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1663–1671",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Animation and artificial intelligence",
		"URL": "https://doi.org/10.1145/3630106.3658995",
		"author": [
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "shiraliParticipatoryObjectiveDesign2024",
		"type": "paper-conference",
		"abstract": "In standard resource allocation problems, the designer sets the objective function, which captures the central allocation goal, in a top-down manner. The agents primarily participate in the allocation mechanism by reporting their preferences over the items; they cannot influence the objective once the designer sets it. Implicitly, this approach presumes that standard ways of eliciting the agents’ preferences adequately represent their true preferences—an assumption which does not hold if agents have preferences not just over the items they receive but also over the objective being optimized. For instance, agents may also have social preferences, such as inequality-aversion, altruism, or similar other-regarding behavior. We cannot express such preferences through standard cardinal utilities or ordinal rankings over the items the designer would typically elicit from the agents. This work examines how we can use this bottom-up preference elicitation stage to enable participants to express preferences over the objectives. We present a versatile framework that elicits agents’ preferences over a possible set of objectives and then minimally alters the underlying optimization problem to solve for a new objective that combines both the standard benchmark objective and the agents’ preferences for other objectives. We show how to evaluate this new participatory approach against the standard approach, using our notions of loss and gain in social welfare as well as individual tradeoffs. We illustrate the potency of this framework using a well-studied fair division problem where the designer aims to allocate m divisible items to n agents. In the standard setting, the designer optimizes for utilitarian social welfare, i.e., the sum of the agents’ cardinal utilities. We assume that some agents are also inequality-averse and may, therefore, have preferences for objectives that minimize inequality. Using the popular Fehr and Schmidt [31] model, we demonstrate how to map this fair division question to our framework, where the participatory approach optimizes both the standard utilitarian social welfare objective and the agents’ heterogeneous preferences over the level of inequality. We examine this problem theoretically to show that there can be large gains in social welfare if the designer uses this participatory approach. Further, we show that the loss in social welfare is linear in the level of inequality aversion and independent of the number of agents. We present a tighter bound in both cases under further natural assumptions on the preferences. We also examine the worst-case cost an individual agent might incur. Our results indicate that the loss in social welfare (measured by the standard objective) and gain in social welfare (measured by the participatory one) can favor the participatory approach in several natural settings. Throughout the work, we highlight various promising avenues for examining this participatory approach in the specific case study tackled in this paper and a broader range of resource allocation problems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658994",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 26\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1637–1662",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participatory objective design via preference elicitation",
		"URL": "https://doi.org/10.1145/3630106.3658994",
		"author": [
			{
				"family": "Shirali",
				"given": "Ali"
			},
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			},
			{
				"family": "Abebe",
				"given": "Rediet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "smallEqualisedOddsNot2024",
		"type": "paper-conference",
		"abstract": "Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different – a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving the chances of a positive outcome available to individuals from another sub-population, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model’s predictive power, individual fairness and robustness while ensuring group fairness.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658989",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1559–1578",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalised odds is not equal individual odds: Post-processing for group and individual fairness",
		"URL": "https://doi.org/10.1145/3630106.3658989",
		"author": [
			{
				"family": "Small",
				"given": "Edward"
			},
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Manning",
				"given": "Daniel"
			},
			{
				"family": "Salim",
				"given": "Flora D."
			},
			{
				"family": "Chan",
				"given": "Jeffrey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "loeffladHowTypesConsequences2024",
		"type": "paper-conference",
		"abstract": "In the context of the rise of algorithmic decision-making (ADM) systems, social scoring systems are particularly controversial. They aim to encourage socially desirable behaviors by rewarding people with a good score in various decision-making contexts. In this paper, we report the results of a survey following a social scoring experiment, to predominantly understand the impact of the scoring outcome and the decision importance on people’s perceptions and behavioral intentions within an abstract social scoring system. We find that the outcome was pivotal for creating opinion differences regarding people’s perceptions, and behavioral reactions. In contrast, the decision importance did not exert a systematic impact on people’s perceptions and behavioral reactions, but exacerbated existing opinion differences in terms of perceived effectiveness. Specifically, the outcome strongly shaped the structural relationship between people’s experiences, perceptions, and behavioral reactions, creating a substantial outcome favorability bias for people with a bad outcome. Although people with a bad outcome reported an intention to adapt their behaviors, their intention to engage in desired behaviors could not be attributed to a perceived legitimacy of the system. For those with a good outcome, perceptions of procedural justice and legitimacy were weakened by the privacy-invading character of the social scoring system. Our work shows that the outcome people receive might create a pivotal disparate impact on people’s overall attitudes towards social scoring, shape their behavioral reactions, and create divergent behavioral motives, suggesting that very distinct societal dynamics may arise.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658986",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1515–1530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How the types of consequences in social scoring systems shape people's perceptions and behavioral reactions",
		"URL": "https://doi.org/10.1145/3630106.3658986",
		"author": [
			{
				"family": "Loefflad",
				"given": "Carmen"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mirowskiRobotWalksBar2024",
		"type": "paper-conference",
		"abstract": "We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on “AI x Comedy” conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians’ motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to “cruise ship comedy material from the 1950s, but a bit less racist”. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, “offensive” language as a practice of resistance, satire and “punching up”. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists’ needs. Warning: this study may contain offensive language and discusses self-harm.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658993",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1622–1636",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A robot walks into a bar: Can language models serve as creativity SupportTools for comedy? An evaluation of llms’ humour alignment with comedians",
		"URL": "https://doi.org/10.1145/3630106.3658993",
		"author": [
			{
				"family": "Mirowski",
				"given": "Piotr"
			},
			{
				"family": "Love",
				"given": "Juliette"
			},
			{
				"family": "Mathewson",
				"given": "Kory"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sureshParticipationAgeFoundation2024",
		"type": "paper-conference",
		"abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of services, from banking to healthcare. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to historically marginalized groups. The larger scale and domain-agnostic manner in which these models operate further heightens the stakes: any errors or harms are liable to reoccur across use cases. In AI & ML more broadly, participatory approaches hold promise to lend agency and decision-making power to marginalized stakeholders, leading to systems that better benefit justice through equitable and distributed governance. But existing approaches in participatory AI/ML are typically grounded in a specific application and set of relevant stakeholders, and it is not straightforward how to apply these lessons to the context of foundation models. Our paper aims to fill this gap. First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the “foundation” layer, our framework proposes the “subfloor” layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain such as clinical care, journalism, or finance, and the “surface” (or application) layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate “subfloor” layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658992",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1609–1621",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participation in the age of foundation models",
		"URL": "https://doi.org/10.1145/3630106.3658992",
		"author": [
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Tseng",
				"given": "Emily"
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Gray",
				"given": "Mary"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ziosiEvidenceWhatWhom2024",
		"type": "paper-conference",
		"abstract": "This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool’s algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders’ positionality and political ends. Drawing inspiration from Catherine D’Ignazio’s taxonomy of “refusing and using” data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658991",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1596–1608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evidence of what, for whom? The socially contested role of algorithmic bias in a predictive policing tool",
		"URL": "https://doi.org/10.1145/3630106.3658991",
		"author": [
			{
				"family": "Ziosi",
				"given": "Marta"
			},
			{
				"family": "Pruss",
				"given": "Dasha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "deckCriticalSurveyFairness2024",
		"type": "paper-conference",
		"abstract": "In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 scientific articles on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. Importantly, we notice that claims are often (i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly aligned with the actual capabilities of XAI. We suggest to conceive XAI not as an ethical panacea but as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness. Moreover, when making a claim about XAI and fairness, we emphasize the need to be more specific about what kind of XAI method is used, which fairness desideratum it refers to, how exactly it enables fairness, and who is the stakeholder that benefits from XAI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658990",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1579–1595",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A critical survey on fairness benefits of explainable AI",
		"URL": "https://doi.org/10.1145/3630106.3658990",
		"author": [
			{
				"family": "Deck",
				"given": "Luca"
			},
			{
				"family": "Schoeffer",
				"given": "Jakob"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Kühl",
				"given": "Niklas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "madaioLearningResponsibleAI2024",
		"type": "paper-conference",
		"abstract": "Prior work has developed responsible AI (RAI) toolkits and studied how AI practitioners use such resources when practicing RAI. However, AI practitioners may not have the relevant skills or knowledge to effectively use RAI resources—particularly as pre-trained AI models have enabled more people to develop AI-based applications. In this paper, we explore current practices and aspirations for learning about RAI on-the-job, by interviewing 16 AI practitioners and 24 RAI educators across 16 organizations. We identify AI practitioners’ learning pathways for RAI, including information foraging and interpersonal learning; the orientations of RAI learning resources towards computational and procedural approaches to RAI; and aspirations for RAI learning, including desires for more sociotechnical approaches to understand potential harms of AI systems—aspirations that can be in tension with organizational priorities. We contribute empirical evidence of what and how AI practitioners are learning about RAI, and we suggest opportunities for the field to better support sociotechnical approaches to learning about RAI on-the-job.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658988",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1544–1558",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning about responsible AI on-the-job: Learning pathways, orientations, and aspirations",
		"URL": "https://doi.org/10.1145/3630106.3658988",
		"author": [
			{
				"family": "Madaio",
				"given": "Michael"
			},
			{
				"family": "Kapania",
				"given": "Shivani"
			},
			{
				"family": "Qadri",
				"given": "Rida"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Denton",
				"given": "Remi"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wolfeImpactOpportunitiesGenerative2024",
		"type": "paper-conference",
		"abstract": "Generative AI appears poised to transform white collar professions, with more than 90% of Fortune 500 companies using OpenAI’s flagship GPT models, which have been characterized as “general purpose technologies” capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658987",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1531–1543",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact and opportunities of generative AI in fact-checking",
		"URL": "https://doi.org/10.1145/3630106.3658987",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Mitra",
				"given": "Tanushree"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kinahanAchievingReproducibilityEEGbased2024",
		"type": "paper-conference",
		"abstract": "Despite the inherent complexity of electroencephalogram (EEG) data characterized by its high dimensionality, artifactual noise, and biological variability, many machine learning (ML) studies claim impressive performance in decoding or classifying EEG signals. Recently, several studies have highlighted that flawed data analysis is a prevalent issue in the literature, leading to irreproducible results and exaggerated claims. To address this issue, we propose a framework that addresses three primary obstacles in EEG ML research: data leakage, data scarcity, and flawed model selection. We introduce the EEG ML Model Card, a standardized and transparent EEG ML model documentation tool that aims to directly address these pitfalls and enhance reproducibility and trustworthiness in EEG ML research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658983",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1464–1474",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving reproducibility in EEG-based machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658983",
		"author": [
			{
				"family": "Kinahan",
				"given": "Sean"
			},
			{
				"family": "Saidi",
				"given": "Pouria"
			},
			{
				"family": "Daliri",
				"given": "Ayoub"
			},
			{
				"family": "Liss",
				"given": "Julie"
			},
			{
				"family": "Berisha",
				"given": "Visar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "huangCollectiveConstitutionalAI2024",
		"type": "paper-conference",
		"abstract": "There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658979",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 23\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1395–1417",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Collective constitutional AI: Aligning a language model with public input",
		"URL": "https://doi.org/10.1145/3630106.3658979",
		"author": [
			{
				"family": "Huang",
				"given": "Saffron"
			},
			{
				"family": "Siddarth",
				"given": "Divya"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "akpinarImpactDifferentialFeature2024",
		"type": "paper-conference",
		"abstract": "Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such “differential feature under-reporting” as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, little is known about the setting of data missingness absent indicators (i.e. differential feature under-reporting). In this work, we study an analytically tractable model of differential feature under-reporting to characterizethe impact of under-report on algorithmic fairness. We demonstrate how standard missing data methods typically fail to mitigate bias in this setting, and propose a new set of augmented loss and imputation methods. Our results show that, in real world data settings, under-reporting typically exacerbates disparities. The proposed solution methods show some success in mitigating disparities attributable to feature under-reporting.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658977",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 28\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1355–1382",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of differential feature under-reporting on algorithmic fairness",
		"URL": "https://doi.org/10.1145/3630106.3658977",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "alphertsPerceptiveVisualUrban2024",
		"type": "paper-conference",
		"abstract": "The use of Computer Vision, through a Perceptive Visual Urban Analytics (VUA) paradigm, has been proposed as a way for municipalities to more easily monitor their cities. However, prior studies fall short of actually investigating whether Perceptive VUA is ready for municipal use. In this paper we take a critical look at this paradigm by comparing key methods and evaluating them on usability and trustworthiness with municipal experts as well as Responsible AI and Computer Vision researchers. Based on on this evaluation we find that Perceptive VUA is not (yet) ready for municipal use as they do not incorporate domain knowledge and overly rely on spurious correlations. We conclude by providing recommendations for how to progress Perceptive VUA such that it may actually contribute to improving the liveability and quality of urban environments.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658976",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1341–1354",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptive visual urban analytics is not (yet) suitable for municipalities",
		"URL": "https://doi.org/10.1145/3630106.3658976",
		"author": [
			{
				"family": "Alpherts",
				"given": "Tim"
			},
			{
				"family": "Ghebreab",
				"given": "Sennay"
			},
			{
				"family": "Hsu",
				"given": "Yen-Chia"
			},
			{
				"family": "Van Noord",
				"given": "Nanne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "turriTransparencyWildNavigating2024",
		"type": "paper-conference",
		"abstract": "Transparency is a critical component when building artificial intelligence (AI) decision-support tools, especially for contexts in which AI outputs impact people or policy. Effectively identifying and addressing user transparency needs in practice remains a challenge. While a number of guidelines and processes for identifying transparency needs have emerged, existing methods tend to approach need-finding with a limited focus that centers around a narrow set of stakeholders and transparency techniques. To broaden this perspective, we employ numerous need-finding methods to investigate transparency mechanisms in a widely deployed AI-decision support tool developed by a wildlife conservation non-profit. Throughout our 5-month case study, we conducted need-finding through semi-structured interviews with end-users, analysis of the tool’s community forum, experiments with their ML model, and analysis of training documents created by end-users. We also held regular meetings with the tool’s product and machine learning teams. By approaching transparency need-finding from a broad lens, we uncover insights into end-users’ transparency needs as well as unexpected uses and challenges with current transparency mechanisms. Our study is one of the first to incorporate such diverse perspectives to reveal an unbiased and rich view of transparency needs. Lastly, we offer the FAccT community recommendations on broadening transparency need-finding approaches, contributing to the evolving field of transparency research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658985",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1494–1514",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency in the wild: Navigating transparency in a deployed AI system to broaden need-finding approaches",
		"URL": "https://doi.org/10.1145/3630106.3658985",
		"author": [
			{
				"family": "Turri",
				"given": "Violet"
			},
			{
				"family": "Morrison",
				"given": "Katelyn"
			},
			{
				"family": "Robinson",
				"given": "Katherine-Marie"
			},
			{
				"family": "Abidi",
				"given": "Collin"
			},
			{
				"family": "Perer",
				"given": "Adam"
			},
			{
				"family": "Forlizzi",
				"given": "Jodi"
			},
			{
				"family": "Dzombak",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wangInvestigatingDesigningTrust2024",
		"type": "paper-conference",
		"abstract": "Trust is a crucial factor for the adoption and responsible usage of generative AI tools in complex tasks such as software engineering. However, we have a limited understanding of how software developers evaluate the trustworthiness of AI-powered code generation tools in real-world settings. To address this gap, we conducted Study 1, an interview study with 17 developers who use AI-powered code generation tools in professional or personal settings. We found that developers’ trust is rooted in the AI tool’s perceived ability, integrity, and benevolence, and is situational, varying according to the context of usage. Existing AI code generation tools lack the affordances for developers to efficiently and effectively evaluate the trustworthiness of AI-powered code generation tools. To explore designs that can augment the existing interface of AI-powered code generation tools, we explored three sets of design concepts (suggestion quality indicators, usage stats, and control mechanisms) that derived from Study 1 findings. In Study 2, a design probe study with 12 developers, we investigated the potential of these design concepts to help developers make effective trust judgments. We discuss the implication of our findings on the design of AI-powered code generation tools and future research on trust in AI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658984",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1475–1493",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating and designing for trust in AI-powered code generation tools",
		"URL": "https://doi.org/10.1145/3630106.3658984",
		"author": [
			{
				"family": "Wang",
				"given": "Ruotong"
			},
			{
				"family": "Cheng",
				"given": "Ruijia"
			},
			{
				"family": "Ford",
				"given": "Denae"
			},
			{
				"family": "Zimmermann",
				"given": "Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "antoniakNLPMaternalHealthcare2024",
		"type": "paper-conference",
		"abstract": "Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658982",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1446–1463",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "NLP for maternal healthcare: Perspectives and guiding principles in the age of llms",
		"URL": "https://doi.org/10.1145/3630106.3658982",
		"author": [
			{
				"family": "Antoniak",
				"given": "Maria"
			},
			{
				"family": "Naik",
				"given": "Aakanksha"
			},
			{
				"family": "Alvarado",
				"given": "Carla S."
			},
			{
				"family": "Wang",
				"given": "Lucy Lu"
			},
			{
				"family": "Chen",
				"given": "Irene Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kraftKnowledgeenhancedLanguageModels2024",
		"type": "paper-conference",
		"abstract": "The factual inaccuracies (\"hallucinations\") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of \"objective\" or \"neutral\" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of \"knowledge\" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658981",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1433–1445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Knowledge-enhanced language models are not bias-proof: Situated knowledge and epistemic injustice in AI",
		"URL": "https://doi.org/10.1145/3630106.3658981",
		"author": [
			{
				"family": "Kraft",
				"given": "Angelie"
			},
			{
				"family": "Soulier",
				"given": "Eloı̈se"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "baumannFairnessOnlineAd2024",
		"type": "paper-conference",
		"abstract": "Advertising funds a number of services that play a major role in our everyday online experiences, from social networking, to maps, search, and news. As the power and reach of advertising platforms grow, so do the concerns about the potential for discrimination associated with targeted advertising. However, despite our ever-improving ability to measure and describe instances of unfair distribution of high-stakes ads—such as employment, housing, or credit—we lack the tools to model and predict the extent to which alternative systems could address such problems. In this paper, we simulate an ad distribution system to model the effects that enforcing popularly proposed fairness approaches would have on the utility of the advertising platforms and their users. We show that in many realistic scenarios, achieving statistical parity would come at a much higher utility cost to platforms than enforcing predictive parity or equality of opportunity. Additionally, we identify a tradeoff between different notions of fairness, i.e., enforcing one criterion leads to worse outcomes with respect to other criteria. We further describe how pursuing fairness in situations where one group of users is more expensive to advertise to is likely to result in “leveling down” effects, i.e., not benefiting any group of users. We show that these negative effects can be prevented by ensuring that it is the platforms that carry the cost of fairness rather than passing it on to their users or advertisers. Overall, our findings contribute to ongoing discussions on fair ad delivery. We show that fairness is not satisfied by default, that limiting targeting options is not sufficient to address potential discrimination and bias in online ad delivery, and that choices made by regulators and platforms may backfire if potential side-effects are not properly considered.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658980",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1418–1432",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in online ad delivery",
		"URL": "https://doi.org/10.1145/3630106.3658980",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Sapiezynski",
				"given": "Piotr"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Hannak",
				"given": "Aniko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mashiatEvictionPredictionLeveraging2024",
		"type": "paper-conference",
		"abstract": "There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts – informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with a recent history of evictions. We also discuss the importance of neighborhood and ownership features in both risk prediction and targeted outreach.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658978",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1383–1394",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond eviction prediction: Leveraging local spatiotemporal public records to inform action",
		"URL": "https://doi.org/10.1145/3630106.3658978",
		"author": [
			{
				"family": "Mashiat",
				"given": "Tasfia"
			},
			{
				"family": "DiChristofano",
				"given": "Alex"
			},
			{
				"family": "Fowler",
				"given": "Patrick J."
			},
			{
				"family": "Das",
				"given": "Sanmay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leeLargeLanguageModels2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658975",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1321–1340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Large language models portray socially subordinate groups as more homogeneous, consistent with a bias observed in humans",
		"URL": "https://doi.org/10.1145/3630106.3658975",
		"author": [
			{
				"family": "Lee",
				"given": "Messi H.J."
			},
			{
				"family": "Montgomery",
				"given": "Jacob M."
			},
			{
				"family": "Lai",
				"given": "Calvin K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "prinosSpeakingAccentContent2024",
		"type": "paper-conference",
		"abstract": "Automatic speech recognition (ASR) researchers are working to address the differing transcription performance of ASR by accent or dialect. However, research often has a limited view of accent in ways that reproduce discrimination and limit the scope of potential solutions. In this paper we present a content analysis of 22 papers published in 2022 in top conferences and journals on the topic of accent and ASR. We report on how accent is sometimes mistakenly viewed as something some people don’t have; as having a default; and being an attribute only of the speaker, and not of the listener. We discuss the implications on research and provide recommendations to researchers who hope to reduce ASR biases by accent.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658969",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1245–1254",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Speaking of accent: A content analysis of accent misconceptions in ASR research",
		"URL": "https://doi.org/10.1145/3630106.3658969",
		"author": [
			{
				"family": "Prinos",
				"given": "Kerri"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Power",
				"given": "Cathleen A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "birhaneDarkSideDataset2024",
		"type": "paper-conference",
		"abstract": "‘Scale the model, scale the data, scale the GPU farms’ is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658968",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1229–1244",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dark side of dataset scaling: Evaluating racial classification in multimodal models",
		"URL": "https://doi.org/10.1145/3630106.3658968",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Dehdashtian",
				"given": "Sepehr"
			},
			{
				"family": "Prabhu",
				"given": "Vinay"
			},
			{
				"family": "Boddeti",
				"given": "Vishnu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wolfeLaboratoryscaleAIOpenweight2024",
		"type": "paper-conference",
		"abstract": "The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear. We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658966",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1199–1210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Laboratory-scale AI: Open-weight models are competitive with ChatGPT even in low-resource settings",
		"URL": "https://doi.org/10.1145/3630106.3658966",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Slaughter",
				"given": "Isaac"
			},
			{
				"family": "Han",
				"given": "Bin"
			},
			{
				"family": "Wen",
				"given": "Bingbing"
			},
			{
				"family": "Yang",
				"given": "Yiwei"
			},
			{
				"family": "Rosenblatt",
				"given": "Lucas"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Brown",
				"given": "Eva"
			},
			{
				"family": "Qu",
				"given": "Zening"
			},
			{
				"family": "Weber",
				"given": "Nic"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "simsonOneModelMany2024",
		"type": "paper-conference",
		"abstract": "A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. The downstream effects of ADM systems critically depend on the decisions made during a systems’ design, implementation, and evaluation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these decisions are made implicitly, without knowing exactly how they will influence the final system. To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit decisions during design and evaluation into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible “universes” of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can investigate the variability and robustness of fairness scores and see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand fairness implications of design and evaluation decisions using an exemplary case study of predicting public health care coverage for vulnerable populations. Our results highlight how decisions regarding the evaluation of a system can lead to vastly different fairness metrics for the same model. This is problematic, as a nefarious actor could optimise or “hack” a fairness metric to portray a discriminating model as fair merely by changing how it is evaluated. We illustrate how a multiverse analysis can help to address this issue.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658974",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1305–1320",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One model many scores: Using multiverse analysis to prevent fairness hacking and evaluate the influence of model design decisions",
		"URL": "https://doi.org/10.1145/3630106.3658974",
		"author": [
			{
				"family": "Simson",
				"given": "Jan"
			},
			{
				"family": "Pfisterer",
				"given": "Florian"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "widderEpistemicPowerAI2024",
		"type": "paper-conference",
		"abstract": "What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate? Based on 75 interviews with technologists including researchers, developers, open source contributors, and activists, this paper explores the various epistemic bases from which AI ethics is discussed and practiced. In the context of outside attacks on AI ethics as an impediment to “progress,” I show how some AI ethics practices have reached toward authority from automation and quantification, and achieved some legitimacy as a result, while those based on richly embodied and situated lived experience have not. This paper draws together the work of feminist Anthropology and Science and Technology Studies scholars Diana Forsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices. By entrenching the epistemic power of quantification, dominant AI ethics practices—employing Model Cards and similar interventions—risk legitimizing AI ethics as a project in equal and opposite measure to which they marginalize lived experience as a legitimate part of the same project. In response, I propose humble technical practices: quantified or technical practices which specifically seek to make their epistemic limits clear in order to flatten hierarchies of epistemic power.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658973",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1295–1304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic power in AI ethics labor: Legitimizing located complaints",
		"URL": "https://doi.org/10.1145/3630106.3658973",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mhasawadeCausalPerspectiveLabel2024",
		"type": "paper-conference",
		"abstract": "Predictive models developed with machine learning techniques are commonly used to inform decision making and resource allocation in high-stakes contexts, such as healthcare and public health. One means through which this practice may propagate equity-related harms is when the data used for model development or evaluation exhibits label bias. In such cases, the target of prediction is a proxy label of a construct of interest that may be difficult or impossible to measure, while the relationship between the proxy and the construct of interest differs systematically across subgroups. Label bias can be especially challenging to identify and mitigate in practice because consequential fairness violations are masked when the model is evaluated with respect to the proxy label. In this work, we aim to develop further formal understanding of label bias to inform the development of approaches for the identification and mitigation of it. To do so, we present desiderata for unbiased and biased proxy labels, introduce candidate causal graphical criteria for label bias, and consider the extent to which proxy labels can be used to reason about fairness with respect to a true construct of interest. We validate our findings with a simulation study and experiments with synthetic health insurance data used in the context of a care management system.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658972",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1282–1294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A causal perspective on label bias",
		"URL": "https://doi.org/10.1145/3630106.3658972",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "D'Amour",
				"given": "Alexander"
			},
			{
				"family": "Pfohl",
				"given": "Stephen R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "schorMeaningfulTransparencyClinicians2024",
		"type": "paper-conference",
		"abstract": "AI systems can bring great benefits to our healthcare systems, e.g. by improving patient outcomes. Yet implementing them into clinical practice remains challenging. To bridge the gap between academic research and design implementation, we argue clinicians need transparency about such systems that is meaningful—i.e. contextually appropriate—to them. Towards this, we explore recent HCXAI recommendations for building transparent AI systems for users in a specific domain: gynaecology. By better understanding clinicians’ perspectives on meaningful transparency, our aim is to complement and help operationalise such recommendations. We conduct a co-design workshop and interviews with n=15 gynaecologists in the UK and the Netherlands. We show that HCXAI must better account for clinical teams with different types of gynaecologist users, and that the timeliness and relevance of the information provided about the AI-based tool throughout its design lifecycle—in particular before a tool is implemented into clinical practice—is crucial for transparency to become meaningful. Our contributions include: i) testing recommendations from the latest HCXAI literature with a prospective, real-life AI application in a relatively less-studied clinical domain; ii) describing and visualising gynaecologists’ understanding of meaningful transparency for clinicians; iii) outlining four design recommendations towards realising meaningful transparency for clinicians and opportunities for research; and iv) expanding HCI and AI research in women’s health by directly engaging with gynaecologists as users and co-designers. Exploring such issues is key to facilitate the implementation of AI systems that meet clinicians’ information needs and that they can trust.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658971",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1268–1281",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meaningful transparency for clinicians: Operationalising HCXAI research with gynaecologists",
		"URL": "https://doi.org/10.1145/3630106.3658971",
		"author": [
			{
				"family": "Schor",
				"given": "Bianca Giulia Sarah"
			},
			{
				"family": "Kallina",
				"given": "Emma"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Blackwell",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "terzisLawEmergingPolitical2024",
		"type": "paper-conference",
		"abstract": "For almost a decade now, scholarship in and beyond the ACM FAccT community has been focusing on novel and innovative ways and methodologies to audit the functioning of algorithmic systems. Over the years, this research idea and technical project has matured enough to become a regulatory mandate. Today, the Digital Services Act (DSA) and the Online Safety Act (OSA) have established the framework within which technology corporations and (traditional) auditors will develop the ‘practice’ of algorithmic auditing thereby presaging how this ‘ecosystem’ will develop. In this paper, we systematically review the auditing provisions in the DSA and the OSA in light of observations from the emerging industry of algorithmic auditing. Who is likely to occupy this space? What are some political and ethical tensions that are likely to arise? How are the mandates of ‘independent auditing’ or ‘the evaluation of the societal context of an algorithmic function’ likely to play out in practice? By shaping the picture of the emerging political economy of algorithmic auditing, we draw attention to strategies and cultures of traditional auditors that risk eroding important regulatory pillars of the DSA and the OSA. Importantly, we warn that ambitious research ideas and technical projects of/for algorithmic auditing may end up crashed by the standardising grip of traditional auditors and/or diluted within a complex web of (sub-)contractual arrangements, diverse portfolios, and tight timelines.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658970",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1255–1267",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Law and the emerging political economy of algorithmic audits",
		"URL": "https://doi.org/10.1145/3630106.3658970",
		"author": [
			{
				"family": "Terzis",
				"given": "Petros"
			},
			{
				"family": "Veale",
				"given": "Michael"
			},
			{
				"family": "Gaumann",
				"given": "Noëlle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moayeriWorldBenchQuantifyingGeographic2024",
		"type": "paper-conference",
		"abstract": "As large language models (LLMs) continue to improve and gain popularity, some may use the models to recall facts, despite well documented limitations with LLM factuality. Towards ensuring that models work reliably for all, we seek to uncover if geographic disparities emerge when asking an LLM the same question about different countries. To this end, we present WorldBench, a dynamic and flexible benchmark composed of per-country data from the World Bank. In extensive experiments on state of the art open and closed source models, including GPT-4, Gemini, Llama-2, and Vicuna, to name a few, we find significant biases based on region and income level. For example, error rates are 1.5 times higher for countries from Sub-Saharan Africa compared to North American countries. We observe these disparities to be consistent over 20 LLMs and 11 individual World Bank indicators (i.e. specific statistics, such as population or CO2 emissions). WorldBench also enables automatic detection of citation hallucination, where models cite the World Bank itself while providing false statistics, and a manner to assess when an LLM’s stored facts begin to go out of date. We hope our benchmark will draw attention to geographic disparities in existing LLMs and facilitate the remedying of these biases.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658967",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1211–1228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "WorldBench: Quantifying geographic disparities in LLM factual recall",
		"URL": "https://doi.org/10.1145/3630106.3658967",
		"author": [
			{
				"family": "Moayeri",
				"given": "Mazda"
			},
			{
				"family": "Tabassi",
				"given": "Elham"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "manziniShouldUsersTrust2024",
		"type": "paper-conference",
		"abstract": "As AI assistants become increasingly sophisticated and deeply integrated into our lives, questions of trust rise to the forefront. In this paper, we build on philosophical studies of trust to investigate when user trust in AI assistants is justified. By moving beyond a focus on the technical artefact in isolation, we consider the broader societal system in which AI assistants are developed and deployed. We conceptualise user trust in AI assistants as encompassing two main targets, namely AI assistants and their developers. We argue that – as AI assistants become more human like and exhibit increased agency – discerning when user trust is justified requires consideration not only of competence, on the part of AI assistants and their developers, but also alignment between the competing interests, values or incentives of AI assistants, developers and users. To help users understand if and when their trust in the competence and alignment of AI assistants and developers is justified, we propose a sociotechnical approach that requires evidence to be collected at three levels: AI assistant design, organisational practices and third-party governance. Taken together, these measures can help harness the transformative potential of AI assistants while also ensuring their operation is ethical and value aligned.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658964",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1174–1186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should users trust advanced AI assistants? Justified trust as a function of competence and alignment",
		"URL": "https://doi.org/10.1145/3630106.3658964",
		"author": [
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Keeling",
				"given": "Geoff"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leuAuditingImagebasedNSFW2024",
		"type": "paper-conference",
		"abstract": "This paper examines NSFW (Not Safe For Work) image classifiers for content filtering. Through an audit of three prevalent NSFW classifiers, we analyze the relationship between NSFW predictions and three demographic factors: gender, skin-tone, and age. Our study reveals that women are disproportionately more frequently misclassified as NSFW compared to men, even when they appear conducting common daily-life activities. Additionally, we find that NSFW classifiers tend to mispredict images of people with lighter skin-tones and images depicting younger people. We explore the causes of such mispredictions by analyzing the explanatory pixel maps, which reveal some of the reasons behind the misclassifications. Overall, the implications of our findings become particularly salient when considering the application of filters based on NSFW classifiers, which we identified to have a direct impact on image datasets, computer vision models, generative AI, user experience, and artistic creativity. In summary, we hope our study brings attention to the inherent biases within NSFW classifiers and underscores the importance of addressing these issues to ensure fair and equitable outcomes in content filtering.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658963",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1163–1173",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing image-based NSFW classifiers for content filtering",
		"URL": "https://doi.org/10.1145/3630106.3658963",
		"author": [
			{
				"family": "Leu",
				"given": "Warren"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "cachelPreFAIRCombiningPartial2024",
		"type": "paper-conference",
		"abstract": "Preference aggregation mechanisms help decision-makers combine diverse preference rankings produced by multiple voters into a single consensus ranking. Prior work has developed methods for aggregating multiple rankings into a fair consensus over the same set of candidates. Yet few real-world problems present themselves as such precisely formulated aggregation tasks with each voter fully ranking all candidates. Instead, preferences are often expressed as rankings over partial and even disjoint subsets of candidates. For instance, hiring committee members typically opt to rank their top choices instead of exhaustively ordering every single job applicant. However, the existing literature does not offer a framework for characterizing nor ensuring group fairness in such partial preference aggregation tasks. Unlike fully ranked settings, partial preferences imply both a selection decision of whom to rank plus an ordering decision of how to rank the selected candidates. Our work fills this gap by conceptualizing the open problem of fair partial preference aggregation. We introduce an impossibility result for fair selection from partial preferences and design a computational framework showing how we can navigate this obstacle. Inspired by Single Transferable Voting, our proposed solution PreFair produces consensus rankings that are fair in the selection of candidates and also in their relative ordering. Our experimental study demonstrates that PreFair achieves the best performance in this dual fairness objective compared to state-of-the-art alternatives adapted to this new problem while still satisfying voter preferences.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658961",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1133–1149",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "PreFAIR: Combining partial preferences for fair consensus decision-making",
		"URL": "https://doi.org/10.1145/3630106.3658961",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vengroffImpactChartsTool2024",
		"type": "paper-conference",
		"abstract": "We introduce impact charts and apply them to problems of systematic bias encoded in three different data sets. Impact charts are highly visual, making the effects they find easy to understand by both domain experts and non-experts. Impact charts are nonlinear and non-parametric, so they are able to identify structural biases whose functional forms are not a priori well understood. Impact charts are based on SHAP, an interpretability method initially designed to interpret predictions made by Machine Learning (ML) models, which is in turn based on Shapley values, an approach to assigning responsibility for economic outcomes to different factors. Although impact charts use techniques from the ML community, they are intended for use in general settings, whether ML is present or not. Impact charts provide valuable insights even when generated from aggregate data sets. Aggregate data sets typically provide the individuals whose data they are derived from an additional level of privacy as compared to the original unaggregated data. In this work, we relied predominantly on aggregate data from the U.S. Census Bureau, which is known to have a robust privacy program. We introduce and evaluate impact charts using three examples of their use. Our first example uses impact charts to identify racial and ethnic bias in eviction rates. Our second example uses U.S. Census data to identify racial and ethnic bias in housing prices. Our third example assesses the impact of several factors on local access to supermarkets. All three examples not only correct for the effects of income, but also clearly demonstrate the relative impact of income as compared to racial and ethnic features. For example, we demonstrate that in some areas like DeKalb County, GA, the fraction of the population that is Black impacts eviction rates three times more than income does. In addition to the impact charts specifically discussed herein, we have produced thousands of geographically localized impact charts for the data sets mentioned above. There is wide variation in the shape and structure of impact plots built using data from different local areas. We hypothesize that in the future work we will be able to categorize these and identify local policy decisions, whether de jure or de facto that cause the differences from one area to the next.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658965",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1187–1198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impact charts: a tool for identifying systematic bias in social systems and data",
		"URL": "https://doi.org/10.1145/3630106.3658965",
		"author": [
			{
				"family": "Vengroff",
				"given": "Darren Erik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "abduAlgorithmicTransparencyParticipation2024",
		"type": "paper-conference",
		"abstract": "Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust. But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded. Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts. We adopt two theoretical frames, Mulligan and Nissenbaum’s handoff model and Star and Griesemer’s boundary objects, to reveal such shifts during the U.S. Census Bureau’s adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census. This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process. Using publicly available documents concerning the Census’ implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout. We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658962",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1150–1162",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic transparency and participation through the handoff lens: Lessons learned from the U.S. census bureau’s adoption of differential privacy",
		"URL": "https://doi.org/10.1145/3630106.3658962",
		"author": [
			{
				"family": "Abdu",
				"given": "Amina A."
			},
			{
				"family": "Chambers",
				"given": "Lauren M."
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			},
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kaushalAutomatedTransparencyLegal2024",
		"type": "paper-conference",
		"abstract": "The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (‘statements of reasons’ - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658960",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1121–1132",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automated transparency: a legal and empirical analysis of the digital services act transparency database",
		"URL": "https://doi.org/10.1145/3630106.3658960",
		"author": [
			{
				"family": "Kaushal",
				"given": "Rishabh"
			},
			{
				"family": "Van De Kerkhof",
				"given": "Jacob"
			},
			{
				"family": "Goanta",
				"given": "Catalina"
			},
			{
				"family": "Spanakis",
				"given": "Gerasimos"
			},
			{
				"family": "Iamnitchi",
				"given": "Adriana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "grovesAuditingWorkExploring2024",
		"type": "paper-conference",
		"abstract": "In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law’s practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of ‘auditor roles’ that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658959",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1107–1120",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing work: Exploring the new york city algorithmic bias audit regime",
		"URL": "https://doi.org/10.1145/3630106.3658959",
		"author": [
			{
				"family": "Groves",
				"given": "Lara"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Kennedy",
				"given": "Alayna"
			},
			{
				"family": "Vecchione",
				"given": "Briana"
			},
			{
				"family": "Strait",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "devrioBuildingShiftingEmploying2024",
		"type": "paper-conference",
		"abstract": "A large body of research has attempted to ensure that algorithmic systems adhere to notions of fairness and transparency. Increasingly, researchers have highlighted that mitigating algorithmic harms requires explicitly taking power structures into account. Those with power over algorithmic systems often fail to sufficiently address algorithmic harms and rarely consult those directly harmed by algorithmic systems. Left to their own devices, people respond to algorithmic harms they encounter in a wide variety of ways, but we lack broader, overarching understandings of these responses. In this work, we synthesize documented, historical cases into a taxonomy of responses “from below” to algorithmic harm. Our taxonomy connects different types of responses to existing theorizations of power from fields including anthropology, human-computer interaction, and communication, centering how people employ, shift, and build power in their responses to algorithmic harm. Based on our taxonomy, we highlight an opportunity space for the FAccT community to engage with and support such action from below.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658958",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1093–1106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building, shifting, & employing power: a taxonomy of responses from below to algorithmic harm",
		"URL": "https://doi.org/10.1145/3630106.3658958",
		"author": [
			{
				"family": "DeVrio",
				"given": "Alicia"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lamFrameworkAssuranceAudits2024",
		"type": "paper-conference",
		"abstract": "An increasing number of regulations propose ‘AI audits’ as a mechanism for achieving transparency and accountability for artificial intelligence (AI) systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently lacks agreed-upon practices, procedures, taxonomies, and standards. We propose the ‘criterion audit’ as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations’ ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which ‘bias audits’ can be performed for in-scope hiring algorithms, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering a critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion—informed by experiences in performing these audits in practice—highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of audits.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658957",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1078–1092",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for assurance audits of algorithmic systems",
		"URL": "https://doi.org/10.1145/3630106.3658957",
		"author": [
			{
				"family": "Lam",
				"given": "Khoa"
			},
			{
				"family": "Lange",
				"given": "Benjamin"
			},
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Davidovic",
				"given": "Jovana"
			},
			{
				"family": "Brown",
				"given": "Shea"
			},
			{
				"family": "Hasan",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "maedaWhenHumanAIInteractions2024",
		"type": "paper-conference",
		"abstract": "With the continuous improvement of large language models (LLMs), chatbots can produce coherent and continuous word sequences that mirror natural human language. While the use of natural language and human-like conversation styles enables the use of chatbots within a range of everyday settings, these usability-enhancing features can also have unintended consequences, such as making fallible information seem trustworthy by emphasizing friendliness and closeness. This can have serious implications for information retrieval tasks performed with chatbots. In this paper, we provide an overview of the literature on parasociality, social affordance, and trust to bridge these concepts within human-AI interactions. We critically examine how chatbot “roleplaying” and user role projection co-produce a pseudo-interactive, technologically-mediated space with imbalanced dynamics between users and chatbots. Based on the review of the literature, we develop a conceptual framework of parasociality in chatbots that describes interactions between humans and anthropomorphized chatbots. We dissect how chatbots use personal pronouns, conversational conventions, affirmations, and similar strategies to position the chatbots as users’ companions or assistants, and how these tactics induce trust-forming behaviors in users. Finally, based on the conceptual framework, we outline a set of ethical concerns that emerge from parasociality, including illusions of reciprocal engagement, task misalignment, and leaks of sensitive information. This paper argues that these possible consequences arise from a positive feedback cycle wherein anthropomorphized chatbot features encourage users to fill in the context around predictive outcomes.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658956",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1068–1077",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When human-AI interactions become parasocial: Agency and anthropomorphism in affective design",
		"URL": "https://doi.org/10.1145/3630106.3658956",
		"author": [
			{
				"family": "Maeda",
				"given": "Takuya"
			},
			{
				"family": "Quan-Haase",
				"given": "Anabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bhardwajMachineLearningData2024",
		"type": "paper-conference",
		"abstract": "Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658955",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1055–1067",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning data practices through a data curation lens: An evaluation framework",
		"URL": "https://doi.org/10.1145/3630106.3658955",
		"author": [
			{
				"family": "Bhardwaj",
				"given": "Eshta"
			},
			{
				"family": "Gujral",
				"given": "Harshit"
			},
			{
				"family": "Wu",
				"given": "Siyi"
			},
			{
				"family": "Zogheib",
				"given": "Ciara"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Becker",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "cherepanovaDeepDiveDataset2023",
		"type": "paper-conference",
		"abstract": "As the deployment of automated face recognition (FR) systems proliferates, bias in these systems is not just an academic question, but a matter of public concern. Media portrayals often center imbalance as the main source of bias, i.e., that FR models perform worse on images of non-white people or women because these demographic groups are underrepresented in training data. Recent academic research paints a more nuanced picture of this relationship. However, previous studies of data imbalance in FR have focused exclusively on the face verification setting, while the face identification setting has been largely ignored, despite being deployed in sensitive applications such as law enforcement. This is an unfortunate omission, as ‘imbalance’ is a more complex matter in identification; imbalance may arise in not only the training data, but also the testing data, and furthermore may affect the proportion of identities belonging to each demographic group or the number of images belonging to each identity. In this work, we address this gap in the research by thoroughly exploring the effects of each kind of imbalance possible in face identification, and discuss other factors which may impact bias in this setting.",
		"container-title": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3600211.3604691",
		"event-place": "Montr\\'{e}al QC Canada",
		"event-title": "AIES '23: AAAI/ACM Conference on AI, Ethics, and Society",
		"ISBN": "979-8-4007-0231-0",
		"language": "en",
		"page": "229-247",
		"publisher": "ACM",
		"publisher-place": "Montr\\'{e}al QC Canada",
		"source": "DOI.org (Crossref)",
		"title": "A Deep Dive into Dataset Imbalance and Bias in Face Identification",
		"URL": "https://dl.acm.org/doi/10.1145/3600211.3604691",
		"author": [
			{
				"family": "Cherepanova",
				"given": "Valeriia"
			},
			{
				"family": "Reich",
				"given": "Steven"
			},
			{
				"family": "Dooley",
				"given": "Samuel"
			},
			{
				"family": "Souri",
				"given": "Hossein"
			},
			{
				"family": "Dickerson",
				"given": "John"
			},
			{
				"family": "Goldblum",
				"given": "Micah"
			},
			{
				"family": "Goldstein",
				"given": "Tom"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					7,
					14
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					8
				]
			]
		}
	},
	{
		"id": "agizaPoliTuneAnalyzingImpact2024",
		"type": "article-journal",
		"abstract": "In an era where language models are increasingly integrated into decision-making and communication, understanding the biases within Large Language Models (LLMs) becomes imperative, especially when these models are applied in the economic and political domains. This work investigates the impact of fine-tuning and data selection on economic and political biases in LLMs. In this context, we introduce PoliTune, a fine-tuning methodology to explore the systematic aspects of aligning LLMs with specific ideologies, mindful of the biases that arise from their extensive training on diverse datasets. Distinct from earlier efforts that either focus on smaller models or entail resource-intensive pre-training, PoliTune employs Parameter-Efficient Fine-Tuning (PEFT) techniques, which allow for the alignment of LLMs with targeted ideologies by modifying a small subset of parameters. We introduce a systematic method for using the open-source LLM Llama3-70B for dataset selection, annotation, and synthesizing a preferences dataset for Direct Preference Optimization (DPO) to align the model with a given political ideology. We assess the effectiveness of PoliTune through both quantitative and qualitative evaluations of aligning open-source LLMs (Llama3-8B and Mistral-7B) to different ideologies. Our work analyzes the potential of embedding specific biases into LLMs and contributes to the dialogue on the ethical application of AI, highlighting the importance of deploying AI in a manner that aligns with societal values.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "2-12",
		"source": "ojs.aaai.org",
		"title": "PoliTune: Analyzing the Impact of Data Selection and Fine-Tuning on Economic and Political Biases in Large Language Models",
		"title-short": "PoliTune",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31612",
		"volume": "7",
		"author": [
			{
				"family": "Agiza",
				"given": "Ahmed"
			},
			{
				"family": "Mostagir",
				"given": "Mohamed"
			},
			{
				"family": "Reda",
				"given": "Sherief"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "akbulutAllTooHuman2024",
		"type": "article-journal",
		"abstract": "The development of highly-capable conversational agents, underwritten by large language models, has the potential to shape user interaction with this technology in profound ways, particularly when the technology is anthropomorphic, or appears human-like. Although the effects of anthropomorphic AI are often benign, anthropomorphic design features also create new kinds of risk. For example, users may form emotional connections to human-like AI, creating the risk of infringing on user privacy and autonomy through over-reliance. To better understand the possible pitfalls of anthropomorphic AI systems, we make two contributions: first, we explore anthropomorphic features that have been embedded in interactive systems in the past, and leverage this precedent to highlight the current implications of anthropomorphic design. Second, we propose research directions for informing the ethical design of anthropomorphic AI. In advancing  the responsible development of AI, we promote approaches to the ethical foresight, evaluation, and mitigation of harms arising from user interactions with anthropomorphic AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "13-26",
		"source": "ojs.aaai.org",
		"title": "All Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI",
		"title-short": "All Too Human?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31613",
		"volume": "7",
		"author": [
			{
				"family": "Akbulut",
				"given": "Canfer"
			},
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "alcarazEstimatingWeightsReasons2024",
		"type": "article-journal",
		"abstract": "We present a new approach to representation and acquisition of normative information for machine ethics. It combines an influential philosophical account of the fundamental structure of morality with argumentation theory and machine learning. According to the philosophical account, the deontic status of an action -- whether it is required, forbidden, or permissible -- is determined through the interaction of \"normative reasons\" of varying strengths or weights. We first provide a formal characterization of this account, by modeling it in (weighted) argumentation graphs. We then use it to model ethical learning: the basic idea is to use a set of cases for which deontic statuses are known to estimate the weights of normative reasons in operation in these cases, and to use these weight estimates to determine the deontic statuses of actions in new cases. The result is an approach that has the advantages of both bottom-up and top-down approaches to machine ethics: normative information is acquired through the interaction with training data, and its meaning is clear. We also report the results of some initial experiments with the model.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "27-38",
		"source": "ojs.aaai.org",
		"title": "Estimating Weights of Reasons Using Metaheuristics: A Hybrid Approach to Machine Ethics",
		"title-short": "Estimating Weights of Reasons Using Metaheuristics",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31614",
		"volume": "7",
		"author": [
			{
				"family": "Alcaraz",
				"given": "Benoît"
			},
			{
				"family": "Knoks",
				"given": "Aleks"
			},
			{
				"family": "Streit",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arnoldIntroducingAIGovernance2024",
		"type": "article-journal",
		"abstract": "AI-related laws, standards, and norms are emerging rapidly. However, a lack of shared descriptive concepts and monitoring infrastructure undermine efforts to track, understand, and improve AI governance. We introduce AGORA (the AI Governance and Regulatory Archive), a rigorously compiled and enriched dataset of AI-focused laws and policies encompassing diverse jurisdictions, institutions, and contexts related to AI. AGORA is oriented around an original taxonomy describing risks, potential harms, governance strategies, incentives for compliance, and application domains addressed in AI regulatory documents. At launch, AGORA included data on over 330 instruments, with new entries being added continuously. We describe the manual and automated processes through which these data are systematically compiled, screened, annotated, and validated, enabling deep, efficient, and reliable analysis of the emerging AI governance landscape. The dataset, supporting information, and analyses are available through a public web interface (https://agora.eto.tech) and bulk dataset.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "39-48",
		"source": "ojs.aaai.org",
		"title": "Introducing the AI Governance and Regulatory Archive (AGORA): An Analytic Infrastructure for Navigating the Emerging AI Governance Landscape",
		"title-short": "Introducing the AI Governance and Regulatory Archive (AGORA)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31615",
		"volume": "7",
		"author": [
			{
				"family": "Arnold",
				"given": "Zachary"
			},
			{
				"family": "Schiff",
				"given": "Daniel S."
			},
			{
				"family": "Schiff",
				"given": "Kaylyn Jackson"
			},
			{
				"family": "Love",
				"given": "Brian"
			},
			{
				"family": "Melot",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Neha"
			},
			{
				"family": "Jenkins",
				"given": "Lindsay"
			},
			{
				"family": "Lin",
				"given": "Ashley"
			},
			{
				"family": "Pilz",
				"given": "Konstantin"
			},
			{
				"family": "Enweareazu",
				"given": "Ogadinma"
			},
			{
				"family": "Girard",
				"given": "Tyler"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arzaghiUnderstandingIntrinsicSocioeconomic2024",
		"type": "article-journal",
		"abstract": "Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications. Warning: This paper discusses and contains content that can be offensive or upsetting.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "49-60",
		"source": "ojs.aaai.org",
		"title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31616",
		"volume": "7",
		"author": [
			{
				"family": "Arzaghi",
				"given": "Mina"
			},
			{
				"family": "Carichon",
				"given": "Florian"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arzbergerNothingComesIts2024",
		"type": "article-journal",
		"abstract": "Work on value alignment aims to ensure that human values\nare respected by AI systems. However, existing approaches\ntend to rely on universal framings of human values that obscure\nthe question of which values the systems should capture\nand align with, given the variety of operational situations.\nThis often results in AI systems that privilege only a\nselected few while perpetuating problematic norms grounded\non biases, ultimately causing equity and justice issues. In this\nperspective paper, we unpack the limitations of predominant\nalignment practices of reinforcement learning from human\nfeedback (RLHF) for LLMs through the lens of situated values.\nWe build on feminist epistemology to argue that at the\ndesign-time, RLHF has problems with representation in the\nsubjects providing feedback and implicitness in the conceptualization\nof values and situations of real-world users while\nlacking system adaptation to real user situations at the use time.\nTo address these shortcomings, we propose three research\ndirections: 1) situated annotation to capture information\nabout the crowdworker’s and user’s values and judgments\nin relation to specific situations at both the design and\nuse-time, 2) expressive instruction to encode plural values\nfor instructing LLMs systems at design-time, and 3) reflexive\nadaptation to leverage situational knowledge for system\nadaption at use-time. We conclude by reflecting on the\npractical challenges of pursuing these research directions and\nsituated value alignment of AI more broadly.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "61-73",
		"source": "ojs.aaai.org",
		"title": "Nothing Comes Without Its World – Practical Challenges of Aligning LLMs to Situated Human Values through RLHF",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31617",
		"volume": "7",
		"author": [
			{
				"family": "Arzberger",
				"given": "Anne"
			},
			{
				"family": "Buijsman",
				"given": "Stefan"
			},
			{
				"family": "Lupetti",
				"given": "Maria Luce"
			},
			{
				"family": "Bozzon",
				"given": "Alessandro"
			},
			{
				"family": "Yang",
				"given": "Jie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "attiaKidWhisperBridgingPerformance2024",
		"type": "article-journal",
		"abstract": "Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn’t readily extend to ASR for children due to the lim- ited availability of suitable child-specific databases and the distinct characteristics of children’s speech. A recent study investigated leveraging the My Science Tutor (MyST) chil- dren’s speech corpus to enhance Whisper’s performance in recognizing children’s speech. They were able to demon- strate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children’s ASR performance and the effect of fine-tuning in improving the transcription of disfluent speech.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "74-80",
		"source": "ojs.aaai.org",
		"title": "Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults",
		"title-short": "Kid-Whisper",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31618",
		"volume": "7",
		"author": [
			{
				"family": "Attia",
				"given": "Ahmed Adel"
			},
			{
				"family": "Liu",
				"given": "Jing"
			},
			{
				"family": "Ai",
				"given": "Wei"
			},
			{
				"family": "Demszky",
				"given": "Dorottya"
			},
			{
				"family": "Espy-Wilson",
				"given": "Carol"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bansakPublicAttitudesPerformance2024",
		"type": "article-journal",
		"abstract": "This study explores public preferences between algorithmic and human decision-makers (DMs) in high-stakes contexts, how these preferences are impacted by performance metrics, and whether the public's evaluation of performance differs when considering algorithmic versus human DMs. Leveraging a conjoint experimental design, respondents (n = 9,000) chose between pairs of DM profiles in two scenarios: pre-trial release decisions and bank loan decisions. DM profiles varied on the DM’s type (human vs. algorithm) and on three metrics—defendant crime rate/loan default rate, false positive rate (FPR) among white defendants/applicants, and FPR among minority defendants/applicants—as well as an implicit (un)fairness metric defined by the absolute difference between the two FPRs. Controlling for performance, we observe a general tendency to favor human DMs, though this is driven by a subset of respondents who expect human DMs to perform better in the real world, and there is an analogous group with the opposite preference for algorithmic DMs. We also find that the relative importance of the four performance metrics remains consistent across DM type, suggesting that the public's preferences related to DM performance do not vary fundamentally between algorithmic and human DMs. Taken together, the results collectively suggest that people have very different beliefs about what type of DM (human or algorithmic) will deliver better performance and should be preferred, but they have similar desires in terms of what they want that performance to be regardless of DM type.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "81-81",
		"source": "ojs.aaai.org",
		"title": "Public Attitudes on Performance for Algorithmic and Human Decision-Makers (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31619",
		"volume": "7",
		"author": [
			{
				"family": "Bansak",
				"given": "Kirk"
			},
			{
				"family": "Paulson",
				"given": "Elisabeth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "barnettSimulatingPolicyImpacts2024",
		"type": "article-journal",
		"abstract": "The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "82-93",
		"source": "ojs.aaai.org",
		"title": "Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation",
		"title-short": "Simulating Policy Impacts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31620",
		"volume": "7",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			},
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bartschOriginOpportunitiesDevelopers2024",
		"type": "article-journal",
		"abstract": "Open source (OS) software projects in artificial intelligence (AI), such as TensorFlow and scikit-learn, depend on developers' continuous, voluntary code contributions. However, recent security incidents highlighted substantial risks in such software, requiring examinations of factors motivating developers to continuously contribute high-quality code (i.e., providing secure and reliable code fulfilling its functions). Prior research suggests code accountability (i.e., requirements to explain and justify contributed code) to improve code quality, enforced through external accountability mechanisms such as sanctions and rewards. However, the OS domain often lacks such mechanisms, questioning whether and how code accountability arises in this domain and how it affects code contributions. To address these questions, we conducted 26 semi-structured interviews with developers contributing to OS AI software projects. Our findings reveal that despite the absence of external accountability mechanisms, system-, project-, and individual-related factors evoke developers' perceived code accountability. Notably, we discovered a trade-off as high perceived code accountability is associated with higher code quality but discourages developers from participating in OS AI software projects. Overall, this study contributes to understanding the nuanced roles of perceived code accountability in continuously contributing high-quality code without external accountability mechanisms and highlights the complex trade-offs developers face in OS AI software projects.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "94-106",
		"source": "ojs.aaai.org",
		"title": "The Origin and Opportunities of Developers’ Perceived Code Accountability in Open Source AI Software Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31621",
		"volume": "7",
		"author": [
			{
				"family": "Bartsch",
				"given": "Sebastian Clemens"
			},
			{
				"family": "Lother",
				"given": "Moritz"
			},
			{
				"family": "Schmidt",
				"given": "Jan-Hendrik"
			},
			{
				"family": "Adam",
				"given": "Martin"
			},
			{
				"family": "Benlian",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "berettaGenderPixelsPathways2024",
		"type": "article-journal",
		"abstract": "In the field of Computer Vision (CV), the study of bias, including gender bias, has received a significant area of attention in recent years. However, these studies predominantly operate within a binary, cisnormative framework, often neglecting the complexities of non-binary gender identities. To date, there is no comprehensive analysis of how CV is addressing the mitigation of bias for non-binary individuals or how it seeks solutions that transcend a binary view of gender. This systematic scoping review aims to fill this gap by analyzing over 60 papers that delve into gender biases in CV, with a particular emphasis on non-binary perspectives. Our findings indicate that despite the increasing recognition of gender as a multifaceted and complex construct, practical applications of this understanding in CV remain limited and fragmented. The review critically examines the foundational research critiquing the binarism in CV and explores emerging approaches that challenge and move beyond this limited perspective. We highlight innovative solutions, including algorithmic adaptations and the creation of more inclusive and diverse datasets. Furthermore, the study emphasizes the importance of integrating gender theory into CV practices to develop more accurate and representative models.\nOur recommendations advocate for interdisciplinary collaboration, particularly with Gender Studies, to foster a more nuanced understanding of gender in CV. This study serves as a pivotal step towards redefining gender representation in CV, encouraging researchers and practitioners to embrace and incorporate a broader spectrum of gender identities in their work.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "107-119",
		"source": "ojs.aaai.org",
		"title": "Gender in Pixels: Pathways to Non-binary Representation in Computer Vision",
		"title-short": "Gender in Pixels",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31622",
		"volume": "7",
		"author": [
			{
				"family": "Beretta",
				"given": "Elena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bignottiLegalMindsAlgorithmic2024",
		"type": "article-journal",
		"abstract": "In this paper, we conduct an empirical analysis of how large language models (LLMs), specifically GPT-4, interpret constitutional principles in complex decision-making scenarios.\nWe examine rulings from the Italian Constitutional Court on bioethics issues that involve trade-offs between competing values and compare GPT’s legal arguments on these issues to those presented by the State, the Court, and the applicants.\nOur results indicate that GPT consistently aligns more closely with progressive interpretations of the Constitution, often overlooking competing values and mirroring the applicants’ views rather than the more conservative perspectives of the State or the Court’s moderate positions. Our findings raise important questions about the value alignment of LLMs in scenarios where societal values are in conflict, as our experiment demonstrates GPT’s tendency to align with progressive legal interpretations. We thus underscore the importance of testing alignment in real-world scenarios and considering the implications of deploying LLMs in decision-making processes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "120-130",
		"source": "ojs.aaai.org",
		"title": "Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios",
		"title-short": "Legal Minds, Algorithmic Decisions",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31623",
		"volume": "7",
		"author": [
			{
				"family": "Bignotti",
				"given": "Camilla"
			},
			{
				"family": "Camassa",
				"given": "Carolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "biscontiFormalAccountTrustworthiness2024",
		"type": "article-journal",
		"abstract": "This paper proposes a formal account of AI trustworthiness, connecting both intrinsic and perceived trustworthiness in an operational schematization. We argue that trustworthiness extends beyond the inherent capabilities of an AI system to include significant influences from observers' perceptions, such as perceived transparency, agency locus, and human oversight. While the concept of perceived trustworthiness is discussed in the literature, few attempts have been made to connect it with the intrinsic trustworthiness of AI systems. Our analysis introduces a novel schematization to quantify trustworthiness by assessing the discrepancies between expected and observed behaviors and how these affect perceived uncertainty and trust. The paper provides a formalization for measuring trustworthiness, taking into account both perceived and intrinsic characteristics. By detailing the factors that influence trust, this study aims to foster more ethical and widely accepted AI technologies, ensuring they meet both functional and ethical criteria.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "131-140",
		"source": "ojs.aaai.org",
		"title": "A Formal Account of Trustworthiness: Connecting Intrinsic and Perceived Trustworthiness",
		"title-short": "A Formal Account of Trustworthiness",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31624",
		"volume": "7",
		"author": [
			{
				"family": "Bisconti",
				"given": "Piercosma"
			},
			{
				"family": "Aquilino",
				"given": "Letizia"
			},
			{
				"family": "Marchetti",
				"given": "Antonella"
			},
			{
				"family": "Nardi",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "blili-hamelinUnsocialIntelligenceInvestigation2024",
		"type": "article-journal",
		"abstract": "Dreams of machines rivaling human intelligence have shaped the field of AI since its inception. Yet, the very meaning of human-level AI or artificial general intelligence (AGI) remains elusive and contested. Definitions of AGI embrace a diverse range of incompatible values and assumptions. Contending with the fractured worldviews of AGI discourse is vital for critiques that pursue different values and futures. To that end, we provide a taxonomy of AGI definitions, laying the ground for examining the key social, political, and ethical assumptions they make. We highlight instances in which these definitions frame AGI or human-level AI as a technical topic and expose the value-laden choices being implicitly made. Drawing on feminist, STS, and social science scholarship on the political and social character of intelligence in both humans and machines, we propose contextual, democratic, and participatory paths to imagining future forms of machine intelligence. The development of future forms of AI must involve explicit attention to the values it encodes, the people it includes or excludes, and a commitment to epistemic justice.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "141-155",
		"source": "ojs.aaai.org",
		"title": "Unsocial Intelligence: An Investigation of the Assumptions of AGI Discourse",
		"title-short": "Unsocial Intelligence",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31625",
		"volume": "7",
		"author": [
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "boerstlerStabilityMoralPreferences2024",
		"type": "article-journal",
		"abstract": "Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants’ responses are stable over time such that, all other things being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants’ true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don’t track. If participants’ moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how participants’ true moral preferences, opinions, and judgments can be ascertained. We address this possibility here by asking the same survey participants the same moral questions about which patient should receive a kidney when only one is available ten times in ten different sessions over two weeks, varying only presentation order across sessions. We measured how often participants gave different responses to simple (Study One) and more complicated (Study Two) controversial and uncontroversial repeated scenarios. On average, the fraction of times participants changed their responses to controversial scenarios (i.e., were unstable) was around 10-18% (±14-15%) across studies, and this instability is observed to have positive associations with response time and decision-making difficulty. We discuss the implications of these results for the efficacy of common moral preference elicitation methods, highlighting the role of response instability in potentially causing value misalignment between the stakeholders and AI tools trained on their moral judgments.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "156-167",
		"source": "ojs.aaai.org",
		"title": "On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods",
		"title-short": "On The Stability of Moral Preferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31626",
		"volume": "7",
		"author": [
			{
				"family": "Boerstler",
				"given": "Kyle"
			},
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Chan",
				"given": "Lok"
			},
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "boguckaCodesigningAIImpact2024",
		"type": "article-journal",
		"abstract": "In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of AI systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "168-180",
		"source": "ojs.aaai.org",
		"title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31627",
		"volume": "7",
		"author": [
			{
				"family": "Bogucka",
				"given": "Edyta"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Šćepanović",
				"given": "Sanja"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniFoundationModelTransparency2024",
		"type": "article-journal",
		"abstract": "Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "181-195",
		"source": "ojs.aaai.org",
		"title": "Foundation Model Transparency Reports",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31628",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Klyman",
				"given": "Kevin"
			},
			{
				"family": "Longpre",
				"given": "Shayne"
			},
			{
				"family": "Xiong",
				"given": "Betty"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Maslej",
				"given": "Nestor"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniEcosystemGraphsDocumenting2024",
		"type": "article-journal",
		"abstract": "Foundation models (e.g. GPT-4, Gemini, Llama 3) pervasively influence society, warranting greater understanding. While the models garner much attention, accurately characterizing their impact requires considering the broader sociotechnical ecosystem in which they are created and deployed. We propose Ecosystem Graphs as a documentation framework to centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical and social relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata, such as the model’s estimated training emissions or licensing guidelines. Since its release in March 2023, Ecosystem Graphs represents an ongoing effort to document 568 assets (112 datasets, 359 models, 97 applications) from 117 organizations. Ecosystem Graphs functions as a multifunctional resource: we discuss two major uses by the 2024 AI Index and the UK’s Competition and Markets Authority that demonstrate the value of Ecosystem Graphs.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "196-209",
		"source": "ojs.aaai.org",
		"title": "Ecosystem Graphs: Documenting the Foundation Model Supply Chain",
		"title-short": "Ecosystem Graphs",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31629",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Soylu",
				"given": "Dilara"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Creel",
				"given": "Kathleen A."
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniTrustworthySocialBias2024",
		"type": "article-journal",
		"abstract": "How do we design measures of social bias that we trust?\nWhile prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. In this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. To combat the frequently fuzzy treatment of social bias in natural language processing, we explicitly define social bias, grounded in principles drawn from social science research. We operationalize our definition by proposing a general bias measurement framework DivDist, which we use to instantiate 5 concrete bias measures. To validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in US employment?). Through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "210-224",
		"source": "ojs.aaai.org",
		"title": "Trustworthy Social Bias Measurement",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31630",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bristowViewsAIArent2024",
		"type": "article-journal",
		"abstract": "Recent developments in AI have brought broader attention to tensions between two overlapping communities, “AI Ethics” and “AI Safety.” In this article we (i) characterize this false binary, (ii) argue that a simple binary is not an accurate model of AI discourse, and (iii) provide concrete suggestions for how individuals can help avoid the emergence of us-vs-them conflict in the broad community of people working on AI development and governance. While we focus on “AI Ethics” and “AI Safety,” the general lessons apply to related tensions, including those between accelerationist (“e/acc”) and cautious stances on AI development.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "225-225",
		"source": "ojs.aaai.org",
		"title": "Views on AI Aren't Binary — They’re Plural (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31631",
		"volume": "7",
		"author": [
			{
				"family": "Bristow",
				"given": "Thorin"
			},
			{
				"family": "Thorburn",
				"given": "Luke"
			},
			{
				"family": "Acosta-Navas",
				"given": "Diana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "brownQualitativeStudyCultural2024",
		"type": "article-journal",
		"abstract": "Understanding the future consequences of artificial intelligence requires a holistic consideration of its cultural dimensions, on par with its technological intricacies and potential applications. Individuals and institutions working closely with AI, and with considerable resources, have significant influence on how impact is considered, particularly with regard to how much attention is paid to epistemic concerns (including issues of bias in datasets or potential misinterpretations of data, for example) versus normative concerns (such as societal and ecological effects of AI in the medium- and long-term). In this paper we review qualitative studies conducted with AI researchers and developers to understand how they position themselves relative to each of these two dimensions of impact, and how geographies and conditions of work influence their positions. Our findings underscore the need to gather more perspectives from low- and middle-income countries, whose notions of impact extend beyond the immediate technical concerns or impacts in the short- to medium-term. Rather, they encapsulate a broader spectrum of impact considerations, including the deleterious effects perpetrated by global corporate entities, the unwarranted influence of wealthy nations, the encroachment of philanthrocapitalism, and the adverse consequences of excluding communities affected by these phenomena from active participation in discussions surrounding impact.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "226-238",
		"source": "ojs.aaai.org",
		"title": "A Qualitative Study on Cultural Hegemony and the Impacts of AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31632",
		"volume": "7",
		"author": [
			{
				"family": "Brown",
				"given": "Venetia"
			},
			{
				"family": "Larasati",
				"given": "Retno"
			},
			{
				"family": "Third",
				"given": "Aisling"
			},
			{
				"family": "Farrell",
				"given": "Tracie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "carpenterFDAAIPitfalls2024",
		"type": "article-journal",
		"abstract": "Observers and practitioners of artificial intelligence (AI) have proposed an FDA-style licensing regime for the most advanced AI models, or 'frontier' models.  In this paper, we explore the applicability of approval regulation -- that is, regulation of a product that combines experimental minima with government licensure conditioned partially or fully upon that experimentation -- to the regulation of frontier AI.  There are a number of reasons to believe that approval regulation, simplistically applied, would be inapposite for frontier AI risks.  Domains of weak fit include the difficulty of defining the regulated product, the presence of Knightian uncertainty or deep ambiguity about harms from AI, the potentially transmissible nature of risks, and distributed activities among actors involved in the AI lifecycle. We conclude by highlighting the role of policy learning and experimentation in regulatory development, describing how learning from other forms of AI regulation and improvements in evaluation and testing methods can help to overcome some of the challenges we identify.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "239-254",
		"source": "ojs.aaai.org",
		"title": "An FDA for AI? Pitfalls and Plausibility of Approval Regulation for Frontier Artificial Intelligence",
		"title-short": "An FDA for AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31633",
		"volume": "7",
		"author": [
			{
				"family": "Carpenter",
				"given": "Daniel"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "castlemanWhyAmStill2024",
		"type": "article-journal",
		"abstract": "Recently, Meta has shifted towards AI-mediated ad targeting mechanisms that do not require advertisers to provide detailed targeting criteria. The shift is likely driven by excitement over AI capabilities as well as the need to address new data privacy policies and targeting changes agreed upon in civil rights settlements. At the same time, in response to growing public concern about the harms of targeted advertising, Meta has touted their ad preference controls as an effective mechanism for users to exert control over the advertising they see. Furthermore, Meta markets their \"Why this ad\" targeting explanation as a transparency tool that allows users to understand the reasons for seeing particular ads and inform their actions to control what ads they see in the future. \n\nOur study evaluates the effectiveness of Meta's \"See less\" ad control, as well as the actionability of ad targeting explanations following the shift to AI-mediated targeting. We conduct a large-scale study, randomly assigning participants the intervention of marking \"See less\" to either Body Weight Control or Parenting topics, and collecting the ads Meta shows to participants and their targeting explanations before and after the intervention. We find that utilizing the \"See less\" ad control for the topics we study does not significantly reduce the number of ads shown by Meta on these topics, and that the control is less effective for some users whose demographics are correlated with the topic. Furthermore, we find that the majority of ad targeting explanations for local ads made no reference to location-specific targeting criteria, and did not inform users why ads related to the topics they requested to \"See less\" of continued to be delivered. We hypothesize that the poor effectiveness of controls and lack of actionability and comprehensiveness in explanations are the result of the shift to AI-mediated targeting, for which explainability and transparency tools have not yet been developed by Meta. Our work thus provides evidence for the need of new methods for transparency and user control, suitable and reflective of how the increasingly complex and AI-mediated ad delivery systems operate.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "255-266",
		"source": "ojs.aaai.org",
		"title": "Why Am I Still Seeing This: Measuring the Effectiveness of Ad Controls and Explanations in AI-Mediated Ad Targeting Systems",
		"title-short": "Why Am I Still Seeing This",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31634",
		"volume": "7",
		"author": [
			{
				"family": "Castleman",
				"given": "Jane"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "cattellCoordinatedFlawDisclosure2024",
		"type": "article-journal",
		"abstract": "Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML and AI issues. This paper reviews the evolution of ML disclosure practices, from ad hoc reporting to emerging participatory auditing methods, and compares them with cybersecurity norms. Our framework introduces innovations such as extended model cards, dynamic scope expansion, an independent adjudication panel, and an automated verification process. We also outline a forthcoming real-world pilot of CFD. We argue that CFD could significantly enhance public trust in AI systems. By balancing organizational and community interests, CFD aims to improve AI accountability in a rapidly evolving technological landscape.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "267-280",
		"source": "ojs.aaai.org",
		"title": "Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities",
		"title-short": "Coordinated Flaw Disclosure for AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31635",
		"volume": "7",
		"author": [
			{
				"family": "Cattell",
				"given": "Sven"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kaffee",
				"given": "Lucie-Aimée"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "chengAlgorithmAssistedDecisionMaking2024",
		"type": "article-journal",
		"abstract": "The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource.  To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide.  Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "281-292",
		"source": "ojs.aaai.org",
		"title": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool",
		"title-short": "Algorithm-Assisted Decision Making and Racial Disparities in Housing",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31636",
		"volume": "7",
		"author": [
			{
				"family": "Cheng",
				"given": "Lingwei"
			},
			{
				"family": "Drayton",
				"given": "Cameron"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Vaithianathan",
				"given": "Rhema"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "collinsThumbsUntanglingChallenges2024",
		"type": "article-journal",
		"abstract": "Human feedback plays a critical role in learning and refining reward models for text-to-image generation, but the optimal form the feedback should take for learning an accurate reward function has not been conclusively established. This paper investigates the effectiveness of fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment, compared to traditional coarse-grained feedback (for example, thumbs up/down or ranking between a set of options). While fine-grained feedback holds promise, particularly for systems catering to diverse societal preferences, we show that demonstrating its superiority to coarse-grained feedback is not automatic. Through experiments on real and synthetic preference data, we surface the complexities of building effective models due to the interplay of model choice, feedback type, and the alignment between human judgment and computational interpretation. We identify key challenges in eliciting and utilizing fine-grained feedback, prompting a reassessment of its assumed benefits and practicality. Our findings -- e.g., that fine-grained feedback can lead to worse models for a fixed budget, in some settings; however, in controlled settings with known attributes, fine grained rewards can indeed be more helpful -- call for careful consideration of feedback attributes and potentially beckon novel modeling approaches to appropriately unlock the potential value of fine-grained feedback in-the-wild.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "293-303",
		"source": "ojs.aaai.org",
		"title": "Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation",
		"title-short": "Beyond Thumbs Up/Down",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31637",
		"volume": "7",
		"author": [
			{
				"family": "Collins",
				"given": "Katherine M."
			},
			{
				"family": "Kim",
				"given": "Najoung"
			},
			{
				"family": "Bitton",
				"given": "Yonatan"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Omidshafiei",
				"given": "Shayegan"
			},
			{
				"family": "Hu",
				"given": "Yushi"
			},
			{
				"family": "Chen",
				"given": "Sherol"
			},
			{
				"family": "Dutta",
				"given": "Senjuti"
			},
			{
				"family": "Chang",
				"given": "Minsuk"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Liang",
				"given": "Youwei"
			},
			{
				"family": "Evans",
				"given": "Georgina"
			},
			{
				"family": "Singla",
				"given": "Sahil"
			},
			{
				"family": "Li",
				"given": "Gang"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "He",
				"given": "Junfeng"
			},
			{
				"family": "Ramachandran",
				"given": "Deepak"
			},
			{
				"family": "Dvijotham",
				"given": "Krishnamurthy Dj"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "cornacchiaMoJEMixtureJailbreak2024",
		"type": "article-journal",
		"abstract": "The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting  jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "304-315",
		"source": "ojs.aaai.org",
		"title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
		"title-short": "MoJE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31638",
		"volume": "7",
		"author": [
			{
				"family": "Cornacchia",
				"given": "Giandomenico"
			},
			{
				"family": "Zizzo",
				"given": "Giulio"
			},
			{
				"family": "Fraser",
				"given": "Kieran"
			},
			{
				"family": "Hameed",
				"given": "Muhammad Zaid"
			},
			{
				"family": "Rawat",
				"given": "Ambrish"
			},
			{
				"family": "Purcell",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "dashSponsoredNewOrganic2024",
		"type": "article-journal",
		"abstract": "Interleaving sponsored results (advertisements) amongst organic results on search engine result pages (SERP) has become a common practice across multiple digital platforms. Advertisements have catered to consumer satisfaction and fostered competition in digital public spaces; making them an appealing gateway for businesses to reach their consumers. However, especially in the context of digital marketplaces, due to the competitive nature of the sponsored results with the organic ones, multiple unwanted repercussions have surfaced affecting different stakeholders. From the consumers' perspective the sponsored ads/results may cause degradation of search quality and nudge consumers to potentially irrelevant and costlier products. The sponsored ads may also affect the level playing field of the competition in the marketplaces among sellers. To understand and unravel these potential concerns, we analyse the Amazon digital marketplace in four different countries by simulating 4,800 search operations. Our analyses over SERPs consisting 2M organic and 638K sponsored results show items with poor organic ranks (beyond 100th position) appear as sponsored results even before the top organic results on the first page of Amazon SERP. Moreover, we also observe that in majority of the cases, these top sponsored results are costlier and are of poorer quality than the top organic results. We believe these observations can motivate researchers for further deliberation to bring in more transparency and guard rails in the advertising practices followed in digital marketplaces.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "316-327",
		"source": "ojs.aaai.org",
		"title": "Sponsored is the New Organic: Implications of Sponsored Results on Quality of Search Results in the Amazon Marketplace",
		"title-short": "Sponsored is the New Organic",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31639",
		"volume": "7",
		"author": [
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "deyAPPRAISEGovernanceFramework2024",
		"type": "article-journal",
		"abstract": "As artificial intelligence (AI) systems increasingly impact society, the EU Artificial Intelligence Act (AIA) is the first legislative attempt to regulate AI systems. This paper proposes a governance framework for organizations innovating with AI systems. Building upon secondary research, the framework aims at driving a balance between four types of pressures that organizations, innovating with AI, experience, and thereby creating responsible value. These pressures encompass AI/technology, normative, value creation, and regulatory aspects. The framework is partially validated through primary research in two phases. In the first phase, a conceptual model is proposed that measures the extent to which organizational tasks result in AIA compliance, using elements from the AIA as mediators and strategic variables such as organization size, extent of outsourcing, and offshoring as moderators. 34 organizations in the Netherlands are surveyed to test the conceptual model. The average actual compliance score of the 34 participants is low, and most participants exaggerate their compliance. Organization size is found to have significant impact on AIA compliance. In phase 2, two case studies are conducted with the purpose of generating in-depth insights to validate the proposed framework. The case studies confirm the interplay of the four pressures on organizations innovating with AI, and furthermore substantiate the governance framework.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "328-340",
		"source": "ojs.aaai.org",
		"title": "APPRAISE: a Governance Framework for Innovation with Artificial Intelligence Systems",
		"title-short": "APPRAISE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31640",
		"volume": "7",
		"author": [
			{
				"family": "Dey",
				"given": "Diptish"
			},
			{
				"family": "Bhaumik",
				"given": "Debarati"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazScalingLawsNot2024",
		"type": "article-journal",
		"abstract": "Recent work has advocated for training AI models on ever-larger datasets, arguing that as the size of a dataset increases, the performance of a model trained on that dataset will correspondingly increase (referred to as “scaling laws”). In this paper, we draw on literature from the social sciences and machine learning to critically interrogate these claims. We argue that this scaling law relationship depends on metrics used to measure performance that may not correspond with how different groups of people perceive the quality of models' output. As the size of datasets used to train large AI models grows and AI systems impact ever larger groups of people, the number of distinct communities represented in training or evaluation datasets grows. It is thus even more likely that communities represented in datasets may have values or preferences not reflected in (or at odds with) the metrics used to evaluate model performance in scaling laws. Different communities may also have values in tension with each other, leading to difficult, potentially irreconcilable choices about metrics used for model evaluations---threatening the validity of claims that model performance is improving at scale. We end the paper with implications for AI development: that the motivation for scraping ever-larger datasets may be based on fundamentally flawed assumptions about model performance. That is, models may not, in fact, continue to improve as the datasets get larger---at least not for all people or communities impacted by those models. We suggest opportunities for the field to rethink norms and values in AI development, resisting claims for universality of large models, fostering more local, small-scale designs, and other ways to resist the impetus towards scale in AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "341-357",
		"source": "ojs.aaai.org",
		"title": "Scaling Laws Do Not Scale",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31641",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Fernando"
			},
			{
				"family": "Madaio",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazWhatMakesExpert2024",
		"type": "article-journal",
		"abstract": "Human experts are often engaged in the development of machine learning systems to collect and validate data, consult on algorithm development, and evaluate system performance. At the same time, who counts as an ‘expert’ and what constitutes ‘expertise’ is not always explicitly defined. In this work, we review 112 academic publications that explicitly reference ‘expert’ and ‘expertise’ and that describe the development of machine learning (ML) systems to survey how expertise is characterized and the role experts play. We find that expertise is often undefined and forms of knowledge outside of formal education and professional certification are rarely sought, which has implications for the kinds of knowledge that are recognized and legitimized in ML development. Moreover, we find that expert knowledge tends to be utilized in ways focused on mining textbook knowledge, such as through data annotation. We discuss the ways experts are engaged in ML development in relation to deskilling, the social construction of expertise, and implications for responsible AI development. We point to a need for reflection and specificity in justifications of domain expert engagement, both as a matter of documentation and reproducibility, as well as a matter of broadening the range of recognized expertise.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "358-370",
		"source": "ojs.aaai.org",
		"title": "What Makes An Expert? Reviewing How ML Researchers Define \"Expert\"",
		"title-short": "What Makes An Expert?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31642",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Smith",
				"given": "Angela D. R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazSoUnDFrameworkAnalyzing2024",
		"type": "article-journal",
		"abstract": "Decisions about how to responsibly collect, use and document data often rely upon understanding how people are represented in data. Yet, the unlabeled  nature and scale of data used in foundation model development poses a direct challenge to systematic analyses of downstream risks, such as representational harms.  We provide a framework designed to help RAI practitioners more easily plan and structure analyses of how people are represented in unstructured data and identify downstream risks. The framework is organized into groups of analyses that map to 3 basic questions: 1) Who is represented in the data, 2) What content is in the data, and 3) How are the two associated. We use the framework to analyze human representation in two commonly used datasets: the Common Crawl web corpus (C4) of 356 billion tokens, and the LAION-400M dataset of 400 million text-image pairs, both developed in the English language. We illustrate how the framework informs action steps for hypothetical teams faced with data use, development, and documentation decisions. Ultimately, the framework structures human representation analyses and maps out analysis planning considerations, goals, and risk mitigation actions at different stages of dataset and model development.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "371-383",
		"source": "ojs.aaai.org",
		"title": "SoUnD Framework: Analyzing (So)cial Representation in (Un)structured (D)ata",
		"title-short": "SoUnD Framework",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31643",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Reif",
				"given": "Emily"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "dingOutlierDetectionBias2024",
		"type": "article-journal",
		"abstract": "The astonishing successes of ML  have raised growing concern for the fairness of modern methods when deployed in real world settings. However, studies on fairness have mostly focused on supervised ML, while unsupervised outlier detection (OD), with numerous applications in finance, security, etc., have attracted little attention. While a few studies proposed fairness-enhanced OD algorithms, they remain agnostic to the underlying driving mechanisms or sources of unfairness. Even within the supervised ML literature, there exists debate on whether unfairness stems solely from algorithmic biases (i.e. design choices)  or from the  biases encoded in the data on which they are trained.  \n\nTo close this gap, this work aims to shed light on the possible sources of unfairness in OD by auditing detection models under different data-centric factors.By injecting various known biases into the input data---as pertain to sample size disparity, under-representation, feature measurement noise, and group membership obfuscation---we find that the OD algorithms under the study all exhibit fairness pitfalls, although differing in which types of data bias they are more susceptible to. Most notable of our study is to demonstrate that OD algorithm bias is not merely a data bias problem. A key realization is that the data properties that emerge from bias injection could as well be organic---as pertain to natural group differences w.r.t. sparsity, base rate, variance, and multi-modality. Either natural or biased, such data properties can \ngive rise to unfairness as they interact with certain algorithmic design choices.\n\nOur work provides a deeper  understanding of the possible sources of OD unfairness, and\nserves as a framework for assessing the unfairness of future OD algorithms under specific data-centric factors. It also paves the way for future work on mitigation strategies by underscoring the susceptibility of various design choices.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "384-395",
		"source": "ojs.aaai.org",
		"title": "Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors",
		"title-short": "Outlier Detection Bias Busted",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31644",
		"volume": "7",
		"author": [
			{
				"family": "Ding",
				"given": "Xueying"
			},
			{
				"family": "Xi",
				"given": "Rui"
			},
			{
				"family": "Akoglu",
				"given": "Leman"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "doerflerLegitimatingEmotionTracking2024",
		"type": "article-journal",
		"abstract": "Contemporary automobiles are now incorporating digital technologies, including emotion recognition technologies intended to monitor and sometimes intervene on the driver’s mood, attentiveness, or emotional state. We investigate how the firms producing these technologies justify and legitimate their design, production, and use, and how these discourses of legitimation paint a picture of the desired social role of emotion recognition in the automotive sector. Through a critical discourse analysis of patents, advertising, and promotional materials from industry-leading companies Cerence and Affectiva/Smart Eye, we argue both companies use  potentially spurious arguments about the accuracy of emotion recognition to rationalize their products. Both companies also use a variety of other legitimation techniques around driver safety, individual personalization, and increased productivity to re-frame the social aspects of digitally mediated autonomous vehicles on their terms.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "396-410",
		"source": "ojs.aaai.org",
		"title": "Legitimating Emotion Tracking Technologies in Driver Monitoring Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31645",
		"volume": "7",
		"author": [
			{
				"family": "Doerfler",
				"given": "Aaron"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "fangRepresentationMagnitudeHas2024",
		"type": "article-journal",
		"abstract": "The privacy-preserving approaches to machine learning (ML) models have made substantial progress in recent years. However, it is still opaque in which circumstances and conditions the model becomes privacy-vulnerable, leading to a challenge for ML models to maintain both performance and privacy. In this paper, we first explore the disparity between member and non-member data in the representation of models under common training frameworks.We identify how the representation magnitude disparity correlates with privacy vulnerability and address how this correlation impacts privacy vulnerability. Based on the observations, we propose Saturn Ring Classifier Module (SRCM), a plug-in model-level solution to mitigate membership privacy leakage. Through a confined yet effective representation space, our approach ameliorates models’ privacy vulnerability while maintaining generalizability. The code of this work can be found here:\nhttps://github.com/JEKimLab/AIES2024SRCM",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "411-420",
		"source": "ojs.aaai.org",
		"title": "Representation Magnitude Has a Liability to Privacy Vulnerability",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31646",
		"volume": "7",
		"author": [
			{
				"family": "Fang",
				"given": "Xingli"
			},
			{
				"family": "Kim",
				"given": "Jung-Eun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "fefferRedTeamingGenerativeAI2024",
		"type": "article-journal",
		"abstract": "In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming’s central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "421-437",
		"source": "ojs.aaai.org",
		"title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?",
		"title-short": "Red-Teaming for Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31647",
		"volume": "7",
		"author": [
			{
				"family": "Feffer",
				"given": "Michael"
			},
			{
				"family": "Sinha",
				"given": "Anusha"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "freszHowShouldAI2024",
		"type": "article-journal",
		"abstract": "This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, which was adopted by the European Parliament in March 2024, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary duties, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI methods are derived from each of the legal fields, resulting in the conclusion that each legal field requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI methods.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "438-450",
		"source": "ojs.aaai.org",
		"title": "How Should AI Decisions Be Explained? Requirements for Explanations from the Perspective of European Law",
		"title-short": "How Should AI Decisions Be Explained?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31648",
		"volume": "7",
		"author": [
			{
				"family": "Fresz",
				"given": "Benjamin"
			},
			{
				"family": "Dubovitskaya",
				"given": "Elena"
			},
			{
				"family": "Brajovic",
				"given": "Danilo"
			},
			{
				"family": "Huber",
				"given": "Marco F."
			},
			{
				"family": "Horz",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "gaoSurvivingDiverseBiases2024",
		"type": "article-journal",
		"abstract": "The online data markets have emerged as a valuable source of diverse datasets for training machine learning (ML) models.  However, datasets from different data providers may exhibit varying levels of bias with respect to certain sensitive attributes in the population (such as race, sex, age, and marital status).  Recent dataset acquisition research has focused on maximizing accuracy improvements for downstream model training,  ignoring the negative impact of biases in the acquired datasets, which can lead to an unfair model.  Can a consumer obtain an unbiased dataset from datasets with diverse biases? In this work, we propose a fairness-aware data acquisition framework  (FAIRDA) to acquire high-quality datasets that maximize both accuracy and fairness for consumer local classifier training while remaining within a limited budget.  Given the biases of data commodities remain opaque to consumers,  the data acquisition in FAIRDA employs explore-exploit strategies.  Based on whether exploration and exploitation are conducted sequentially or alternately, we introduce two algorithms: the knowledge-based offline data acquisition (KDA) and the reward-based online data acquisition algorithms (RDA).  Each algorithm is tailored to specific customer needs, giving the former an advantage in computational efficiency and the latter an advantage in robustness.  We conduct experiments to demonstrate the effectiveness of the proposed data acquisition framework in steering users toward fairer model training compared to existing baselines under varying market settings.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "451-462",
		"source": "ojs.aaai.org",
		"title": "Surviving in Diverse Biases: Unbiased Dataset Acquisition in Online Data Market for Fair Model Training",
		"title-short": "Surviving in Diverse Biases",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31649",
		"volume": "7",
		"author": [
			{
				"family": "Gao",
				"given": "Jiashi"
			},
			{
				"family": "Wang",
				"given": "Ziwei"
			},
			{
				"family": "Zhao",
				"given": "Xiangyu"
			},
			{
				"family": "Yao",
				"given": "Xin"
			},
			{
				"family": "Wei",
				"given": "Xuetao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshDontSeeMyself2024",
		"type": "article-journal",
		"abstract": "Though research into text-to-image generators (T2Is) such as Stable Diffusion has demonstrated their amplification of societal biases and potentials to cause harm, such research has primarily relied on computational methods instead of seeking information from real users who experience harm, which is a significant knowledge gap. In this paper, we conduct the largest human subjects study of Stable Diffusion, with a combination of crowdsourced data from 133 crowdworkers and 14 semi-structured interviews across diverse countries and genders. Through a mixed-methods approach of intra-set cosine similarity hierarchies (i.e., comparing multiple Stable Diffusion outputs for the same prompt with each other to examine which result is `closest' to the prompt) and qualitative thematic analysis, we first demonstrate a large disconnect between user expectations for Stable Diffusion outputs with those generated, evidenced by a set of Stable Diffusion renditions of `a Person' providing images far away from such expectations. We then extend this finding of general dissatisfaction into highlighting representational harms caused by Stable Diffusion upon our subjects, especially those with traditionally marginalized identities, subjecting them to incorrect and often dehumanizing stereotypes about their identities. We provide recommendations for a harm-aware approach to (re)design future versions of Stable Diffusion and other T2Is.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "463-475",
		"source": "ojs.aaai.org",
		"title": "“I Don’t See Myself Represented Here at All”: User Experiences of Stable Diffusion Outputs Containing Representational Harms across Gender Identities and Nationalities",
		"title-short": "“I Don’t See Myself Represented Here at All”",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31650",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Lutz",
				"given": "Nina"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshGenerativeAIModels2024",
		"type": "article-journal",
		"abstract": "Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a Non-Western community-centered approach\nand grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English input prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, contributing to the development of more equitable and representative GAI technologies globally. Our work underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise as these models are deployed on a global scale.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "476-489",
		"source": "ojs.aaai.org",
		"title": "Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach",
		"title-short": "Do Generative AI Models Output Harm while Representing Non-Western Cultures",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31651",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Venkit",
				"given": "Pranav Narayanan"
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Wilson",
				"given": "Shomir"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshInterpretationsRepresentationsStereotypes2024",
		"type": "article-journal",
		"abstract": "The surge in the popularity of text-to-image generators (T2Is) has been matched by extensive research into ensuring fairness and equitable outcomes, with a focus on how they impact society. However, such work has typically focused on globally-experienced identities or centered Western contexts. In this paper, we address interpretations, representations, and stereotypes surrounding a tragically underexplored context in T2I research: caste. We examine how the T2I Stable Diffusion displays people of various castes, and what professions they are depicted as performing. Generating 100 images per prompt, we perform CLIP-cosine similarity comparisons with default depictions of an `Indian person’ by Stable Diffusion, and explore patterns of similarity. Our findings reveal how Stable Diffusion outputs perpetuate systems of `castelessness’, equating Indianness with high-castes and depicting caste-oppressed identities with markers of poverty. In particular, we note the stereotyping and representational harm towards the historically-marginalized Dalits, prominently depicted as living in rural areas and always at protests. Our findings underscore a need for a caste-aware approach towards T2I design, and we conclude with design recommendations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "490-502",
		"source": "ojs.aaai.org",
		"title": "Interpretations, Representations, and Stereotypes of Caste within Text-to-Image Generators",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31652",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "goldsteinPPOuFrameworkStructured2024",
		"type": "article-journal",
		"abstract": "The diffusion of increasingly capable AI systems has produced concern that bad actors could intentionally misuse current or future AI systems for harm. Governments have begun to create new entities—such as AI Safety Institutes—tasked with assessing these risks. However, approaches for risk assessment are currently fragmented and would benefit from broader disciplinary expertise. As it stands, it is often unclear whether concerns about malicious use misestimate the likelihood and severity of the risks. This article advances a conceptual framework to review and structure investigation into the likelihood of an AI system (X) being applied to a malicious use (Y). We introduce a three-stage framework of (1) Plausibility (can X be used to do Y at all?), (2) Performance (how well does X do Y?), and (3) Observed use (do actors use X to do Y in practice?). At each stage, we outline key research questions, methodologies, benefits and limitations, and the types of uncertainty addressed. We also offer ideas for directions to improve risk assessment moving forward.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "503-518",
		"source": "ojs.aaai.org",
		"title": "The PPOu Framework: A Structured Approach for Assessing the Likelihood of Malicious Use of Advanced AI Systems",
		"title-short": "The PPOu Framework",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31653",
		"volume": "7",
		"author": [
			{
				"family": "Goldstein",
				"given": "Josh A."
			},
			{
				"family": "Sastry",
				"given": "Girish"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "grabbRisksLanguageModels2024",
		"type": "article-journal",
		"abstract": "In the United States and other countries exists a “national mental health crisis”: Rates of suicide, depression, anxiety, substance use, and more continue to increase – exacerbated by isolation, the COVID pandemic, and, most importantly, lack of access to mental healthcare. Therefore, many are looking to AI-enabled digital mental health tools, which have the potential to reach many patients who would otherwise remain on wait lists or without care. The main drive behind these new tools is the focus on large language models that could enable real-time, personalized support and advice for patients. With a trend towards language models entering the mental healthcare delivery apparatus, questions arise about how a robust, high-level framework to guide ethical implementations would look like and whether existing language models are ready for this high-stakes application where individual failures can lead to dire consequences.\n\nThis paper addresses the ethical and practical challenges custom to mental health applications and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s) with defined rubrics and criteria for each question that would define \"safe,\" \"unsafe,\" and \"borderline\" (between safe and unsafe) for reproducibility.\n\nWe find that all tested language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. \nThis is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models based on system prompt engineering and model-generated self-critiques.\n\nBefore the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.\n\nOur code and the redacted data set are available on Github (github.com/maxlampe/taimh_eval, MIT License). The full, unredacted data set is available upon request due to the harmful content contained.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "519-519",
		"source": "ojs.aaai.org",
		"title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation (Extended Abstract)",
		"title-short": "Risks from Language Models for Automated Mental Healthcare",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31654",
		"volume": "7",
		"author": [
			{
				"family": "Grabb",
				"given": "Declan"
			},
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Vasan",
				"given": "Nina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "gravesCompassionateAIMoral2024",
		"type": "article-journal",
		"abstract": "The rapid expansion of artificial intelligence (AI) technology promises plausible increases to human flourishing, health, and well-being but raises concerns about possible harms and increased suffering. By making AI compassionate, the alleviation of suffering becomes explicit, rather than proxied, and potential harms caused by AI automation can be turned into benefits. Compassionate healthcare is beneficial for patient health outcomes and satisfaction and improves caregiver resilience and burnout. AI automation has many benefits but may interfere with patient care and autonomy. Incorporating compassion into healthcare reduces potential harms, increases health benefits and well-being, and can protect patient autonomy while providing more responsive and equitable care.\n\nWhether and how one conceives of AI as plausibly compassionate depends on ethical concerns and cultural context, including assumptions about human nature and AI personhood. Insights from Buddhism have contributed to scholarship on compassion and can extend incomplete Western perspectives on AI possibilities and limitations. Psychological research on the elements of compassion can guide development of compassionate AI and its incorporation into healthcare. Compassionate AI can be deployed especially into application areas where compassion plays an essential role with high demands on the compassion capacity of caregivers, such as dementia eldercare and palliative care.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "520-533",
		"source": "ojs.aaai.org",
		"title": "Compassionate AI for Moral Decision-Making, Health, and Well-Being",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31655",
		"volume": "7",
		"author": [
			{
				"family": "Graves",
				"given": "Mark"
			},
			{
				"family": "Compson",
				"given": "Jane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "guptaConceptualFrameworkEthical2024",
		"type": "article-journal",
		"abstract": "Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "534-546",
		"source": "ojs.aaai.org",
		"title": "A Conceptual Framework for Ethical Evaluation of Machine Learning Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31656",
		"volume": "7",
		"author": [
			{
				"family": "Gupta",
				"given": "Neha R."
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Subramonyam",
				"given": "Hariharan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hamidiehIdentifyingImplicitSocial2024",
		"type": "article-journal",
		"abstract": "Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-It, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a \"terrorist\". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "547-561",
		"source": "ojs.aaai.org",
		"title": "Identifying Implicit Social Biases in Vision-Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31657",
		"volume": "7",
		"author": [
			{
				"family": "Hamidieh",
				"given": "Kimia"
			},
			{
				"family": "Zhang",
				"given": "Haoran"
			},
			{
				"family": "Gerych",
				"given": "Walter"
			},
			{
				"family": "Hartvigsen",
				"given": "Thomas"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hanCausalFrameworkEvaluate2024",
		"type": "article-journal",
		"abstract": "We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the \"causal chain of interactions\" rather than simply focusing on the end outcome; this can help guide reforms. \n\nIn this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and criminality. Our framework identifies three canonical scenarios with distinct characteristics: in settings like (1) airport security, the primary source of observed bias against a race is likely to be bias in law enforcement against innocents of that race; (2) AI-empowered policing, the primary source of observed bias against a race is likely to be bias in law enforcement against criminals of that race; and (3) police-civilian interaction, the primary source of observed bias against a race could be bias in law enforcement against that race or bias from the general public in reporting (e.g. via 911 calls) against the other race. Through an extensive empirical study using police-civilian interaction (stop) data and 911 call data, we And an instance of such a counter-intuitive phenomenon: in New Orleans, the observed bias is against the majority race and the likely reason for it is the over-reporting (via 911 calls) of incidents involving the minority race by the general public.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "562-572",
		"source": "ojs.aaai.org",
		"title": "A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31658",
		"volume": "7",
		"author": [
			{
				"family": "Han",
				"given": "Jessy Xinyi"
			},
			{
				"family": "Miller",
				"given": "Andrew Cesare"
			},
			{
				"family": "Watkins",
				"given": "S. Craig"
			},
			{
				"family": "Winship",
				"given": "Christopher"
			},
			{
				"family": "Christia",
				"given": "Fotini"
			},
			{
				"family": "Shah",
				"given": "Devavrat"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hardalupasContributoryInjusticeEpistemic2024",
		"type": "article-journal",
		"abstract": "AI systems have long been touted as a means to transform the healthcare system and improve service user outcomes. However, these claims frequently ignore the social context that leaves service users subject to epistemic oppression. This paper introduces the term “epistemic calcification” to describe how the use of AI systems leads to our epistemological systems becoming stuck in fixed frameworks for understanding the world. Epistemic calcification leads to contributory injustice as it reduces the ability of healthcare systems to meaningfully consider alternative understandings of people’s health experiences. By analysing examples of algorithmic prognosis and diagnosis, this paper demonstrates the challenges of addressing contributory injustice in AI systems and the need for contestability to focus on more than the AI system and on the underlying epistemologies of AI systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "573-583",
		"source": "ojs.aaai.org",
		"title": "Contributory Injustice, Epistemic Calcification and the Use of AI Systems in Healthcare",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31659",
		"volume": "7",
		"author": [
			{
				"family": "Hardalupas",
				"given": "Mahi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "herdelExploreGenLargeLanguage2024",
		"type": "article-journal",
		"abstract": "Responsible AI design is increasingly seen as an imperative by both AI developers and AI compliance experts. One of the key tasks is envisioning AI technology uses and risks. Recent studies on the model and data cards reveal that AI practitioners struggle with this task due to its inherently challenging nature. Here, we demonstrate that leveraging a Large Language Model (LLM) can support AI practitioners in this task by enabling reflexivity, brainstorming, and deliberation, especially in the early design stages of the AI development process. We developed an LLM framework, ExploreGen, which generates realistic and varied uses of AI technology, including those overlooked by research, and classifies their risk level based on the EU AI Act regulation. We evaluated our framework using the case of Facial Recognition and Analysis technology in nine user studies with 25 AI practitioners. Our findings show that ExploreGen is helpful to both developers and compliance experts. They rated the uses as realistic and their risk classification as accurate (94.5%). Moreover, while unfamiliar with many of the uses, they rated them as having high adoption potential and transformational impact.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "584-596",
		"source": "ojs.aaai.org",
		"title": "ExploreGen: Large Language Models for Envisioning the Uses and Risks of AI Technologies",
		"title-short": "ExploreGen",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31660",
		"volume": "7",
		"author": [
			{
				"family": "Herdel",
				"given": "Viviane"
			},
			{
				"family": "Šćepanović",
				"given": "Sanja"
			},
			{
				"family": "Bogucka",
				"given": "Edyta"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hertweckWhatsDistributiveJustice2024",
		"type": "article-journal",
		"abstract": "In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "597-608",
		"source": "ojs.aaai.org",
		"title": "What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from a Perspective of Approximate Justice",
		"title-short": "What's Distributive Justice Got to Do with It?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31661",
		"volume": "7",
		"author": [
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hice-fromilleAfrofuturistValuesMetaverse2024",
		"type": "article-journal",
		"abstract": "Many emerging technologies, such as the immersive VR and AR devices forming the metaverse, are not just reminiscent of but inspired by devices found in popular science fiction texts. Yet, the stories that these technologies are drawn from do not often center marginalized communities and people of color. In this article, we propose that builders and users of these technologies turn to diverse creative texts as inspiration for the ethical codes that will shape the ways that these technologies are built and used.  A study of 39 speculative fiction texts, including 20 that we identified as Afrofuturist, revealed three overarching themes that serve as recommendations for the creation and maintenance of a diverse and inclusive metaverse: Collective Power, Inclusive Engagement, and Cultural Specificity. We outline each recommendation through a textual analysis of three Afrofuturist texts – Esi Edugyan’s Washington Black (2018), Roger Ross Williams’ Traveling While Black (2019), and Ryan Coogler’s Black Panther (2018) – and specify the undercurrents of collectivity and co-production that bind them together. We suggest collaborative and critical reading methods for industry professionals and community members which may help to shape democratic processes governing the future of AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "609-609",
		"source": "ojs.aaai.org",
		"title": "Afrofuturist Values for the Metaverse (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31662",
		"volume": "7",
		"author": [
			{
				"family": "Hice-Fromille",
				"given": "Theresa"
			},
			{
				"family": "Papazoglakis",
				"given": "Sarah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hollanekEthicoPoliticsDesignToolkits2024",
		"type": "article-journal",
		"abstract": "This paper interrogates the belief in toolkitting as a method for translating AI ethics theory into practice and assesses the toolkit paradigm’s effect on the understanding of ethics in AI research and AI-related policy. I start by exploring the ethico-political assumptions that underly most ethical AI toolkits. Through a meta-critique of toolkits (drawing on a review of existing ‘toolkit-scoping’ work), I demon-strate that most toolkits embody a reductionist conception of ethics and that, because of this, their capacity for facili-tating change and challenging the status quo is limited. Then, I analyze the features of several ‘alternative’ toolkits – informed by feminist theory, posthumanism, and critical design – whose creators recognize that ethics cannot be-come a box-ticking exercise for engineers, while the ethical should not be dissociated from the political. Finally, in the concluding section, referring to broader theories and cri-tiques of toolkitting as a method for structuring the design process, I suggest how different stakeholders can draw on the myriad of available tools, ranging from big tech com-panies’ guidelines to feminist design ideation cards, to achieve positive, socially desirable results, while rejecting the oversimplification of ethical practice and technosolu-tionism that many responsible AI toolkits embody. The analysis thus serves to provide suggestions for future toolkit creators and users on how to meaningfully adopt the toolkit format in AI ethics work without overselling its transformative potential.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "610-610",
		"source": "ojs.aaai.org",
		"title": "The Ethico-Politics of Design Toolkits: Responsible AI Tools, From Big Tech Guidelines to Feminist Ideation Cards (Extended Abstract)",
		"title-short": "The Ethico-Politics of Design Toolkits",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31663",
		"volume": "7",
		"author": [
			{
				"family": "Hollanek",
				"given": "Tomasz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "iqbalLLMPlatformSecurity2024",
		"type": "article-journal",
		"abstract": "Large language model (LLM) platforms, such as ChatGPT, have recently begun offering an app ecosystem to interface with third-party services on the internet. While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Apps also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future third-party integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin (apps) ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms. The full version of this paper is available online at https://arxiv.org/abs/2309.10254",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "611-623",
		"source": "ojs.aaai.org",
		"title": "LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins",
		"title-short": "LLM Platform Security",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31664",
		"volume": "7",
		"author": [
			{
				"family": "Iqbal",
				"given": "Umar"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Roesner",
				"given": "Franziska"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jainAILanguageModel2024",
		"type": "article-journal",
		"abstract": "We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations. Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos. We evaluate the decisions of three state-of-the-art LLMs — GPT-4, Gemini 1.0, and Claude 3 Sonnet — in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded. Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods. These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "624-633",
		"source": "ojs.aaai.org",
		"title": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\": Norm Inconsistency in LLM Decision-Making",
		"title-short": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\"",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31665",
		"volume": "7",
		"author": [
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Calacci",
				"given": "D."
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jaiswalBreakingGlobalNorth2024",
		"type": "article-journal",
		"abstract": "Facial Recognition Systems (FRSs) are being developed and deployed all around the world at unprecedented rates. Most platforms are designed in a limited set of countries, but deployed in other regions too, without adequate checkpoints for region-specific requirements. This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems. A combination of unavailability of datasets, lack of understanding of how FRSs function and low-resource bias mitigation measures accentuate the problems at hand. In this work, we propose a self-curated face dataset composed of 6,579 unique male and female sports-persons (cricket players) from eight countries around the world. More than 50% of the dataset is composed of individuals from the Global South countries and is demographically diverse. To aid adversarial audits and robust model training, we curate four adversarial variants of each image in the dataset, leading to more than 40,000 distinct images. We also use this dataset to benchmark five popular facial recognition systems (FRSs), including both commercial and open-source FRSs, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming). Experiments on industrial FRSs reveal accuracies ranging from 98.2% (in case of Azure) to 38.1% (in case of Face++), with a large disparity between males and females in the Global South (max difference of 38.5% in case of Face++). Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%). A Grad-CAM analysis shows that the nose, forehead and mouth are the regions of interest for one of the open-source FRSs.  Based on this crucial observation, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques that demonstrate a significant improvement in accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings. For the red-teaming experiment using the open-source Deepface model we observe that simple fine-tuning is not very useful while contrastive learning brings steady benefits.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "634-646",
		"source": "ojs.aaai.org",
		"title": "Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems",
		"title-short": "Breaking the Global North Stereotype",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31666",
		"volume": "7",
		"author": [
			{
				"family": "Jaiswal",
				"given": "Siddharth"
			},
			{
				"family": "Ganai",
				"given": "Animesh"
			},
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jensenReflectionItsCreators2024",
		"type": "article-journal",
		"abstract": "The increasing prevalence of artificial intelligence (AI) will likely lead to new interactions and impacts for the general public. An understanding of people’s perceptions of AI can be leveraged to design and deploy AI systems toward human needs and values. We conducted semi-structured interviews with 25 individuals in the general public and 20 AI experts in the United States (U.S.) to assess perceptions of AI across levels of expertise. Qualitative analysis revealed that ideas about humanness and ethics were central to perceptions of AI in both groups. Humanness, the set of traits considered to distinguish humans from other intelligent actors, was used to articulate beliefs about AI’s characteristics. Ethics arose in discussions of the role of technology in society and centered around views of AI as made and used by people. General public and expert participants expressed similar perceptions of AI, but articulated beliefs slightly differently. We discuss the implications of humanness-related beliefs and ethical concerns for AI development and deployment.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "647-658",
		"source": "ojs.aaai.org",
		"title": "Reflection of Its Creators: Qualitative Analysis of General Public and Expert Perceptions of Artificial Intelligence",
		"title-short": "Reflection of Its Creators",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31667",
		"volume": "7",
		"author": [
			{
				"family": "Jensen",
				"given": "Theodore"
			},
			{
				"family": "Theofanos",
				"given": "Mary"
			},
			{
				"family": "Greene",
				"given": "Kristen"
			},
			{
				"family": "Williams",
				"given": "Olivia"
			},
			{
				"family": "Goad",
				"given": "Kurtis"
			},
			{
				"family": "Fofang",
				"given": "Janet Bih"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jorgensonVirtualAssistantsAre2024",
		"type": "article-journal",
		"abstract": "The ethical use of AI typically involves setting boundaries on its deployment. Ethical guidelines advise against practices that involve deception, privacy infringement, or discriminatory actions. However, ethical considerations can also identify areas where using AI is desirable and morally necessary. For instance, it has been argued that AI could contribute to more equitable justice systems. Another area where ethical considerations can make AI deployment imperative is healthcare. For example, patients often withhold pertinent details from healthcare providers due to fear of judgment. However, utilizing virtual assistants to gather patients' health histories could be a potential solution. Ethical imperatives support using such technology if patients are more inclined to disclose information to an AI system. This article presents findings from several survey studies investigating whether virtual assistants can reduce non-disclosure behaviors. Unfortunately, the evidence suggests that virtual assistants are unlikely to minimize non-disclosure. Therefore, the potential benefits of virtual assistants due to reduced non-disclosure are unlikely to outweigh their ethical risks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "659-669",
		"source": "ojs.aaai.org",
		"title": "Virtual Assistants Are Unlikely to Reduce Patient Non-Disclosure",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31668",
		"volume": "7",
		"author": [
			{
				"family": "Jorgenson",
				"given": "Corinne"
			},
			{
				"family": "Ozkes",
				"given": "Ali I."
			},
			{
				"family": "Willems",
				"given": "Jurgen"
			},
			{
				"family": "Vanderelst",
				"given": "Dieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kawakamiResponsibleAIArtifacts2024",
		"type": "article-journal",
		"abstract": "The responsible AI (RAI) community has introduced numerous processes and artifacts---such as Model Cards, Transparency Notes, and Data Cards---to facilitate transparency and support the governance of AI systems.  While originally designed to scaffold and document AI development processes in technology companies, these artifacts are becoming central components of regulatory compliance under recent regulations such as the EU AI Act. Much of the existing literature has focussed primarily on the design of new RAI artifacts, or an examination of their use by practitioners within technology companies.  However, as RAI artifacts begin to play key roles in enabling external oversight, it becomes critical to understand how stakeholders---particularly stakeholders situated outside of technology companies who govern and audit industry AI deployments---perceive the efficacy of RAI artifacts.  In this study, we conduct semi-structured interviews and design activities with 19 government, legal, and civil society stakeholders who inform policy and advocacy around responsible AI efforts. While participants believe that RAI artifacts are a valuable contribution to the RAI ecosystem, many have concerns around their potential unintended and longer-term impacts on actors outside of technology companies (e.g., downstream end-users, policymakers, civil society stakeholders). We organized these beliefs into four barriers that help explain how RAI artifacts may (inadvertently) reconfigure power relations across civil society, government, and industry, impeding civil society and legal stakeholders' ability to protect downstream end-users from potential AI harms. Participants envision how structural changes, along with changes in how RAI artifacts are designed, used, and governed, could help re-direct the role and impacts of artifacts in the RAI ecosystem. Drawing on these findings, we discuss research and policy implications for RAI artifacts.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "670-682",
		"source": "ojs.aaai.org",
		"title": "Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers Perceived by Legal and Civil Stakeholders",
		"title-short": "Do Responsible AI Artifacts Advance Stakeholder Goals?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31669",
		"volume": "7",
		"author": [
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Wilkinson",
				"given": "Daricia"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kawakamiAIFailureLoops2024",
		"type": "article-journal",
		"abstract": "A growing body of literature has focused on understanding and addressing workplace AI design failures. However, past work has largely overlooked the role of occupational devaluation in shaping the dynamics of AI development and deployment. In this paper, we examine the case of feminized labor: a class of devalued occupations historically misnomered as ``women's work,'' such as social work, K-12 teaching, and home healthcare. Drawing on literature on AI deployments in feminized labor contexts, we conceptualize AI Failure Loops: a set of interwoven, socio-technical failures that help explain how the systemic devaluation of workers' expertise negatively impacts, and is impacted by, AI design, evaluation, and governance practices. These failures demonstrate how misjudgments on the automatability of workers' skills can lead to AI deployments that fail to bring value and, instead, further diminish the visibility of workers' expertise. We discuss research and design implications for workplace AI, especially for devalued occupations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "683-683",
		"source": "ojs.aaai.org",
		"title": "AI Failure Loops in Feminized Labor: Understanding the Interplay of Workplace AI and Occupational Devaluation",
		"title-short": "AI Failure Loops in Feminized Labor",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31670",
		"volume": "7",
		"author": [
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Taylor",
				"given": "Jordan"
			},
			{
				"family": "Fox",
				"given": "Sarah"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			},
			{
				"family": "Holstein",
				"given": "Ken"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kayEpistemicInjusticeGenerative2024",
		"type": "article-journal",
		"abstract": "This paper investigates how generative AI can potentially undermine the integrity of collective knowledge and the processes we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinformation, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlighting these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable information ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "684-697",
		"source": "ojs.aaai.org",
		"title": "Epistemic Injustice in Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31671",
		"volume": "7",
		"author": [
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kennedyVernacularizingTaxonomiesHarm2024",
		"type": "article-journal",
		"abstract": "Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel “holistic” methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies are still too general to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or “high-risk” sectors. This is because many sectors are constituted by discourses, norms, and values that “refract” or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of “vernacularization”—a participatory, decolonial practice distinct from doctrinary “translation” (the dominant mode of AI safety operationalization)—can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading  taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "698-710",
		"source": "ojs.aaai.org",
		"title": "Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31672",
		"volume": "7",
		"author": [
			{
				"family": "Kennedy",
				"given": "Wm Matthew"
			},
			{
				"family": "Campos",
				"given": "Daniel Vargas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "keswaniProsConsActive2024",
		"type": "article-journal",
		"abstract": "Computational preference elicitation methods are tools used to learn people’s preferences quantitatively in a given context. Recent works on preference elicitation advocate for active learning as an efficient method to iteratively construct queries (framed as comparisons between context-specific cases) that are likely to be most informative about an agent’s underlying preferences. In this work, we argue that the use of active learning for moral preference elicitation relies on certain assumptions about the underlying moral preferences, which can be violated in practice. Specifically, we highlight the following common assumptions (a) preferences are stable over time and not sensitive to the sequence of presented queries, (b) the appropriate hypothesis class is chosen to model moral preferences, and (c) noise in the agent’s responses is limited. While these assumptions can be appropriate for preference elicitation in certain domains, prior research on moral psychology suggests they may not be valid for moral judgments. Through a synthetic simulation of preferences that violate the above assumptions, we observe that active learning can have similar or worse performance than a basic random query selection method in certain settings. Yet, simulation results also demonstrate that active learning can still be viable if the degree of instability or noise is relatively small and when the agent’s preferences can be approximately represented with the hypothesis class used for learning. Our study highlights the nuances associated with effective moral preference elicitation in practice and advocates for the cautious use of active learning as a methodology to learn moral preferences.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "711-723",
		"source": "ojs.aaai.org",
		"title": "On the Pros and Cons of Active Learning for Moral Preference Elicitation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31673",
		"volume": "7",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "keswaniAlgorithmicFairnessPerspective2024",
		"type": "article-journal",
		"abstract": "Real-world applications of machine learning (ML) algorithms often propagate negative stereotypes and social biases against marginalized groups. In response, the field of fair machine learning has proposed technical solutions for a variety of settings that aim to correct the biases in algorithmic predictions. These solutions remove the dependence of the final prediction on the protected attributes (like gender or race) and/or ensure that prediction performance is similar across demographic groups. Yet, recent studies assessing the impact of these solutions in practice demonstrate their ineffectiveness in tackling real-world inequalities. Given this lack of real-world success, it is essential to take a step back and question the design motivations of algorithmic fairness interventions. \n\nWe use popular legal anti-discriminatory principles, specifically anti-classification and anti-subordination principles, to study the motivations of fairness interventions and their applications. The anti-classification principle suggests addressing discrimination by ensuring that decision processes and outcomes are independent of the protected attributes of individuals. The anti-subordination principle, on the other hand, argues that decision-making policies can provide equal protection to all only by actively tackling societal hierarchies that enable structural discrimination, even if that requires using protected attributes to address historical inequalities. Through a survey of the fairness mechanisms and applications, we assess different components of fair ML approaches from the perspective of these principles. We argue that the observed shortcomings of fair ML algorithms are similar to the failures of anti-classification policies and that these shortcomings constitute violations of the anti-subordination principle. Correspondingly, we propose guidelines for algorithmic fairness interventions to adhere to the anti-subordination principle. In doing so, we hope to bridge critical concepts between legal frameworks for non-discrimination and fairness in machine learning.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "724-737",
		"source": "ojs.aaai.org",
		"title": "Algorithmic Fairness From the Perspective of Legal Anti-discrimination Principles",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31674",
		"volume": "7",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Celis",
				"given": "L. Elisa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kimWhatsYourStake2024",
		"type": "article-journal",
		"abstract": "It's no secret that AI systems come with a significant environmental cost. This raises the question: What are the roles and responsibilities of computing professionals regarding the sustainability of AI? Informed by a year-long informal literature review on the subject, we employ stakeholder identification, analysis, and mapping to highlight the complex and interconnected roles that five major stakeholder groups (industry, practitioners, regulatory, advocacy, and the general public) play in the sustainability of AI. Swapping the traditional final step of stakeholder methods (stakeholder engagement) for entanglement, we demonstrate the inherent entwinement of choices made with regard to the development and maintenance of AI systems and the people who impact (or are impacted by) these choices. This entanglement should be understood as a system of human and non-human agents, with the implications of each choice ricocheting into the use of natural resources and climate implications. We argue that computing professionals (AI-focused or not) may belong to multiple stakeholder groups, and that we all have multiple roles to play in the sustainability of AI. Further, we argue that the nature of regulation in this domain will look unlike others in environmental preservation (e.g., legislation around water contaminants). As a result, we call for ongoing, flexible bodies and policies to move towards the regulation of AI from a sustainability angle, as well as suggest ways in which individual computing professionals can contribute to fighting the environmental and climate effects of AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "738-750",
		"source": "ojs.aaai.org",
		"title": "What’s Your Stake in Sustainability of AI?: An Informed Insider’s Guide",
		"title-short": "What’s Your Stake in Sustainability of AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31675",
		"volume": "7",
		"author": [
			{
				"family": "Kim",
				"given": "Grace C."
			},
			{
				"family": "Rothschild",
				"given": "Annabel"
			},
			{
				"family": "DiSalvo",
				"given": "Carl"
			},
			{
				"family": "DiSalvo",
				"given": "Betsy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kirfelAnticipatingRisksBenefits2024",
		"type": "article-journal",
		"abstract": "This paper examines the transformative potential of Counterfactual World Simulation Models (CWSMs). CWSMs use pieces of multi-modal evidence, such as the CCTV footage or sound recordings of a road accident, to build a high-fidelity 3D reconstruction of the scene. They can also answer causal questions, such as whether the accident happened because the driver was speeding, by simulating what would have happened in relevant counterfactual situations. CWSMs will enhance our capacity to envision alternate realities and investigate the outcomes of counterfactual alterations to how events unfold. This also, however, raises questions about what alternative scenarios we should be considering and what to do with that knowledge. We present a normative and ethical framework that guides and constrains the simulation of counterfactuals. We address the challenge of ensuring fidelity in reconstructions while simultaneously preventing stereotype perpetuation during counterfactual simulations. We anticipate different modes of how users will interact with CWSMs and discuss how their outputs may be presented. Finally, we address the prospective applications of CWSMs in the legal domain, recognizing both their potential to revolutionize legal proceedings as well as the ethical concerns they engender. Anticipating a new type of AI, this paper seeks to illuminate a path forward for responsible and effective use of CWSMs.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "751-751",
		"source": "ojs.aaai.org",
		"title": "Anticipating the Risks and Benefits of Counterfactual World Simulation Models (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31676",
		"volume": "7",
		"author": [
			{
				"family": "Kirfel",
				"given": "Lara"
			},
			{
				"family": "MacCoun",
				"given": "Rob"
			},
			{
				"family": "Icard",
				"given": "Thomas"
			},
			{
				"family": "Gerstenberg",
				"given": "Tobias"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "klymanAcceptableUsePolicies2024",
		"type": "article-journal",
		"abstract": "As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies—legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers’ acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Companies also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the AI ecosystem.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "752-767",
		"source": "ojs.aaai.org",
		"title": "Acceptable Use Policies for Foundation Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31677",
		"volume": "7",
		"author": [
			{
				"family": "Klyman",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "koltResponsibleReportingFrontier2024",
		"type": "article-journal",
		"abstract": "Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "768-783",
		"source": "ojs.aaai.org",
		"title": "Responsible Reporting for Frontier AI Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31678",
		"volume": "7",
		"author": [
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Barnhart",
				"given": "Joslyn"
			},
			{
				"family": "Brass",
				"given": "Asher"
			},
			{
				"family": "Esvelt",
				"given": "Kevin"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Rodriguez",
				"given": "Mikel"
			},
			{
				"family": "Sandbrink",
				"given": "Jonas B."
			},
			{
				"family": "Woodside",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "krishnaTradeoffsAdversarialRobustness2024",
		"type": "article-journal",
		"abstract": "As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-robust linear and non-linear models. Our empirical results with multiple real-world datasets validate our theoretical results and show the impact of varying degrees of model robustness on the cost and validity of the resulting recourses. Our analyses demonstrate that adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses, thus shedding light on the inherent trade-offs between adversarial robustness and actionable explanations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "784-795",
		"source": "ojs.aaai.org",
		"title": "On the Trade-offs between Adversarial Robustness and Actionable Explanations",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31679",
		"volume": "7",
		"author": [
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Agarwal",
				"given": "Chirag"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kwegyir-aggreyObservingContextImproves2024",
		"type": "article-journal",
		"abstract": "In many domains, it is difficult to obtain the race data that is required to estimate racial disparity.  To address this problem, practitioners have adopted the use of proxy methods which predict race using non-protected covariates.  However, these proxies often yield biased estimates, especially for minority groups, limiting their real-world utility.  In this paper, we introduce two new contextual proxy models that advance existing methods by incorporating contextual features in order to improve race estimates. We show that these algorithms demonstrate significant performance improvements in estimating disparities, on real-world home loan and voter data. We establish that achieving unbiased disparity estimates with contextual proxies relies on mean-consistency, a calibration-like condition.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "796-806",
		"source": "ojs.aaai.org",
		"title": "Observing Context Improves Disparity Estimation when Race is Unobserved",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31680",
		"volume": "7",
		"author": [
			{
				"family": "Kwegyir-Aggrey",
				"given": "Kweku"
			},
			{
				"family": "Durvasula",
				"given": "Naveen"
			},
			{
				"family": "Wang",
				"given": "Jennifer"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lamparthHumanVsMachine2024",
		"type": "article-journal",
		"abstract": "To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to examine crisis escalation in a fictional US-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as “pacifist” or “aggressive sociopath.” When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "807-817",
		"source": "ojs.aaai.org",
		"title": "Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
		"title-short": "Human vs. Machine",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31681",
		"volume": "7",
		"author": [
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Corso",
				"given": "Anthony"
			},
			{
				"family": "Ganz",
				"given": "Jacob"
			},
			{
				"family": "Mastro",
				"given": "Oriana Skylar"
			},
			{
				"family": "Schneider",
				"given": "Jacquelyn"
			},
			{
				"family": "Trinkunas",
				"given": "Harold"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yejasRacialNeighborhoodDisparities2024",
		"type": "article-journal",
		"abstract": "Legal financial obligations (LFOs) such as court fees and fines are commonly levied on individuals who are convicted of crimes. It is expected that LFO amounts should be similar across social, racial, and geographic subpopulations convicted of the same crime.  This work analyzes the distribution of LFOs in Jefferson County, Alabama and highlights disparities across different individual and neighborhood demographic characteristics. Data-driven discovery methods are used to detect subpopulations that experience higher LFOs than the overall population of offenders. Critically, these discovery methods do not rely on pre-specified groups and can assist scientists and researchers investigate socially-sensitive hypotheses in a disciplined way.  Some findings, such as individuals who are Black, live in Black-majority neighborhoods, or live in low-income neighborhoods tending to experience higher LFOs, are commensurate with prior expectation. However others, such as high LFO amounts in worthless instrument (bad check) cases experienced disproportionately by individuals living in affluent majority-white neighborhoods, are more surprising. More broadly than the specific findings, the methodology is shown to identify structural weaknesses that undermine the goal of equal justice under law that can be addressed through policy interventions.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "818-827",
		"source": "ojs.aaai.org",
		"title": "Racial and Neighborhood Disparities in Legal Financial Obligations in Jefferson County, Alabama",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31682",
		"volume": "7",
		"author": [
			{
				"family": "Yejas",
				"given": "Óscar Lara"
			},
			{
				"family": "Joshi",
				"given": "Aakanksha"
			},
			{
				"family": "Martinez",
				"given": "Andrew"
			},
			{
				"family": "Nelson",
				"given": "Leah"
			},
			{
				"family": "Speakman",
				"given": "Skyler"
			},
			{
				"family": "Thompson",
				"given": "Krysten"
			},
			{
				"family": "Nishimura",
				"given": "Yuki"
			},
			{
				"family": "Bond",
				"given": "Jordan"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lehdonvirtaComputeNorthVs2024",
		"type": "article-journal",
		"abstract": "Governments have begun to view AI compute infrastructures, including advanced AI chips, as a geostrategic resource. This is partly because “compute governance” is believed to be emerging as an important tool for governing AI systems. In this governance model, states that host AI compute capacity within their territorial jurisdictions are likely to be better placed to impose their rules on AI systems than states that do not. In this study, we provide the first attempt at mapping the global geography of public cloud GPU compute, one particularly important category of AI compute infrastructure. Using a census of hyperscale cloud providers’ cloud regions, we observe that the world is divided into “Compute North” countries that host AI compute relevant for AI development (ie. training), “Compute South” countries whose AI compute is more relevant for AI deployment (ie. running inferencing), and “Compute Desert” countries that host no public cloud AI compute at all. We generate potential explanations for the results using expert interviews, discuss the implications to AI governance and technology geopolitics, and consider possible future trajectories.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "828-838",
		"source": "ojs.aaai.org",
		"title": "Compute North vs. Compute South: The Uneven Possibilities of Compute-based AI Governance Around the Globe",
		"title-short": "Compute North vs. Compute South",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31683",
		"volume": "7",
		"author": [
			{
				"family": "Lehdonvirta",
				"given": "Vili"
			},
			{
				"family": "Wú",
				"given": "Bóxī"
			},
			{
				"family": "Hawkins",
				"given": "Zoe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "leidingerHowAreLLMs2024",
		"type": "article-journal",
		"abstract": "With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on `safety' training concerning legal liabilities at the expense of social impact evaluation. This mimics a similar trend which we could observe for search engine autocompletion some years prior. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by using four metrics, namely refusal rates, toxicity, sentiment and regard, with and without safety system prompts. Our findings indicate an improvement to stereotyping outputs with the system prompt, but overall a lack of attention by LLMs under study to certain harms classified as toxic, particularly for prompts about peoples/ethnicities and sexual orientation. Mentions of intersectional identities trigger a disproportionate amount of stereotyping. Finally, we discuss the implications of these findings about stereotyping harms in light of the coming intermingling of LLMs and search and the choice of stereotyping mitigation policy to adopt. We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "839-854",
		"source": "ojs.aaai.org",
		"title": "How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies",
		"title-short": "How Are LLMs Mitigating Stereotyping Harms?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31684",
		"volume": "7",
		"author": [
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Rogers",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "liFeasibilityIntentObfuscating2024",
		"type": "article-journal",
		"abstract": "Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose using intent obfuscation to generate adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors---YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN---using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attacks. Finally, we discuss main takeaways and legal repercussions. If you are reading the AAAI/ACM version, please download the technical appendix on arXiv at https://arxiv.org/abs/2408.02674",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "855-866",
		"source": "ojs.aaai.org",
		"title": "On Feasibility of Intent Obfuscating Attacks",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31685",
		"volume": "7",
		"author": [
			{
				"family": "Li",
				"given": "Zhaobin"
			},
			{
				"family": "Shafto",
				"given": "Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "linDemocratizingAIConcern2024",
		"type": "article-journal",
		"abstract": "The call to make artificial intelligence (AI) more democratic, or to “democratize AI,” is sometimes framed as a promising response for mitigating algorithmic injustice or making AI more aligned with social justice. However, the notion of “democratizing AI” is elusive, as the phrase has been associated with multiple meanings and practices, and the extent to which it may help mitigate algorithmic injustice is still underexplored. In this paper, based on a socio-technical understanding of algorithmic injustice, I examine three notable notions of democratizing AI and their associated measures—democratizing AI use, democratizing AI development, and democratizing AI governance—regarding their respective prospects and limits in response to algorithmic injustice. My examinations reveal that while some versions of democratizing AI bear the prospect of mitigating the concern of algorithmic injustice, others are somewhat limited and might even function to perpetuate unjust power hierarchies. This analysis thus urges a more fine-grained discussion on how to democratize AI and suggests that closer scrutiny of the power dynamics embedded in the socio-technical structure can help guide such explorations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "867-867",
		"source": "ojs.aaai.org",
		"title": "“Democratizing AI” and the Concern of Algorithmic Injustice (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31686",
		"volume": "7",
		"author": [
			{
				"family": "Lin",
				"given": "Ting-an"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "livanosFoundationsUnfairnessAnomaly2024",
		"type": "article-journal",
		"abstract": "Deep anomaly detection (AD) is perhaps the most controversial of data analytic tasks as it identifies entities that are specifically targeted for further investigation or exclusion. Also controversial is the application of AI to facial data, in particular facial recognition. This work explores the intersection of these two areas to understand two core questions: Who these algorithms are being unfair to and equally important why. Recent work has shown that deep AD can be unfair to different groups despite being unsupervised with a recent study showing that for portraits of people: men of color are far more likely to be chosen to be outliers. We study the two main categories of AD algorithms: autoencoder-based and single-class-based which effectively try to compress all the instances and those that can not be easily compressed are deemed to be outliers. We experimentally verify sources of unfairness such as the under-representation of a group (e.g people of color are relatively rare), spurious group features (e.g. men are often photographed with hats) and group labeling noise (e.g. race is subjective). We conjecture that lack of compressibility is the main foundation and the others cause it but experimental results show otherwise and we present a natural hierarchy amongst them.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "868-877",
		"source": "ojs.aaai.org",
		"title": "Foundations for Unfairness in Anomaly Detection - Case Studies in Facial Imaging Data",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31687",
		"volume": "7",
		"author": [
			{
				"family": "Livanos",
				"given": "Michael"
			},
			{
				"family": "Davidson",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "albaredaUncoveringGapChalleging2024",
		"type": "article-journal",
		"abstract": "In this paper, I will argue that the responsibility gap arising from new AI systems is reducible to the problem of many hands and collective agency. Systematic analysis of the agential dimension of AI will lead me to outline a disjunctive between the two problems. Either we reduce individual responsibility gaps to the many hands, or we abandon the individual dimension and accept the possibility of responsible collective agencies. Moreover, I will adduce that this conclusion reveals an underlying weakness in AI ethics: the lack of attention to the question of the disciplinary boundaries of AI ethics. This absence has made it difficult to identify the specifics of the responsibility gap arising from new AI systems as compared to the responsibility gaps of other applied ethics. Lastly, I will be concerned with outlining these specific aspects.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "878-878",
		"source": "ojs.aaai.org",
		"title": "Uncovering the Gap: Challeging the Agential Nature of AI Responsibility Problems (Extended Abstract)",
		"title-short": "Uncovering the Gap",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31688",
		"volume": "7",
		"author": [
			{
				"family": "Albareda",
				"given": "Joan Llorca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "locatelliExaminingBehaviorLLM2024",
		"type": "article-journal",
		"abstract": "The Exame Nacional do Ensino Médio (ENEM) is a pivotal test for Brazilian students, required for admission to a significant number of universities in Brazil. The test consists of four objective high-school level tests on Math, Humanities, Natural Sciences and Languages, and one writing essay. Students' answers to the test and to the accompanying socioeconomic status questionnaire are made public every year (albeit anonymized) due to transparency policies from the Brazilian Government. In the context of large language models (LLMs), these data lend themselves nicely to comparing different groups of humans with AI, as we can have access to human and machine answer distributions. We leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4, and MariTalk, a model trained using Portuguese data, to humans, aiming to ascertain how their answers relate to real societal groups and what that may reveal about the model biases. We divide the human groups by using socioeconomic status (SES), and compare their answer distribution with LLMs for each question and for the essay. We find no significant biases when comparing LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as the distance between model and human answers is mostly determined by the human accuracy. A similar conclusion is found by looking at the generated text as, when analyzing the essays, we observe that human and LLM essays differ in a few key factors, one being the choice of words where model essays were easily separable from human ones. The texts also differ syntactically, with LLM generated essays exhibiting, on average, smaller sentences and less thought units, among other differences. These results suggest that, for Brazilian Portuguese in the ENEM context, LLM outputs represent no group of humans, being significantly different from the answers from Brazilian students across all tests. The appendices may be found at https://arxiv.org/abs/2408.05035.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "879-890",
		"source": "ojs.aaai.org",
		"title": "Examining the Behavior of LLM Architectures Within the Framework of Standardized National Exams in Brazil",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31689",
		"volume": "7",
		"author": [
			{
				"family": "Locatelli",
				"given": "Marcelo Sartori"
			},
			{
				"family": "Miranda",
				"given": "Matheus Prado"
			},
			{
				"family": "Costa",
				"given": "Igor Joaquim da Silva"
			},
			{
				"family": "Prates",
				"given": "Matheus Torres"
			},
			{
				"family": "Thomé",
				"given": "Victor"
			},
			{
				"family": "Monteiro",
				"given": "Mateus Zaparoli"
			},
			{
				"family": "Lacerda",
				"given": "Tomas"
			},
			{
				"family": "Pagano",
				"given": "Adriana"
			},
			{
				"family": "Neto",
				"given": "Eduardo Rios"
			},
			{
				"family": "Jr",
				"given": "Wagner Meira"
			},
			{
				"family": "Almeida",
				"given": "Virgilio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "loeffladSocialScoringSystems2024",
		"type": "article-journal",
		"abstract": "Recent developments in artificial intelligence research have advanced the spread of automated decision-making (ADM) systems used for regulating human behaviors. In this context, prior work has focused on the determinants of human trust in and the legitimacy of ADM systems, e.g., when used for decision support. However, studies assessing people's perceptions of ADM systems used for behavioral regulation, as well as the effect on behaviors and the overall impact on human communities are largely absent. In this paper, we experimentally investigate people's behavioral adaptations to, and their perceptions of an institutionalized decision-making system, which resembled a social scoring system. Using social scores as incentives, the system aimed at ensuring mutual fair treatment between members of experimental communities. We explore how the provision of transparency affected people’s perceptions, behaviors, as well as the well-being of the communities. While a non-transparent scoring system led to disparate impacts both within as well as across communities, transparency helped people develop trust in each other, create wealth, and enabled them to benefit from the system in a more uniform manner. A transparent system was perceived as more effective, procedurally just, and legitimate, and led people to rely more strongly on the system. However, transparency also made people strongly discipline those with a low score. This suggests that social scoring systems that precisely disclose past behaviors may also impose significant discriminatory consequences on individuals deemed non-compliant.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "891-904",
		"source": "ojs.aaai.org",
		"title": "Social Scoring Systems for Behavioral Regulation: An Experiment on the Role of Transparency in Determining Perceptions and Behaviors",
		"title-short": "Social Scoring Systems for Behavioral Regulation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31690",
		"volume": "7",
		"author": [
			{
				"family": "Loefflad",
				"given": "Carmen"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lovatoForegroundingArtistOpinions2024",
		"type": "article-journal",
		"abstract": "Generative AI tools are used to create art-like outputs and sometimes aid in the creative process. These tools have potential benefits for artists, but they also have the potential to harm the art workforce and infringe upon artistic and intellectual property rights. Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like outputs at scale. These outputs are now being used to compete with human artists in the marketplace as well as being used by some artists in their generative processes to create art. We surveyed 459 artists to investigate the tension between artists' opinions on Generative AI art's potential utility and harm. This study surveys artists' opinions on the utility and threat of Generative AI art models, fair practices in the disclosure of artistic works in AI art training models, ownership and rights of AI art derivatives, and fair compensation. Results show that a majority of artists believe creators should disclose what art is being used in AI training, that AI outputs should not belong to model creators, and express concerns about AI's impact on the art workforce and who profits from their art. We hope the results of this work will further meaningful collaboration and alignment between the art community and Generative AI researchers and developers.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "905-916",
		"source": "ojs.aaai.org",
		"title": "Foregrounding Artist Opinions: A Survey Study on Transparency, Ownership, and Fairness in AI Generative Art",
		"title-short": "Foregrounding Artist Opinions",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31691",
		"volume": "7",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper"
			},
			{
				"family": "Zimmerman",
				"given": "Julia Witte"
			},
			{
				"family": "Smith",
				"given": "Isabelle"
			},
			{
				"family": "Dodds",
				"given": "Peter"
			},
			{
				"family": "Karson",
				"given": "Jennifer L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lunaNavigatingGovernanceParadigms2024",
		"type": "article-journal",
		"abstract": "As Generative Artificial Intelligence (GenAI) technologies\nevolve at an unprecedented rate, global governance approaches\nstruggle to keep pace with the technology, highlighting\na critical issue in the governance adaptation of significant\nchallenges. Depicting the nuances of nascent and\ndiverse governance approaches based on risks, rules, outcomes,\nprinciples, or a mix, across different regions around\nthe globe, is fundamental to discern discrepancies and convergences,\nand to shed light on specific limitations that need\nto be addressed, thereby facilitating the safe and trustworthy\nadoption of GenAI. In response to the need and the evolving\nnature of GenAI, this paper seeks to provide a collective\nview of different governance approaches around the world.\nOur research introduces a Harmonized GenAI Framework,\n“H-GenAIGF”, based on the current governance approaches\nof six regions: (European Union (EU), United States (US),\nChina (CN), Canada (CA), United Kingdom (UK), and Singapore\n(SG)). We have identified four constituents, fifteen\nprocesses, twenty-five sub-processes, and nine principles that\naid the governance of GenAI, thus providing a comprehensive\nperspective on the current state of GenAI governance. In\naddition, we present a comparative analysis to facilitate identification\nof common ground and distinctions based on coverage\nof the processes by each region. The results show that\nrisk-based approaches allow for better coverage of the processes,\nfollowed by mixed approaches. Other approaches lag\nbehind, covering less than 50% of the processes. Most prominently,\nthe analysis demonstrates that amongst the regions,\nonly one process aligns across all approaches, highlighting\nthe lack of consistent and executable provisions. Moreover,\nour case study on ChatGPT reveals process coverage deficiency,\nshowing that harmonization of approaches is necessary\nto find alignment for GenAI governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "917-931",
		"source": "ojs.aaai.org",
		"title": "Navigating Governance Paradigms: A Cross-Regional Comparative Study of Generative AI Governance Processes & Principles",
		"title-short": "Navigating Governance Paradigms",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31692",
		"volume": "7",
		"author": [
			{
				"family": "Luna",
				"given": "Jose"
			},
			{
				"family": "Tan",
				"given": "Ivan"
			},
			{
				"family": "Xie",
				"given": "Xiaofei"
			},
			{
				"family": "Jiang",
				"given": "Lingxiao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "maasParticipatoryAI2024",
		"type": "article-journal",
		"abstract": "The ‘participatory turn’ in AI design has received much attention in the literature. In this paper, we provide various arguments and proposals to move the discussion of participatory AI beyond its current state and towards stakeholder empowerment. The participatory AI literature points to Arnstein’s understanding of ‘citizen power’ as the right approach to participation. Although we agree with this general idea, we argue that there is a lack of depth in analyzing the legal, economic, and political arrangements required for a genuine redistribution of power to prioritize AI stakeholders. We highlight two domains on which the current discourse on participatory AI needs to expand. These are (1) the legal-institutional background that could provide ‘participation teeth’ for stakeholder empowerment and (2) the political economy of AI production that fosters such power asymmetries between AI developers and other stakeholders. We conclude by offering ways forward to explore alternative legal arrangements and ownership models for participatory AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "932-942",
		"source": "ojs.aaai.org",
		"title": "Beyond Participatory AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31693",
		"volume": "7",
		"author": [
			{
				"family": "Maas",
				"given": "Jonne"
			},
			{
				"family": "Inglés",
				"given": "Aarón Moreno"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "manziniCodeThatBinds2024",
		"type": "article-journal",
		"abstract": "The development of increasingly agentic and human-like AI assistants, capable of performing a wide range of tasks on user's behalf over time, has sparked heightened interest in the nature and bounds of human interactions with AI. Such systems may indeed ground a transition from task-oriented interactions with AI, at discrete time intervals, to ongoing relationships -- where users develop a deeper sense of connection with and attachment to the technology. This paper investigates what it means for relationships between users and advanced AI assistants to be appropriate and proposes a new framework to evaluate both users' relationships with AI and developers' design choices. We first provide an account of advanced AI assistants, motivating the question of appropriate relationships by exploring several distinctive features of this technology. These include anthropomorphic cues and the longevity of interactions with users, increased AI agency, generality and context ambiguity, and the forms and depth of dependence the relationship could engender. Drawing upon various ethical traditions, we then consider a series of values, including benefit, flourishing, autonomy and care, that characterise appropriate human interpersonal relationships. These values guide our analysis of how the distinctive features of AI assistants may give rise to inappropriate relationships with users. Specifically, we discuss a set of concrete risks arising from user--AI assistant relationships that: (1) cause direct emotional or physical harm to users, (2) limit opportunities for user personal development, (3) exploit user emotional dependence, and (4) generate material dependencies without adequate commitment to user needs. We conclude with a set of recommendations to address these risks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "943-957",
		"source": "ojs.aaai.org",
		"title": "The Code That Binds Us: Navigating the Appropriateness of Human-AI Assistant Relationships",
		"title-short": "The Code That Binds Us",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31694",
		"volume": "7",
		"author": [
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Keeling",
				"given": "Geoff"
			},
			{
				"family": "Alberts",
				"given": "Lize"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "menonLessonsClinicalCommunications2024",
		"type": "article-journal",
		"abstract": "One of the major challenges in the use of opaque, complex AI models is the need or desire to provide an explanation to the end-user (and other stakeholders) as to how the system arrived at the answer it did. While there is significant research in the development of explainability techniques for AI, the question remains as to who needs an explanation, what an explanation consists of, and how to communicate this to a lay user who lacks direct expertise in the area. In this position paper, an interdisciplinary team of researchers argue that the example of clinical communications offers lessons to those interested in improving the transparency and interpretability of AI systems. We identify five lessons from clinical communications: (1) offering explanations for AI systems and disclosure of their use recognizes the dignity of those using and impacted by it; (2) AI explanations can be productively targeted rather than totally comprehensive; (3) AI explanations can be enforced through codified rules but also norms, guided by core values; (4) what constitutes a “good” AI explanation will require repeated updating due to changes in technology and social expectations; 5) AI explanations will have impacts beyond defining any one AI system, shaping and being shaped by broader perceptions of AI. We review the history, debates and consequences surrounding the institutionalization of one type of clinical communication, informed consent, in order to illustrate the challenges and opportunities that may await attempts to offer explanations of opaque AI models. We highlight takeaways and implications for computer scientists and policymakers in the context of growing concerns and moves toward AI governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "958-970",
		"source": "ojs.aaai.org",
		"title": "Lessons from Clinical Communications for Explainable AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31695",
		"volume": "7",
		"author": [
			{
				"family": "Menon",
				"given": "Alka V."
			},
			{
				"family": "Omar",
				"given": "Zahra Abba"
			},
			{
				"family": "Nahar",
				"given": "Nadia"
			},
			{
				"family": "Papademetris",
				"given": "Xenophon"
			},
			{
				"family": "Fiellin",
				"given": "Lynn E."
			},
			{
				"family": "Kästner",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "michelPayAttentionCall2024",
		"type": "article-journal",
		"abstract": "Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "971-983",
		"source": "ojs.aaai.org",
		"title": "Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance",
		"title-short": "Pay Attention",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31696",
		"volume": "7",
		"author": [
			{
				"family": "Michel",
				"given": "Franck"
			},
			{
				"family": "Gandon",
				"given": "Fabien"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "muellerLLMsMemorizationQuality2024",
		"type": "article-journal",
		"abstract": "Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act. In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example. Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario. Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions. The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data. We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors. We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations. Code can be found at github.com/felixbmuller/llms-memorization-copyright.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "984-996",
		"source": "ojs.aaai.org",
		"title": "LLMs and Memorization: On Quality and Specificity of Copyright Compliance",
		"title-short": "LLMs and Memorization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31697",
		"volume": "7",
		"author": [
			{
				"family": "Mueller",
				"given": "Felix B."
			},
			{
				"family": "Görge",
				"given": "Rebekka"
			},
			{
				"family": "Bernzen",
				"given": "Anna K."
			},
			{
				"family": "Pirk",
				"given": "Janna C."
			},
			{
				"family": "Poretschkin",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "munParticipAIDemocraticSurveying2024",
		"type": "article-journal",
		"abstract": "General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without a comprehensive assessment of risks. As a first step towards democratic risk assessment and design of general purpose AI, we introduce PARTICIP-AI, a carefully designed framework for laypeople to speculate and assess AI use cases and their impacts. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI devel- opment through making a concluding choice on its development. To showcase the promise of our framework towards informing democratic AI development, we run a medium-scale study with inputs from 295 demographically diverse participants. Our analyses show that participants’ responses emphasize applications for personal life and society, contrasting with most current AI development’s business focus. We also surface diverse set of envisioned harms such as distrust in AI and institutions, complementary to those defined by experts. Furthermore, we found that perceived impact of not developing use cases significantly predicted participants’ judgements of whether AI use cases should be developed, and highlighted lay users’ concerns of techno-solutionism. We conclude with a discussion on how frameworks like PARTICIP-AI can further guide democratic AI development and governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "997-1010",
		"source": "ojs.aaai.org",
		"title": "Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits",
		"title-short": "Particip-AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31698",
		"volume": "7",
		"author": [
			{
				"family": "Mun",
				"given": "Jimin"
			},
			{
				"family": "Jiang",
				"given": "Liwei"
			},
			{
				"family": "Liang",
				"given": "Jenny"
			},
			{
				"family": "Cheong",
				"given": "Inyoung"
			},
			{
				"family": "DeCairo",
				"given": "Nicole"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Sap",
				"given": "Maarten"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nakajimaQuantifyingGenderedCitation2024",
		"type": "article-journal",
		"abstract": "The number of citations received by papers often exhibits imbalances in terms of author attributes such as country of affiliation and gender. While recent studies have quantified citation imbalance in terms of the authors' gender in journal papers, the computer science discipline, where researchers frequently present their work at conferences, may exhibit unique patterns in gendered citation imbalance. Additionally, understanding how network properties in citations influence citation imbalances remains challenging due to a lack of suitable reference models. In this paper, we develop a family of reference models for citation networks and investigate gender imbalance in citations between papers published in computer science conferences. By deploying these reference models, we found that homophily in citations is strongly associated with gendered citation imbalance in computer science, whereas heterogeneity in the number of citations received per paper has a relatively minor association with it. Furthermore, we found that the gendered citation imbalance is most pronounced in papers published in the highest-ranked conferences, is present across different subfields, and extends to citation-based rankings of papers. Our study provides a framework for investigating associations between network properties and citation imbalances, aiming to enhance our understanding of the structure and dynamics of citations between research publications.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1011-1022",
		"source": "ojs.aaai.org",
		"title": "Quantifying Gendered Citation Imbalance in Computer Science Conferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31699",
		"volume": "7",
		"author": [
			{
				"family": "Nakajima",
				"given": "Kazuki"
			},
			{
				"family": "Sasaki",
				"given": "Yuya"
			},
			{
				"family": "Tokuno",
				"given": "Sohei"
			},
			{
				"family": "Fletcher",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nanniniHabemusRightExplanation2024",
		"type": "article-journal",
		"abstract": "The European Union's Artificial Intelligence Act (AI Act), finalized in February 2024, mandates comprehensive transparency and explainability requirements for AI systems to enable effective oversight and safeguard fundamental rights. However, the practical implementation of these requirements faces challenges due to tensions between the need for meaningful explanations and the potential risks to intellectual property and commercial interests of AI providers. This research proposes the Transparency-Explainability Functionality and Tensions (TEFT) framework to systematically analyze the complex interplay of legal, technical, and socio-ethical factors shaping the realization of algorithmic transparency and explainability in the EU context.\nThrough a two-pronged approach combining a focused literature review and an in-depth examination of the AI Act's provisions, we identify key friction points and challenges in operationalizing the right to explanation. The TEFT framework maps the interests and incentives of various stakeholders, including AI providers & deployers, oversight bodies, and affected individuals, while considering their goals, expected benefits, risks, possible negative impacts, and context to algorithmic explainability.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1023-1035",
		"source": "ojs.aaai.org",
		"title": "Habemus a Right to an Explanation: so What? – A Framework on Transparency-Explainability Functionality and Tensions in the EU AI Act",
		"title-short": "Habemus a Right to an Explanation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31700",
		"volume": "7",
		"author": [
			{
				"family": "Nannini",
				"given": "Luca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nejadgholiHumanCenteredAIApplications2024",
		"type": "article-journal",
		"abstract": "While AI has been frequently applied in the context of immigration, most of these applications focus on selection and screening, which primarily serve to empower states and authorities, raising concerns due to their understudied reliability and high impact on immigrants' lives. In contrast, this paper emphasizes the potential of AI in Canada’s immigration settlement phase, a stage where access to information is crucial and service providers are overburdened. By highlighting the settlement sector as a prime candidate for reliable AI applications, we demonstrate its unique capacity to empower immigrants directly, yet it remains under-explored in AI research. We outline a vision for human-centred and responsible AI solutions that facilitate the integration of newcomers. We call on AI researchers to build upon our work and engage in multidisciplinary research and active collaboration with service providers and government organizations to develop tailored AI tools that are empowering, inclusive and safe.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1036-1050",
		"source": "ojs.aaai.org",
		"title": "Human-Centered AI Applications for Canada’s Immigration Settlement Sector",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31701",
		"volume": "7",
		"author": [
			{
				"family": "Nejadgholi",
				"given": "Isar"
			},
			{
				"family": "Molamohammadi",
				"given": "Maryam"
			},
			{
				"family": "Missaghi",
				"given": "Kimiya"
			},
			{
				"family": "Bakhtawar",
				"given": "Samir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nematovAIDEAntitheticalIntentbased2024",
		"type": "article-journal",
		"abstract": "For many use-cases, it is often important to explain the prediction of a black-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's reasoning from different angles. \n\nIn this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explanations for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, investigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast.\nTo provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and increase coverage of the training data. \n\nWe demonstrate the effectiveness of AIDE on image and text classification tasks,\nin three ways: \nquantitatively, assessing correctness and continuity; \nqualitatively, comparing anecdotal evidence from AIDE and other example-based approaches;\nand via a user study, evaluating multiple aspects of AIDE.\nThe results show that AIDE addresses the limitations of existing methods and exhibits desirable traits for an explainability method.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1051-1062",
		"source": "ojs.aaai.org",
		"title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations",
		"title-short": "AIDE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31702",
		"volume": "7",
		"author": [
			{
				"family": "Nematov",
				"given": "Ikhtiyor"
			},
			{
				"family": "Sacharidis",
				"given": "Dimitris"
			},
			{
				"family": "Hose",
				"given": "Katja"
			},
			{
				"family": "Sagi",
				"given": "Tomer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "norhashimMeasuringHumanAIValue2024",
		"type": "article-journal",
		"abstract": "This paper seeks to quantify the human-AI value alignment in large language models. Alignment between humans and AI has become a critical area of research to mitigate potential harm posed by AI. In tandem with this need, developers have incorporated a values-based approach towards model development where ethical principles are integrated from its inception. However, ensuring that these values are reflected in outputs remains a challenge. In addition, studies have noted that models lack consistency when producing outputs, which in turn can affect their function. Such variability in responses would impact human-AI value alignment as well, particularly where consistent alignment is critical. Fundamentally, the task of uncovering a model’s alignment is one of explainability – where understanding how these complex models behave is essential in order to assess their alignment. \n\nThis paper examines the problem through a case study of GPT-3.5. By repeatedly prompting the model with scenarios based on a dataset of moral stories, we aggregate the model’s alignment with human values to produce a human-AI value alignment metric. Moreover, by using a comprehensive taxonomy of human values, we uncover the latent value profile represented by these outputs, thereby determining the extent of human-AI value alignment.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1063-1073",
		"source": "ojs.aaai.org",
		"title": "Measuring Human-AI Value Alignment in Large Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31703",
		"volume": "7",
		"author": [
			{
				"family": "Norhashim",
				"given": "Hakim"
			},
			{
				"family": "Hahn",
				"given": "Jungpil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nunesAreLargeLanguage2024",
		"type": "article-journal",
		"abstract": "Large language models (LLMs) have taken centre stage in debates on Artificial Intelligence. Yet there remains a gap in how to assess LLMs' conformity to important human values. In this paper, we investigate whether state-of-the-art LLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid results) are moral hypocrites. We employ two research instruments based on the Moral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which investigates which values are considered morally relevant in abstract moral judgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate moral cognition in concrete scenarios related to each moral foundation. We characterise conflicts in values between these different abstractions of moral evaluation as hypocrisy. We found that both models displayed reasonable consistency within each instrument compared to humans, but they displayed contradictory and hypocritical behaviour when we compared the abstract values present in the MFQ to the evaluation of concrete moral violations of the MFV.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1074-1087",
		"source": "ojs.aaai.org",
		"title": "Are Large Language Models Moral Hypocrites? A Study Based on Moral Foundations",
		"title-short": "Are Large Language Models Moral Hypocrites?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31704",
		"volume": "7",
		"author": [
			{
				"family": "Nunes",
				"given": "José Luiz"
			},
			{
				"family": "Almeida",
				"given": "Guilherme F. C. F."
			},
			{
				"family": "Araujo",
				"given": "Marcelo",
				"dropping-particle": "de"
			},
			{
				"family": "Barbosa",
				"given": "Simone D. J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "olulanaHiddenInferredFair2024",
		"type": "article-journal",
		"abstract": "As learning-to-rank models are increasingly deployed for decision-making in areas with profound life implications, the FairML community has been developing fair learning-to-rank (LTR) models. These models rely on the availability of sensitive demographic features such as race or sex. However, in practice, regulatory obstacles and privacy concerns protect this data from collection and use. As a result, practitioners may either need to promote fairness despite the absence of these features or turn to demographic inference tools to attempt to infer them. Given that these tools are fallible, this paper aims to further understand how errors in demographic inference impact the fairness performance of popular fair LTR strategies. In which cases would it be better to keep such demographic attributes hidden from models versus infer them? We examine a spectrum of fair LTR strategies ranging from fair LTR with and without demographic features hidden versus inferred to fairness-unaware LTR followed by fair re-ranking. We conduct a controlled empirical investigation modeling different levels of inference errors by systematically perturbing the inferred sensitive attribute. We also perform three case studies with real-world datasets and popular open-source inference methods. Our findings reveal that as inference noise grows, LTR-based methods that incorporate fairness considerations into the learning process may increase bias. In contrast, fair re-ranking strategies are more robust to inference errors. All source code, data, and experimental artifacts of our experimental study are available here: https://github.com/sewen007/hoiltr.git",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1088-1099",
		"source": "ojs.aaai.org",
		"title": "Hidden or Inferred: Fair Learning-To-Rank With Unknown Demographics",
		"title-short": "Hidden or Inferred",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31705",
		"volume": "7",
		"author": [
			{
				"family": "Olulana",
				"given": "Oluseun"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Murai",
				"given": "Fabricio"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "oudahPerceptionExperienceInfluences2024",
		"type": "article-journal",
		"abstract": "It has been argued that human social and economic interactions depend on the perception of mind of the interacting partner. Minds are perceived along two dimensions: experience, i.e., the ability to feel, and agency, i.e., the ability to act and take responsibility for one’s actions. Here, we pair participants with bots in a dictator game (to measure altruism) and a trust game (to measure trust) while varying the bots’ perceived experience and agency.  Here, we pair participants with bots in a dictator game (to measure altruism) and a trust game (to measure trust) while varying the bots' perceived experience and agency. Results demonstrate that the perception of experience influences altruism, while the perception of agency influences trust.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1100-1100",
		"source": "ojs.aaai.org",
		"title": "Perception of Experience Influences Altruism and Perception of Agency Influences Trust in Human-Machine Interactions (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31706",
		"volume": "7",
		"author": [
			{
				"family": "Oudah",
				"given": "Mayada"
			},
			{
				"family": "Makovi",
				"given": "Kinga"
			},
			{
				"family": "Gray",
				"given": "Kurt"
			},
			{
				"family": "Battu",
				"given": "Balaraju"
			},
			{
				"family": "Rahwan",
				"given": "Talal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "owensFaceFactsUsing2024",
		"type": "article-journal",
		"abstract": "We applied techniques from psychology --- typically used to visualize human bias --- to facial analysis systems, providing novel approaches for diagnosing and communicating algorithmic bias. First, we aggregated a diverse corpus of human facial images (N=1492) with self-identified gender and race. We tested four automated gender recognition (AGR) systems and found that some exhibited intersectional gender-by-race biases. Employing a technique developed by psychologists --- face averaging --- we created composite images to visualize these systems' outputs. For example, we visualized what an \"average woman\" looks like, according to a system's output. Second, we conducted two online experiments wherein participants judged the bias of hypothetical AGR systems. The first experiment involved participants (N=228) from a convenience sample. When depicting the same results in different formats, facial visualizations communicated bias to the same magnitude as statistics. In the second experiment with only Black participants (N=223), facial visualizations communicated bias significantly more than statistics, suggesting that face averages are meaningful for communicating algorithmic bias.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1101-1111",
		"source": "ojs.aaai.org",
		"title": "Face the Facts: Using Face Averaging to Visualize Gender-by-Race Bias in Facial Analysis Algorithms",
		"title-short": "Face the Facts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31707",
		"volume": "7",
		"author": [
			{
				"family": "Owens",
				"given": "Kentrell"
			},
			{
				"family": "Freiburger",
				"given": "Erin"
			},
			{
				"family": "Hutchings",
				"given": "Ryan"
			},
			{
				"family": "Sim",
				"given": "Mattea"
			},
			{
				"family": "Hugenberg",
				"given": "Kurt"
			},
			{
				"family": "Roesner",
				"given": "Franziska"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "papageorgiouProxyFairnessEuropean2024",
		"type": "article-journal",
		"abstract": "This paper navigates the convergence of the European Data Protection Regulation and the AI Act within the paradigm of computational methods that operationalise fairness in the absence of demographic data, notably through the use of proxy variables and inferential techniques (Proxy Fairness). Particularly, it explores the legal nature of the data involved in Proxy Fairness under the European Data Protection Regulation, focusing on the legal notion of Sensitivity. Moreover, it examines the lawfulness of processing sensitive personal data for Proxy Fairness purposes under the AI Act, particularly focusing on the legal requirement of Necessity. Through this analysis, the paper aims to shed light on core aspects of the legitimacy of Proxy Fairness in the context of EU law, providing a normative foundation to this line of Fair-AI approaches.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1112-1122",
		"source": "ojs.aaai.org",
		"title": "Proxy Fairness under the European Data Protection Regulation and the AI Act: A Perspective of Sensitivity and Necessity",
		"title-short": "Proxy Fairness under the European Data Protection Regulation and the AI Act",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31708",
		"volume": "7",
		"author": [
			{
				"family": "Papageorgiou",
				"given": "Ioanna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "pinkavaModelDataAgnosticDebiasing2024",
		"type": "article-journal",
		"abstract": "As reliance on Machine Learning (ML) systems in real-world decision-making processes grows, ensuring these systems are free of bias against sensitive demographic groups is of increasing importance. Existing techniques for automatically debiasing ML models generally require access to either the models’ internal architectures, the models’ training datasets, or both. In this paper we outline the reasons why such requirements are disadvantageous, and present an alternative novel debiasing system that is both data- and model-agnostic. We implement this system as a Reinforcement Learning Agent and through extensive experiments show that we can debias a variety of target ML model architectures over three benchmark datasets. Our results show performance comparable to data- and/or model-gnostic state-of-the-art debiasers.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1123-1131",
		"source": "ojs.aaai.org",
		"title": "A Model- and Data-Agnostic Debiasing System for Achieving Equalized Odds",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31709",
		"volume": "7",
		"author": [
			{
				"family": "Pinkava",
				"given": "Thomas"
			},
			{
				"family": "McFarland",
				"given": "Jack"
			},
			{
				"family": "Mashhadi",
				"given": "Afra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "pistilliCIVICSBuildingDataset2024",
		"type": "article-journal",
		"abstract": "This paper introduces the \"CIVICS: Culturally-Informed \\& Values-Inclusive Corpus for Societal impacts\" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) towards socially sensitive topics across multiple languages and cultures. The hand-crafted, multilingual dataset of statements addresses value-laden topics, including LGBTQI rights, social welfare, immigration, disability rights, and surrogacy. CIVICS is designed to elicit responses from LLMs to shed light on how values encoded in their parameters shape their behaviors. Through our dynamic annotation processes, tailored prompt design, and experiments, we investigate how open-weight LLMs respond to these issues, exploring their behavior across diverse linguistic and cultural contexts.\nUsing two experimental set-ups based on log-probabilities and long-form responses, we show social and cultural variability across different LLMs. Specifically, different topics and sources lead to more pronounced differences across model answers, particularly on immigration, LGBTQI rights, and social welfare. Experiments on generating long-form responses from models tuned for user chat demonstrate that refusals are triggered disparately across different models, but consistently and more frequently in English or translated statements. As shown by our initial experimentation, the CIVICS dataset can serve as a tool for future research, promoting reproducibility and transparency across broader linguistic settings, and furthering the development of AI technologies that respect and reflect global cultural diversities and value pluralism. \nThe CIVICS dataset and tools are made available under open licenses at hf.co/CIVICS-dataset.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1132-1144",
		"source": "ojs.aaai.org",
		"title": "CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models",
		"title-short": "CIVICS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31710",
		"volume": "7",
		"author": [
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "poirouxDisengagementAlgorithmsHow2024",
		"type": "article-journal",
		"abstract": "This study examines the use of algorithmic tools in traditional organizational decision-making processes. Through forty semi-structured interviews with managers, engineers, and (expert) users across six European projects, we suggest that initiators deploy algorithms not to automate actions or replace users, but to disengage themselves from prescriptive decision-making. Consequently, the responsibility to choose, select, and decide falls upon the users; they become engaged. Therefore, algorithm evaluation is oriented towards utility, interpretability, and, more broadly, user satisfaction. Further research is encouraged to analyze the advent of a 'satisfaction regime', from platforms to traditional organizations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1145-1156",
		"source": "ojs.aaai.org",
		"title": "Disengagement through Algorithms: How Traditional Organizations Aim for Experts' Satisfaction",
		"title-short": "Disengagement through Algorithms",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31711",
		"volume": "7",
		"author": [
			{
				"family": "Poiroux",
				"given": "Jérémie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "probascoNotOraclesBattlefield2024",
		"type": "article-journal",
		"abstract": "AI-based military decision support systems that help commanders observe, orient, decide, and act on the battlefield are highly sought after by military leadership. With the advent of large language models, AI developers have begun advertising automated AI-based decision support systems designed to both analyze and act on data from the battlefield. While the desire to use decision support systems to make better decisions on the battlefield is unsurprising, the responsible deployment of such systems requires a clear understanding of the capabilities and limitations of modern machine learning models. This paper reviews recently proposed uses of AI-enables decision support systems (DSS), provides a simplified framework for considering AI-DSS capabilities and limitations, and recommends practical risk mitigations commanders might employ when operating with an AI-enabled DSS.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1157-1165",
		"source": "ojs.aaai.org",
		"title": "Not Oracles of the Battlefield: Safety Considerations for AI-Based Military Decision Support Systems",
		"title-short": "Not Oracles of the Battlefield",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31712",
		"volume": "7",
		"author": [
			{
				"family": "Probasco",
				"given": "Emelia"
			},
			{
				"family": "Burtell",
				"given": "Matthew"
			},
			{
				"family": "Toner",
				"given": "Helen"
			},
			{
				"family": "Rudner",
				"given": "Tim G. J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "purvesWhatTrustWhen2024",
		"type": "article-journal",
		"abstract": "What to Trust When We Trust Artificial Intelligence\n\nAbstract:\nSo-called “trustworthy AI” has emerged as a guiding aim of industry leaders, computer and data science researchers, and policy makers in the US and Europe. Often, trustworthy AI is characterized in terms of a list of criteria. These lists usually include at least fairness, accountability, and transparency. Fairness, accountability, and transparency are valuable objectives, and they have begun to receive attention from philosophers and legal scholars. However, those who put forth criteria for trustworthy AI have failed to explain why satisfying the criteria makes an AI system—or the organizations that make use of the AI system—worthy of trust. Nor do they explain why the aim of trustworthy AI is important enough to justify devoting resources to achieve it. It even remains unclear whether an AI system is the sort of thing that can be trustworthy or not.\n\nTo explain why fairness, accountability, and transparency are suitable criteria for trustworthy AI one needs an analysis of trustworthy AI. Providing an analysis of trustworthy AI is a distinct task from providing criteria. Criteria are diagnostic; they provide a useful test for the phenomenon of interest, but they do not purport to explain the nature of the phenomenon. It is conceivable that an AI system could lack transparency, accountability, or fairness while remaining trustworthy. An analysis of trustworthy AI provides the fundamental features of an AI system in virtue of which it is (or is not) worthy of trust. An AI system that lacks these features will, necessarily, fail to be worthy of trust. This paper puts forward an analysis of trustworthy AI that can be used to critically evaluate criteria for trustworthy AI such as fairness, accountability, and transparency. \n\nIn this paper we first make clear the target concept to be analyzed: trustworthy AI. We argue that AI, at least in its current form, should be understood as a distributed, complex system embedded in a larger institutional context. This characterization of AI is consistent with recent definitions proposed by national and international regulatory bodies, and it eliminates some unhappy ambiguity in the common usage of the term. We further limit the scope of our discussion to AI systems which are used to inform decision-making about qualification problems, problems wherein a decision-maker must decide whether an individual is qualified for some beneficial or harmful treatment. We argue that, given reasonable assumptions about the nature of trust and trustworthiness, only AI systems that are used to inform decision-making about qualification problems are appropriate candidates for attributions of (un)trustworthiness.\n\nWe then distinguish between two models of trust and trustworthiness that we find in the existing literature. We motivate our account by highlighting this as a dilemma in in the accounts of trustworthy AI that have previously been offered. These accounts claim that trustworthiness is either exclusive to full agents (and it is thus nonsense when we talk of trustworthy AI), or they offer an account of trustworthiness that collapses into mere reliability. The first sort of account we refer to as an agential account and the second sort we refer to as a reliability account. We offer that one of the core challenges of putting forth an account of trustworthy AI is to avoid reducing to one of these two camps. It is thus a desideratum of our account that it avoids being exclusive to full moral agents, while it simultaneously avoids capturing things such as mere tools. We go on to propose our positive account which we submit avoids these twin pitfalls.\n\nWe subsequently argue that if AI can be trustworthy, then it will be trustworthy on an institutional model. Starting from an account of institutional trust offered by Purves and Davis, we argue that trustworthy AI systems have three features: they are competent with regard to the task they are assigned, they are responsive to the morally salient facts governing the decision-making context in which they are deployed, and they publicly provide evidence of these features. As noted, this account builds on a model of institutional trust offered by Purves and Davis and an account of default trust from Margaret Urban Walker. The resulting account allows us to accommodate the core challenge of finding a balance between agential accounts and reliability accounts. We go on to refine our account, answer objections, and revisit the list criteria from above as explained in terms of competence, responsiveness, and evidence.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1166-1166",
		"source": "ojs.aaai.org",
		"title": "What to Trust When We Trust Artificial Intelligence (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31713",
		"volume": "7",
		"author": [
			{
				"family": "Purves",
				"given": "Duncan"
			},
			{
				"family": "Sturm",
				"given": "Schuyler"
			},
			{
				"family": "Madock",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "qianPPSPersonalizedPolicy2024",
		"type": "article-journal",
		"abstract": "AI-enabled agents designed to assist humans are gaining traction in a variety of domains such as healthcare and disaster response. It is evident that, as we move forward, these agents will play increasingly vital roles in our lives. To realize this future successfully and mitigate its unintended consequences, it is imperative that humans have a clear understanding of the agents that they work with. Policy summarization methods help facilitate this understanding by showcasing key examples of agent behaviors to their human users. Yet, existing methods produce “one-size-fits-all” summaries for a generic audience ahead of time. Drawing inspiration from research in pedagogy, we posit that personalized policy summaries can more effectively enhance user understanding. To evaluate this hypothesis, this paper presents and benchmarks a novel technique: Personalized Policy Summarization (PPS). PPS discerns a user’s mental model of the agent through a series of algorithmically generated questions and crafts customized policy summaries to enhance user understanding. Unlike existing methods, PPS actively engages with users to gauge their comprehension of the agent behavior, subsequently generating tailored explanations on the fly. Through a combination of numerical and human subject experiments, we confirm the utility of this personalized approach to explainable AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1167-1179",
		"source": "ojs.aaai.org",
		"title": "PPS: Personalized Policy Summarization for Explaining Sequential Behavior of Autonomous Agents",
		"title-short": "PPS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31714",
		"volume": "7",
		"author": [
			{
				"family": "Qian",
				"given": "Peizhu"
			},
			{
				"family": "Huang",
				"given": "Harrison"
			},
			{
				"family": "Unhelkar",
				"given": "Vaibhav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rajBreakingBiasBuilding2024",
		"type": "article-journal",
		"abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model’s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1180-1189",
		"source": "ojs.aaai.org",
		"title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
		"title-short": "Breaking Bias, Building Bridges",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31715",
		"volume": "7",
		"author": [
			{
				"family": "Raj",
				"given": "Chahat"
			},
			{
				"family": "Mukherjee",
				"given": "Anjishnu"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			},
			{
				"family": "Anastasopoulos",
				"given": "Antonios"
			},
			{
				"family": "Zhu",
				"given": "Ziwei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rathjeLearningWhenNot2024",
		"type": "article-journal",
		"abstract": "LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine – with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1190-1199",
		"source": "ojs.aaai.org",
		"title": "Learning When Not to Measure: Theorizing Ethical Alignment in LLMs",
		"title-short": "Learning When Not to Measure",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31716",
		"volume": "7",
		"author": [
			{
				"family": "Rathje",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rauhGapsSafetyEvaluation2024",
		"type": "article-journal",
		"abstract": "Generative AI systems produce a range of ethical and social risks. Evaluation of these risks is a critical step on the path to ensuring the safety of these systems. However, evaluation requires the availability of validated and established measurement approaches and tools. In this paper, we provide an empirical review of the methods and tools that are available for evaluating known safety of generative AI systems to date. To this end, we review more than 200 safety-related evaluations that have been applied to generative AI systems. We categorise each evaluation along multiple axes to create a detailed snapshot of the safety evaluation landscape to date. We release this data for researchers and AI safety practitioners (https://bitly.ws/3hUzu). Analysing the current safety evaluation landscape reveals three systemic ”evaluation gaps”. First, a ”modality gap” emerges as few safety evaluations exist for non-text modalities. Second, a ”risk coverage gap” arises as evaluations for several ethical and social risks are simply lacking. Third, a ”context gap” arises as most safety evaluations are model-centric and fail to take into account the broader context in which AI systems operate. Devising next steps for safety practitioners based on these findings, we present tactical ”low-hanging fruit” steps towards closing the identified evaluation gaps and their limitations. We close by discussing the role and limitations of safety evaluation to ensure the safety of generative AI systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1200-1217",
		"source": "ojs.aaai.org",
		"title": "Gaps in the Safety Evaluation of Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31717",
		"volume": "7",
		"author": [
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Comanescu",
				"given": "Ramona"
			},
			{
				"family": "Akbulut",
				"given": "Canfer"
			},
			{
				"family": "Stepleton",
				"given": "Tom"
			},
			{
				"family": "Mateos-Garcia",
				"given": "Juan"
			},
			{
				"family": "Bergman",
				"given": "Stevie"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Bariach",
				"given": "Ben"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Isaac",
				"given": "William"
			},
			{
				"family": "Weidinger",
				"given": "Laura"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "reuelFairnessReinforcementLearning2024",
		"type": "article-journal",
		"abstract": "While our understanding of fairness in machine learning has significantly progressed, our understanding of fairness in reinforcement learning (RL) remains nascent. Most of the attention has been on fairness in one-shot classification tasks; however, real-world, RL-enabled systems (e.g., autonomous vehicles) are much more complicated in that agents operate in dynamic environments over a long period of time. To ensure the responsible development and deployment of these systems, we must better understand fairness in RL. In this paper, we survey the literature to provide the most up-to-date snapshot of the frontiers of fairness in RL. We start by reviewing where fairness considerations can arise in RL, then discuss the various definitions of fairness in RL that have been put forth thus far. We continue to highlight the methodologies researchers used to implement fairness in single- and multi-agent RL systems and showcase the distinct application domains that fair RL has been investigated in. Finally, we critically examine gaps in the literature, such as understanding fairness in the context of RLHF, that still need to be addressed in future work to truly operationalize fair RL in real-world systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1218-1230",
		"source": "ojs.aaai.org",
		"title": "Fairness in Reinforcement Learning: A Survey",
		"title-short": "Fairness in Reinforcement Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31718",
		"volume": "7",
		"author": [
			{
				"family": "Reuel",
				"given": "Anka"
			},
			{
				"family": "Ma",
				"given": "Devin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "robertsonHumanintheLoopFairnessAwareModel2024",
		"type": "article-journal",
		"abstract": "Fairness-aware Machine Learning (FairML) applications are often characterized by complex social objectives and legal requirements, frequently involving multiple, potentially conflicting notions of fairness. Despite the well-known Impossibility Theorem of Fairness and extensive theoretical research on the statistical and socio-technical trade-offs between fairness metrics, many FairML tools still optimize or constrain for a single fairness objective. However, this one-sided optimization can inadvertently lead to violations of other relevant notions of fairness. In this socio-technical and empirical study, we frame fairness as a Many-Objective (MaO) problem by treating fairness metrics as conflicting objectives in a multi-objective (MO) sense. We introduce ManyFairHPO, a human-in-the-loop, fairness-aware model selection framework that enables practitioners to effectively navigate complex and nuanced fairness objective landscapes. ManyFairHPO aids in the identification, evaluation, and balancing of fairness metric conflicts and their related social consequences, leading to more informed and socially responsible model-selection decisions. Through a comprehensive empirical evaluation and a case study on the Law School Admissions problem, we demonstrate the effectiveness of ManyFairHPO in balancing multiple fairness objectives, mitigating risks such as self-fulfilling prophecies, and providing interpretable insights to guide stakeholders in making fairness-aware modeling decisions.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1231-1242",
		"source": "ojs.aaai.org",
		"title": "A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31719",
		"volume": "7",
		"author": [
			{
				"family": "Robertson",
				"given": "Jake"
			},
			{
				"family": "Schmidt",
				"given": "Thorsten"
			},
			{
				"family": "Hutter",
				"given": "Frank"
			},
			{
				"family": "Awad",
				"given": "Noor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "roccaIntroducingELLIPSEthicsCentered2024",
		"type": "article-journal",
		"abstract": "As mental health care systems worldwide struggle to meet demand, there is increasing focus on using language models (LM) to infer neuropsychiatric conditions or psychopathological traits from language production. Yet, so far, this research has only delivered solutions with limited clinical applicability, due to insufficient consideration of ethical questions crucial to ensuring the synergy between possible applications and model design.\nTo accelerate progress towards clinically applicable models, our paper charts the ethical landscape of research on language-based inference of psychopathology and provides a practical tool for researchers to navigate it. We identify seven core ethical principles that should guide model development and deployment in this domain, translate them into ELLIPS, an ethical toolkit operationalizing these principles into questions that can guide researchers' choices with respect to data selection, architectures, evaluation, and model deployment, and provide a case study exemplifying its use. With this, we aim to facilitate the emergence of model technology with concrete potential for real-world applicability.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1243-1254",
		"source": "ojs.aaai.org",
		"title": "Introducing ELLIPS: An Ethics-Centered Approach to Research on LLM-Based Inference of Psychiatric Conditions",
		"title-short": "Introducing ELLIPS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31720",
		"volume": "7",
		"author": [
			{
				"family": "Rocca",
				"given": "Roberta"
			},
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Maheshwari",
				"given": "Kritika"
			},
			{
				"family": "Fusaroli",
				"given": "Riccardo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rothschildProblemsProxiesMaking2024",
		"type": "article-journal",
		"abstract": "Fairness in AI and ML systems is increasingly linked to the proper treatment and recognition of data workers involved in training dataset development. Yet, those who collect and annotate the data, and thus have the most intimate knowledge of its development, are often excluded from critical discussions. This exclusion prevents data annotators, who are domain experts, from contributing effectively to dataset contextualization. Our investigation into the hiring and engagement practices of 52 data work requesters on platforms like Amazon Mechanical Turk reveals a gap: requesters frequently hold naive or unchallenged notions of worker identities and capabilities and rely on ad-hoc qualification tasks that fail to respect the workers’ expertise. These practices not only undermine the quality of data but also the ethical standards of AI development. To rectify these issues, we advocate for policy changes to enhance how data annotation tasks are designed and managed and to ensure data workers are treated with the respect they deserve.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1255-1268",
		"source": "ojs.aaai.org",
		"title": "The Problems with Proxies: Making Data Work Visible through Requester Practices",
		"title-short": "The Problems with Proxies",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31721",
		"volume": "7",
		"author": [
			{
				"family": "Rothschild",
				"given": "Annabel"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Vilvanathan",
				"given": "Niveditha Jayakumar"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			},
			{
				"family": "DiSalvo",
				"given": "Carl"
			},
			{
				"family": "DiSalvo",
				"given": "Betsy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "salavatiReducingBiasesMinoritized2024",
		"type": "article-journal",
		"abstract": "Biased information (recently termed bisinformation) continues to be taught in medical curricula, often long after having been debunked. In this paper, we introduce bricc, a first-in-class initiative that seeks to mitigate medical bisinformation using machine learning to systematically identify and flag text with potential biases, for subsequent review in an expert-in-the-loop fashion, thus greatly accelerating an otherwise labor-intensive process. We have developed a gold-standard bricc dataset throughout several years containing over 12K pages of instructional materials.  Medical experts meticulously annotated these documents for bias according to comprehensive coding guidelines, emphasizing gender, sex, age, geography, ethnicity, and race. Using this labeled dataset, we trained, validated, and tested medical bias classifiers. We test three classifier approaches: a binary type-specific classifier, a general bias classifier; an ensemble combining bias type-specific classifiers independently-trained; and a multi-task learning (MTL) model tasked with predicting both general and type-specific biases. While MTL led to some improvement on race bias detection in terms of F1-score, it did not outperform binary classifiers trained specifically on each task.\nOn general bias detection, the binary classifier achieves up to 0.923 of AUC, a 27.8% improvement over the baseline.\nThis work lays the foundations for debiasing medical curricula by exploring a novel dataset and evaluating different training model strategies. Hence, it offers new pathways for more nuanced and effective mitigation of bisinformation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1269-1280",
		"source": "ojs.aaai.org",
		"title": "Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31722",
		"volume": "7",
		"author": [
			{
				"family": "Salavati",
				"given": "Chiman"
			},
			{
				"family": "Song",
				"given": "Shannon"
			},
			{
				"family": "Diaz",
				"given": "Willmar Sosa"
			},
			{
				"family": "Hale",
				"given": "Scott A."
			},
			{
				"family": "Montenegro",
				"given": "Roberto E."
			},
			{
				"family": "Murai",
				"given": "Fabricio"
			},
			{
				"family": "Dori-Hacohen",
				"given": "Shiri"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "sangaryaEstimatingEnvironmentalCost2024",
		"type": "article-journal",
		"abstract": "With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with and can be used to estimate other resource indicators associated with deep learning, such as epochs, gradient norm, and magnitude of model parameter change. PreIndex requires only one forward pass of the data, following which it provides a single concise value to estimate resources associated with retraining to the new distribution shifted data. We show that PreIndex can be reliably used across various datasets, model architectures, different types, and intensities of distribution shifts. Thus, PreIndex enables users to make informed decisions for retraining to different distribution shifts and determine the most cost-effective and sustainable option, allowing for the reuse of a model with a much smaller footprint in the environment. The code for this work is available here: \nhttps://github.com/JEKimLab/AIES2024PreIndex",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1281-1291",
		"source": "ojs.aaai.org",
		"title": "Estimating Environmental Cost Throughout Model’s Adaptive Life Cycle",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31723",
		"volume": "7",
		"author": [
			{
				"family": "Sangarya",
				"given": "Vishwesh"
			},
			{
				"family": "Bradford",
				"given": "Richard"
			},
			{
				"family": "Kim",
				"given": "Jung-Eun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "scariaAlgorithmsRecidivismMultidisciplinary2024",
		"type": "article-journal",
		"abstract": "The adoption of algorithms across different jurisdictions have transformed the workings of the criminal justice system, particularly in predicting recidivism risk for bail, sentencing, and parole decisions. This shift from human decision-making to statistical or algorithmic tool-assisted decision-making has prompted discussions regarding the legitimacy of such adoption. Our paper presents the results of a systematic review of the literature on criminal recidivism, spanning both legal and empirical perspectives. By coalescing different approaches, we highlight the most prominent themes that have garnered the attention of researchers so far and some that warrant further investigation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1292-1305",
		"source": "ojs.aaai.org",
		"title": "Algorithms and Recidivism: A Multi-disciplinary Systematic Review",
		"title-short": "Algorithms and Recidivism",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31724",
		"volume": "7",
		"author": [
			{
				"family": "Scaria",
				"given": "Arul George"
			},
			{
				"family": "Subramanian",
				"given": "Vidya"
			},
			{
				"family": "George",
				"given": "Nevin K."
			},
			{
				"family": "Sengupta",
				"given": "Nandana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "borgWhatRequiredEmpathic2024",
		"type": "article-journal",
		"abstract": "Interest is growing in artificial empathy, but so is confusion about what artificial empathy is or needs to be. This confusion makes it challenging to navigate the technical and ethical issues that accompany empathic AI development. Here, we outline a framework for thinking about empathic AI based on the premise that different constellations of capabilities associated with empathy are important for different empathic AI applications. We describe distinctions of capabilities that we argue belong under the empathy umbrella, and show how three medical empathic AI use cases require different sets of these capabilities. We conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both AI creators and users.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1306-1318",
		"source": "ojs.aaai.org",
		"title": "What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users",
		"title-short": "What Is Required for Empathic AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31725",
		"volume": "7",
		"author": [
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Read",
				"given": "Hannah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "schmer-galunderAnnotatorLoopCase2024",
		"type": "article-journal",
		"abstract": "With the growing prevalence of large language models, it is increasingly common to annotate datasets for machine learning using pools of crowd raters. However, these raters often work in isolation as individual crowdworkers. In this work, we regard annotation not merely as inexpensive, scalable labor, but rather as a nuanced interpretative effort to discern the meaning of what is being said in a text. We describe a novel, collaborative, and iterative annotator-in-the-loop methodology for annotation, resulting in a 'Bridging Benchmark Dataset' of comments relevant to bridging divides, annotated from 11,973 textual posts in the Civil Comments dataset. The methodology differs from popular anonymous crowd-rating annotation processes due to its use of an in-depth, iterative engagement with seven US-based raters to (1) collaboratively refine the definitions of the to-be-annotated concepts and then (2) iteratively annotate complex social concepts, with check-in meetings and discussions. This approach addresses some shortcomings of current anonymous crowd-based annotation work, and we present empirical evidence of the performance of our annotation process in the form of inter-rater reliability. Our findings indicate that collaborative engagement with annotators can enhance annotation methods, as opposed to relying solely on isolated work conducted remotely. We provide an overview of the input texts, attributes, and annotation process, along with the empirical results and the resulting benchmark dataset, categorized according to the following attributes: Alienation, Compassion, Reasoning, Curiosity, Moral Outrage, and Respect.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1319-1328",
		"source": "ojs.aaai.org",
		"title": "Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Prosocial Benchmark Dataset",
		"title-short": "Annotator in the Loop",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31726",
		"volume": "7",
		"author": [
			{
				"family": "Schmer-Galunder",
				"given": "Sonja"
			},
			{
				"family": "Wheelock",
				"given": "Ruta"
			},
			{
				"family": "Jalan",
				"given": "Zaria"
			},
			{
				"family": "Chvasta",
				"given": "Alyssa"
			},
			{
				"family": "Friedman",
				"given": "Scott"
			},
			{
				"family": "Saltz",
				"given": "Emily"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "septiandriImpactResponsibleAI2024",
		"type": "article-journal",
		"abstract": "Translational research, especially in the fast-evolving field of Artificial Intelligence (AI), is key to converting scientific findings into practical innovations. In Responsible AI (RAI) research, translational impact is often viewed through various pathways, including research papers, blogs, news articles, and the drafting of forthcoming AI legislation (e.g., the EU AI Act). However, the real-world impact of RAI research remains an underexplored area. Our study aims to capture it through two pathways: patents and code repositories, both of which provide a rich and structured source of data. Using a dataset of 200,000 papers from 1980 to 2022 in AI and related fields, including Computer Vision, Natural Language Processing, and Human-Computer Interaction, we developed a Sentence-Transformers Deep Learning framework to identify RAI papers. This framework calculates the semantic similarity between paper abstracts and a set of RAI keywords, which are derived from the NIST's AI Risk Management Framework; a framework that aims to enhance trustworthiness considerations in the design, development, use, and evaluation of AI products, services, and systems. We identified 1,747 RAI papers published in top venues such as CHI, CSCW, NeurIPS, FAccT, and AIES between 2015 and 2022. By analyzing these papers, we found that a small subset that goes into patents or repositories is highly cited, with the translational process taking between 1 year for repositories and up to 8 years for patents. Interestingly, impactful RAI research is not limited to top U.S. institutions, but significant contributions come from European and Asian institutions. Finally, the multidisciplinary nature of RAI papers, often incorporating knowledge from diverse fields of expertise, was evident as these papers tend to build on unconventional combinations of prior knowledge.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1329-1342",
		"source": "ojs.aaai.org",
		"title": "The Impact of Responsible AI Research on Innovation and Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31727",
		"volume": "7",
		"author": [
			{
				"family": "Septiandri",
				"given": "Ali Akbar"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shangTrustingYourAI2024",
		"type": "article-journal",
		"abstract": "Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLM-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1343-1356",
		"source": "ojs.aaai.org",
		"title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust",
		"title-short": "Trusting Your AI Agent Emotionally and Cognitively",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31728",
		"volume": "7",
		"author": [
			{
				"family": "Shang",
				"given": "Ruoxi"
			},
			{
				"family": "Hsieh",
				"given": "Gary"
			},
			{
				"family": "Shah",
				"given": "Chirag"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shastriAutomatingTransparencyMechanisms2024",
		"type": "article-journal",
		"abstract": "Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1357-1367",
		"source": "ojs.aaai.org",
		"title": "Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges",
		"title-short": "Automating Transparency Mechanisms in the Judicial System Using LLMs",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31729",
		"volume": "7",
		"author": [
			{
				"family": "Shastri",
				"given": "Ishana"
			},
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Engelhardt",
				"given": "Barbara"
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shea-blymyerFormalEthicalObligations2024",
		"type": "article-journal",
		"abstract": "When designing agents for operation in uncertain environments, designers need tools to automatically reason about what agents ought to do, how that conflicts with what is actually happening, and how a policy might be modified to remove the conflict.\nThese obligations include ethical and social obligations, permissions and prohibitions, which constrain how the agent achieves its mission and executes its policy.\nWe propose a new deontic logic, Expected Act Utilitarian deontic logic, for enabling this reasoning at design time: for specifying and verifying the agent's strategic obligations, then modifying its policy from a reference policy to meet those obligations.\nUnlike approaches that work at the reward level, working at the logical level increases the transparency of the trade-offs.\nWe introduce two algorithms: one for model-checking whether an RL agent has the right strategic obligations, and one for modifying a reference decision policy to make it meet obligations expressed in our logic.\nWe illustrate our algorithms on DAC-MDPs which accurately abstract neural decision policies, and on toy gridworld environments.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1368-1378",
		"source": "ojs.aaai.org",
		"title": "Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates",
		"title-short": "Formal Ethical Obligations in Reinforcement Learning Agents",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31730",
		"volume": "7",
		"author": [
			{
				"family": "Shea-Blymyer",
				"given": "Colin"
			},
			{
				"family": "Abbas",
				"given": "Houssam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "siumIndividualFairnessGraphs2024",
		"type": "article-journal",
		"abstract": "Graph neural networks are powerful graph representation learners in which node representations are highly influenced by features of neighboring nodes. Prior work on individual fairness in graphs has focused only on node features rather than structural issues. However, from the perspective of fairness in high-stakes applications, structural fairness is also important, and the learned representations may be systematically and undesirably biased against unprivileged individuals due to a lack of structural awareness in the learning process. In this work, we propose a pre-processing bias mitigation approach for individual fairness that gives importance to local and global structural features. We mitigate the local structure discrepancy of the graph embedding via a locally fair PageRank method. We address the global structure disproportion between pairs of nodes by introducing truncated singular value decomposition-based pairwise node similarities. Empirically, the proposed pre-processed fair structural features have superior performance in individual fairness metrics compared to the state-of-the-art methods while maintaining prediction performance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1379-1389",
		"source": "ojs.aaai.org",
		"title": "Individual Fairness in Graphs Using Local and Global Structural Information",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31731",
		"volume": "7",
		"author": [
			{
				"family": "Sium",
				"given": "Yonas"
			},
			{
				"family": "Li",
				"given": "Qi"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "soganciogluFairnessAIBasedMental2024",
		"type": "article-journal",
		"abstract": "There is limited research on fairness in automated decision-making systems in the clinical domain, particularly in the mental health domain. Our study explores clinicians' perceptions of AI fairness through two distinct scenarios: violence risk assessment and depression phenotype recognition using textual clinical notes. We engage with clinicians through semi-structured interviews to understand their fairness perceptions and to identify appropriate quantitative fairness objectives for these scenarios. Then, we compare a set of bias mitigation strategies developed to improve at least one of the four selected fairness objectives. Our findings underscore the importance of carefully selecting fairness measures, as prioritizing less relevant measures can have a detrimental rather than a beneficial effect on model behavior in real-world clinical use.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1390-1400",
		"source": "ojs.aaai.org",
		"title": "Fairness in AI-Based Mental Health: Clinician Perspectives and Bias Mitigation",
		"title-short": "Fairness in AI-Based Mental Health",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31732",
		"volume": "7",
		"author": [
			{
				"family": "Sogancioglu",
				"given": "Gizem"
			},
			{
				"family": "Mosteiro",
				"given": "Pablo"
			},
			{
				"family": "Salah",
				"given": "Albert Ali"
			},
			{
				"family": "Scheepers",
				"given": "Floortje"
			},
			{
				"family": "Kaya",
				"given": "Heysem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "steinPublicVsPrivate2024",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both. \nAuditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms’ compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively. \nFirst, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of advanced AI’s risk profile, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight.\nSecondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies’ capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1401-1415",
		"source": "ojs.aaai.org",
		"title": "Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries",
		"title-short": "Public vs Private Bodies",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31733",
		"volume": "7",
		"author": [
			{
				"family": "Stein",
				"given": "Merlin"
			},
			{
				"family": "Gandhi",
				"given": "Milan"
			},
			{
				"family": "Kriecherbauer",
				"given": "Theresa"
			},
			{
				"family": "Oueslati",
				"given": "Amin"
			},
			{
				"family": "Trager",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "tahaeiSurveysConsideredHarmful2024",
		"type": "article-journal",
		"abstract": "Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1416-1433",
		"source": "ojs.aaai.org",
		"title": "Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance",
		"title-short": "Surveys Considered Harmful?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31734",
		"volume": "7",
		"author": [
			{
				"family": "Tahaei",
				"given": "Mohammad"
			},
			{
				"family": "Wilkinson",
				"given": "Daricia"
			},
			{
				"family": "Frik",
				"given": "Alisa"
			},
			{
				"family": "Muller",
				"given": "Michael"
			},
			{
				"family": "Abu-Salma",
				"given": "Ruba"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "taibEnhancingEquitableAccess2024",
		"type": "article-journal",
		"abstract": "The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing. An HHSC typically consists of many agencies serving the same population.  Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another. Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not. To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data. We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC. This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent. Our experimental results using real-world HHSC data from a North American city demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1434-1443",
		"source": "ojs.aaai.org",
		"title": "Enhancing Equitable Access to AI in Housing and Homelessness System of Care through Federated Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31735",
		"volume": "7",
		"author": [
			{
				"family": "Taib",
				"given": "Musa"
			},
			{
				"family": "Wu",
				"given": "Jiajun"
			},
			{
				"family": "Drew",
				"given": "Steve"
			},
			{
				"family": "Messier",
				"given": "Geoffrey G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "tennantDynamicsMoralBehavior2024",
		"type": "article-journal",
		"abstract": "Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1444-1454",
		"source": "ojs.aaai.org",
		"title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31736",
		"volume": "7",
		"author": [
			{
				"family": "Tennant",
				"given": "Elizaveta"
			},
			{
				"family": "Hailes",
				"given": "Stephen"
			},
			{
				"family": "Musolesi",
				"given": "Mirco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "thaisMisrepresentedTechnologicalSolutions2024",
		"type": "article-journal",
		"abstract": "Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1455-1465",
		"source": "ojs.aaai.org",
		"title": "Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community",
		"title-short": "Misrepresented Technological Solutions in Imagined Futures",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31737",
		"volume": "7",
		"author": [
			{
				"family": "Thais",
				"given": "Savannah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "valdiviaSupplyChainCapitalism2024",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) is woven into a supply chain of capital, resources and human labour that has been neglected in debates about the social impact of this technology. Given the current surge in generative AI—which is estimated to use more natural resources than classic machine learning algorithms—it is vital that we better understand its production networks. Building on Tsing’s concept of supply chain capitalism, this paper offers a journey through the AI industry by illustrating the complex, diverse, opaque and global structures of the AI supply chain. The paper then illustrates an ethnographic research in Latin America revealing that AI’s rapid infrastructural growth may be precipitating environmental struggles. Investigating the supply chain capitalism of AI  reveals that eco-political frictions are arising. This demands broad critical perspectives on AI studies from a critical perspective by considering the entire capitalist production line of its industry.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1466-1466",
		"source": "ojs.aaai.org",
		"title": "The Supply Chain Capitalism of AI: A Call to (Re)think Algorithmic Harms and Resistance (Extended Abstract)",
		"title-short": "The Supply Chain Capitalism of AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31738",
		"volume": "7",
		"author": [
			{
				"family": "Valdivia",
				"given": "Ana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "varshneyDecolonialAIAlignment2024",
		"type": "article-journal",
		"abstract": "Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment through mechanisms such as extractivism, automation, sociological essentialism, surveillance, and containment. However, that work has not engaged much with alignment: teaching behaviors to a large language model (LLM) in line with desired values, and has not considered a mechanism that arises within that process: moral absolutism---a part of the coloniality of knowledge. Colonialism has a history of altering the beliefs and values of colonized peoples; in this paper, I argue that this history is recapitulated in current LLM alignment practices and technologies. Furthermore, I suggest that AI alignment be decolonialized using three forms of openness: openness of models, openness to society, and openness to excluded knowledges. This suggested approach to decolonial AI alignment uses ideas from the argumentative moral philosophical tradition of Hinduism, which has been described as an open-source religion. One concept used is viśeṣa-dharma, or particular context-specific notions of right and wrong. At the end of the paper, I provide a suggested reference architecture to work toward the proposed framework.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1467-1481",
		"source": "ojs.aaai.org",
		"title": "Decolonial AI Alignment: Openness, Visesa-Dharma, and Including Excluded Knowledges",
		"title-short": "Decolonial AI Alignment",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31739",
		"volume": "7",
		"author": [
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "victorMedicalAICategories2024",
		"type": "article-journal",
		"abstract": "It is becoming clear that, in the process of aligning AI with human values, one glaring ethical problem is that of value conflict. It is not obvious what we should do when two compelling values (such as autonomy and safety) come into conflict with one another in the design or implementation of a medical AI technology. This paper shares findings from a scoping review at the intersection of three concepts—AI, moral value, and health—that have to do with value conflict and arbitration. The paper looks at some important and unique cases of value conflict, and then describes three possible categories of value conflict: personal value conflict, interpersonal or intercommunal value conflict, and definitional value conflict. It then describes three general paths forward in addressing value conflict: additional ethical theory, additional empirical evidence, and bypassing the conflict altogether. Finally, it reflects on the efficacy of these three paths forward as ways of addressing the three categories of value conflict, and motions toward what is needed for better approaching value conflicts in medical AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1482-1489",
		"source": "ojs.aaai.org",
		"title": "Medical AI, Categories of Value Conflict, and Conflict Bypasses",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31740",
		"volume": "7",
		"author": [
			{
				"family": "Victor",
				"given": "Gavin"
			},
			{
				"family": "Bélisle-Pipon",
				"given": "Jean-Christophe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "vidaDecodingMultilingualMoral2024",
		"type": "article-journal",
		"abstract": "Large language models (LLMs) increasingly find their way into the most diverse areas of our everyday lives. They indirectly influence people's decisions or opinions through their daily use. Therefore, understanding how and which moral judgements these LLMs make is crucial. However, morality is not universal and depends on the cultural background. This raises the question of  whether these cultural preferences are also reflected in LLMs when prompted in different languages or whether moral decision-making is consistent across different languages. So far, most research has focused on investigating the inherent values of LLMs in English. While a few works conduct multilingual analyses of moral bias in LLMs in a multilingual setting, these analyses do not go beyond atomic actions. To the best of our knowledge, a multilingual analysis of moral bias in dilemmas has not yet been conducted.\n\nTo address this, our paper builds on the moral machine experiment (MME) to investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilingual setting and compares them with the preferences collected from humans belonging to different cultures. To accomplish this, we generate 6500 scenarios of the MME and prompt the models in ten languages on which action to take. Our analysis reveals that all LLMs inhibit different moral biases to some degree and that they not only differ from the human preferences but also across multiple languages within the models themselves. Moreover, we find that almost all models, particularly Llama 3, divert greatly from human values and, for instance, prefer saving fewer people over saving more.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1490-1501",
		"source": "ojs.aaai.org",
		"title": "Decoding Multilingual Moral Preferences: Unveiling LLM's Biases through the Moral Machine Experiment",
		"title-short": "Decoding Multilingual Moral Preferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31741",
		"volume": "7",
		"author": [
			{
				"family": "Vida",
				"given": "Karina"
			},
			{
				"family": "Damken",
				"given": "Fabian"
			},
			{
				"family": "Lauscher",
				"given": "Anne"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "villaniPICEPolyhedralComplex2024",
		"type": "article-journal",
		"abstract": "Polyhedral geometry can be used to shed light on the behaviour of piecewise linear neural networks, such as ReLU-based architectures. \nCounterfactual explanations are a popular class of methods for examining model behaviour by comparing a query to the closest point with a different label, subject to constraints. \nWe present a new algorithm, Polyhedral-complex Informed Counterfactual Explanations (PICE), which leverages the decomposition of the piecewise linear neural network into a polyhedral complex to find counterfactuals that are provably minimal in the Euclidean norm and exactly on the decision boundary for any given query. \nMoreover, we develop variants of the algorithm that target popular counterfactual desiderata such as sparsity, robustness, speed, plausibility, and actionability. \nWe empirically show on four publicly available real-world datasets that our method outperforms other popular techniques to find counterfactuals and adversarial attacks by distance to decision boundary and distance to query.\nMoreover, we successfully improve our baseline method in the dimensions of the desiderata we target, as supported by experimental evaluations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1502-1513",
		"source": "ojs.aaai.org",
		"title": "PICE: Polyhedral Complex Informed Counterfactual Explanations",
		"title-short": "PICE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31742",
		"volume": "7",
		"author": [
			{
				"family": "Villani",
				"given": "Mattia Jacopo"
			},
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Mishra",
				"given": "Saumitra"
			},
			{
				"family": "Amoukou",
				"given": "Salim Ibrahim"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			},
			{
				"family": "Veloso",
				"given": "Manuela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wangStrategiesIncreasingCorporate2024",
		"type": "article-journal",
		"abstract": "Responsible artificial intelligence (RAI) is increasingly recognized as a critical concern. However, the level of corporate RAI prioritization has not kept pace. In this work, we conduct 16 semi-structured interviews with practitioners to investigate what has historically motivated companies to increase the prioritization of RAI. What emerges is a complex story of conflicting and varied factors, but we bring structure to the narrative by highlighting the different strategies available to employ, and point to the actors with access to each. While there are no guaranteed steps for increasing RAI prioritization, we paint the current landscape of motivators so that practitioners can learn from each other, and put forth our own selection of promising directions forward.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1514-1526",
		"source": "ojs.aaai.org",
		"title": "Strategies for Increasing Corporate Responsible AI Prioritization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31743",
		"volume": "7",
		"author": [
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Datta",
				"given": "Teresa"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "weiOperationalizingContentModeration2024",
		"type": "article-journal",
		"abstract": "The Digital Services Act, recently adopted by the EU, requires social media platforms to report the ``accuracy'' of their automated content moderation systems. The colloquial term is vague, or open-textured---the literal accuracy (number of correct predictions divided by the total) is not suitable for problems with large class imbalance, and the ground truth and dataset to measure accuracy against is unspecified. Without further specification, the regulatory requirement allows for deficient reporting. In this interdisciplinary work, we operationalize ``accuracy'' reporting by refining legal concepts and relating them to technical implementation. We start by elucidating the legislative purpose of the Act to legally justify an interpretation of ``accuracy'' as precision and recall. These metrics remain informative in class imbalanced settings, and reflect the proportional balancing of Fundamental Rights of the EU Charter. We then focus on the estimation of recall, as its naive estimation can incur extremely high annotation costs and disproportionately interfere with the platform's right to conduct business. Through a simulation study, we show that recall can be efficiently estimated using stratified sampling with trained classifiers, and provide concrete recommendations for its application. Finally, we present a case study of recall reporting for a subset of Reddit under the Act. Based on the language in the Act, we identify a number of ways recall could be reported due to underspecification. We report on one possibility using our improved estimator, and discuss the implications and areas for further legal clarification.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1527-1538",
		"source": "ojs.aaai.org",
		"title": "Operationalizing Content Moderation “Accuracy” in the Digital Services Act",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31744",
		"volume": "7",
		"author": [
			{
				"family": "Wei",
				"given": "Johnny Tian-Zheng"
			},
			{
				"family": "Zufall",
				"given": "Frederike"
			},
			{
				"family": "Jia",
				"given": "Robin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "weiHowAICompanies2024",
		"type": "article-journal",
		"abstract": "Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1539-1555",
		"source": "ojs.aaai.org",
		"title": "How Do AI Companies “Fine-Tune” Policy? Examining Regulatory Capture in AI Governance",
		"title-short": "How Do AI Companies “Fine-Tune” Policy?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31745",
		"volume": "7",
		"author": [
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Gabrieli",
				"given": "Nick"
			},
			{
				"family": "Deshpande",
				"given": "Chinmay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wen-yiAutomateAssistRole2024",
		"type": "article-journal",
		"abstract": "The language used by US courtroom actors in criminal trials has long been studied for biases. However, systematic studies for bias in high-stakes court trials have been difficult, due to the nuanced nature of bias and the legal expertise required.  Large language models offer the possibility to automate annotation. But validating the computational approach requires both an understanding of how automated methods fit in existing annotation workflows and what they really offer.  We present a case study of adding a computational model to a complex and high-stakes problem: identifying gender-biased language in US capital trials for women defendants.\nOur team of experienced death-penalty lawyers and NLP technologists pursue a three-phase study: first annotating manually, then training and evaluating computational models, and finally comparing expert annotations to model predictions.  Unlike many typical NLP tasks, annotating for gender bias in months-long capital trials is complicated, with many individual judgment calls.  Contrary to standard arguments for automation that are based on efficiency and scalability, legal experts find the computational models most useful in providing opportunities to reflect on their own bias in annotation and to build consensus on annotation rules.  This experience suggests that seeking to replace experts with computational models for complex annotation is both unrealistic and undesirable. Rather, computational models offer valuable opportunities to assist the legal experts in annotation-based studies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1556-1566",
		"source": "ojs.aaai.org",
		"title": "Automate or Assist? The Role of Computational Models in Identifying Gendered Discourse in US Capital Trial Transcripts",
		"title-short": "Automate or Assist?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31746",
		"volume": "7",
		"author": [
			{
				"family": "Wen-Yi",
				"given": "Andrea W."
			},
			{
				"family": "Adamson",
				"given": "Kathryn"
			},
			{
				"family": "Greenfield",
				"given": "Nathalie"
			},
			{
				"family": "Goldberg",
				"given": "Rachel"
			},
			{
				"family": "Babcock",
				"given": "Sandra"
			},
			{
				"family": "Mimno",
				"given": "David"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wielingaRelationalJustificationAI2024",
		"type": "article-journal",
		"abstract": "While much has been written about what democratized AI should look like, there has been surprisingly little attention for the normative grounds of AI democratization. Existing calls for AI democratization that do make explicit arguments broadly fall into two categories: outcome-based and legitimacy-based, corresponding to outcome-based and process-based views of procedural justice respectively. This paper argues that we should favor relational justifications of AI democratization to outcome-based ones, because the former additionally provide outcome-independent reasons for AI democratization. Moreover, existing legitimacy-based arguments often leave the why of AI democratization implicit and instead focus on the how. We present two relational arguments for AI democratization: one based on empirical findings regarding the perceived importance of relational features of decision-making procedures, and one based on Iris Marion Young’s conception of justice, according to which the main forms of injustice are domination and oppression. We show how these arguments lead to requirements for procedural fairness and thus also offer guidance on the how of AI democratization. Finally, we consider several objections to AI democratization, including worries concerning epistemic exploitation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1567-1577",
		"source": "ojs.aaai.org",
		"title": "A Relational Justification of AI Democratization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31747",
		"volume": "7",
		"author": [
			{
				"family": "Wielinga",
				"given": "Bauke"
			},
			{
				"family": "Buijsman",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wilsonGenderRaceIntersectional2024",
		"type": "article-journal",
		"abstract": "Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have  implications for widely used AI tools that are automating employment, fairness, and tech policy.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1578-1590",
		"source": "ojs.aaai.org",
		"title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31748",
		"volume": "7",
		"author": [
			{
				"family": "Wilson",
				"given": "Kyra"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wojtowiczWhenWhyPersuasion2024",
		"type": "article-journal",
		"abstract": "As generative foundation models improve, they also tend to become more persuasive, raising concerns that AI automation will enable governments, firms, and other actors to manipulate beliefs with unprecedented scale and effectiveness at virtually no cost. The full economic and social ramifications of this trend have been difficult to foresee, however, given that we currently lack a complete theoretical understanding of why persuasion is costly for human labor to produce in the first place. This paper places human and AI agents on a common conceptual footing by formalizing informational persuasion as a mathematical decision problem and characterizing its computational complexity. A novel proof establishes that persuasive messages are challenging to discover (NP-Hard) but easy to adopt if supplied by others (NP). This asymmetry helps explain why people are susceptible to persuasion, even in contexts where all relevant information is publicly available. The result also illuminates why litigation, strategic communication, and other persuasion-oriented activities have historically been so human capital intensive, and it provides a new theoretical basis for studying how AI will impact various industries.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1591-1594",
		"source": "ojs.aaai.org",
		"title": "When and Why is Persuasion Hard? A Computational Complexity Result",
		"title-short": "When and Why is Persuasion Hard?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31749",
		"volume": "7",
		"author": [
			{
				"family": "Wojtowicz",
				"given": "Zachary"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeImplicationsOpenGenerative2024",
		"type": "article-journal",
		"abstract": "Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1595-1607",
		"source": "ojs.aaai.org",
		"title": "The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations",
		"title-short": "The Implications of Open Generative Models in Human-Centered Data Science Work",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31750",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Mitra",
				"given": "Tanushree"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeMLEATMultilevelEmbedding2024",
		"type": "article-journal",
		"abstract": "This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1608-1620",
		"source": "ojs.aaai.org",
		"title": "ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science",
		"title-short": "ML-EAT",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31751",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeRepresentationBiasAdolescents2024",
		"type": "article-journal",
		"abstract": "Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while not free of such associations, are less dominated by social problems. Data from workshops with N=13 U.S. adolescents and N=18 Nepalese adolescents show that AI presentations are disconnected from teenage life, which revolves around activities like school and friendship. Participant ratings of how well 20 trait words describe teens are decorrelated from SWE associations, with Pearson's rho=.02, n.s. in English FastText and rho=.06, n.s. GloVe; and rho=.06, n.s. in Nepali FastText and rho=-.23, n.s. in GloVe. U.S. participants suggested AI could fairly present teens by highlighting diversity, while Nepalese participants centered positivity. Participants were optimistic that, if it learned from adolescents, rather than media sources, AI could help mitigate stereotypes. Our work offers an understanding of the ways SWEs and GLMs misrepresent a developmentally vulnerable group and provides a template for less sensationalized characterization.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1621-1634",
		"source": "ojs.aaai.org",
		"title": "Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study",
		"title-short": "Representation Bias of Adolescents in AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31752",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Dangol",
				"given": "Aayushi"
			},
			{
				"family": "Howe",
				"given": "Bill"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeDatasetScaleSocietal2024",
		"type": "article-journal",
		"abstract": "Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1635-1647",
		"source": "ojs.aaai.org",
		"title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31753",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Dangol",
				"given": "Aayushi"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wuStableDiffusionExposed2024",
		"type": "article-journal",
		"abstract": "Several studies have raised awareness about social biases in image generative models, demonstrating their predisposition towards stereotypes and imbalances. This paper contributes to this growing body of research by introducing an evaluation protocol that analyzes the impact of gender indicators at every step of the generation process on Stable Diffusion images. Leveraging insights from prior work, we explore how gender indicators not only affect gender presentation but also the representation of objects and layouts within the generated images. Our findings include the existence of differences in the depiction of objects, such as instruments tailored for specific genders, and shifts in overall layouts. We also reveal that neutral prompts tend to produce images more aligned with masculine prompts than their feminine counterparts. We further explore where bias originates through representational disparities and how it manifests in the images via prompt-image dependencies, and provide recommendations for developers and users to mitigate potential bias in image generation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1648-1659",
		"source": "ojs.aaai.org",
		"title": "Stable Diffusion Exposed: Gender Bias from Prompt to Image",
		"title-short": "Stable Diffusion Exposed",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31754",
		"volume": "7",
		"author": [
			{
				"family": "Wu",
				"given": "Yankun"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xieNonlinearWelfareAwareStrategic2024",
		"type": "article-journal",
		"abstract": "This paper studies algorithmic decision-making in the presence of strategic individual behaviors, where an ML model is used to make decisions about human agents and the latter can adapt their behavior strategically to improve their future data. Existing results on strategic learning have largely focused on the linear setting where agents with linear labeling functions best respond to a (noisy) linear decision policy. Instead, this work focuses on general non-linear settings where agents respond to the decision policy with only \"local information\" of the policy. Moreover, we simultaneously consider objectives of maximizing decision-maker welfare (model prediction accuracy), social welfare (agent improvement caused by strategic behaviors), and agent welfare (the extent that ML underestimates the agents). We first generalize the agent best response model in previous works to the non-linear setting and then investigate the compatibility of welfare objectives. We show the three welfare can attain the optimum simultaneously only under restrictive conditions which are challenging to achieve in non-linear settings. The theoretical results imply that existing works solely maximizing the welfare of a subset of parties usually diminish the welfare of others. We thus claim the necessity of balancing the welfare of each party in non-linear settings and propose an irreducible optimization algorithm suitable for general strategic learning. Experiments on synthetic and real data validate the proposed algorithm.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1660-1671",
		"source": "ojs.aaai.org",
		"title": "Non-linear Welfare-Aware Strategic Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31755",
		"volume": "7",
		"author": [
			{
				"family": "Xie",
				"given": "Tian"
			},
			{
				"family": "Zhang",
				"given": "Xueru"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xieAlgorithmicDecisionMakingAgents2024",
		"type": "article-journal",
		"abstract": "This paper studies algorithmic decision-making under human strategic behavior, where a decision-maker uses an algorithm to make decisions about human agents, and the latter with information about the algorithm may exert effort strategically and improve to receive favorable decisions. Unlike prior works that assume agents benefit from their efforts immediately, we consider realistic scenarios where the impacts of these efforts are persistent and agents benefit from efforts by making improvements gradually. We first develop a dynamic model to characterize persistent improvements and based on this construct a Stackelberg game to model the interplay between agents and the decision-maker. We analytically characterize the equilibrium strategies and identify conditions under which agents have incentives to invest efforts to improve their qualifications. With the dynamics, we then study how the decision-maker can design an optimal policy to incentivize the largest improvements inside the agent population. We also extend the model to settings where 1) agents may be dishonest and game the algorithm into making favorable but erroneous decisions; 2) honest efforts are forgettable and not sufficient to guarantee persistent improvements. With the extended models, we further examine conditions under which agents prefer honest efforts over dishonest behavior and the impacts of forgettable efforts.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1672-1683",
		"source": "ojs.aaai.org",
		"title": "Algorithmic Decision-Making under Agents with Persistent Improvement",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31756",
		"volume": "7",
		"author": [
			{
				"family": "Xie",
				"given": "Tian"
			},
			{
				"family": "Tan",
				"given": "Xuwei"
			},
			{
				"family": "Zhang",
				"given": "Xueru"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xuTracingEvolutionInformation2024",
		"type": "article-journal",
		"abstract": "Information transparency, the open disclosure of information about models, is crucial for proactively evaluating the potential societal harm of large language models (LLMs) and developing effective risk mitigation measures. Adapting the biographies of artifacts and practices (BOAP) method from science and technology studies, this study analyzes the evolution of information transparency within OpenAI’s Generative Pre-trained Transformers (GPT) model reports and usage policies from its inception in 2018 to GPT-4, one of today’s most capable LLMs. To assess the breadth and depth of transparency practices, we develop a 9-dimensional, 3-level analytical framework to evaluate the comprehensiveness and accessibility of information disclosed to various stakeholders. Findings suggest that while model limitations and downstream usages are increasingly clarified, model development processes have become more opaque. Transparency remains minimal in certain aspects, such as model explainability and real-world evidence of LLM impacts, and the discussions on safety measures such as technical interventions and regulation pipelines lack in-depth details. The findings emphasize the need for enhanced transparency to foster accountability and ensure responsible technological innovations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1684-1695",
		"source": "ojs.aaai.org",
		"title": "Tracing the Evolution of Information Transparency for OpenAI’s GPT Models through a Biographical Approach",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31757",
		"volume": "7",
		"author": [
			{
				"family": "Xu",
				"given": "Zhihan"
			},
			{
				"family": "Mustafaraj",
				"given": "Eni"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yangLLMVotingHuman2024",
		"type": "article-journal",
		"abstract": "This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1696-1708",
		"source": "ojs.aaai.org",
		"title": "LLM Voting: Human Choices and AI Collective Decision-Making",
		"title-short": "LLM Voting",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31758",
		"volume": "7",
		"author": [
			{
				"family": "Yang",
				"given": "Joshua C."
			},
			{
				"family": "Dailisan",
				"given": "Damian"
			},
			{
				"family": "Korecki",
				"given": "Marcin"
			},
			{
				"family": "Hausladen",
				"given": "Carina I."
			},
			{
				"family": "Helbing",
				"given": "Dirk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yewYouStillSee2024",
		"type": "article-journal",
		"abstract": "Data forms the backbone of artificial intelligence (AI). Privacy and data protection laws thus have strong bearing on AI systems. Shielded by the rhetoric of compliance with data protection and privacy regulations, privacy-preserving techniques have enabled the extraction of more and new forms of data. We illustrate how the application of privacy-preserving techniques in the development of AI systems--from private set intersection as part of dataset curation to homomorphic encryption and federated learning as part of model computation--can further support surveillance infrastructure under the guise of regulatory permissibility. Finally, we propose technology and policy strategies to evaluate privacy-preserving techniques in light of the protections they actually confer. We conclude by highlighting the role that technologists could play in devising policies that combat surveillance AI technologies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1709-1722",
		"source": "ojs.aaai.org",
		"title": "You Still See Me: How Data Protection Supports the Architecture of AI Surveillance",
		"title-short": "You Still See Me",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31759",
		"volume": "7",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Qin",
				"given": "Lucy"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "zhangMitigatingUrbanRuralDisparities2024",
		"type": "article-journal",
		"abstract": "Satellite imagery is being leveraged for many societally critical tasks across climate, economics, and public health. Yet, because of heterogeneity in landscapes (e.g. how a road looks in different places), models can show disparate performance across geographic areas. Given the important potential of disparities in algorithmic systems used in societal contexts, here we consider the risk of urban-rural disparities in identification of land-cover features. This is via semantic segmentation (a common computer vision task in which image regions are labelled according to what is being shown) which uses pre-trained image representations generated via contrastive self-supervised learning. We propose fair dense representation with contrastive learning (FairDCL) as a method for de-biasing the multi-level latent space of a convolution neural network. The method improves feature identification by removing spurious latent representations which are disparately distributed across urban and rural areas, and is achieved in an unsupervised way by contrastive pre-training. The pre-trained image representation mitigates downstream urban-rural prediction disparities and outperforms state-of-the-art baselines on real-world satellite images. Embedding space evaluation and ablation studies further demonstrate FairDCL’s robustness. As generalizability and robustness in geographic imagery is a nascent topic, our work motivates researchers to consider metrics beyond average accuracy in such applications.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1723-1734",
		"source": "ojs.aaai.org",
		"title": "Mitigating Urban-Rural Disparities in Contrastive Representation Learning with Satellite Imagery",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31760",
		"volume": "7",
		"author": [
			{
				"family": "Zhang",
				"given": "Miao"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "zhangOntologyBeliefDiversity2024",
		"type": "article-journal",
		"abstract": "AI applications across classification, fairness, and human interaction  often  implicitly  require  ontologies  of  social  concepts.  Constructing  these  well  –  especially  when  there  are many relevant categories – is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which isa complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological  methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology’s utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1735-1743",
		"source": "ojs.aaai.org",
		"title": "Ontology of Belief Diversity: A Community-Based Epistemological Approach",
		"title-short": "Ontology of Belief Diversity",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31761",
		"volume": "7",
		"author": [
			{
				"family": "Zhang",
				"given": "Richard"
			},
			{
				"family": "Liemt",
				"given": "Erin Van"
			},
			{
				"family": "Fischella",
				"given": "Tyler"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	}
]