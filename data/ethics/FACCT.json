[
	{
		"id": "liaoDesigningResponsibleTrust2022",
		"type": "paper-conference",
		"abstract": "Current literature and public discourse on “trust in AI” are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments. Besides AI-generated content, we highlight transparency and interaction as AI systems’ affordances that present a wide range of trustworthiness cues to users. By bringing to light the variety of users’ cognitive processes to make trust judgments and their potential limitations, we urge technology creators to make conscious decisions in choosing reliable trustworthiness cues for target users and, as an industry, to regulate this space and prevent malicious use. Towards these goals, we define the concepts of warranted trustworthiness cues and expensive trustworthiness cues, and propose a checklist of requirements to help technology creators identify appropriate cues to use. We present a hypothetical use case to illustrate how practitioners can use MATCH to design AI systems responsibly, and discuss future directions for research and industry efforts aimed at promoting responsible trust in AI.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533182",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1257–1268",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing for responsible trust in AI systems: A communication perspective",
		"URL": "https://doi.org/10.1145/3531146.3533182",
		"author": [
			{
				"family": "Liao",
				"given": "Q.Vera"
			},
			{
				"family": "Sundar",
				"given": "S. Shyam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "stapletonImaginingNewFutures2022",
		"type": "paper-conference",
		"abstract": "Child welfare agencies across the United States are turning to data-driven predictive technologies (commonly called predictive analytics) which use government administrative data to assist workers’ decision-making. While some prior work has explored impacted stakeholders’ concerns with current uses of data-driven predictive risk models (PRMs), less work has asked stakeholders whether such tools ought to be used in the first place. In this work, we conducted a set of seven design workshops with 35 stakeholders who have been impacted by the child welfare system or who work in it to understand their beliefs and concerns around PRMs, and to engage them in imagining new uses of data and technologies in the child welfare system. We found that participants worried current PRMs perpetuate or exacerbate existing problems in child welfare. Participants suggested new ways to use data and data-driven tools to better support impacted communities and suggested paths to mitigate possible harms of these tools. Participants also suggested low-tech or no-tech alternatives to PRMs to address problems in child welfare. Our study sheds light on how researchers and designers can work in solidarity with impacted communities, possibly to circumvent or oppose child welfare agencies.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533177",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "1162–1177",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders",
		"URL": "https://doi.org/10.1145/3531146.3533177",
		"author": [
			{
				"family": "Stapleton",
				"given": "Logan"
			},
			{
				"family": "Lee",
				"given": "Min Hun"
			},
			{
				"family": "Qing",
				"given": "Diana"
			},
			{
				"family": "Wright",
				"given": "Marya"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Holstein",
				"given": "Ken"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blumMultiStageScreening2022",
		"type": "paper-conference",
		"abstract": "Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533178",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "1178–1193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi stage screening: Enforcing fairness and maximizing efficiency in a pre-existing pipeline",
		"URL": "https://doi.org/10.1145/3531146.3533178",
		"author": [
			{
				"family": "Blum",
				"given": "Avrim"
			},
			{
				"family": "Stangl",
				"given": "Kevin"
			},
			{
				"family": "Vakilian",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rahmattalabiLearningResourceAllocation2022",
		"type": "paper-conference",
		"abstract": "We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533181",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "1240–1256",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning resource allocation policies from observational data with an application to homeless services delivery",
		"URL": "https://doi.org/10.1145/3531146.3533181",
		"author": [
			{
				"family": "Rahmattalabi",
				"given": "Aida"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Dullerud",
				"given": "Kathryn"
			},
			{
				"family": "Rice",
				"given": "Eric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeMarkednessVisualSemantic2022",
		"type": "paper-conference",
		"abstract": "We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533183",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1269–1279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Markedness in visual semantic AI",
		"URL": "https://doi.org/10.1145/3531146.3533183",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeEvidenceHypodescentVisual2022",
		"type": "paper-conference",
		"abstract": "We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p &lt; 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p &lt; 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p &lt; 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533185",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1293–1304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evidence for hypodescent in visual semantic AI",
		"URL": "https://doi.org/10.1145/3531146.3533185",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Banaji",
				"given": "Mahzarin R."
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ehsanAlgorithmicImprint2022",
		"type": "paper-conference",
		"abstract": "When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533186",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1305–1317",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The algorithmic imprint",
		"URL": "https://doi.org/10.1145/3531146.3533186",
		"author": [
			{
				"family": "Ehsan",
				"given": "Upol"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Riedl",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hirotaGenderRacialBias2022",
		"type": "paper-conference",
		"abstract": "Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533184",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1280–1292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender and racial bias in visual question answering datasets",
		"URL": "https://doi.org/10.1145/3531146.3533184",
		"author": [
			{
				"family": "Hirota",
				"given": "Yusuke"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangDualCFEfficientModel2022",
		"type": "paper-conference",
		"abstract": "Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533188",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1318–1329",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "DualCF: Efficient model extraction attack from counterfactual explanations",
		"URL": "https://doi.org/10.1145/3531146.3533188",
		"author": [
			{
				"family": "Wang",
				"given": "Yongjie"
			},
			{
				"family": "Qian",
				"given": "Hangwei"
			},
			{
				"family": "Miao",
				"given": "Chunyan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goantaCaseLegalCompliance2022",
		"type": "paper-conference",
		"abstract": "In the course of under a year, the European Commission has launched some of the most important regulatory proposals to date on platform governance. The Commission’s goals behind cross-sectoral regulation of this sort include the protection of markets and democracies alike. While all these acts propose sophisticated rules for setting up new enforcement institutions and procedures, one aspect remains highly unclear: how digital enforcement will actually take place in practice. Focusing on the Digital Services Act (DSA), this discussion paper critically addresses issues around social media data access for the purpose of digital enforcement and proposes the use of a legal compliance application programming interface (API) as a means to facilitate compliance with the DSA and complementary European and national regulation. To contextualize this discussion, the paper pursues two scenarios that exemplify the harms arising out of content monetization affecting a particularly vulnerable category of social media users: children. The two scenarios are used to further reflect upon essential issues surrounding data access and legal compliance with the DSA and further applicable legal standards in the field of labour and consumer law.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533190",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "1341–1349",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The case for a legal compliance API for the enforcement of the EU’s digital services act on social media platforms",
		"URL": "https://doi.org/10.1145/3531146.3533190",
		"author": [
			{
				"family": "Goanta",
				"given": "Catalina"
			},
			{
				"family": "Bertaglia",
				"given": "Thales"
			},
			{
				"family": "Iamnitchi",
				"given": "Adriana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fogliatoWhoGoesFirst2022",
		"type": "paper-conference",
		"abstract": "Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients’ X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported that the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533193",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1362–1374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who goes first? Influences of human-AI workflow on decision making in clinical imaging",
		"URL": "https://doi.org/10.1145/3531146.3533193",
		"author": [
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Chappidi",
				"given": "Shreya"
			},
			{
				"family": "Lungren",
				"given": "Matthew"
			},
			{
				"family": "Fisher",
				"given": "Paul"
			},
			{
				"family": "Wilson",
				"given": "Diane"
			},
			{
				"family": "Fitzke",
				"given": "Michael"
			},
			{
				"family": "Parkinson",
				"given": "Mark"
			},
			{
				"family": "Horvitz",
				"given": "Eric"
			},
			{
				"family": "Inkpen",
				"given": "Kori"
			},
			{
				"family": "Nushi",
				"given": "Besmira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "thorntonAlchemyTrustCreative2022",
		"type": "paper-conference",
		"abstract": "Trust is recognised as a significant and valuable component of socio-technical systems, facilitating numerous important benefits. Many trust models have been created throughout various streams of literature, describing trust for different stakeholders in different contexts. However, when designing a system with multiple stakeholders in their multiple contexts, how does one decide which trust model(s) to apply? And furthermore, how does one go from selecting a model or models to translating those into design? We review and analyse two prominent trust models, and apply them to the design of a trustworthy socio-technical system, namely virtual research environments. We show that a singular model cannot easily be imported and directly implemented into the design of such a system. We introduce the concept of alchemy as the most apt characterization of a successful design process, illustrating the need for designers to engage with the richness of the trust landscape and creatively experiment with components from multiple models to create the perfect blend for their context. We provide a demonstrative case study illustrating the process through which designers of socio-technical systems can become alchemists of trust.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533196",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1387–1398",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The alchemy of trust: The creative act of designing trustworthy socio-technical systems",
		"URL": "https://doi.org/10.1145/3531146.3533196",
		"author": [
			{
				"family": "Thornton",
				"given": "Lauren"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Blair",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shangWhyAmNot2022",
		"type": "paper-conference",
		"abstract": "Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533189",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1330–1340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why am I not seeing it? Understanding users’ needs for counterfactual explanations in everyday recommendations",
		"URL": "https://doi.org/10.1145/3531146.3533189",
		"author": [
			{
				"family": "Shang",
				"given": "Ruoxi"
			},
			{
				"family": "Feng",
				"given": "K. J. Kevin"
			},
			{
				"family": "Shah",
				"given": "Chirag"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schramowskiCanMachinesHelp2022",
		"type": "paper-conference",
		"abstract": "This paper contains images and descriptions that are offensive in nature.Large datasets underlying much of current machine learning raise serious issues concerning inappropriate content such as offensive, insulting, threatening, or might otherwise cause anxiety. This calls for increased dataset documentation, e.g., using datasheets. They, among other topics, encourage to reflect on the composition of the datasets. So far, this documentation, however, is done manually and therefore can be tedious and error-prone, especially for large image datasets. Here we ask the arguably “circular” question of whether a machine can help us reflect on inappropriate content, answering Question 16 in Datasheets. To this end, we propose to use the information stored in pre-trained transformer models to assist us in the documentation process. Specifically, prompt-tuning based on a dataset of socio-moral values steers CLIP to identify potentially inappropriate content, therefore reducing human labor. We then document the inappropriate images found using word clouds, based on captions generated using a vision-language model. The documentations of two popular, large-scale computer vision datasets—ImageNet and OpenImages—produced this way suggest that machines can indeed help dataset creators to answer Question 16 on inappropriate image content.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533192",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1350–1361",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?",
		"URL": "https://doi.org/10.1145/3531146.3533192",
		"author": [
			{
				"family": "Schramowski",
				"given": "Patrick"
			},
			{
				"family": "Tauchmann",
				"given": "Christopher"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "youngConfrontingPowerCorporate2022",
		"type": "paper-conference",
		"abstract": "Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533194",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1375–1386",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Confronting power and corporate capture at the FAccT conference",
		"URL": "https://doi.org/10.1145/3531146.3533194",
		"author": [
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Krafft",
				"given": "P.M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "buet-golfouseFairUnsupervisedLearning2022",
		"type": "paper-conference",
		"abstract": "Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533197",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1399–1409",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fair unsupervised learning",
		"URL": "https://doi.org/10.1145/3531146.3533197",
		"author": [
			{
				"family": "Buet-Golfouse",
				"given": "Francois"
			},
			{
				"family": "Utyagulov",
				"given": "Islam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "susserDecisionTimeNormative2022",
		"type": "paper-conference",
		"abstract": "Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533198",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1410–1420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Decision time: Normative dimensions of algorithmic speed",
		"URL": "https://doi.org/10.1145/3531146.3533198",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rateikeDontThrowIt2022",
		"type": "paper-conference",
		"abstract": "Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533199",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1421–1433",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Don’t throw it away! The utility of unlabeled data in fair decision making",
		"URL": "https://doi.org/10.1145/3531146.3533199",
		"author": [
			{
				"family": "Rateike",
				"given": "Miriam"
			},
			{
				"family": "Majumdar",
				"given": "Ayan"
			},
			{
				"family": "Mineeva",
				"given": "Olga"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "caiAdaptiveSamplingStrategies2022",
		"type": "paper-conference",
		"abstract": "In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data—an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533203",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1467–1478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adaptive sampling strategies to construct equitable training datasets",
		"URL": "https://doi.org/10.1145/3531146.3533203",
		"author": [
			{
				"family": "Cai",
				"given": "William"
			},
			{
				"family": "Encarnacion",
				"given": "Ro"
			},
			{
				"family": "Chern",
				"given": "Bobbie"
			},
			{
				"family": "Corbett-Davies",
				"given": "Sam"
			},
			{
				"family": "Bogen",
				"given": "Miranda"
			},
			{
				"family": "Bergman",
				"given": "Stevie"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "neumannJusticeMisinformationDetection2022",
		"type": "paper-conference",
		"abstract": "Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533205",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1504–1515",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Justice in misinformation detection systems: An analysis of algorithms, stakeholders, and potential harms",
		"URL": "https://doi.org/10.1145/3531146.3533205",
		"author": [
			{
				"family": "Neumann",
				"given": "Terrence"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Fazelpour",
				"given": "Sina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "klumbytundefinedCriticalToolsMachine2022",
		"type": "paper-conference",
		"abstract": "This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of “situating/situated knowledges”, \"figuration\", \"diffraction\", and “critical fabulation/speculation” were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533207",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1528–1541",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Critical tools for machine learning: Working with intersectional critical concepts in machine learning systems design",
		"URL": "https://doi.org/10.1145/3531146.3533207",
		"author": [
			{
				"family": "Klumbytundefined",
				"given": "Goda"
			},
			{
				"family": "Draude",
				"given": "Claude"
			},
			{
				"family": "Taylor",
				"given": "Alex S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "agarwalPowerRandomizationFair2022",
		"type": "paper-conference",
		"abstract": "Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533209",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1542–1551",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the power of randomization in fair classification and representation",
		"URL": "https://doi.org/10.1145/3531146.3533209",
		"author": [
			{
				"family": "Agarwal",
				"given": "Sushant"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pngTensionsSouthNorth2022",
		"type": "paper-conference",
		"abstract": "This paper aims to present a landscape of AI governance for and from the Global South, advanced by critical and decolonial-informed practitioners and scholars, and contrast this with the Inclusive AI Governance discourse led out of Global North institutions. By doing so, it identifies gaps in the dominant AI governance discourse, and bridges these gaps with relevant discourses of technology and power, localisation, and historical-geopolitical analyses of inequality led by Global South aligned actors. Specific areas of concern addressed by this paper include infrastructural and regulatory monopolies, harms associated with the labour and material supply chains of AI infrastructure, and commercial exploitation. By contrasting Global South and Global North discourses surrounding AI risks, this paper proposes a systemic restructuring of AI governance processes beyond current frameworks of Inclusive AI governance, offering three roles for Global South actors to substantively engage in AI governance processes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533200",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1434–1445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "At the tensions of south and north: Critical roles of global south stakeholders in AI governance",
		"URL": "https://doi.org/10.1145/3531146.3533200",
		"author": [
			{
				"family": "Png",
				"given": "Marie-Therese"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "poirierAccountableDataPolitics2022",
		"type": "paper-conference",
		"abstract": "This paper attends specifically to what I call “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit – what is ultimately being counted in the data – is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533201",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1446–1456",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accountable data: The politics and pragmatics of disclosure datasets",
		"URL": "https://doi.org/10.1145/3531146.3533201",
		"author": [
			{
				"family": "Poirier",
				"given": "Lindsay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ferrarioHowExplainabilityContributes2022",
		"type": "paper-conference",
		"abstract": "We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as “explainability fosters trust in AI,” that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called “trust as anti-monitoring,” that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that “explainability fosters trust in AI” if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user’s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533202",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1457–1466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How explainability contributes to trust in AI",
		"URL": "https://doi.org/10.1145/3531146.3533202",
		"author": [
			{
				"family": "Ferrario",
				"given": "Andrea"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blackAlgorithmicFairnessVertical2022",
		"type": "paper-conference",
		"abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533204",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 25\npublisher-place: Seoul, Republic of Korea",
		"page": "1479–1503",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness and vertical equity: Income fairness with IRS tax audit models",
		"URL": "https://doi.org/10.1145/3531146.3533204",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Goldin",
				"given": "Jacob"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fishmanShouldAttentionBe2022",
		"type": "paper-conference",
		"abstract": "“Attention is all you need” has become a fundamental precept in machine learning research. Originally designed for machine translation, transformers and the attention mechanisms that underpin them now find success across many problem domains. With the apparent domain-agnostic success of transformers, many researchers are excited that similar model architectures can be successfully deployed across diverse applications in vision, language and beyond. We consider the benefits and risks of these waves of unification on both epistemic and ethical fronts. On the epistemic side, we argue that many of the arguments in favor of unification in the natural sciences fail to transfer over to the machine learning case, or transfer over only under assumptions that might not hold. Unification also introduces epistemic risks related to portability, path dependency, methodological diversity, and increased black-boxing. On the ethical side, we discuss risks emerging from epistemic concerns, further marginalizing underrepresented perspectives, the centralization of power, and having fewer models across more domains of application.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533206",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1516–1527",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should attention be all we need? The epistemic and ethical implications of unification in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533206",
		"author": [
			{
				"family": "Fishman",
				"given": "Nic"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "almuzainiABCinMLAnticipatoryBias2022",
		"type": "paper-conference",
		"abstract": "The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533211",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "1552–1560",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ABCinML: Anticipatory bias correction in machine learning applications",
		"URL": "https://doi.org/10.1145/3531146.3533211",
		"author": [
			{
				"family": "Almuzaini",
				"given": "Abdulaziz A."
			},
			{
				"family": "Bhatt",
				"given": "Chidansh A."
			},
			{
				"family": "Pennock",
				"given": "David M."
			},
			{
				"family": "Singh",
				"given": "Vivek K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "irionAlgorithmsOfflimitsIf2022",
		"type": "paper-conference",
		"abstract": "Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533212",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1561–1570",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms Off-limits? If digital trade law restricts access to source code of software then accountability will suffer",
		"URL": "https://doi.org/10.1145/3531146.3533212",
		"author": [
			{
				"family": "Irion",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "costanza-chockWhoAuditsAuditors2022",
		"type": "paper-conference",
		"abstract": "Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533213",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1571–1583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem",
		"URL": "https://doi.org/10.1145/3531146.3533213",
		"author": [
			{
				"family": "Costanza-Chock",
				"given": "Sasha"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "brubachCharacterizingPropertiesTradeoffs2022",
		"type": "paper-conference",
		"abstract": "Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533219",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1629–1638",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Characterizing properties and trade-offs of centralized delegation mechanisms in liquid democracy",
		"URL": "https://doi.org/10.1145/3531146.3533219",
		"author": [
			{
				"family": "Brubach",
				"given": "Brian"
			},
			{
				"family": "Ballarin",
				"given": "Audrey"
			},
			{
				"family": "Nazeer",
				"given": "Heeba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "donahueHumanalgorithmCollaborationAchieving2022",
		"type": "paper-conference",
		"abstract": "Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533221",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1639–1656",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human-algorithm collaboration: Achieving complementarity and avoiding unfairness",
		"URL": "https://doi.org/10.1145/3531146.3533221",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangStopSpreadContextual2022",
		"type": "paper-conference",
		"abstract": "We present an empirical study exploring how privacy influences the acceptance of vaccination certificate (VC) deployments across different realistic usage scenarios. The study employed the privacy framework of Contextual Integrity, which has been shown to be particularly effective in capturing people’s privacy expectations across different contexts. We use a vignette methodology, where we selectively manipulate salient contextual parameters to learn whether and how they affect people’s attitudes towards VCs. We surveyed 890 participants from a demographically-stratified sample of the US population to gauge the acceptance and overall attitudes towards possible VC deployments to enforce vaccination mandates and the different information flows VCs might entail. Analysis of results collected as part of this study is used to derive general normative observations about different possible VC practices and to provide guidance for the possible deployments of VCs in different contexts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533222",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1657–1670",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stop the spread: A contextual integrity perspective on the appropriateness of COVID-19 vaccination certificates",
		"URL": "https://doi.org/10.1145/3531146.3533222",
		"author": [
			{
				"family": "Zhang",
				"given": "Shikun"
			},
			{
				"family": "Shvartzshnaider",
				"given": "Yan"
			},
			{
				"family": "Feng",
				"given": "Yuanyuan"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			},
			{
				"family": "Sadeh",
				"given": "Norman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "johnsonWhatBureaucraticCounterfactual2022",
		"type": "paper-conference",
		"abstract": "There is growing concern about governments’ use of algorithms to make high-stakes decisions. While an early wave of research focused on algorithms that predict risk to allocate punishment and suspicion, a newer wave of research studies algorithms that predict “need” or “benefit” to target beneficial resources, such as ranking those experiencing homelessness by their need for housing. The present paper argues that existing research on the role of algorithms in social policy could benefit from a counterfactual perspective that asks: given that a social service bureaucracy needs to make some decision about whom to help, what status quo prioritization method would algorithms replace? While a large body of research contrasts human versus algorithmic decision-making, social service bureaucracies target help not by giving street-level bureaucrats full discretion. Instead, they primarily target help through pre-algorithmic, rule-based methods. In this paper, we outline social policy’s current status quo method—categorical prioritization—where decision-makers manually (1) decide which attributes of help seekers should give those help seekers priority, (2) simplify any continuous measures of need into categories (e.g., household income falls below a threshold), and (3) manually choose the decision rules that map categories to priority levels. We draw on novel data and quantitative and qualitative social science methods to outline categorical prioritization in two case studies of United States social policy: waitlists for scarce housing vouchers and K-12 school finance formulas. We outline three main differences between categorical and algorithmic prioritization: is the basis for prioritization formalized; what role does power play in prioritization; and are decision rules for priority manually chosen or inductively derived from a predictive model. Concluding, we show how the counterfactual perspective underscores both the understudied costs of categorical prioritization in social policy and the understudied potential of predictive algorithms to narrow inequalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533223",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1671–1682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What is the bureaucratic counterfactual? Categorical versus algorithmic prioritization in U.S. social policy",
		"URL": "https://doi.org/10.1145/3531146.3533223",
		"author": [
			{
				"family": "Johnson",
				"given": "Rebecca Ann"
			},
			{
				"family": "Zhang",
				"given": "Simone"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sachdevaAssessingAnnotatorIdentity2022",
		"type": "paper-conference",
		"abstract": "Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533216",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1585–1603",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing annotator identity sensitivity via item response theory: A case study in a hate speech corpus",
		"URL": "https://doi.org/10.1145/3531146.3533216",
		"author": [
			{
				"family": "Sachdeva",
				"given": "Pratik S."
			},
			{
				"family": "Barreto",
				"given": "Renata"
			},
			{
				"family": "Vacano",
				"given": "Claudia",
				"non-dropping-particle": "von"
			},
			{
				"family": "Kennedy",
				"given": "Chris J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kambhatlaSurfacingRacialStereotypes2022",
		"type": "paper-conference",
		"abstract": "Content warning: this paper discusses and contains content that may be offensive or upsetting.People express racial stereotypes through conversations with others, increasingly in a digital format; as a result, the ability to computationally identify racial stereotypes could be beneficial to help mitigate some of the harmful effects of stereotyping. In this work, we seek to better understand how we can computationally surface racial stereotypes in text by identifying linguistic features associated with differences in racial identity portrayal, focused on two races (Black and White). We collect novel data of individuals’ self-presentation via crowdsourcing, where each crowdworker answers a set of prompts from their own perspective (real identity), and from the perspective of another racial identity (portrayed identity), keeping the gender constant. We use these responses as a dataset to identify stereotypes. Through a series of experiments based on classifications between real and portrayed identities, we show that generalizations and stereotypes appear to be more prevalent amongst white participants than black participants. Through analyses of predictive words and word usage patterns, we find that some of the most predictive features of an author portraying a different racial identity are known stereotypes, and reveal how people of different identities see themselves and others.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533217",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1604–1615",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Surfacing racial stereotypes through identity portrayal",
		"URL": "https://doi.org/10.1145/3531146.3533217",
		"author": [
			{
				"family": "Kambhatla",
				"given": "Gauri"
			},
			{
				"family": "Stewart",
				"given": "Ian"
			},
			{
				"family": "Mihalcea",
				"given": "Rada"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schoefferThereNotEnough2022",
		"type": "paper-conference",
		"abstract": "Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533218",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1616–1628",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“There is not enough information”: On the effects of explanations on perceptions of informational fairness and trustworthiness in automated decision-making",
		"URL": "https://doi.org/10.1145/3531146.3533218",
		"author": [
			{
				"family": "Schoeffer",
				"given": "Jakob"
			},
			{
				"family": "Kuehl",
				"given": "Niklas"
			},
			{
				"family": "Machowski",
				"given": "Yvette"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "andrusDemographicreliantAlgorithmicFairness2022",
		"type": "paper-conference",
		"abstract": "Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533226",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1709–1721",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Demographic-reliant algorithmic fairness: Characterizing the risks of demographic data collection in the pursuit of fairness",
		"URL": "https://doi.org/10.1145/3531146.3533226",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Villeneuve",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "matiasTestingConcernsTechnologys2022",
		"type": "paper-conference",
		"abstract": "As public trust in technology companies has declined, people are questioning the effects of digital technologies in their lives. In this context, many evidence-free claims from corporations and tech critics are widely circulated. How can members of the public make evidence-based decisions about digital technology in their lives? In clinical fields, N -of-one trials enable participant-investigators to make personalized causal discoveries about managing health, improving fitness, and improving their education. Similar methods could help community scientists understand and manage how they use digital technologies. In this paper, we introduce Conjecture, a system for coordinating N -of-one trials that can guide personal decisions about technology use and contribute to science. We describe N -of-one trials as a design challenge and present the design of the Conjecture system. We evaluate the system with a field experiment that tests folk theories about the influence of colorful screens on alleged phone addiction. We present findings on the design of N -of-one-trial systems based on submitted data, interviews, and surveys with 14 participants. Taken together, this paper introduces N -of-one trials as a fruitful direction for computer scientists designing industry-independent systems for evidence-based technology governance and accountability.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533227",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1722–1732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Testing concerns about technology's behavioral impacts with N-of-one trials",
		"URL": "https://doi.org/10.1145/3531146.3533227",
		"author": [
			{
				"family": "Matias",
				"given": "Nathan"
			},
			{
				"family": "Pennington",
				"given": "Eric"
			},
			{
				"family": "Chan",
				"given": "Zenobia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "readerModelsUnderstandingQuantifying2022",
		"type": "paper-conference",
		"abstract": "When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533230",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1765–1775",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Models for understanding and quantifying feedback in societal systems",
		"URL": "https://doi.org/10.1145/3531146.3533230",
		"author": [
			{
				"family": "Reader",
				"given": "Lydia"
			},
			{
				"family": "Nokhiz",
				"given": "Pegah"
			},
			{
				"family": "Power",
				"given": "Cathleen"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Friedler",
				"given": "Sorelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pushkarnaDataCardsPurposeful2022",
		"type": "paper-conference",
		"abstract": "As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset’s origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset’s lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models—such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533231",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 51\npublisher-place: Seoul, Republic of Korea",
		"page": "1776–1826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data cards: Purposeful and transparent dataset documentation for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533231",
		"author": [
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Kjartansson",
				"given": "Oddur"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "semenovaExistenceSimplerMachine2022",
		"type": "paper-conference",
		"abstract": "It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533232",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 32\npublisher-place: Seoul, Republic of Korea",
		"page": "1827–1858",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the existence of simpler machine learning models",
		"URL": "https://doi.org/10.1145/3531146.3533232",
		"author": [
			{
				"family": "Semenova",
				"given": "Lesia"
			},
			{
				"family": "Rudin",
				"given": "Cynthia"
			},
			{
				"family": "Parr",
				"given": "Ronald"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "barberanNeuroViewRNNItsTime2022",
		"type": "paper-conference",
		"abstract": "Recurrent Neural Networks (RNNs) are important tools for processing sequential data such as time-series or video. Interpretability is defined as the ability to be understood by a person and is different from explainability, which is the ability to be explained in a mathematical formulation. A key interpretability issue with RNNs is that it is not clear how each hidden state per time step contributes to the decision-making process in a quantitative manner. We propose NeuroView-RNN as a family of new RNN architectures that explains how all the time steps are used for the decision-making process. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier. The global linear classifier has all the hidden states as the input, so the weights of the classifier have a linear mapping to the hidden states. Hence, from the weights, NeuroView-RNN can quantify how important each time step is to a particular decision. As a bonus, NeuroView-RNN also offers higher accuracy in many cases compared to the RNNs and their variants. We showcase the benefits of NeuroView-RNN by evaluating on a multitude of diverse time-series datasets.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533224",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "1683–1697",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "NeuroView-RNN: It’s about time",
		"URL": "https://doi.org/10.1145/3531146.3533224",
		"author": [
			{
				"family": "Barberan",
				"given": "Cj"
			},
			{
				"family": "Alemmohammad",
				"given": "Sina"
			},
			{
				"family": "Liu",
				"given": "Naiming"
			},
			{
				"family": "Balestriero",
				"given": "Randall"
			},
			{
				"family": "Baraniuk",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wuFairnessawareModelagnosticPositive2022",
		"type": "paper-conference",
		"abstract": "With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533225",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1698–1708",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness-aware model-agnostic positive and unlabeled learning",
		"URL": "https://doi.org/10.1145/3531146.3533225",
		"author": [
			{
				"family": "Wu",
				"given": "Ziwei"
			},
			{
				"family": "He",
				"given": "Jingrui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "abebeAdversarialScrutinyEvidentiary2022",
		"type": "paper-conference",
		"abstract": "The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533228",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1733–1746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adversarial scrutiny of evidentiary statistical software",
		"URL": "https://doi.org/10.1145/3531146.3533228",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			},
			{
				"family": "Jin",
				"given": "Angela"
			},
			{
				"family": "Miller",
				"given": "John"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Wexler",
				"given": "Rebecca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ganguliPredictabilitySurpriseLarge2022a",
		"type": "paper-conference",
		"abstract": "Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533229",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1747–1764",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Predictability and surprise in large generative models",
		"URL": "https://doi.org/10.1145/3531146.3533229",
		"author": [
			{
				"family": "Ganguli",
				"given": "Deep"
			},
			{
				"family": "Hernandez",
				"given": "Danny"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Askell",
				"given": "Amanda"
			},
			{
				"family": "Bai",
				"given": "Yuntao"
			},
			{
				"family": "Chen",
				"given": "Anna"
			},
			{
				"family": "Conerly",
				"given": "Tom"
			},
			{
				"family": "Dassarma",
				"given": "Nova"
			},
			{
				"family": "Drain",
				"given": "Dawn"
			},
			{
				"family": "Elhage",
				"given": "Nelson"
			},
			{
				"family": "El Showk",
				"given": "Sheer"
			},
			{
				"family": "Fort",
				"given": "Stanislav"
			},
			{
				"family": "Hatfield-Dodds",
				"given": "Zac"
			},
			{
				"family": "Henighan",
				"given": "Tom"
			},
			{
				"family": "Johnston",
				"given": "Scott"
			},
			{
				"family": "Jones",
				"given": "Andy"
			},
			{
				"family": "Joseph",
				"given": "Nicholas"
			},
			{
				"family": "Kernian",
				"given": "Jackson"
			},
			{
				"family": "Kravec",
				"given": "Shauna"
			},
			{
				"family": "Mann",
				"given": "Ben"
			},
			{
				"family": "Nanda",
				"given": "Neel"
			},
			{
				"family": "Ndousse",
				"given": "Kamal"
			},
			{
				"family": "Olsson",
				"given": "Catherine"
			},
			{
				"family": "Amodei",
				"given": "Daniela"
			},
			{
				"family": "Brown",
				"given": "Tom"
			},
			{
				"family": "Kaplan",
				"given": "Jared"
			},
			{
				"family": "McCandlish",
				"given": "Sam"
			},
			{
				"family": "Olah",
				"given": "Christopher"
			},
			{
				"family": "Amodei",
				"given": "Dario"
			},
			{
				"family": "Clark",
				"given": "Jack"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hutchinsonEvaluationGapsMachine2022",
		"type": "paper-conference",
		"abstract": "Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533233",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1859–1876",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluation gaps in machine learning practice",
		"URL": "https://doi.org/10.1145/3531146.3533233",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Heller",
				"given": "Katherine"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dodgeMeasuringCarbonIntensity2022",
		"type": "paper-conference",
		"abstract": "The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions. In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1877–1894",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring the carbon intensity of AI in cloud instances",
		"URL": "https://doi.org/10.1145/3531146.3533234",
		"author": [
			{
				"family": "Dodge",
				"given": "Jesse"
			},
			{
				"family": "Prewitt",
				"given": "Taylor"
			},
			{
				"family": "Tachet des Combes",
				"given": "Remi"
			},
			{
				"family": "Odmark",
				"given": "Erika"
			},
			{
				"family": "Schwartz",
				"given": "Roy"
			},
			{
				"family": "Strubell",
				"given": "Emma"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			},
			{
				"family": "DeCario",
				"given": "Nicole"
			},
			{
				"family": "Buchanan",
				"given": "Will"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rameshHowPlatformuserPower2022",
		"type": "paper-conference",
		"abstract": "Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533237",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1917–1928",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How platform-user power relations shape algorithmic accountability: A case study of instant loan platforms and financially stressed users in india",
		"URL": "https://doi.org/10.1145/3531146.3533237",
		"author": [
			{
				"family": "Ramesh",
				"given": "Divya"
			},
			{
				"family": "Kameswaran",
				"given": "Vaishnav"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Sambasivan",
				"given": "Nithya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tschantzWhatProxyDiscrimination2022",
		"type": "paper-conference",
		"abstract": "The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533242",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1993–2003",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What is proxy discrimination?",
		"URL": "https://doi.org/10.1145/3531146.3533242",
		"author": [
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "patelModelExplanationsDifferential2022",
		"type": "paper-conference",
		"abstract": "Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533235",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1895–1904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model explanations with differential privacy",
		"URL": "https://doi.org/10.1145/3531146.3533235",
		"author": [
			{
				"family": "Patel",
				"given": "Neel"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "grabowiczMarryingFairnessExplainability2022",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533236",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1905–1916",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Marrying fairness and explainability in supervised learning",
		"URL": "https://doi.org/10.1145/3531146.3533236",
		"author": [
			{
				"family": "Grabowicz",
				"given": "Przemyslaw A."
			},
			{
				"family": "Perello",
				"given": "Nicholas"
			},
			{
				"family": "Mishra",
				"given": "Aarshee"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "patroFairRankingCritical2022",
		"type": "paper-conference",
		"abstract": "Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large “fair ranking” research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533238",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1929–1942",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair ranking: a critical review, challenges, and future directions",
		"URL": "https://doi.org/10.1145/3531146.3533238",
		"author": [
			{
				"family": "Patro",
				"given": "Gourab K."
			},
			{
				"family": "Porcaro",
				"given": "Lorenzo"
			},
			{
				"family": "Mitchell",
				"given": "Laura"
			},
			{
				"family": "Zhang",
				"given": "Qiuyue"
			},
			{
				"family": "Zehlike",
				"given": "Meike"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rostamzadehHealthsheetDevelopmentTransparency2022",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) approaches have demonstrated promising results in a wide range of healthcare applications. Data plays a crucial role in developing ML-based healthcare systems that directly affect people’s lives. Many of the ethical issues surrounding the use of ML in healthcare stem from structural inequalities underlying the way we collect, use, and handle data. Developing guidelines to improve documentation practices regarding the creation, use, and maintenance of ML healthcare datasets is therefore of critical importance. In this work, we introduce Healthsheet, a contextualized adaptation of the original datasheet questionnaire &nbsp;[22] for health-specific applications. Through a series of semi-structured interviews, we adapt the datasheets for healthcare data documentation. As part of the Healthsheet development process and to understand the obstacles researchers face in creating datasheets, we worked with three publicly-available healthcare datasets as our case studies, each with different types of structured data: Electronic health Records (EHR), clinical trial study data, and smartphone-based performance outcome measures. Our findings from the interviewee study and case studies show 1) that datasheets should be contextualized for healthcare, 2) that despite incentives to adopt accountability practices such as datasheets, there is a lack of consistency in the broader use of these practices 3) how the ML for health community views datasheets and particularly Healthsheets as diagnostic tool to surface the limitations and strength of datasets and 4) the relative importance of different fields in the datasheet to healthcare concerns.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533239",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1943–1961",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Healthsheet: Development of a transparency artifact for health datasets",
		"URL": "https://doi.org/10.1145/3531146.3533239",
		"author": [
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Mincu",
				"given": "Diana"
			},
			{
				"family": "Roy",
				"given": "Subhrajit"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			},
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Schrouff",
				"given": "Jessica"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			},
			{
				"family": "Moorosi",
				"given": "Nyalleng"
			},
			{
				"family": "Heller",
				"given": "Katherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "deonSpotlightGeneralMethod2022",
		"type": "paper-conference",
		"abstract": "Supervised learning models often make systematic errors on rare subsets of the data. When these subsets correspond to explicit labels in the data (e.g., gender, race) such poor performance can be identified straightforwardly. This paper introduces a method for discovering systematic errors that do not correspond to such explicitly labelled subgroups. The key idea is that similar inputs tend to have similar representations in the final hidden layer of a neural network. We leverage this structure by “shining a spotlight” on this representation space to find contiguous regions in which the model performs poorly. We show that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and we verify its performance through quantitative experiments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533240",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 20\npublisher-place: Seoul, Republic of Korea",
		"page": "1962–1981",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The spotlight: A general method for discovering systematic errors in deep learning models",
		"URL": "https://doi.org/10.1145/3531146.3533240",
		"author": [
			{
				"family": "Eon",
				"given": "Greg",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Eon",
				"given": "Jason",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Wright",
				"given": "James R."
			},
			{
				"family": "Leyton-Brown",
				"given": "Kevin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ganskyCounterFAccTualHowFAccT2022",
		"type": "paper-conference",
		"abstract": "This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles.We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems.In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533241",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1982–1992",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CounterFAccTual: How FAccT undermines its organizing principles",
		"URL": "https://doi.org/10.1145/3531146.3533241",
		"author": [
			{
				"family": "Gansky",
				"given": "Ben"
			},
			{
				"family": "McDonald",
				"given": "Sean"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cousinsUncertaintySocialPlanners2022",
		"type": "paper-conference",
		"abstract": "Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533243",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2004–2015",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncertainty and the social planner’s problem: Why sample complexity matters",
		"URL": "https://doi.org/10.1145/3531146.3533243",
		"author": [
			{
				"family": "Cousins",
				"given": "Cyrus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "widderLimitsPossibilitiesEthical2022",
		"type": "paper-conference",
		"abstract": "Open source software communities are a significant site of AI development, but “Ethical AI” discourses largely focus on the problems that arise in software produced by private companies. Design, policy and tooling interventions to encourage “Ethical AI” based on studies in private companies risk being ill-suited for an open source context, which operates under radically different organizational structures, cultural norms, and incentives. In this paper, we show that significant and understudied harms and possibilities originate from differing practices of transparency and accountability in the open source community. We conducted an interview study of an AI-enabled open source Deepfake project to understand how members of that community reason about the ethics of their work. We found that notions of the “Freedom 0” to use code without any restriction, alongside beliefs about technology neutrality and technological inevitability, were central to how community members framed their responsibilities, and the actions they believed were and were not available to them. We propose a continuum between harms resulting from how a system is implemented versus how it is used, and show how commitments to radical transparency in open source allow great ethical scrutiny for harms wrought by implementation bugs, but allow harms through (mis)use to proliferate, requiring a deeper toolbox for disincentivizing harmful use. We discuss how an assumption of control over downstream uses is often implicit in discourses of “Ethical AI”, but outline alternative possibilities for action in cases such as open source where this assumption may not hold.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533779",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2035–2046",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Limits and possibilities for “Ethical AI” in open source: A study of deepfakes",
		"URL": "https://doi.org/10.1145/3531146.3533779",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			},
			{
				"family": "Nafus",
				"given": "Dawn"
			},
			{
				"family": "Dabbish",
				"given": "Laura"
			},
			{
				"family": "Herbsleb",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ashurstAIEthicsStatements2022",
		"type": "paper-conference",
		"abstract": "Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533780",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2047–2056",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI ethics statements: Analysis and lessons learnt from NeurIPS broader impact statements",
		"URL": "https://doi.org/10.1145/3531146.3533780",
		"author": [
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Hine",
				"given": "Emmie"
			},
			{
				"family": "Sedille",
				"given": "Paul"
			},
			{
				"family": "Carlier",
				"given": "Alexis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "boydDesigningValuesensitiveDesign2022",
		"type": "paper-conference",
		"abstract": "If “studying up,” or researching powerful actors in a social system, can offer insight into the workings and effects of power in social systems, this paper argues that “designing up” will give researchers and designers a tool to intervene. This paper offers a conception of “designing up,” applies the structure of Value Sensitive Design (VSD) to accomplish it, and submits an example of a tool designed to support ethical sensitivity, especially particularization and judgment. The designed artifact is a field guide for ethical mitigation strategies that uses tool profiles and filters to aid machine learning (ML) engineers as they build understanding of an ethical issue they have recognized and as they match the particulars of their problem to a technical ethical mitigation. This guide may broaden its users’ awareness of potential ethical issues, important features of ethical issues and their mitigations, and the breadth of available mitigations. Additionally, it may encourage ethical sensitivity in future ML projects. Feedback from ML engineers and technology ethics researchers rendered several usability improvements and ideas for future development. The tool can be found at: https://ml-ethics-tool.web.app/.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534626",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "2069–2082",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing up with value-sensitive design: Building a field guide for ethical ML development",
		"URL": "https://doi.org/10.1145/3531146.3534626",
		"author": [
			{
				"family": "Boyd",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "devinneyTheoriesGenderNLP2022",
		"type": "paper-conference",
		"abstract": "The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534627",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 20\npublisher-place: Seoul, Republic of Korea",
		"page": "2083–2102",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Theories of “Gender” in NLP bias research",
		"URL": "https://doi.org/10.1145/3531146.3534627",
		"author": [
			{
				"family": "Devinney",
				"given": "Hannah"
			},
			{
				"family": "Björklund",
				"given": "Jenny"
			},
			{
				"family": "Björklund",
				"given": "Henrik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "drawsEffectsCrowdWorker2022",
		"type": "paper-conference",
		"abstract": "Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534629",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2114–2124",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effects of crowd worker biases in fact-checking tasks",
		"URL": "https://doi.org/10.1145/3531146.3534629",
		"author": [
			{
				"family": "Draws",
				"given": "Tim"
			},
			{
				"family": "La Barbera",
				"given": "David"
			},
			{
				"family": "Soprano",
				"given": "Michael"
			},
			{
				"family": "Roitero",
				"given": "Kevin"
			},
			{
				"family": "Ceolin",
				"given": "Davide"
			},
			{
				"family": "Checco",
				"given": "Alessandro"
			},
			{
				"family": "Mizzaro",
				"given": "Stefano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kuhlKeepYourFriends2022",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534630",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2125–2137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Keep your friends close and your counterfactuals closer: Improved learning from closest rather than plausible counterfactual explanations in an abstract setting",
		"URL": "https://doi.org/10.1145/3531146.3534630",
		"author": [
			{
				"family": "Kuhl",
				"given": "Ulrike"
			},
			{
				"family": "Artelt",
				"given": "André"
			},
			{
				"family": "Hammer",
				"given": "Barbara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mehandruReliableSafeUse2022",
		"type": "paper-conference",
		"abstract": "Language barriers between patients and clinicians contribute to disparities in quality of care. Machine Translation (MT) tools are widely used in healthcare settings, but even small mistranslations can have life-threatening consequences. We study how MT is currently used in medical settings through a qualitative interview study with 20 clinicians–physicians, surgeons, nurses, and midwives. We find that clinicians face challenges stemming from lack of time and resources, cultural barriers, and medical literacy rates, as well as accountability in cases of miscommunication. Clinicians have devised strategies to aid communication in the face of language barriers including back translation, non-verbal communication, and testing patient understanding. We propose design implications for machine translation systems including combining neural MT with pre-translated medical phrases, integrating translation support with multimodal communication, and providing interactive support for testing mutual understanding.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533244",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2016–2025",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reliable and safe use of machine translation in medical settings",
		"URL": "https://doi.org/10.1145/3531146.3533244",
		"author": [
			{
				"family": "Mehandru",
				"given": "Nikita"
			},
			{
				"family": "Robertson",
				"given": "Samantha"
			},
			{
				"family": "Salehi",
				"given": "Niloufar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "loiCalibrationFairnessRequirement2022",
		"type": "paper-conference",
		"abstract": "In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533245",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "2026–2034",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Is calibration a fairness requirement? An argument from the point of view of moral philosophy and decision theory",
		"URL": "https://doi.org/10.1145/3531146.3533245",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ashurstDisentanglingComponentsEthical2022",
		"type": "paper-conference",
		"abstract": "While practical applications of machine learning have been the target of considerable normative scrutiny over the past decade, there is growing concern with machine learning research as well. Debates are currently unfolding about how the research community should develop its research agendas, conduct its research, evaluate its research contributions, and handle the publication and dissemination of its findings, among other matters. At times, these debates have been quite heated, with different actors adopting different positions on what it means to do machine learning research ethically. In this paper, we show that some of the disagreement owes to a lack of clarity about what ethical issues are at stake in machine learning research, how these issues—in particular, the concerns with research integrity, research process harms, and downstream consequences—relate to (or, more often, differ from) one another. We then explore which mechanisms are most appropriate for dealing with the different types of ethical issues, and highlight which ethical issues require more attention than they are currently receiving. Ultimately, we hope to foster more productive discussions about the responsibilities that the community bears in addressing the ethical challenges tied to machine learning research and how to best fulfil these responsibilities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533781",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2057–2068",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling the components of ethical research in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533781",
		"author": [
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Campbell",
				"given": "Rosie"
			},
			{
				"family": "Raji",
				"given": "Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "limaConflictExplainableAccountable2022",
		"type": "paper-conference",
		"abstract": "Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534628",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2103–2113",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The conflict between explainable and accountable decision-making algorithms",
		"URL": "https://doi.org/10.1145/3531146.3534628",
		"author": [
			{
				"family": "Lima",
				"given": "Gabriel"
			},
			{
				"family": "Grgić-Hlača",
				"given": "Nina"
			},
			{
				"family": "Jeong",
				"given": "Jin Keun"
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "scottAlgorithmicToolsPublic2022",
		"type": "paper-conference",
		"abstract": "Data-driven and algorithmic systems have been introduced to support Public Employment Services (PES) throughout the world. Their deployment has sparked public controversy and, as a consequence, some of these systems have been removed from use or their role was reduced. Yet the implementation of similar systems continues. In this paper, we use a participatory approach to determine a course forward for research and development in this area. We draw attention to the needs and expectations of people directly affected by these systems, i.e., jobseekers. Our investigation comprises two workshops: the first a fact-finding workshop with academics, system developers, the public sector, and civil-society organizations, the second a co-design workshop with 13 unemployed migrants to Germany. Based on the discussion in the fact-finding workshop we identified challenges of existing PES (algorithmic) systems. From the co-design workshop we identified our participants’ needs and desires when contacting PES: the need for human contact, the expectation to receive genuine orientation, and the desire to be seen as a whole human being. We map these expectations to three design considerations for data-driven and algorithmic systems for PES: the importance of interpersonal interaction, jobseeker assessment as direction, and the challenge of mitigating misrepresentation. Finally, we argue that the limitations and risks of current systems cannot be addressed through minor adjustments but require a more fundamental change to the role of PES.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534631",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2138–2148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic tools in public employment services: Towards a jobseeker-centric perspective",
		"URL": "https://doi.org/10.1145/3531146.3534631",
		"author": [
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Wang",
				"given": "Sonja Mei"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Delobelle",
				"given": "Pieter"
			},
			{
				"family": "Sztandar-Sztanderska",
				"given": "Karolina"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sharafPromotingFairnessLearned2022",
		"type": "paper-conference",
		"abstract": "Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534632",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "2149–2156",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Promoting fairness in learned models by learning to active learn under parity constraints",
		"URL": "https://doi.org/10.1145/3531146.3534632",
		"author": [
			{
				"family": "Sharaf",
				"given": "Amr"
			},
			{
				"family": "Daume III",
				"given": "Hal"
			},
			{
				"family": "Ni",
				"given": "Renkun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bilstrupDemoDesignTeaching2022",
		"type": "paper-conference",
		"abstract": "The prevalence of artificial intelligence (AI) and machine learning (ML) technologies in digital ecosystems has led to a push for AI literacy, giving everybody, including K-12 students, the necessary knowledge and abilities to engage critically with these new technologies. While there is an increasing focus on designing tools and activities for teaching machine learning, most tools sidestep engaging with the complexity and trade-offs inherent in the design of ML models in favor of demonstrating the power and functionality of the technology. In this paper, we investigate how a design perspective can inform the design of educational tools and activities for teaching machine learning. Through a literature review, we identify 34 tools and activities for teaching ML, and using a design perspective on ML system development, we examine strengths and limitations in how they engage students in the complex design considerations linked to the different components of machine learners. Based on this work, we suggest directions for furthering AI literacy through adopting a design approach in teaching ML.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534634",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2168–2178",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From demo to design in teaching machine learning",
		"URL": "https://doi.org/10.1145/3531146.3534634",
		"author": [
			{
				"family": "Bilstrup",
				"given": "Karl-Emi Kjær"
			},
			{
				"family": "Kaspersen",
				"given": "Magnus Høholt"
			},
			{
				"family": "Assent",
				"given": "Ira"
			},
			{
				"family": "Enni",
				"given": "Simon"
			},
			{
				"family": "Petersen",
				"given": "Marianne Graves"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "laranjeiradasilvaSeeingLookingAnalysis2022",
		"type": "paper-conference",
		"abstract": "The online sharing and viewing of Child Sexual Abuse Material (CSAM) are growing fast, such that human experts can no longer handle the manual inspection. However, the automatic classification of CSAM is a challenging field of research, largely due to the inaccessibility of target data that is — and should forever be — private and in sole possession of law enforcement agencies. To aid researchers in drawing insights from unseen data and safely providing further understanding of CSAM images, we propose an analysis template that goes beyond the statistics of the dataset and respective labels. It focuses on the extraction of automatic signals, provided both by pre-trained machine learning models, e.g., object categories and pornography detection, as well as image metrics such as luminance and sharpness. Only aggregated statistics of sparse signals are provided to guarantee the anonymity of children and adolescents victimized. The pipeline allows filtering the data by applying thresholds to each specified signal and provides the distribution of such signals within the subset, correlations between signals, as well as a bias evaluation. We demonstrated our proposal on the Region-based annotated Child Pornography Dataset (RCPD), one of the few CSAM benchmarks in the literature, composed of over 2000 samples among regular and CSAM images, produced in partnership with Brazil’s Federal Police. Although noisy and limited in several senses, we argue that automatic signals can highlight important aspects of the overall distribution of data, which is valuable for databases that can not be disclosed. Our goal is to safely publicize the characteristics of CSAM datasets, encouraging researchers to join the field and perhaps other institutions to provide similar reports on their benchmarks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534636",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2189–2205",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Seeing without looking: Analysis pipeline for child sexual abuse datasets",
		"URL": "https://doi.org/10.1145/3531146.3534636",
		"author": [
			{
				"family": "Laranjeira da Silva",
				"given": "Camila"
			},
			{
				"family": "Macedo",
				"given": "Joao"
			},
			{
				"family": "Avila",
				"given": "Sandra"
			},
			{
				"family": "Santos",
				"given": "Jefersson",
				"non-dropping-particle": "dos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "usunierFastOnlineRanking2022",
		"type": "paper-conference",
		"abstract": "As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534633",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "2157–2167",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fast online ranking with fairness of exposure",
		"URL": "https://doi.org/10.1145/3531146.3534633",
		"author": [
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Do",
				"given": "Virginie"
			},
			{
				"family": "Dohmatob",
				"given": "Elvis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schwobelLongArcFairness2022",
		"type": "paper-conference",
		"abstract": "In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application – the current European efforts to increase the number of women on company boards, e&nbsp;.g.&nbsp; via quota solutions – and present early technical work that fits within our framework.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534635",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2179–2188",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The long arc of fairness: Formalisations and ethical discourse",
		"URL": "https://doi.org/10.1145/3531146.3534635",
		"author": [
			{
				"family": "Schwöbel",
				"given": "Pola"
			},
			{
				"family": "Remmers",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "jerniteDataGovernanceAge2022",
		"type": "paper-conference",
		"abstract": "The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534637",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2206–2222",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data governance in the age of large-scale data-driven language technology",
		"URL": "https://doi.org/10.1145/3531146.3534637",
		"author": [
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Nguyen",
				"given": "Huu"
			},
			{
				"family": "Biderman",
				"given": "Stella"
			},
			{
				"family": "Rogers",
				"given": "Anna"
			},
			{
				"family": "Masoud",
				"given": "Maraim"
			},
			{
				"family": "Danchev",
				"given": "Valentin"
			},
			{
				"family": "Tan",
				"given": "Samson"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Subramani",
				"given": "Nishant"
			},
			{
				"family": "Johnson",
				"given": "Isaac"
			},
			{
				"family": "Dupont",
				"given": "Gerard"
			},
			{
				"family": "Dodge",
				"given": "Jesse"
			},
			{
				"family": "Lo",
				"given": "Kyle"
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Radev",
				"given": "Dragomir"
			},
			{
				"family": "Gokaslan",
				"given": "Aaron"
			},
			{
				"family": "Nikpoor",
				"given": "Somaieh"
			},
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "robertsonUnderstandingBeingUnderstood2022",
		"type": "paper-conference",
		"abstract": "Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534638",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "2223–2238",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding and being understood: User strategies for identifying and recovering from mistranslations in machine translation-mediated chat",
		"URL": "https://doi.org/10.1145/3531146.3534638",
		"author": [
			{
				"family": "Robertson",
				"given": "Samantha"
			},
			{
				"family": "Díaz",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "speithReviewTaxonomiesExplainable2022",
		"type": "paper-conference",
		"abstract": "The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534639",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2239–2250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A review of taxonomies of explainable artificial intelligence (XAI) methods",
		"URL": "https://doi.org/10.1145/3531146.3534639",
		"author": [
			{
				"family": "Speith",
				"given": "Timo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lovatoLimitsIndividualConsent2022",
		"type": "paper-conference",
		"abstract": "Personal data are not discrete in socially-networked digital environments. A user who consents to allow access to their profile can expose the personal data of their network connections to non-consented access. Therefore, the traditional consent model (informed and individual) is not appropriate in social networks where informed consent may not be possible for all users affected by data processing and where information is distributed across users. Here, we outline the adequacy of consent for data transactions. Informed by the shortcomings of individual consent, we introduce both a platform-specific model of “distributed consent” and a cross-platform model of a “consent passport.” In both models, individuals and groups can coordinate by giving consent conditional on that of their network connections. We simulate the impact of these distributed consent models on the observability of social networks and find that low adoption would allow macroscopic subsets of networks to preserve their connectivity and privacy.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534640",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2251–2262",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Limits of individual consent and models of distributed consent in online social networks",
		"URL": "https://doi.org/10.1145/3531146.3534640",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper L."
			},
			{
				"family": "Allard",
				"given": "Antoine"
			},
			{
				"family": "Harp",
				"given": "Randall"
			},
			{
				"family": "Onaolapo",
				"given": "Jeremiah"
			},
			{
				"family": "Hébert-Dufresne",
				"given": "Laurent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghazimatinMeasuringFairnessRankings2022",
		"type": "paper-conference",
		"abstract": "Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534641",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "2263–2279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring fairness of rankings under noisy sensitive information",
		"URL": "https://doi.org/10.1145/3531146.3534641",
		"author": [
			{
				"family": "Ghazimatin",
				"given": "Azin"
			},
			{
				"family": "Kleindessner",
				"given": "Matthaus"
			},
			{
				"family": "Russell",
				"given": "Chris"
			},
			{
				"family": "Abedjan",
				"given": "Ziawasch"
			},
			{
				"family": "Golebiowski",
				"given": "Jacek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "brownWhatDoesIt2022",
		"type": "paper-conference",
		"abstract": "Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534642",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2280–2292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What does it mean for a language model to preserve privacy?",
		"URL": "https://doi.org/10.1145/3531146.3534642",
		"author": [
			{
				"family": "Brown",
				"given": "Hannah"
			},
			{
				"family": "Lee",
				"given": "Katherine"
			},
			{
				"family": "Mireshghallah",
				"given": "Fatemehsadat"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Tramèr",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pastaltzidisDataAugmentationFairnessaware2022",
		"type": "paper-conference",
		"abstract": "Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534644",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "2302–2314",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems",
		"URL": "https://doi.org/10.1145/3531146.3534644",
		"author": [
			{
				"family": "Pastaltzidis",
				"given": "Ioannis"
			},
			{
				"family": "Dimitriou",
				"given": "Nikolaos"
			},
			{
				"family": "Quezada-Tavarez",
				"given": "Katherine"
			},
			{
				"family": "Aidinlis",
				"given": "Stergios"
			},
			{
				"family": "Marquenie",
				"given": "Thomas"
			},
			{
				"family": "Gurzawska",
				"given": "Agata"
			},
			{
				"family": "Tzovaras",
				"given": "Dimitrios"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "baumannEnforcingGroupFairness2022",
		"type": "paper-conference",
		"abstract": "Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534645",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "2315–2326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enforcing group fairness in algorithmic decision making: Utility maximization under sufficiency",
		"URL": "https://doi.org/10.1145/3531146.3534645",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Hannák",
				"given": "Anikó"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "viganoPeopleAreNot2022",
		"type": "paper-conference",
		"abstract": "In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534643",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "2293–2301",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "People are not coins: Morally distinct types of predictions necessitate different fairness constraints",
		"URL": "https://doi.org/10.1145/3531146.3534643",
		"author": [
			{
				"family": "Viganò",
				"given": "Eleonora"
			},
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cruzcortesLocalityTechnicalObjects2022",
		"type": "paper-conference",
		"abstract": "Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534646",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "2327–2341",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Locality of technical objects and the role of structural interventions for systemic change",
		"URL": "https://doi.org/10.1145/3531146.3534646",
		"author": [
			{
				"family": "Cruz Cortés",
				"given": "Efrén"
			},
			{
				"family": "Rajtmajer",
				"given": "Sarah"
			},
			{
				"family": "Ghosh",
				"given": "Debashis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "diazCrowdWorkSheetsAccountingIndividual2022",
		"type": "paper-conference",
		"abstract": "Human annotated data plays a crucial role in machine learning (ML) research and development. However, the ethical considerations around the processes and decisions that go into dataset annotation have not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this space along two layers: (1) who the annotator is, and how the annotators’ lived experiences can impact their annotations, and (2) the relationship between the annotators and the crowdsourcing platforms, and what that relationship affords them. Finally, we introduce a novel framework, CrowdWorkSheets, for dataset developers to facilitate transparent documentation of key decisions points at various stages of the data annotation pipeline: task formulation, selection of annotators, platform and infrastructure choices, dataset analysis and evaluation, and dataset release and maintenance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3534647",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "2342–2351",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CrowdWorkSheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation",
		"URL": "https://doi.org/10.1145/3531146.3534647",
		"author": [
			{
				"family": "Díaz",
				"given": "Mark"
			},
			{
				"family": "Kivlichan",
				"given": "Ian"
			},
			{
				"family": "Rosen",
				"given": "Rachel"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cohenPriceDiscriminationFairness2021",
		"type": "paper-conference",
		"abstract": "Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "2",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Price discrimination with fairness constraints",
		"URL": "https://doi.org/10.1145/3442188.3445864",
		"author": [
			{
				"family": "Cohen",
				"given": "Maxime C."
			},
			{
				"family": "Elmachtoub",
				"given": "Adam N."
			},
			{
				"family": "Lei",
				"given": "Xiao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "singhFairnessViolationsMitigation2021",
		"type": "paper-conference",
		"abstract": "We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "3–13",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness violations and mitigation under covariate shift",
		"URL": "https://doi.org/10.1145/3442188.3445865",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Singh",
				"given": "Rina"
			},
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "parkDesigningOnlineInfrastructure2021",
		"type": "paper-conference",
		"abstract": "AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "52–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing an online infrastructure for collecting AI data from people with disabilities",
		"URL": "https://doi.org/10.1145/3442188.3445870",
		"author": [
			{
				"family": "Park",
				"given": "Joon Sung"
			},
			{
				"family": "Bragg",
				"given": "Danielle"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cheongComputerScienceCommunities2021",
		"type": "paper-conference",
		"abstract": "Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "106–115",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computer science communities: Who is speaking, and who is listening to the women? Using an ethics of care to promote diverse voices",
		"URL": "https://doi.org/10.1145/3442188.3445874",
		"author": [
			{
				"family": "Cheong",
				"given": "Marc"
			},
			{
				"family": "Leins",
				"given": "Kobi"
			},
			{
				"family": "Coghlan",
				"given": "Simon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ball-burackDifferentialTweetmentMitigating2021",
		"type": "paper-conference",
		"abstract": "Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "116–128",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Differential tweetment: Mitigating racial dialect bias in harmful tweet detection",
		"URL": "https://doi.org/10.1145/3442188.3445875",
		"author": [
			{
				"family": "Ball-Burack",
				"given": "Ari"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehReasonsValuesStakeholders2021",
		"type": "paper-conference",
		"abstract": "The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence",
		"URL": "https://doi.org/10.1145/3442188.3445866",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "heidariAllocatingOpportunitiesDynamic2021",
		"type": "paper-conference",
		"abstract": "Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status — a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "15–25",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Allocating opportunities in a dynamic model of intergenerational mobility",
		"URL": "https://doi.org/10.1145/3442188.3445867",
		"author": [
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ronCorporateSocialResponsibility2021",
		"type": "paper-conference",
		"abstract": "We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "26–40",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Corporate social responsibility via multi-armed bandits",
		"URL": "https://doi.org/10.1145/3442188.3445868",
		"author": [
			{
				"family": "Ron",
				"given": "Tom"
			},
			{
				"family": "Ben-Porat",
				"given": "Omer"
			},
			{
				"family": "Shalit",
				"given": "Uri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "srinivasanBiasesGenerativeArt2021",
		"type": "paper-conference",
		"abstract": "With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "41–51",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Biases in generative art: A causal look from the lens of art history",
		"URL": "https://doi.org/10.1145/3442188.3445869",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Ramya"
			},
			{
				"family": "Uchino",
				"given": "Kanji"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "thorntonFiftyShadesGrey2021",
		"type": "paper-conference",
		"abstract": "Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs—easily accessible online environments provisioning access to datasets, analysis and visualisations—are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust—transparency and provenance in particular—appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "64–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fifty shades of grey: In praise of a nuanced approach towards trustworthy design",
		"URL": "https://doi.org/10.1145/3442188.3445871",
		"author": [
			{
				"family": "Thornton",
				"given": "Lauren"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Blair",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chasalowRepresentativenessStatisticsPolitics2021",
		"type": "paper-conference",
		"abstract": "Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "77–89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representativeness in statistics, politics, and machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445872",
		"author": [
			{
				"family": "Chasalow",
				"given": "Kyla"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "benamiDistributiveEffectsRisk2021",
		"type": "paper-conference",
		"abstract": "Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 16\npublisher-place: Virtual Event, Canada",
		"page": "90–105",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The distributive effects of risk prediction in environmental compliance: Algorithmic design, environmental justice, and public policy",
		"URL": "https://doi.org/10.1145/3442188.3445873",
		"author": [
			{
				"family": "Benami",
				"given": "Elinor"
			},
			{
				"family": "Whitaker",
				"given": "Reid"
			},
			{
				"family": "La",
				"given": "Vincent"
			},
			{
				"family": "Lin",
				"given": "Hongjin"
			},
			{
				"family": "Anderson",
				"given": "Brandon R."
			},
			{
				"family": "Ho",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "razGroupFairnessIndependence2021",
		"type": "paper-conference",
		"abstract": "This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "129–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness: Independence revisited",
		"URL": "https://doi.org/10.1145/3442188.3445876",
		"author": [
			{
				"family": "Räz",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "akpinarEffectDifferentialVictim2021",
		"type": "paper-conference",
		"abstract": "Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogotá, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445877",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "838–849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effect of differential victim crime reporting on predictive policing systems",
		"URL": "https://doi.org/10.1145/3442188.3445877",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "canettiSoftClassifiersHard2019",
		"type": "paper-conference",
		"abstract": "A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary \"scoring\" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain \"nice\" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for \"nice\" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287561",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "309–318",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From soft classifiers to hard decisions: How fair can we be?",
		"URL": "https://doi.org/10.1145/3287560.3287561",
		"author": [
			{
				"family": "Canetti",
				"given": "Ran"
			},
			{
				"family": "Cohen",
				"given": "Aloni"
			},
			{
				"family": "Dikkala",
				"given": "Nishanth"
			},
			{
				"family": "Ramnarayan",
				"given": "Govind"
			},
			{
				"family": "Scheffler",
				"given": "Sarah"
			},
			{
				"family": "Smith",
				"given": "Adam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "milliModelReconstructionModel2019",
		"type": "paper-conference",
		"abstract": "We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations.On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive.Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287562",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "1–9",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model reconstruction from model explanations",
		"URL": "https://doi.org/10.1145/3287560.3287562",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Schmidt",
				"given": "Ludwig"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "greenDisparateInteractionsAlgorithmintheloop2019",
		"type": "paper-conference",
		"abstract": "Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions—they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with \"disparate interactions,\" whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new \"algorithm-in-the-loop\" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287563",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "90–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments",
		"URL": "https://doi.org/10.1145/3287560.3287563",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			},
			{
				"family": "Chen",
				"given": "Yiling"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "madrasFairnessCausalAwareness2019",
		"type": "paper-conference",
		"abstract": "How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287564",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "349–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness through causal awareness: Learning causal latent-variable models for biased data",
		"URL": "https://doi.org/10.1145/3287560.3287564",
		"author": [
			{
				"family": "Madras",
				"given": "David"
			},
			{
				"family": "Creager",
				"given": "Elliot"
			},
			{
				"family": "Pitassi",
				"given": "Toniann"
			},
			{
				"family": "Zemel",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jiangWhoGuineaPig2019",
		"type": "paper-conference",
		"abstract": "A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287565",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "201–210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who's the guinea pig? Investigating online A/B/n tests in-the-Wild",
		"URL": "https://doi.org/10.1145/3287560.3287565",
		"author": [
			{
				"family": "Jiang",
				"given": "Shan"
			},
			{
				"family": "Martin",
				"given": "John"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ensignRunawayFeedbackLoops2018",
		"type": "paper-conference",
		"abstract": "Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "160-171",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Runaway Feedback Loops in Predictive Policing",
		"URL": "https://proceedings.mlr.press/v81/ensign18a.html",
		"author": [
			{
				"family": "Ensign",
				"given": "Danielle"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Neville",
				"given": "Scott"
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "burkeBalancedNeighborhoodsMultisided2018",
		"type": "paper-conference",
		"abstract": "Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "202-214",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Balanced Neighborhoods for Multi-sided Fairness in Recommendation",
		"URL": "https://proceedings.mlr.press/v81/burke18a.html",
		"author": [
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Sonboli",
				"given": "Nasim"
			},
			{
				"family": "Ordonez-Gauger",
				"given": "Aldo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "kamishimaRecommendationIndependence2018",
		"type": "paper-conference",
		"abstract": "This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "187-201",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Recommendation Independence",
		"URL": "https://proceedings.mlr.press/v81/kamishima18a.html",
		"author": [
			{
				"family": "Kamishima",
				"given": "Toshihiro"
			},
			{
				"family": "Akaho",
				"given": "Shotaro"
			},
			{
				"family": "Asoh",
				"given": "Hideki"
			},
			{
				"family": "Sakuma",
				"given": "Jun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "ekstrandAllCoolKids2018",
		"type": "paper-conference",
		"abstract": "In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complex—and likely detrimental—interaction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naïve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "172-186",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness",
		"title-short": "All The Cool Kids, How Do They Fit In?",
		"URL": "https://proceedings.mlr.press/v81/ekstrand18b.html",
		"author": [
			{
				"family": "Ekstrand",
				"given": "Michael D."
			},
			{
				"family": "Tian",
				"given": "Mucun"
			},
			{
				"family": "Azpiazu",
				"given": "Ion Madrazo"
			},
			{
				"family": "Ekstrand",
				"given": "Jennifer D."
			},
			{
				"family": "Anuyah",
				"given": "Oghenemaro"
			},
			{
				"family": "McNeill",
				"given": "David"
			},
			{
				"family": "Pera",
				"given": "Maria Soledad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "dworkDecoupledClassifiersGroupFair2018",
		"type": "paper-conference",
		"abstract": "When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "119-133",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Decoupled Classifiers for Group-Fair and Efficient Machine Learning",
		"URL": "https://proceedings.mlr.press/v81/dwork18a.html",
		"author": [
			{
				"family": "Dwork",
				"given": "Cynthia"
			},
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Leiserson",
				"given": "Max"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "binnsFairnessMachineLearning2018",
		"type": "paper-conference",
		"abstract": "What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "149-159",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Fairness in Machine Learning: Lessons from Political Philosophy",
		"title-short": "Fairness in Machine Learning",
		"URL": "https://proceedings.mlr.press/v81/binns18a.html",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "menonCostFairnessBinary2018",
		"type": "paper-conference",
		"abstract": "Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "107-118",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "The cost of fairness in binary classification",
		"URL": "https://proceedings.mlr.press/v81/menon18a.html",
		"author": [
			{
				"family": "Menon",
				"given": "Aditya Krishna"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "duarteMixedMessagesLimits2018",
		"type": "paper-conference",
		"abstract": "Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, “fake news” or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Today’s tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker.  A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service.  This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakers’ knowledge about what is possible with automated text analysis.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "106-106",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Mixed Messages? The Limits of Automated Social Media Content Analysis",
		"title-short": "Mixed Messages?",
		"URL": "https://proceedings.mlr.press/v81/duarte18a.html",
		"author": [
			{
				"family": "Duarte",
				"given": "Natasha"
			},
			{
				"family": "Llanso",
				"given": "Emma"
			},
			{
				"family": "Loup",
				"given": "Anna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "madaanAnalyzeDetectRemove2018",
		"type": "paper-conference",
		"abstract": "The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry (\\it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "92-105",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies",
		"URL": "https://proceedings.mlr.press/v81/madaan18a.html",
		"author": [
			{
				"family": "Madaan",
				"given": "Nishtha"
			},
			{
				"family": "Mehta",
				"given": "Sameep"
			},
			{
				"family": "Agrawaal",
				"given": "Taneea"
			},
			{
				"family": "Malhotra",
				"given": "Vrinda"
			},
			{
				"family": "Aggarwal",
				"given": "Aditi"
			},
			{
				"family": "Gupta",
				"given": "Yatin"
			},
			{
				"family": "Saxena",
				"given": "Mayank"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "buolamwiniGenderShadesIntersectional2018",
		"type": "paper-conference",
		"abstract": "Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "77-91",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification",
		"title-short": "Gender Shades",
		"URL": "https://proceedings.mlr.press/v81/buolamwini18a.html",
		"author": [
			{
				"family": "Buolamwini",
				"given": "Joy"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "barabasInterventionsPredictionsReframing2018",
		"type": "paper-conference",
		"abstract": "Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "62-76",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment",
		"title-short": "Interventions over Predictions",
		"URL": "https://proceedings.mlr.press/v81/barabas18a.html",
		"author": [
			{
				"family": "Barabas",
				"given": "Chelsea"
			},
			{
				"family": "Virza",
				"given": "Madars"
			},
			{
				"family": "Dinakar",
				"given": "Karthik"
			},
			{
				"family": "Ito",
				"given": "Joichi"
			},
			{
				"family": "Zittrain",
				"given": "Jonathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "phillipsInterpretableActiveLearning2018",
		"type": "paper-conference",
		"abstract": "Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. These explanations can also be used to generate batches based on common sources of uncertainty. These regions of common uncertainty can be useful for understanding a model’s current weaknesses.  In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model’s predictions between one subgroup and another.  Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. We also measure how the choice of initial labeled examples effects groups over time.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "49-61",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Interpretable Active Learning",
		"URL": "https://proceedings.mlr.press/v81/phillips18a.html",
		"author": [
			{
				"family": "Phillips",
				"given": "Richard"
			},
			{
				"family": "Chang",
				"given": "Kyu Hyun"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "selbstMeaningfulInformationRight2018",
		"type": "paper-conference",
		"abstract": "There is no single, neat statutory provision labeled the “right to explanation” in Europe’s new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13–15 provide rights to “meaningful information about the logic involved” in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "48-48",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "“Meaningful Information” and the Right to Explanation",
		"URL": "https://proceedings.mlr.press/v81/selbst18a.html",
		"author": [
			{
				"family": "Selbst",
				"given": "Andrew"
			},
			{
				"family": "Powles",
				"given": "Julia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "ekstrandPrivacyAllEnsuring2018",
		"type": "paper-conference",
		"abstract": "In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "35-47",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Privacy for All: Ensuring Fair and Equitable Privacy Protections",
		"title-short": "Privacy for All",
		"URL": "https://proceedings.mlr.press/v81/ekstrand18a.html",
		"author": [
			{
				"family": "Ekstrand",
				"given": "Michael D."
			},
			{
				"family": "Joshaghani",
				"given": "Rezvan"
			},
			{
				"family": "Mehrpouyan",
				"given": "Hoda"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "dattaDiscriminationOnlineAdvertising2018",
		"type": "paper-conference",
		"abstract": "We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "20-34",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Discrimination in Online Advertising: A Multidisciplinary Inquiry",
		"title-short": "Discrimination in Online Advertising",
		"URL": "https://proceedings.mlr.press/v81/datta18a.html",
		"author": [
			{
				"family": "Datta",
				"given": "Amit"
			},
			{
				"family": "Datta",
				"given": "Anupam"
			},
			{
				"family": "Makagon",
				"given": "Jael"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "speicherPotentialDiscriminationOnline2018",
		"type": "paper-conference",
		"abstract": "Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.",
		"container-title": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency",
		"event-title": "Conference on Fairness, Accountability and Transparency",
		"language": "en",
		"note": "ISSN: 2640-3498",
		"page": "5-19",
		"publisher": "PMLR",
		"source": "proceedings.mlr.press",
		"title": "Potential for Discrimination in Online Targeted Advertising",
		"URL": "https://proceedings.mlr.press/v81/speicher18a.html",
		"author": [
			{
				"family": "Speicher",
				"given": "Till"
			},
			{
				"family": "Ali",
				"given": "Muhammad"
			},
			{
				"family": "Venkatadri",
				"given": "Giridhari"
			},
			{
				"family": "Ribeiro",
				"given": "Filipe Nunes"
			},
			{
				"family": "Arvanitakis",
				"given": "George"
			},
			{
				"family": "Benevenuto",
				"given": "Fabrício"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Loiseau",
				"given": "Patrick"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					21
				]
			]
		}
	},
	{
		"id": "chouldechovaCaseStudyAlgorithmassisted2018",
		"type": "paper-conference",
		"abstract": "Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.",
		"collection-title": "Proceedings of Machine Learning Research",
		"container-title": "Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA",
		"page": "134–148",
		"publisher": "PMLR",
		"source": "DBLP Computer Science Bibliography",
		"title": "A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions",
		"URL": "http://proceedings.mlr.press/v81/chouldechova18a.html",
		"volume": "81",
		"author": [
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Prado",
				"given": "Diana Benavides"
			},
			{
				"family": "Fialko",
				"given": "Oleksandr"
			},
			{
				"family": "Vaithianathan",
				"given": "Rhema"
			}
		],
		"editor": [
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "mardaDataNewDelhis2020",
		"type": "paper-conference",
		"abstract": "In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 8\npublisher-place: Barcelona, Spain",
		"page": "317–324",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data in new delhi's predictive policing system",
		"URL": "https://doi.org/10.1145/3351095.3372865",
		"author": [
			{
				"family": "Marda",
				"given": "Vidushi"
			},
			{
				"family": "Narayan",
				"given": "Shivangi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "marcinkowskiImplicationsAIUnfairness2020",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "122–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation",
		"URL": "https://doi.org/10.1145/3351095.3372867",
		"author": [
			{
				"family": "Marcinkowski",
				"given": "Frank"
			},
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Starke",
				"given": "Christopher"
			},
			{
				"family": "Lünich",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "geigerGarbageGarbageOut2020",
		"type": "paper-conference",
		"abstract": "Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing — specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data — give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a \"gold standard\" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372862",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "325–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from?",
		"URL": "https://doi.org/10.1145/3351095.3372862",
		"author": [
			{
				"family": "Geiger",
				"given": "R. Stuart"
			},
			{
				"family": "Yu",
				"given": "Kevin"
			},
			{
				"family": "Yang",
				"given": "Yanlai"
			},
			{
				"family": "Dai",
				"given": "Mindy"
			},
			{
				"family": "Qiu",
				"given": "Jie"
			},
			{
				"family": "Tang",
				"given": "Rebekah"
			},
			{
				"family": "Huang",
				"given": "Jenny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "binnsApparentConflictIndividual2020",
		"type": "paper-conference",
		"abstract": "A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "514–524",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the apparent conflict between individual and group fairness",
		"URL": "https://doi.org/10.1145/3351095.3372864",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sokolExplainabilityFactSheets2020",
		"type": "paper-conference",
		"abstract": "Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "56–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability fact sheets: a framework for systematic assessment of explainable approaches",
		"URL": "https://doi.org/10.1145/3351095.3372870",
		"author": [
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Flach",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rajiClosingAIAccountability2020a",
		"type": "paper-conference",
		"abstract": "Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "33–44",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing",
		"URL": "https://doi.org/10.1145/3351095.3372873",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "White",
				"given": "Rebecca N."
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Smith-Loud",
				"given": "Jamila"
			},
			{
				"family": "Theron",
				"given": "Daniel"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kaminskiMultilayeredExplanationsAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability—individual rights and systemic governance— and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "68–79",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-layered explanations from algorithmic impact assessments in the GDPR",
		"URL": "https://doi.org/10.1145/3351095.3372875",
		"author": [
			{
				"family": "Kaminski",
				"given": "Margot E."
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "venkatasubramanianPhilosophicalBasisAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse.We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "284–293",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The philosophical basis of algorithmic recourse",
		"URL": "https://doi.org/10.1145/3351095.3372876",
		"author": [
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Alfano",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "greenFalsePromiseRisk2020",
		"type": "paper-conference",
		"abstract": "Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an \"epistemic reform,\" the path forward for criminal justice reform. I reinterpret recent results regarding the \"impossibility of fairness\" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how \"fair\" algorithms can reinforce discrimination.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "594–606",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The false promise of risk assessments: epistemic reform and the limits of fairness",
		"URL": "https://doi.org/10.1145/3351095.3372869",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "terzisOnwardFreedomOthers2020",
		"type": "paper-conference",
		"abstract": "The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "220–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Onward for the freedom of others: marching beyond the AI ethics",
		"URL": "https://doi.org/10.1145/3351095.3373152",
		"author": [
			{
				"family": "Terzis",
				"given": "Petros"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kulynychPOTsProtectiveOptimization2020",
		"type": "paper-conference",
		"abstract": "Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372853",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "177–188",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "POTs: protective optimization technologies",
		"URL": "https://doi.org/10.1145/3351095.3372853",
		"author": [
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Overdorf",
				"given": "Rebekah"
			},
			{
				"family": "Troncoso",
				"given": "Carmela"
			},
			{
				"family": "Gürses",
				"given": "Seda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wagnerRegulatingTransparencyFacebook2020",
		"type": "paper-conference",
		"abstract": "Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns — design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG.This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372856",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "261–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating transparency? Facebook, twitter and the german network enforcement act",
		"URL": "https://doi.org/10.1145/3351095.3372856",
		"author": [
			{
				"family": "Wagner",
				"given": "Ben"
			},
			{
				"family": "Rozgonyi",
				"given": "Krisztina"
			},
			{
				"family": "Sekwenz",
				"given": "Marie-Therese"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "huFairClassificationSocial2020",
		"type": "paper-conference",
		"abstract": "Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of \"fairness-to-welfare\" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring \"more fair\" classifiers does not abide by the Pareto Principle—a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372857",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "535–545",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair classification and social welfare",
		"URL": "https://doi.org/10.1145/3351095.3372857",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Chen",
				"given": "Yiling"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mothilalExplainingMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372850",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "607–617",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explaining machine learning classifiers through diverse counterfactual explanations",
		"URL": "https://doi.org/10.1145/3351095.3372850",
		"author": [
			{
				"family": "Mothilal",
				"given": "Ramaravind K."
			},
			{
				"family": "Sharma",
				"given": "Amit"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "costonCounterfactualRiskAssessments2020",
		"type": "paper-conference",
		"abstract": "Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may—and as we show empirically, do—induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372851",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "582–593",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual risk assessments, evaluation, and fairness",
		"URL": "https://doi.org/10.1145/3351095.3372851",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barabasStudyingReorientingStudy2020",
		"type": "paper-conference",
		"abstract": "Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of \"studying up\". We reflect on the contributions that the call to \"study up\" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation \"upward\". A case study from our own work illustrates what it looks like to reorient one's research questions \"up\" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that \"study up\". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372859",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "167–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Studying up: reorienting the study of algorithmic fairness around issues of power",
		"URL": "https://doi.org/10.1145/3351095.3372859",
		"author": [
			{
				"family": "Barabas",
				"given": "Chelsea"
			},
			{
				"family": "Doyle",
				"given": "Colin"
			},
			{
				"family": "Rubinovitz",
				"given": "JB"
			},
			{
				"family": "Dinakar",
				"given": "Karthik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rodolfaCaseStudyPredictive2020",
		"type": "paper-conference",
		"abstract": "The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372863",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "142–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions",
		"URL": "https://doi.org/10.1145/3351095.3372863",
		"author": [
			{
				"family": "Rodolfa",
				"given": "Kit T."
			},
			{
				"family": "Salomon",
				"given": "Erika"
			},
			{
				"family": "Haynes",
				"given": "Lauren"
			},
			{
				"family": "Mendieta",
				"given": "Iván Higuera"
			},
			{
				"family": "Larson",
				"given": "Jamie"
			},
			{
				"family": "Ghani",
				"given": "Rayid"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "biettiEthicsWashingEthics2020",
		"type": "paper-conference",
		"abstract": "The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, \"ethics\" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called \"ethics washing\" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in \"ethics bashing.\" This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and \"ethics\" are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere \"ivory tower\" intellectualization of complex problems that need to be dealt with in practice.This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of \"ethics washing,\" or by scholars and policy-makers in the form of \"ethics bashing.\" Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372860",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "210–219",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy",
		"URL": "https://doi.org/10.1145/3351095.3372860",
		"author": [
			{
				"family": "Bietti",
				"given": "Elettra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "deanRecommendationsUserAgency2020",
		"type": "paper-conference",
		"abstract": "Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "436–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Recommendations and user agency: the reachability of collaboratively-filtered information",
		"URL": "https://doi.org/10.1145/3351095.3372866",
		"author": [
			{
				"family": "Dean",
				"given": "Sarah"
			},
			{
				"family": "Rich",
				"given": "Sarah"
			},
			{
				"family": "Recht",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "malgieriConceptFairnessGDPR2020",
		"type": "paper-conference",
		"abstract": "There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world \"fair\" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of \"Treu und Glaube\") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of \"bona fide\".Taking into account both the value of \"bona fide\" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of \"vulnerability\". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "154–166",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The concept of fairness in the GDPR: a linguistic and contextual interpretation",
		"URL": "https://doi.org/10.1145/3351095.3372868",
		"author": [
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "damourFairnessNotStatic2020",
		"type": "paper-conference",
		"abstract": "As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372878",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "525–534",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness is not static: deeper understanding of long term fairness via simulation studies",
		"URL": "https://doi.org/10.1145/3351095.3372878",
		"author": [
			{
				"family": "D'Amour",
				"given": "Alexander"
			},
			{
				"family": "Srinivasan",
				"given": "Hansa"
			},
			{
				"family": "Atwood",
				"given": "James"
			},
			{
				"family": "Baljekar",
				"given": "Pallavi"
			},
			{
				"family": "Sculley",
				"given": "D."
			},
			{
				"family": "Halpern",
				"given": "Yoni"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "abebeRolesComputingSocial2020",
		"type": "paper-conference",
		"abstract": "A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined — changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "252–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Roles for computing in social change",
		"URL": "https://doi.org/10.1145/3351095.3372871",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Robinson",
				"given": "David G."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pujolFairDecisionMaking2020",
		"type": "paper-conference",
		"abstract": "Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "189–199",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair decision making using privacy-protected data",
		"URL": "https://doi.org/10.1145/3351095.3372872",
		"author": [
			{
				"family": "Pujol",
				"given": "David"
			},
			{
				"family": "McKenna",
				"given": "Ryan"
			},
			{
				"family": "Kuppam",
				"given": "Satya"
			},
			{
				"family": "Hay",
				"given": "Michael"
			},
			{
				"family": "Machanavajjhala",
				"given": "Ashwin"
			},
			{
				"family": "Miklau",
				"given": "Gerome"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "katellSituatedInterventionsAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is \"scalable\" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "45–55",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward situated interventions for algorithmic equity: lessons from the field",
		"URL": "https://doi.org/10.1145/3351095.3372874",
		"author": [
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Krafft",
				"given": "P. M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bogenAwarenessPracticeTensions2020",
		"type": "paper-conference",
		"abstract": "Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372877",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "492–500",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination",
		"URL": "https://doi.org/10.1145/3351095.3372877",
		"author": [
			{
				"family": "Bogen",
				"given": "Miranda"
			},
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Ahmed",
				"given": "Shazeda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ribeiroAuditingRadicalizationPathways2020",
		"type": "paper-conference",
		"abstract": "Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372879",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "131–141",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing radicalization pathways on YouTube",
		"URL": "https://doi.org/10.1145/3351095.3372879",
		"author": [
			{
				"family": "Ribeiro",
				"given": "Manoel Horta"
			},
			{
				"family": "Ottoni",
				"given": "Raphael"
			},
			{
				"family": "West",
				"given": "Robert"
			},
			{
				"family": "Almeida",
				"given": "Virgílio A. F."
			},
			{
				"family": "Meira",
				"given": "Wagner"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kangAlgorithmicAccountabilityPublic2020",
		"type": "paper-conference",
		"abstract": "The EU General Data Protection Regulation (\"GDPR\") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration.Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation.The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "32",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic accountability in public administration: the GDPR paradox",
		"URL": "https://doi.org/10.1145/3351095.3373153",
		"author": [
			{
				"family": "Kang",
				"given": "Sunny Seon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "castelleSocialLivesGenerative2020",
		"type": "paper-conference",
		"abstract": "Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus—a \"durably installed generative principle of regulated improvisations\"—that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill \"deeply interiorized master patterns\" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because \"sometimes we don't follow the rules... language is full of exceptions to the rules\"; and in the case of Bourdieu, the habitus was an answer to a long-standing question: \"how can behaviour be regulated without being the product of obedience to rules?\" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations—or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a \"two-player minimax game with value function V(G,D)\", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, \"the degree zero of sociology\", by which he means an isolated, inert, and amodal—and therefore not particularly sociological—starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and \"selling out\" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the \"value functions\" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The social lives of generative adversarial networks",
		"URL": "https://doi.org/10.1145/3351095.3373156",
		"author": [
			{
				"family": "Castelle",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dotanValueladenDisciplinaryShifts2020",
		"type": "paper-conference",
		"abstract": "As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required.This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value-laden disciplinary shifts in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3373157",
		"author": [
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Milli",
				"given": "Smitha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bhattExplainableMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375624",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "648–657",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable machine learning in deployment",
		"URL": "https://doi.org/10.1145/3351095.3375624",
		"author": [
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			},
			{
				"family": "Jia",
				"given": "Yunhan"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			},
			{
				"family": "Puri",
				"given": "Ruchir"
			},
			{
				"family": "Moura",
				"given": "José M. F."
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cathLeapFATEHuman2020",
		"type": "paper-conference",
		"abstract": "The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375665",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "702",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leap of FATE: human rights as a complementary framework for AI policy and practice",
		"URL": "https://doi.org/10.1145/3351095.3375665",
		"author": [
			{
				"family": "Cath",
				"given": "Corinne"
			},
			{
				"family": "Latonero",
				"given": "Mark"
			},
			{
				"family": "Marda",
				"given": "Vidushi"
			},
			{
				"family": "Pakzad",
				"given": "Roya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kaeser-chenPositionalityawareMachineLearning2020",
		"type": "paper-conference",
		"abstract": "Positionality is a person's unique and always partial view of the world which is shaped by social and political contexts. Machine Learning (ML) systems have positionality, too, as a consequence of the choices we make when we develop ML systems. Being positionality-aware is key for ML practitioners to acknowledge and embrace the necessary choices embedded in ML by its creators.When groups form a shared view of the world, or group positionality, they have the power to embed and institutionalize their unique perspectives in artifacts such as standards and ontologies. For example, the international standard for reporting diseases and health conditions (International Classification of Diseases, ICD) is shaped by a distinctly medical, European and North American perspective. It dictates how we collect data, and limits what questions we can ask of data and what ML systems we can develop. Researchers struggle to study the effects of social factors on health outcomes because of what the ICD renders legible (usually in medicalized terms) and what it renders invisible (usually social contexts) in data. The ICD, as with all information infrastructures, promotes and propagates the perspective(s) of its creators. Over time, it establishes what counts as \"truth\".Positionality, and how it embeds itself in standards, ontologies, and data collection, is the root for bias in our data and algorithms. Every perspective has its limits - there is no view from nowhere. Without an awareness of positionality, the current debate on bias in machine learning is quite limited: adding more data to the set cannot remove bias. Instead, we propose positionality-aware ML, a new workflow focused on continuous evaluation and improvement of the fit between the positionality embedded in ML systems and the scenarios within which it is deployed.To demonstrate how to uncover positionality in standards, ontologies, data, and ML systems, we discuss recent work on online harassment of Canadian journalists and politicians on Twitter. Using legal definitions of hate speech and harassment, Twitter's community standards, and insight from interviews with journalists and politicians, we created standards and annotation guidelines for labeling the intensity of harassment in tweets. We then hand labeled a sample of data and through this process identified instances where positionality impacts choices about how many categories of harassment should exist, how to label boundary cases, and how to interpret messy data. We take three perspectives—technical, systems, socio-technical—that when combined illuminate areas of tension which serve as a signal of misalignment between the positionality embedded in the ML system and the deployment context. We demonstrate how the concept of positionality allows us to delineate sets of use cases that may not be suited for automated, ML solutions. Finally, we discuss strategies for developing positionality-aware ML systems, which embed a positionality appropriate for the application context, and continuously evolve to maintain this contextual fit, with an emphasis on the need for of democratic, egalitarian dialogues between knowledge-producing groups.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375666",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Positionality-aware machine learning: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375666",
		"author": [
			{
				"family": "Kaeser-Chen",
				"given": "Christine"
			},
			{
				"family": "Dubois",
				"given": "Elizabeth"
			},
			{
				"family": "Schüür",
				"given": "Friederike"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "duartePolicy101Introduction2020",
		"type": "paper-conference",
		"abstract": "Navigating the rules, processes, and venues through which public policy is made can seem daunting. But public participation in these processes is a crucial part of democratic governance. With a general understanding of when, where, and how to engage in policymaking, anyone can become a policy advocate. This tutorial will introduce some of the most common US (federal and state) and EU policymaking processes and provide guidance to experts in other domains (such as data and computer science) who want to get involved in policymaking. We will discuss the practical considerations involved in identifying and choosing among policymaking opportunities and discuss how to maximize the impact of policymaking interventions. This tutorial is intended to be interactive and will be improved by audience participation and questions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375668",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "703",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Policy 101: an introduction to public policymaking in the EU and US",
		"URL": "https://doi.org/10.1145/3351095.3375668",
		"author": [
			{
				"family": "Duarte",
				"given": "Natasha"
			},
			{
				"family": "Adams",
				"given": "Stan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "senTotalSurveyError2020",
		"type": "paper-conference",
		"abstract": "The digital traces of hundreds of millions of people offer increasingly comprehensive pictures of both individuals and groups on different platforms, but also allow inferences about broader target populations beyond those platforms. Studying the errors that can occur when digital traces are used to learn about humans and social phenomena is essential. Many similar errors also affect survey estimates, which survey designers have been addressing for decades, most notably using the Total Survey Error Framework (TSE). In this tutorial, we first introduce the audience to the concepts and guidelines of the TSE and how they are applied by survey practitioners in the social sciences. Second, we introduce our own conceptual framework to diagnose, understand, and avoid errors that may occur in studies that are based on digital traces of humans.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375669",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "701",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From the total survey error framework to an error framework for digital traces of humans: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375669",
		"author": [
			{
				"family": "Sen",
				"given": "Indira"
			},
			{
				"family": "Flöck",
				"given": "Fabian"
			},
			{
				"family": "Weller",
				"given": "Katrin"
			},
			{
				"family": "Weiß",
				"given": "Bernd"
			},
			{
				"family": "Wagner",
				"given": "Claudia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jacobsMeaningMeasurementBias2020",
		"type": "paper-conference",
		"abstract": "The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different—and occasionally incomparable—proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring \"bias.\" We show that this framework helps to clarify the way unobservable theoretical constructs—such as \"creditworthiness,\" \"risk to society,\" or \"tweet toxicity\"—are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining \"bias\" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring \"bias.\" In so doing, we illustrate the limits of current \"debiasing\" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375671",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "706",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The meaning and measurement of bias: lessons from natural language processing",
		"URL": "https://doi.org/10.1145/3351095.3375671",
		"author": [
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Daumé",
				"given": "Hal"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wexlerProbingMLModels2020",
		"type": "paper-conference",
		"abstract": "As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375662",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "705",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Probing ML models for fairness with the what-if tool and SHAP: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375662",
		"author": [
			{
				"family": "Wexler",
				"given": "James"
			},
			{
				"family": "Pushkarna",
				"given": "Mahima"
			},
			{
				"family": "Robinson",
				"given": "Sara"
			},
			{
				"family": "Bolukbasi",
				"given": "Tolga"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ganeshTwoComputerScientists2020",
		"type": "paper-conference",
		"abstract": "In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375663",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "707",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Two computer scientists and a cultural scientist get hit by a driver-less car: a method for situating knowledge in the cross-disciplinary study of F-A-T in machine learning: translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375663",
		"author": [
			{
				"family": "Ganesh",
				"given": "Maya Indira"
			},
			{
				"family": "Dechesne",
				"given": "Francien"
			},
			{
				"family": "Waseem",
				"given": "Zeerak"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "gadeExplainableAIIndustry2020",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375664",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "699",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI in industry: practical challenges and lessons learned: implications tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375664",
		"author": [
			{
				"family": "Gade",
				"given": "Krishna"
			},
			{
				"family": "Geyik",
				"given": "Sahin Cem"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Mithal",
				"given": "Varun"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "aryaAIExplainability3602020",
		"type": "paper-conference",
		"abstract": "This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models.Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space.Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below).In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general.Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks.Here is a rough agenda of the tutorial:1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms.2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers).3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications.4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions.5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks.6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit.7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375667",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "696",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI explainability 360: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375667",
		"author": [
			{
				"family": "Arya",
				"given": "Vijay"
			},
			{
				"family": "Bellamy",
				"given": "Rachel K. E."
			},
			{
				"family": "Chen",
				"given": "Pin-Yu"
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Hind",
				"given": "Michael"
			},
			{
				"family": "Hoffman",
				"given": "Samuel C."
			},
			{
				"family": "Houde",
				"given": "Stephanie"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Luss",
				"given": "Ronny"
			},
			{
				"family": "Mojsilović",
				"given": "Aleksandra"
			},
			{
				"family": "Mourad",
				"given": "Sami"
			},
			{
				"family": "Pedemonte",
				"given": "Pablo"
			},
			{
				"family": "Raghavendra",
				"given": "Ramya"
			},
			{
				"family": "Richards",
				"given": "John"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Shanmugam",
				"given": "Karthikeyan"
			},
			{
				"family": "Singh",
				"given": "Moninder"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "burkeExperimentationFairnessawareRecommendation2020",
		"type": "paper-conference",
		"abstract": "The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375670",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Experimentation with fairness-aware recommendation using librec-auto: hands-on tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375670",
		"author": [
			{
				"family": "Burke",
				"given": "Robin Douglas"
			},
			{
				"family": "Mansoury",
				"given": "Masoud"
			},
			{
				"family": "Sonboli",
				"given": "Nasim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "oswaldCanAlgorithmicSystem2020",
		"type": "paper-conference",
		"abstract": "This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375673",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "698",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can an algorithmic system be a 'friend' to a police officer's discretion? ACM FAT 2020 translation tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375673",
		"author": [
			{
				"family": "Oswald",
				"given": "Marion"
			},
			{
				"family": "Powell",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "huWhatsSexGot2020",
		"type": "paper-conference",
		"abstract": "The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group \"female\" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the \"effects\" that sex purportedly \"causes\" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375674",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "513",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's sex got to do with machine learning?",
		"URL": "https://doi.org/10.1145/3351095.3375674",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Kohler-Hausmann",
				"given": "Issa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "givensCenteringDisabilityPerspectives2020",
		"type": "paper-conference",
		"abstract": "It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in \"anonymized\" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the \"long tail\" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt \"reasonable accommodations\" for qualified individuals with a disability. But what is a \"reasonable accommodation\" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375686",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "684",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Centering disability perspectives in algorithmic fairness, accountability, &amp; transparency",
		"URL": "https://doi.org/10.1145/3351095.3375686",
		"author": [
			{
				"family": "Givens",
				"given": "Alexandra Reeve"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "bakerAlgorithmicallyEncodedIdentities2020",
		"type": "paper-conference",
		"abstract": "Our aim with this workshop is to provide a venue within which the FAT* community can thoughtfully engage with identity and the categories which are imposed on people as part of making sense of their identities. Most people have nuanced and deeply personal understandings of what identity categories mean to them; however, sociotechnical systems must, through a set of classification decisions, reduce the nuance and complexity of those identities into discrete categories. The impact of misclassifications can range from the uncomfortable (e.g. displaying ads for items that aren't desirable) to devastating (e.g. being denied medical care; being evaluated as having a high risk of criminal recidivism). However, even the act of being classified can force an individual into categories which feel foreign and othering. Through this workshop, we hope to connect participants' personal understandings of identity to how identity is 'seen' and categorized by sociotechnical systems.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375687",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "681",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmically encoded identities: reframing human classification",
		"URL": "https://doi.org/10.1145/3351095.3375687",
		"author": [
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rakovaAssessingIntersectionOrganizational2020",
		"type": "paper-conference",
		"abstract": "The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375672",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "697",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing the intersection of organizational structure and FAT* efforts within industry: implications tutorial",
		"URL": "https://doi.org/10.1145/3351095.3375672",
		"author": [
			{
				"family": "Rakova",
				"given": "Bogdana"
			},
			{
				"family": "Chowdhury",
				"given": "Rumman"
			},
			{
				"family": "Yang",
				"given": "Jingying"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "baxterBridgingGapAI2020",
		"type": "paper-conference",
		"abstract": "The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375680",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bridging the gap from AI ethics research to practice",
		"URL": "https://doi.org/10.1145/3351095.3375680",
		"author": [
			{
				"family": "Baxter",
				"given": "Kathy"
			},
			{
				"family": "Schlesinger",
				"given": "Yoav"
			},
			{
				"family": "Aerni",
				"given": "Sarah"
			},
			{
				"family": "Baker",
				"given": "Lewis"
			},
			{
				"family": "Dawson",
				"given": "Julie"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Kloumann",
				"given": "Isabel"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "gossManifestingSociotechnicalExperimenting2020",
		"type": "paper-conference",
		"abstract": "Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that \"[fairness work] require[s] technical researchers to learn new skills or partner with social scientists\" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That \"social context\" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375682",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "693",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Manifesting the sociotechnical: experimenting with methods for social context and social justice",
		"URL": "https://doi.org/10.1145/3351095.3375682",
		"author": [
			{
				"family": "Goss",
				"given": "Ezra"
			},
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Sabin",
				"given": "Manuel"
			},
			{
				"family": "Teeple",
				"given": "Stephanie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "szymielewiczWhereAlgorithmicAccountability2020",
		"type": "paper-conference",
		"abstract": "This hands-on session takes academic concepts and their formulation in policy initiatives around algorithmic accountability and explainability and tests them against real cases. In small groups we will (1) test selected frameworks on algorithmic accountability and explainability against a concrete case study (that likely constitutes a human rights violation) and (2) test different formats to explain important aspects of an automated decision-making process (such as input data, type of an algorithm used, design decisions and technical parameters, expected outcomes) to various audiences (end users, affected communities, watchdog organisations, public sector agencies and regulators). We invite participants with various backgrounds: researchers, technologists, human rights advocates, public servants and designers.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375683",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "689",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Where do algorithmic accountability and explainability frameworks take us in the real world? from theory to practice",
		"URL": "https://doi.org/10.1145/3351095.3375683",
		"author": [
			{
				"family": "Szymielewicz",
				"given": "Katarzyna"
			},
			{
				"family": "Bacciarelli",
				"given": "Anna"
			},
			{
				"family": "Hidvegi",
				"given": "Fanny"
			},
			{
				"family": "Foryciarz",
				"given": "Agata"
			},
			{
				"family": "Pénicaud",
				"given": "Soizic"
			},
			{
				"family": "Spielkamp",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barryEthicsGroundPrinciples2020",
		"type": "paper-conference",
		"abstract": "Surveys of public attitudes show that people believe it is possible to design ethical AI. However the everyday professional development context can offer minimal space for ethical reflection or oversight, creating a significant gap between public expectations and the performance of ethics in practice. This 2-part workshop includes an offsite visit to Telefónica Innovation Alpha and uses storytelling and theatre methods to examine how and where ethical reflection happens on the ground. It will explore the gaps in expectations and identify alternative approaches to more effective ethical performance. Bringing social scientists, data scientists, designers, civic rights activists and ethics consultants together to focus on AI/ML in the health context, it will foster critical and creative activities that will bring to the surface the structural, disciplinary, social and epistemological challenges to effective ethical performance in practice. Participants will explore and enact where, when and how meaningful interventions can happen.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375684",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "688",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics on the ground: from principles to practice",
		"URL": "https://doi.org/10.1145/3351095.3375684",
		"author": [
			{
				"family": "Barry",
				"given": "Marguerite"
			},
			{
				"family": "Kerr",
				"given": "Aphra"
			},
			{
				"family": "Smith",
				"given": "Oliver"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wanLostTranslationInteractive2020",
		"type": "paper-conference",
		"abstract": "There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a \"marker for truth\" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375685",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "692",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lost in translation: an interactive workshop mapping interdisciplinary translations for epistemic justice",
		"URL": "https://doi.org/10.1145/3351095.3375685",
		"author": [
			{
				"family": "Wan",
				"given": "Evelyn"
			},
			{
				"family": "Groot",
				"given": "Aviva",
				"non-dropping-particle": "de"
			},
			{
				"family": "Jameson",
				"given": "Shazade"
			},
			{
				"family": "Păun",
				"given": "Mara"
			},
			{
				"family": "Lücking",
				"given": "Phillip"
			},
			{
				"family": "Klumbyte",
				"given": "Goda"
			},
			{
				"family": "Lämmerhirt",
				"given": "Danny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ahmadFairnessAccountabilityTransparency2020",
		"type": "paper-conference",
		"abstract": "The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375690",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, accountability, transparency in AI at scale: lessons from national programs",
		"URL": "https://doi.org/10.1145/3351095.3375690",
		"author": [
			{
				"family": "Ahmad",
				"given": "Muhammad Aurangzeb"
			},
			{
				"family": "Teredesai",
				"given": "Ankur"
			},
			{
				"family": "Eckert",
				"given": "Carly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barocasWhenNotDesign2020",
		"type": "paper-conference",
		"abstract": "Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375691",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "695",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When not to design, build, or deploy",
		"URL": "https://doi.org/10.1145/3351095.3375691",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Biega",
				"given": "Asia J."
			},
			{
				"family": "Fish",
				"given": "Benjamin"
			},
			{
				"family": "Niklas",
				"given": "Jundefineddrzej"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "allhutterDeconstructingFATUsing2020",
		"type": "paper-conference",
		"abstract": "Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas—who gets to speak and to define bias, (un)fairness, and discrimination? \"Deconstructing FAT\" is a CRAFT workshop that aims at complicating this question by asking how \"we\" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375688",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "687",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deconstructing FAT: using memories to collectively explore implicit assumptions, values and context in practices of debiasing and discrimination-awareness",
		"URL": "https://doi.org/10.1145/3351095.3375688",
		"author": [
			{
				"family": "Allhutter",
				"given": "Doris"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sassamanCreatingCommunitybasedTech2020",
		"type": "paper-conference",
		"abstract": "What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375689",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "685",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Creating community-based tech policy: case studies, lessons learned, and what technologists and communities can do together",
		"URL": "https://doi.org/10.1145/3351095.3375689",
		"author": [
			{
				"family": "Sassaman",
				"given": "Hannah"
			},
			{
				"family": "Lee",
				"given": "Jennifer"
			},
			{
				"family": "Irvine",
				"given": "Jenessa"
			},
			{
				"family": "Narayan",
				"given": "Shankar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hannaCtrlZAIZineFair2020",
		"type": "paper-conference",
		"abstract": "The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375692",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CtrlZ.AI zine fair: critical perspectives",
		"URL": "https://doi.org/10.1145/3351095.3375692",
		"author": [
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "williamsHardwiringDiscriminatoryPolice2020",
		"type": "paper-conference",
		"abstract": "We, as activists, as anti-racist organisations, and as racialised communities in Europe, know too well what it means to be over-policed and under-protected. Still, in 2019, we feel the need to evidence racial profiling, to contest narratives placing us as a threat to ‘security’ and essentially to unsettle presumptions as to our criminality. We are still mastering the techniques with which we contest over-policing, brutality and racial profiling. We must now contend with another challenge. When law enforcement resorts to new technology to aid their practice, we find ourselves at further risk. Not only must we consider our physical safety in our relations with the authorities, we also need to be informed about the security of our data. The use of systems to profile, to surveil and to provide a logic to discrimination is not new. What is new is the sense of neutrality afforded to data-driven policing. This report opens a conversation between all those invested in anti-racism, data privacy and non-discrimination in general. It is crucial that we use our collective knowledge to resist.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375695",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "691",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Hardwiring discriminatory police practices: the implications of data-driven technological policing on minority (ethnic and religious) people and communities",
		"URL": "https://doi.org/10.1145/3351095.3375695",
		"author": [
			{
				"family": "Williams",
				"given": "Patrick"
			},
			{
				"family": "Kind",
				"given": "Eric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kallusAssessingAlgorithmicFairness2020",
		"type": "paper-conference",
		"abstract": "The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "110",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing algorithmic fairness with unobserved protected class using data combination",
		"URL": "https://doi.org/10.1145/3351095.3373154",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Mao",
				"given": "Xiaojie"
			},
			{
				"family": "Zhou",
				"given": "Angela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kimPreferenceinformedFairness2020",
		"type": "paper-conference",
		"abstract": "In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3373155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "546",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preference-informed fairness",
		"URL": "https://doi.org/10.1145/3351095.3373155",
		"author": [
			{
				"family": "Kim",
				"given": "Michael P."
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			},
			{
				"family": "Rothblum",
				"given": "Guy N."
			},
			{
				"family": "Yona",
				"given": "Gal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "singhModelAgnosticInterpretability2020",
		"type": "paper-conference",
		"abstract": "A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models.We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "618–628",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model agnostic interpretability of rankers via intent modelling",
		"URL": "https://doi.org/10.1145/3351095.3375234",
		"author": [
			{
				"family": "Singh",
				"given": "Jaspreet"
			},
			{
				"family": "Anand",
				"given": "Avishek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liangArtificialMentalPhenomena2020",
		"type": "paper-conference",
		"abstract": "Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology—meant to relate quantities from the real world (i.e., \"Physics\") into subjective measures in the mind (i.e., \"Psyche\")—to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375623",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "403–412",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models",
		"URL": "https://doi.org/10.1145/3351095.3375623",
		"author": [
			{
				"family": "Liang",
				"given": "Lizhen"
			},
			{
				"family": "Acuna",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pritchardBurnDreamReboot2020",
		"type": "paper-conference",
		"abstract": "Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a \"future otherwise\". We argue that \"speculation\" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes \"how did their dreams make rooms to dream in\"... or not, in the case of coercive practices of computing. And \"what if she changes her dream?\" What if we reboot this dream?1",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375697",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "683",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Burn, dream and reboot! speculating backwards for the missing archive on non-coercive computing",
		"URL": "https://doi.org/10.1145/3351095.3375697",
		"author": [
			{
				"family": "Pritchard",
				"given": "Helen"
			},
			{
				"family": "Snodgrass",
				"given": "Eric"
			},
			{
				"family": "Morrison",
				"given": "Romi Ron"
			},
			{
				"family": "Britton",
				"given": "Loren"
			},
			{
				"family": "Moll",
				"given": "Joana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "noriega-camperoAlgorithmicTargetingSocial2020",
		"type": "paper-conference",
		"abstract": "Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them—and who is not—are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375784",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "241–251",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic targeting of social policies: fairness, accuracy, and distributed governance",
		"URL": "https://doi.org/10.1145/3351095.3375784",
		"author": [
			{
				"family": "Noriega-Campero",
				"given": "Alejandro"
			},
			{
				"family": "Garcia-Bulle",
				"given": "Bernardo"
			},
			{
				"family": "Cantu",
				"given": "Luis Fernando"
			},
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Tejerina",
				"given": "Luis"
			},
			{
				"family": "Pentland",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "yangFairerDatasetsFiltering2020",
		"type": "paper-conference",
		"abstract": "Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375709",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "547–558",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy",
		"URL": "https://doi.org/10.1145/3351095.3375709",
		"author": [
			{
				"family": "Yang",
				"given": "Kaiyu"
			},
			{
				"family": "Qinami",
				"given": "Klint"
			},
			{
				"family": "Fei-Fei",
				"given": "Li"
			},
			{
				"family": "Deng",
				"given": "Jia"
			},
			{
				"family": "Russakovsky",
				"given": "Olga"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nasrBiddingStrategiesGender2020",
		"type": "paper-conference",
		"abstract": "Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3375783",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "337–347",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bidding strategies with gender nondiscrimination constraints for online ad auctions",
		"URL": "https://doi.org/10.1145/3351095.3375783",
		"author": [
			{
				"family": "Nasr",
				"given": "Milad"
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "babaeiAnalyzingBiasesPerception2019",
		"type": "paper-conference",
		"abstract": "Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check.However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories – (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users.The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion.On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking.In summary, we make the following contributions in this work.1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story.2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions.3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth.Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287581",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "139",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing biases in perception of truth in news stories and their implications for fact checking",
		"URL": "https://doi.org/10.1145/3287560.3287581",
		"author": [
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Kulshrestha",
				"given": "Juhi"
			},
			{
				"family": "Redmiles",
				"given": "Elissa M."
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "albarghouthiFairnessawareProgramming2019",
		"type": "paper-conference",
		"abstract": "Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287588",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "211–219",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness-aware programming",
		"URL": "https://doi.org/10.1145/3287560.3287588",
		"author": [
			{
				"family": "Albarghouthi",
				"given": "Aws"
			},
			{
				"family": "Vinitsky",
				"given": "Samuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "babaioffFairAllocationCompetitive2019",
		"type": "paper-conference",
		"abstract": "Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a \"fair\" allocation of the items among them?Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to \"purchase\" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency – prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible.We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287582",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair allocation through competitive equilibrium from generic incomes",
		"URL": "https://doi.org/10.1145/3287560.3287582",
		"author": [
			{
				"family": "Babaioff",
				"given": "Moshe"
			},
			{
				"family": "Nisan",
				"given": "Noam"
			},
			{
				"family": "Talgam-Cohen",
				"given": "Inbal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "celisControllingPolarizationPersonalization2019",
		"type": "paper-conference",
		"abstract": "Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but – subject to these constraints – will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287601",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "160–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Controlling polarization in personalization: An algorithmic framework",
		"URL": "https://doi.org/10.1145/3287560.3287601",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Salehi",
				"given": "Farnood"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "benthallRacialCategoriesMachine2019",
		"type": "paper-conference",
		"abstract": "Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled \"Black\" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287575",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "289–298",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial categories in machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287575",
		"author": [
			{
				"family": "Benthall",
				"given": "Sebastian"
			},
			{
				"family": "Haynes",
				"given": "Bruce D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "bountouridisSIRENSimulationFramework2019",
		"type": "paper-conference",
		"abstract": "The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as \"Matthew effects\", \"filter bubbles\", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287583",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "150–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SIREN: A simulation framework for understanding the effects of recommender systems in online news environments",
		"URL": "https://doi.org/10.1145/3287560.3287583",
		"author": [
			{
				"family": "Bountouridis",
				"given": "Dimitrios"
			},
			{
				"family": "Harambam",
				"given": "Jaron"
			},
			{
				"family": "Makhortykh",
				"given": "Mykola"
			},
			{
				"family": "Marrero",
				"given": "Mónica"
			},
			{
				"family": "Tintarev",
				"given": "Nava"
			},
			{
				"family": "Hauff",
				"given": "Claudia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "celisClassificationFairnessConstraints2019",
		"type": "paper-conference",
		"abstract": "Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex \"linear fractional\" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287586",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "319–328",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Classification with fairness constraints: A meta-algorithm with provable guarantees",
		"URL": "https://doi.org/10.1145/3287560.3287586",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Huang",
				"given": "Lingxiao"
			},
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cardDeepWeightedAveraging2019",
		"type": "paper-conference",
		"abstract": "Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287595",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "369–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deep weighted averaging classifiers",
		"URL": "https://doi.org/10.1145/3287560.3287595",
		"author": [
			{
				"family": "Card",
				"given": "Dallas"
			},
			{
				"family": "Zhang",
				"given": "Michael"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chakrabortyEqualityVoiceFair2019",
		"type": "paper-conference",
		"abstract": "To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups.To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287570",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "129–138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equality of voice: Towards fair representation in crowdsourced top-K recommendations",
		"URL": "https://doi.org/10.1145/3287560.3287570",
		"author": [
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Patro",
				"given": "Gourab K."
			},
			{
				"family": "Ganguly",
				"given": "Niloy"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Loiseau",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "de-arteagaBiasBiosCase2019",
		"type": "paper-conference",
		"abstract": "We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators—such as first names and pronouns—in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287572",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "120–128",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in bios: A case study of semantic representation bias in a high-stakes setting",
		"URL": "https://doi.org/10.1145/3287560.3287572",
		"author": [
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Romanov",
				"given": "Alexey"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			},
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Geyik",
				"given": "Sahin"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chancellorTaxonomyEthicalTensions2019",
		"type": "paper-conference",
		"abstract": "Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287587",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "79–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A taxonomy of ethical tensions in inferring mental health states from social media",
		"URL": "https://doi.org/10.1145/3287560.3287587",
		"author": [
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Birnbaum",
				"given": "Michael L."
			},
			{
				"family": "Caine",
				"given": "Eric D."
			},
			{
				"family": "Silenzio",
				"given": "Vincent M. B."
			},
			{
				"family": "De Choudhury",
				"given": "Munmun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chenFairnessUnawarenessAssessing2019",
		"type": "paper-conference",
		"abstract": "Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287594",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "339–348",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness under unawareness: Assessing disparity when protected class is unobserved",
		"URL": "https://doi.org/10.1145/3287560.3287594",
		"author": [
			{
				"family": "Chen",
				"given": "Jiahao"
			},
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Mao",
				"given": "Xiaojie"
			},
			{
				"family": "Svacha",
				"given": "Geoffry"
			},
			{
				"family": "Udell",
				"given": "Madeleine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "goldenfeinProfilingPotentialComputer2019",
		"type": "paper-conference",
		"abstract": "Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287568",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "110–119",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The profiling potential of computer vision and the challenge of computational empiricism",
		"URL": "https://doi.org/10.1145/3287560.3287568",
		"author": [
			{
				"family": "Goldenfein",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "elzaynFairAlgorithmsLearning2019",
		"type": "paper-conference",
		"abstract": "Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287571",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "170–179",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair algorithms for learning in allocation problems",
		"URL": "https://doi.org/10.1145/3287560.3287571",
		"author": [
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Jabbari",
				"given": "Shahin"
			},
			{
				"family": "Jung",
				"given": "Christopher"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Schutzman",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "glymourMeasuringBiasesThat2019",
		"type": "paper-conference",
		"abstract": "Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of \"procedural bias\" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of \"outcome bias\" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of \"behavior-relative error bias\" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of \"score-relative error bias\" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized.In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287573",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "269–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring the biases that matter: The ethical and casual foundations for measures of fairness in algorithms",
		"URL": "https://doi.org/10.1145/3287560.3287573",
		"author": [
			{
				"family": "Glymour",
				"given": "Bruce"
			},
			{
				"family": "Herington",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "engelmannClearSanctionsVague2019",
		"type": "paper-conference",
		"abstract": "China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, \"good\" behavior can result in material rewards and reputational gain while \"bad\" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines \"good\" and \"bad\" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform \"Credit China\", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on \"good\" behavior) and the Blacklist (information on \"bad\" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287585",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "69–78",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Clear sanctions, vague rewards: How china's social credit system currently defines \"Good\" and \"Bad\" behavior",
		"URL": "https://doi.org/10.1145/3287560.3287585",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Fischer",
				"given": "Felix"
			},
			{
				"family": "Kao",
				"given": "Ching-yu"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "friedlerComparativeStudyFairnessenhancing2019",
		"type": "paper-conference",
		"abstract": "Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287589",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "329–338",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A comparative study of fairness-enhancing interventions in machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287589",
		"author": [
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Choudhary",
				"given": "Sonam"
			},
			{
				"family": "Hamilton",
				"given": "Evan P."
			},
			{
				"family": "Roth",
				"given": "Derek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "heidariMoralFrameworkUnderstanding2019",
		"type": "paper-conference",
		"abstract": "We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)—an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287584",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "181–190",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A moral framework for understanding fair ML through economic models of equality of opportunity",
		"URL": "https://doi.org/10.1145/3287560.3287584",
		"author": [
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Krause",
				"given": "Andreas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kannanDownstreamEffectsAffirmative2019",
		"type": "paper-conference",
		"abstract": "We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287578",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "240–248",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Downstream effects of affirmative action",
		"URL": "https://doi.org/10.1145/3287560.3287578",
		"author": [
			{
				"family": "Kannan",
				"given": "Sampath"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Ziani",
				"given": "Juba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "immorlicaAccessPopulationlevelSignaling2019",
		"type": "paper-conference",
		"abstract": "We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR).We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287579",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "249–258",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Access to population-level signaling as a source of inequality",
		"URL": "https://doi.org/10.1145/3287560.3287579",
		"author": [
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Ligett",
				"given": "Katrina"
			},
			{
				"family": "Ziani",
				"given": "Juba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "laiHumanPredictionsExplanations2019",
		"type": "paper-conference",
		"abstract": "Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (&gt;20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287590",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "29–38",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On human predictions with explanations and predictions of machine learning models: A case study on deception detection",
		"URL": "https://doi.org/10.1145/3287560.3287590",
		"author": [
			{
				"family": "Lai",
				"given": "Vivian"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "huDisparateEffectsStrategic2019",
		"type": "paper-conference",
		"abstract": "When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed \"strategic manipulation,\" analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to \"trick\" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off—even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's \"quality\" when agents' capacities to adaptively respond differ.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287597",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "259–268",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The disparate effects of strategic manipulation",
		"URL": "https://doi.org/10.1145/3287560.3287597",
		"author": [
			{
				"family": "Hu",
				"given": "Lily"
			},
			{
				"family": "Immorlica",
				"given": "Nicole"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kearnsEmpiricalStudyRich2019",
		"type": "paper-conference",
		"abstract": "Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287592",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "100–109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical study of rich subgroup fairness for machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287592",
		"author": [
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hutchinson50YearsTest2019",
		"type": "paper-conference",
		"abstract": "Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287600",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "49–58",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "50 years of test (un)fairness: Lessons for machine learning",
		"URL": "https://doi.org/10.1145/3287560.3287600",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "milliSocialCostStrategic2019",
		"type": "paper-conference",
		"abstract": "Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population.Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287576",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "230–239",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The social cost of strategic classification",
		"URL": "https://doi.org/10.1145/3287560.3287576",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Miller",
				"given": "John"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mittelstadtExplainingExplanationsAI2019",
		"type": "paper-conference",
		"abstract": "Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that \"All models are wrong but some are useful.\" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a \"do it yourself kit\" for explanations, allowing a practitioner to directly answer \"what if questions\" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287574",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "279–288",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explaining explanations in AI",
		"URL": "https://doi.org/10.1145/3287560.3287574",
		"author": [
			{
				"family": "Mittelstadt",
				"given": "Brent"
			},
			{
				"family": "Russell",
				"given": "Chris"
			},
			{
				"family": "Wachter",
				"given": "Sandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "leongRobotEyesWide2019",
		"type": "paper-conference",
		"abstract": "The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, \"Averting Robot Eyes.\" Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287591",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "299–308",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robot eyes wide shut: Understanding dishonest anthropomorphism",
		"URL": "https://doi.org/10.1145/3287560.3287591",
		"author": [
			{
				"family": "Leong",
				"given": "Brenda"
			},
			{
				"family": "Selinger",
				"given": "Evan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "obermeyerDissectingRacialBias2019",
		"type": "paper-conference",
		"abstract": "A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care.To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs).We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate.An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t–1 (features), fine-grained care utilization data in year t – 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks.The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health – not just costs – also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost – for example, race.We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health.The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on – cost – is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care – incentives that induce health systems to focus on dollars rather than health – also has consequences for the way algorithms are built and monitored.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287593",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 1\npublisher-place: Atlanta, GA, USA",
		"page": "89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dissecting racial bias in an algorithm that guides health decisions for 70 million people",
		"URL": "https://doi.org/10.1145/3287560.3287593",
		"author": [
			{
				"family": "Obermeyer",
				"given": "Ziad"
			},
			{
				"family": "Mullainathan",
				"given": "Sendhil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mitchellModelCardsModel2019",
		"type": "paper-conference",
		"abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287596",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "220–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model cards for model reporting",
		"URL": "https://doi.org/10.1145/3287560.3287596",
		"author": [
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Wu",
				"given": "Simone"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Vasserman",
				"given": "Lucy"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Spitzer",
				"given": "Elena"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mouzannarFairDecisionMaking2019",
		"type": "paper-conference",
		"abstract": "The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action.We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287599",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "359–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From fair decision making to social equality",
		"URL": "https://doi.org/10.1145/3287560.3287599",
		"author": [
			{
				"family": "Mouzannar",
				"given": "Hussein"
			},
			{
				"family": "Ohannessian",
				"given": "Mesrob I."
			},
			{
				"family": "Srebro",
				"given": "Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "russellEfficientSearchDiverse2019",
		"type": "paper-conference",
		"abstract": "This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a \"mixed polytope\" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287569",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 9\npublisher-place: Atlanta, GA, USA",
		"page": "20–28",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Efficient search for diverse coherent explanations",
		"URL": "https://doi.org/10.1145/3287560.3287569",
		"author": [
			{
				"family": "Russell",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ustunActionableRecourseLinear2019",
		"type": "paper-conference",
		"abstract": "Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and – more importantly –will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287566",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "10–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable recourse in linear classification",
		"URL": "https://doi.org/10.1145/3287560.3287566",
		"author": [
			{
				"family": "Ustun",
				"given": "Berk"
			},
			{
				"family": "Spangher",
				"given": "Alexander"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "passiProblemFormulationFairness2019",
		"type": "paper-conference",
		"abstract": "Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team—and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases—we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways—and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287567",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "39–48",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Problem formulation and fairness",
		"URL": "https://doi.org/10.1145/3287560.3287567",
		"author": [
			{
				"family": "Passi",
				"given": "Samir"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "youngOpenVsClosed2019",
		"type": "paper-conference",
		"abstract": "Data too sensitive to be \"open\" for analysis and re-purposing typically remains \"closed\" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287577",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "191–200",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond open vs. Closed: Balancing individual privacy and public accountability in data sharing",
		"URL": "https://doi.org/10.1145/3287560.3287577",
		"author": [
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Rodriguez",
				"given": "Luke"
			},
			{
				"family": "Keller",
				"given": "Emily"
			},
			{
				"family": "Sun",
				"given": "Feiyang"
			},
			{
				"family": "Sa",
				"given": "Boyang"
			},
			{
				"family": "Whittington",
				"given": "Jan"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ribeiroMicrotargetingSociallyDivisive2019",
		"type": "paper-conference",
		"abstract": "Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are \"divisive\": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287580",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "140–149",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On microtargeting socially divisive ads: A case study of russia-linked ad campaigns on facebook",
		"URL": "https://doi.org/10.1145/3287560.3287580",
		"author": [
			{
				"family": "Ribeiro",
				"given": "Filipe N."
			},
			{
				"family": "Saha",
				"given": "Koustuv"
			},
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Henrique",
				"given": "Lucas"
			},
			{
				"family": "Messias",
				"given": "Johnnatan"
			},
			{
				"family": "Benevenuto",
				"given": "Fabricio"
			},
			{
				"family": "Goga",
				"given": "Oana"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Redmiles",
				"given": "Elissa M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "selbstFairnessAbstractionSociotechnical2019",
		"type": "paper-conference",
		"abstract": "A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science—such as abstraction and modular design—are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce \"fair\" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five \"traps\" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.",
		"collection-title": "FAT* '19",
		"container-title": "Proceedings of the conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3287560.3287598",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6125-5",
		"note": "number-of-pages: 10\npublisher-place: Atlanta, GA, USA",
		"page": "59–68",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and abstraction in sociotechnical systems",
		"URL": "https://doi.org/10.1145/3287560.3287598",
		"author": [
			{
				"family": "Selbst",
				"given": "Andrew D."
			},
			{
				"family": "Boyd",
				"given": "Danah"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			},
			{
				"family": "Vertesi",
				"given": "Janet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "miceliDocumentingComputerVision2021",
		"type": "paper-conference",
		"abstract": "In industrial computer vision, discretionary decisions surrounding the production of image training data remain widely undocumented. Recent research taking issue with such opacity has proposed standardized processes for dataset documentation. In this paper, we expand this space of inquiry through fieldwork at two data processing companies and thirty interviews with data workers and computer vision practitioners. We identify four key issues that hinder the documentation of image datasets and the effective retrieval of production contexts. Finally, we propose reflexivity, understood as a collective consideration of social and intellectual factors that lead to praxis, as a necessary precondition for documentation. Reflexive documentation can help to expose the contexts, relations, routines, and power structures that shape data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445880",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "161–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Documenting computer vision datasets: An invitation to reflexive data practices",
		"URL": "https://doi.org/10.1145/3442188.3445880",
		"author": [
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Yang",
				"given": "Tianling"
			},
			{
				"family": "Naudts",
				"given": "Laurens"
			},
			{
				"family": "Schuessler",
				"given": "Martin"
			},
			{
				"family": "Serbanescu",
				"given": "Diana"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "costonLeveragingAdministrativeData2021",
		"type": "paper-conference",
		"abstract": "Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data—containing individual-level voter turnout for specific voting locations along with race and age—can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445881",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "173–184",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leveraging administrative data for bias audits: Assessing disparate coverage with mobility data for COVID-19 policy",
		"URL": "https://doi.org/10.1145/3442188.3445881",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Guha",
				"given": "Neel"
			},
			{
				"family": "Ouyang",
				"given": "Derek"
			},
			{
				"family": "Lu",
				"given": "Lisa"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Ho",
				"given": "Daniel E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "khaniRemovingSpuriousFeatures2021",
		"type": "paper-conference",
		"abstract": "Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445883",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "196–205",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Removing spurious features can hurt accuracy and affect groups disproportionately",
		"URL": "https://doi.org/10.1145/3442188.3445883",
		"author": [
			{
				"family": "Khani",
				"given": "Fereshte"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "awasthiEvaluatingFairnessMachine2021",
		"type": "paper-conference",
		"abstract": "Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives.We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445884",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "206–214",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating fairness of machine learning models under uncertain and incomplete information",
		"URL": "https://doi.org/10.1145/3442188.3445884",
		"author": [
			{
				"family": "Awasthi",
				"given": "Pranjal"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Kleindessner",
				"given": "Matthäus"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangFairDeepAnomaly2021",
		"type": "paper-conference",
		"abstract": "Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445878",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "138–148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards fair deep anomaly detection",
		"URL": "https://doi.org/10.1145/3442188.3445878",
		"author": [
			{
				"family": "Zhang",
				"given": "Hongjing"
			},
			{
				"family": "Davidson",
				"given": "Ian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chengCanYouFake2021",
		"type": "paper-conference",
		"abstract": "The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445879",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "149–160",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can you fake it until you make it? Impacts of differentially private synthetic data on downstream classification fairness",
		"URL": "https://doi.org/10.1145/3442188.3445879",
		"author": [
			{
				"family": "Cheng",
				"given": "Victoria"
			},
			{
				"family": "Suriyakumar",
				"given": "Vinith M."
			},
			{
				"family": "Dullerud",
				"given": "Natalie"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "donahueBetterTogetherHow2021",
		"type": "paper-conference",
		"abstract": "Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as \"solidarity\" and \"actuarial fairness\" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory.First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445882",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "185–195",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Better together? How externalities of size complicate notions of solidarity and actuarial fairness",
		"URL": "https://doi.org/10.1145/3442188.3445882",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "vincentDataLeverageFramework2021",
		"type": "paper-conference",
		"abstract": "Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445885",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "215–227",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data leverage: A framework for empowering the public in its relationship with technology companies",
		"URL": "https://doi.org/10.1145/3442188.3445885",
		"author": [
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Li",
				"given": "Hanlin"
			},
			{
				"family": "Tilly",
				"given": "Nicole"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehUseMisuseCounterfactuals2021",
		"type": "paper-conference",
		"abstract": "The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445886",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "228–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The use and misuse of counterfactuals in ethical machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445886",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "gargStandardizedTestsAffirmative2021",
		"type": "paper-conference",
		"abstract": "The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies—such as affirmative action—on admitted class composition.This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action.We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool.Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement.The full paper can be found at https://arxiv.org/abs/2010.04396.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445889",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "261",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Standardized tests and affirmative action: The role of bias and variance",
		"URL": "https://doi.org/10.1145/3442188.3445889",
		"author": [
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Li",
				"given": "Hannah"
			},
			{
				"family": "Monachou",
				"given": "Faidra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "knowlesSanctionAuthorityPromoting2021",
		"type": "paper-conference",
		"abstract": "Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations—both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards—is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445890",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "262–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The sanction of authority: Promoting public trust in AI",
		"URL": "https://doi.org/10.1145/3442188.3445890",
		"author": [
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Richards",
				"given": "John T."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "blackLeaveoneoutUnfairness2021",
		"type": "paper-conference",
		"abstract": "We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445894",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "285–295",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Leave-one-out unfairness",
		"URL": "https://doi.org/10.1145/3442188.3445894",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kallusFairnessWelfareEquity2021",
		"type": "paper-conference",
		"abstract": "We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a \"triple bottom line\": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445895",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 19\npublisher-place: Virtual Event, Canada",
		"page": "296–314",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, welfare, and equity in personalized pricing",
		"URL": "https://doi.org/10.1145/3442188.3445895",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			},
			{
				"family": "Zhou",
				"given": "Angela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mehrotraMitigatingBiasSet2021",
		"type": "paper-conference",
		"abstract": "Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a \"denoised\" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445887",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "237–248",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating bias in set selection with noisy protected attributes",
		"URL": "https://doi.org/10.1145/3442188.3445887",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Celis",
				"given": "L. Elisa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "andrusWhatWeCant2021",
		"type": "paper-conference",
		"abstract": "As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445888",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "249–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What we can't measure, we can't understand: Challenges to demographic data procurement in the pursuit of fairness",
		"URL": "https://doi.org/10.1145/3442188.3445888",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Spitzer",
				"given": "Elena"
			},
			{
				"family": "Brown",
				"given": "Jeffrey"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kilbyAlgorithmicFairnessPredicting2021",
		"type": "paper-conference",
		"abstract": "There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445891",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "272",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness in predicting opioid use disorder using machine learning",
		"URL": "https://doi.org/10.1145/3442188.3445891",
		"author": [
			{
				"family": "Kilby",
				"given": "Angela E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yeomAvoidingDisparityAmplification2021",
		"type": "paper-conference",
		"abstract": "We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445892",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "273–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Avoiding disparity amplification under different worldviews",
		"URL": "https://doi.org/10.1145/3442188.3445892",
		"author": [
			{
				"family": "Yeom",
				"given": "Samuel"
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "martinSpokenCorporaData2021",
		"type": "paper-conference",
		"abstract": "Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in \"I be in my office by 7:30am\", paraphrasable as \"I am usually in my office by 7:30\" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora – Switchboard, Fisher, TIMIT, and LibriSpeech – to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL – the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445893",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Spoken corpora data, automatic speech recognition, and bias against african american language: The case of habitual 'be'",
		"URL": "https://doi.org/10.1145/3442188.3445893",
		"author": [
			{
				"family": "Martin",
				"given": "Joshua L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "sambasivanReimaginingAlgorithmicFairness2021",
		"type": "paper-conference",
		"abstract": "Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445896",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "315–328",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Re-imagining algorithmic fairness in india and beyond",
		"URL": "https://doi.org/10.1145/3442188.3445896",
		"author": [
			{
				"family": "Sambasivan",
				"given": "Nithya"
			},
			{
				"family": "Arnesen",
				"given": "Erin"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Doshi",
				"given": "Tulsee"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abebeNarrativesCounternarrativesData2021",
		"type": "paper-conference",
		"abstract": "As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem.We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445897",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "329–341",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Narratives and counternarratives on data sharing in africa",
		"URL": "https://doi.org/10.1145/3442188.3445897",
		"author": [
			{
				"family": "Abebe",
				"given": "Rediet"
			},
			{
				"family": "Aruleba",
				"given": "Kehinde"
			},
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Kingsley",
				"given": "Sara"
			},
			{
				"family": "Obaido",
				"given": "George"
			},
			{
				"family": "Remy",
				"given": "Sekou L."
			},
			{
				"family": "Sadagopan",
				"given": "Swathi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "albertThisWholeThing2021",
		"type": "paper-conference",
		"abstract": "Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third \"gender\" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445898",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "342–352",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "This whole thing smacks of gender: Algorithmic exclusion in bioimpedance-based body composition analysis",
		"URL": "https://doi.org/10.1145/3442188.3445898",
		"author": [
			{
				"family": "Albert",
				"given": "Kendra"
			},
			{
				"family": "Delano",
				"given": "Maggie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barbosaSemioticsbasedEpistemicTool2021",
		"type": "paper-conference",
		"abstract": "One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445900",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "363–374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development",
		"URL": "https://doi.org/10.1145/3442188.3445900",
		"author": [
			{
				"family": "Barbosa",
				"given": "Simone Diniz Junqueira"
			},
			{
				"family": "Barbosa",
				"given": "Gabriel Diniz Junqueira"
			},
			{
				"family": "Souza",
				"given": "Clarisse Sieckenius",
				"dropping-particle": "de"
			},
			{
				"family": "Leitão",
				"given": "Carla Faria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jacobsMeasurementFairness2021",
		"type": "paper-conference",
		"abstract": "We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them—i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445901",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "375–385",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measurement and fairness",
		"URL": "https://doi.org/10.1145/3442188.3445901",
		"author": [
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mishlerFairnessRiskAssessment2021",
		"type": "paper-conference",
		"abstract": "In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445902",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "386–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds",
		"URL": "https://doi.org/10.1145/3442188.3445902",
		"author": [
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "patelHighDimensionalModel2021",
		"type": "paper-conference",
		"abstract": "Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets.We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model.Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445903",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "401–411",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "High dimensional model explanations: An axiomatic approach",
		"URL": "https://doi.org/10.1145/3442188.3445903",
		"author": [
			{
				"family": "Patel",
				"given": "Neel"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "karimiAlgorithmicRecourseCounterfactual2021",
		"type": "paper-conference",
		"abstract": "As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -\"how the world would have (had) to be different for a desirable outcome to occur\"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445899",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "353–362",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic recourse: from counterfactual explanations to interventions",
		"URL": "https://doi.org/10.1145/3442188.3445899",
		"author": [
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Schölkopf",
				"given": "Bernhard"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ionescuAgentbasedModelEvaluate2021",
		"type": "paper-conference",
		"abstract": "Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445904",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "412–423",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An agent-based model to evaluate interventions on online dating platforms to decrease racial homogamy",
		"URL": "https://doi.org/10.1145/3442188.3445904",
		"author": [
			{
				"family": "Ionescu",
				"given": "Stefania"
			},
			{
				"family": "Hannák",
				"given": "Anikó"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kaciankaDesigningAccountableSystems2021",
		"type": "paper-conference",
		"abstract": "Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445905",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "424–437",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing accountable systems",
		"URL": "https://doi.org/10.1145/3442188.3445905",
		"author": [
			{
				"family": "Kacianka",
				"given": "Severin"
			},
			{
				"family": "Pretschner",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "choCrosslingualGeneralizationTranslation2021",
		"type": "paper-conference",
		"abstract": "Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445907",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "449–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards cross-lingual generalization of translation gender bias",
		"URL": "https://doi.org/10.1145/3442188.3445907",
		"author": [
			{
				"family": "Cho",
				"given": "Won Ik"
			},
			{
				"family": "Kim",
				"given": "Jiwon"
			},
			{
				"family": "Yang",
				"given": "Jaeyeong"
			},
			{
				"family": "Kim",
				"given": "Nam Soo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mulderOperationalizingFramingSupport2021",
		"type": "paper-conference",
		"abstract": "Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445911",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "478–488",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Operationalizing framing to support multiperspective recommendations of opinion pieces",
		"URL": "https://doi.org/10.1145/3442188.3445911",
		"author": [
			{
				"family": "Mulder",
				"given": "Mats"
			},
			{
				"family": "Inel",
				"given": "Oana"
			},
			{
				"family": "Oosterman",
				"given": "Jasper"
			},
			{
				"family": "Tintarev",
				"given": "Nava"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "finocchiaroBridgingMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445912",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 15\npublisher-place: Virtual Event, Canada",
		"page": "489–503",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bridging machine learning and mechanism design towards algorithmic fairness",
		"URL": "https://doi.org/10.1145/3442188.3445912",
		"author": [
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			},
			{
				"family": "Maio",
				"given": "Roland"
			},
			{
				"family": "Monachou",
				"given": "Faidra"
			},
			{
				"family": "Patro",
				"given": "Gourab K"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Stoica",
				"given": "Ana-Andreea"
			},
			{
				"family": "Tsirtsis",
				"given": "Stratis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ghadiriSociallyFairKMeans2021",
		"type": "paper-conference",
		"abstract": "We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445906",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "438–448",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socially fair k-Means clustering",
		"URL": "https://doi.org/10.1145/3442188.3445906",
		"author": [
			{
				"family": "Ghadiri",
				"given": "Mehrdad"
			},
			{
				"family": "Samadi",
				"given": "Samira"
			},
			{
				"family": "Vempala",
				"given": "Santosh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "williamsBayesianModelCash2021",
		"type": "paper-conference",
		"abstract": "The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of \"innocent until proven guilty.\" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445908",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "827–837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A bayesian model of cash bail decisions",
		"URL": "https://doi.org/10.1145/3442188.3445908",
		"author": [
			{
				"family": "Williams",
				"given": "Joshua"
			},
			{
				"family": "Kolter",
				"given": "J. Zico"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "boagPilotStudySurveying2021",
		"type": "paper-conference",
		"abstract": "The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445909",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, Canada",
		"page": "458–465",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A pilot study in surveying clinical judgments to evaluate radiology report generation",
		"URL": "https://doi.org/10.1145/3442188.3445909",
		"author": [
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Kané",
				"given": "Hassan"
			},
			{
				"family": "Rawat",
				"given": "Saumya"
			},
			{
				"family": "Wei",
				"given": "Jesse"
			},
			{
				"family": "Goehler",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nandaFairnessRobustnessInvestigating2021",
		"type": "paper-conference",
		"abstract": "Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445910",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "466–477",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness through robustness: Investigating robustness disparity in deep learning",
		"URL": "https://doi.org/10.1145/3442188.3445910",
		"author": [
			{
				"family": "Nanda",
				"given": "Vedant"
			},
			{
				"family": "Dooley",
				"given": "Samuel"
			},
			{
				"family": "Singla",
				"given": "Sahil"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abbasiFairClusteringEquitable2021",
		"type": "paper-conference",
		"abstract": "What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being \"close\" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common \"use case\" for clustering. For such a clustering to be fair, the centers should \"represent\" different groups equally well. We call such a clustering a group-representative clustering.In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445913",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "504–514",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair clustering via equitable group representations",
		"URL": "https://doi.org/10.1145/3442188.3445913",
		"author": [
			{
				"family": "Abbasi",
				"given": "Mohsen"
			},
			{
				"family": "Bhaskara",
				"given": "Aditya"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "rajiYouCantSit2021",
		"type": "paper-conference",
		"abstract": "Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)—and its implications for AI—has led to the current \"ethics crisis\". However, we claim that the current AI ethics education space relies on a form of \"exclusionary pedagogy,\" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as \"ethical unicorns\" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445914",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "515–525",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "You can't sit with us: Exclusionary pedagogy in AI ethics education",
		"URL": "https://doi.org/10.1145/3442188.3445914",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Scheuerman",
				"given": "Morgan Klaus"
			},
			{
				"family": "Amironesei",
				"given": "Razvan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wangFairClassificationGroupdependent2021",
		"type": "paper-conference",
		"abstract": "This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445915",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "526–536",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair classification with group-dependent label noise",
		"URL": "https://doi.org/10.1145/3442188.3445915",
		"author": [
			{
				"family": "Wang",
				"given": "Jialu"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Levy",
				"given": "Caleb"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hutchinsonAccountabilityMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445918",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 16\npublisher-place: Virtual Event, Canada",
		"page": "560–575",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards accountability for machine learning datasets: Practices from software engineering and infrastructure",
		"URL": "https://doi.org/10.1145/3442188.3445918",
		"author": [
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Kjartansson",
				"given": "Oddur"
			},
			{
				"family": "Barnes",
				"given": "Parker"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasyFairnessEqualityPower2021",
		"type": "paper-conference",
		"abstract": "Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same \"merit.\" Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by \"merit;\" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445919",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "576–586",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness, equality, and power in algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3442188.3445919",
		"author": [
			{
				"family": "Kasy",
				"given": "Maximilian"
			},
			{
				"family": "Abebe",
				"given": "Rediet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cobbeReviewableAutomatedDecisionmaking2021",
		"type": "paper-conference",
		"abstract": "This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445921",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "598–609",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reviewable automated decision-making: A framework for accountable algorithmic systems",
		"URL": "https://doi.org/10.1145/3442188.3445921",
		"author": [
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yangCensorshipOnlineEncyclopedias2021",
		"type": "paper-conference",
		"abstract": "While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445916",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "537–548",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Censorship of online encyclopedias: Implications for NLP models",
		"URL": "https://doi.org/10.1145/3442188.3445916",
		"author": [
			{
				"family": "Yang",
				"given": "Eddie"
			},
			{
				"family": "Roberts",
				"given": "Margaret E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hamonImpossibleExplanationsExplainable2021",
		"type": "paper-conference",
		"abstract": "Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445917",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "549–559",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario",
		"URL": "https://doi.org/10.1145/3442188.3445917",
		"author": [
			{
				"family": "Hamon",
				"given": "Ronan"
			},
			{
				"family": "Junklewitz",
				"given": "Henrik"
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			},
			{
				"family": "Hert",
				"given": "Paul De"
			},
			{
				"family": "Beslay",
				"given": "Laurent"
			},
			{
				"family": "Sanchez",
				"given": "Ignacio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "khanOneLabelOne2021",
		"type": "paper-conference",
		"abstract": "Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445920",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "587–597",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One label, one billion faces: Usage and consistency of racial categories in computer vision",
		"URL": "https://doi.org/10.1145/3442188.3445920",
		"author": [
			{
				"family": "Khan",
				"given": "Zaid"
			},
			{
				"family": "Fu",
				"given": "Yun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "benderDangersStochasticParrots2021",
		"type": "paper-conference",
		"abstract": "The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445922",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "610–623",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the dangers of stochastic parrots: Can language models be too big? 🦜",
		"URL": "https://doi.org/10.1145/3442188.3445922",
		"author": [
			{
				"family": "Bender",
				"given": "Emily M."
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "McMillan-Major",
				"given": "Angelina"
			},
			{
				"family": "Shmitchell",
				"given": "Shmargaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grunewaldTILTGDPRAlignedTransparency2021",
		"type": "paper-conference",
		"abstract": "In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do.We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty.Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445925",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "636–646",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TILT: A GDPR-Aligned transparency information language and toolkit for practical privacy engineering",
		"URL": "https://doi.org/10.1145/3442188.3445925",
		"author": [
			{
				"family": "Grünewald",
				"given": "Elias"
			},
			{
				"family": "Pallas",
				"given": "Frank"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jacoviFormalizingTrustArtificial2021",
		"type": "paper-conference",
		"abstract": "Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445923",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "624–635",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI",
		"URL": "https://doi.org/10.1145/3442188.3445923",
		"author": [
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Marasović",
				"given": "Ana"
			},
			{
				"family": "Miller",
				"given": "Tim"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dhamalaBOLDDatasetMetrics2021",
		"type": "paper-conference",
		"abstract": "Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445924",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "862–872",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "BOLD: Dataset and metrics for measuring biases in open-ended language generation",
		"URL": "https://doi.org/10.1145/3442188.3445924",
		"author": [
			{
				"family": "Dhamala",
				"given": "Jwala"
			},
			{
				"family": "Sun",
				"given": "Tony"
			},
			{
				"family": "Kumar",
				"given": "Varun"
			},
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Pruksachatkun",
				"given": "Yada"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Gupta",
				"given": "Rahul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "lussierPapersProgramsCourts2021",
		"type": "paper-conference",
		"abstract": "This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445926",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "647",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From papers to programs: Courts, corporations, clinics and the battle over computerized psychological testing",
		"URL": "https://doi.org/10.1145/3442188.3445926",
		"author": [
			{
				"family": "Lussier",
				"given": "Kira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "taskesenStatisticalTestProbabilistic2021",
		"type": "paper-conference",
		"abstract": "Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445927",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 18\npublisher-place: Virtual Event, Canada",
		"page": "648–665",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A statistical test for probabilistic fairness",
		"URL": "https://doi.org/10.1145/3442188.3445927",
		"author": [
			{
				"family": "Taskesen",
				"given": "Bahar"
			},
			{
				"family": "Blanchet",
				"given": "Jose"
			},
			{
				"family": "Kuhn",
				"given": "Daniel"
			},
			{
				"family": "Nguyen",
				"given": "Viet Anh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "wilsonBuildingAuditingFair2021",
		"type": "paper-conference",
		"abstract": "Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of \"fairness\" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445928",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "666–677",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building and auditing fair algorithms: A case study in candidate screening",
		"URL": "https://doi.org/10.1145/3442188.3445928",
		"author": [
			{
				"family": "Wilson",
				"given": "Christo"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Jiang",
				"given": "Shan"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			},
			{
				"family": "Baker",
				"given": "Lewis"
			},
			{
				"family": "Szary",
				"given": "Janelle"
			},
			{
				"family": "Trindel",
				"given": "Kelly"
			},
			{
				"family": "Polli",
				"given": "Frida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "celisEffectRooneyRule2021",
		"type": "paper-conference",
		"abstract": "The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change.In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist—independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445930",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "678–689",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effect of the rooney rule on implicit bias in the long term",
		"URL": "https://doi.org/10.1145/3442188.3445930",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Hays",
				"given": "Chris"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "milliOptimizingEngagementMeasuring2021",
		"type": "paper-conference",
		"abstract": "Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of value that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of \"value\".",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445933",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, Canada",
		"page": "714–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From optimizing engagement to measuring value",
		"URL": "https://doi.org/10.1145/3442188.3445933",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Belli",
				"given": "Luca"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hamptonBlackFeministMusings2021",
		"type": "paper-conference",
		"abstract": "This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445929",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "1",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Black feminist musings on algorithmic oppression",
		"URL": "https://doi.org/10.1145/3442188.3445929",
		"author": [
			{
				"family": "Hampton",
				"given": "Lelia Marie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasinidouAgreeDecisionThey2021",
		"type": "paper-conference",
		"abstract": "While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445931",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "690–700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions",
		"URL": "https://doi.org/10.1145/3442188.3445931",
		"author": [
			{
				"family": "Kasinidou",
				"given": "Maria"
			},
			{
				"family": "Kleanthous",
				"given": "Styliani"
			},
			{
				"family": "Barlas",
				"given": "Pınar"
			},
			{
				"family": "Otterbacher",
				"given": "Jahna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "steedImageRepresentationsLearned2021",
		"type": "paper-conference",
		"abstract": "Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445932",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, Canada",
		"page": "701–713",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Image representations learned with unsupervised pre-training contain human-like biases",
		"URL": "https://doi.org/10.1145/3442188.3445932",
		"author": [
			{
				"family": "Steed",
				"given": "Ryan"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "suriyakumarChasingYourLong2021",
		"type": "paper-conference",
		"abstract": "Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445934",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "723–734",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Chasing your long tails: Differentially private prediction in health care settings",
		"URL": "https://doi.org/10.1145/3442188.3445934",
		"author": [
			{
				"family": "Suriyakumar",
				"given": "Vinith M."
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			},
			{
				"family": "Goldenberg",
				"given": "Anna"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "metcalfAlgorithmicImpactAssessments2021",
		"type": "paper-conference",
		"abstract": "Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that \"impacts\" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445935",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "735–746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic impact assessments and accountability: The co-construction of impacts",
		"URL": "https://doi.org/10.1145/3442188.3445935",
		"author": [
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hertweckMoralJustificationStatistical2021",
		"type": "paper-conference",
		"abstract": "A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews \"What You See Is What You Get\" (WYSIWYG) and \"We're All Equal\" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445936",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "747–757",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the moral justification of statistical parity",
		"URL": "https://doi.org/10.1145/3442188.3445936",
		"author": [
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "krafftActionorientedAIPolicy2021",
		"type": "paper-conference",
		"abstract": "Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445938",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "772–781",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An action-oriented AI policy toolkit for technology audits by community advocates and activists",
		"URL": "https://doi.org/10.1145/3442188.3445938",
		"author": [
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Lee",
				"given": "Jennifer E."
			},
			{
				"family": "Narayan",
				"given": "Shankar"
			},
			{
				"family": "Epstein",
				"given": "Micah"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Jobe",
				"given": "Pa Ousman"
			},
			{
				"family": "Putz",
				"given": "Franziska"
			},
			{
				"family": "Robick",
				"given": "Brian"
			},
			{
				"family": "Barghouti",
				"given": "Bissan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jesusHowCanChoose2021",
		"type": "paper-conference",
		"abstract": "There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445941",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "805–815",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How can I choose an explainer? An application-grounded evaluation of post-hoc explanations",
		"URL": "https://doi.org/10.1145/3442188.3445941",
		"author": [
			{
				"family": "Jesus",
				"given": "Sérgio"
			},
			{
				"family": "Belém",
				"given": "Catarina"
			},
			{
				"family": "Balayan",
				"given": "Vladimir"
			},
			{
				"family": "Bento",
				"given": "João"
			},
			{
				"family": "Saleiro",
				"given": "Pedro"
			},
			{
				"family": "Bizarro",
				"given": "Pedro"
			},
			{
				"family": "Gama",
				"given": "João"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "creelAlgorithmicLeviathanArbitrariness2021",
		"type": "paper-conference",
		"abstract": "Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are \"fair\" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445942",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, Canada",
		"page": "816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The algorithmic leviathan: Arbitrariness, fairness, and opportunity in algorithmic decision making systems",
		"URL": "https://doi.org/10.1145/3442188.3445942",
		"author": [
			{
				"family": "Creel",
				"given": "Kathleen"
			},
			{
				"family": "Hellman",
				"given": "Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shenValueCardsEducational2021",
		"type": "paper-conference",
		"abstract": "Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445971",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "850–861",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value cards: An educational toolkit for teaching social impacts of machine learning through deliberation",
		"URL": "https://doi.org/10.1145/3442188.3445971",
		"author": [
			{
				"family": "Shen",
				"given": "Hong"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Chattopadhyay",
				"given": "Aditi"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Wang",
				"given": "Xu"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "krollOutliningTraceabilityPrinciple2021",
		"type": "paper-conference",
		"abstract": "Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445937",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 14\npublisher-place: Virtual Event, Canada",
		"page": "758–771",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Outlining traceability: A principle for operationalizing accountability in computing systems",
		"URL": "https://doi.org/10.1145/3442188.3445937",
		"author": [
			{
				"family": "Kroll",
				"given": "Joshua A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "starkEthicsEmotionArtificial2021",
		"type": "paper-conference",
		"abstract": "In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445939",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "782–793",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethics of emotion in artificial intelligence systems",
		"URL": "https://doi.org/10.1145/3442188.3445939",
		"author": [
			{
				"family": "Stark",
				"given": "Luke"
			},
			{
				"family": "Hoey",
				"given": "Jesse"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "berettaDetectingDiscriminatoryRisk2021",
		"type": "paper-conference",
		"abstract": "Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445940",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, Canada",
		"page": "794–804",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting discriminatory risk through data annotation based on Bayesian inferences",
		"URL": "https://doi.org/10.1145/3442188.3445940",
		"author": [
			{
				"family": "Beretta",
				"given": "Elena"
			},
			{
				"family": "Vetrò",
				"given": "Antonio"
			},
			{
				"family": "Lepri",
				"given": "Bruno"
			},
			{
				"family": "Martin",
				"given": "Juan Carlos De"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hancox-liEpistemicValuesFeature2021",
		"type": "paper-conference",
		"abstract": "As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445943",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, Canada",
		"page": "817–826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic values in feature importance methods: Lessons from feminist epistemology",
		"URL": "https://doi.org/10.1145/3442188.3445943",
		"author": [
			{
				"family": "Hancox-Li",
				"given": "Leif"
			},
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dashWhenUmpireAlso2021",
		"type": "paper-conference",
		"abstract": "Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.",
		"collection-title": "FAccT '21",
		"container-title": "Proceedings of the 2021 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3442188.3445944",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8309-7",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, Canada",
		"page": "873–884",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When the umpire is also a player: Bias in private label product recommendations on E-commerce marketplaces",
		"URL": "https://doi.org/10.1145/3442188.3445944",
		"author": [
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "roldanDirichletUncertaintyWrappers2020",
		"type": "paper-conference",
		"abstract": "Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community.In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps:Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*)   Dir(α).Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y.Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i   Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels.Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction.Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system.We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality.Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372825",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "581",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability",
		"URL": "https://doi.org/10.1145/3351095.3372825",
		"author": [
			{
				"family": "Roldán",
				"given": "José Mena"
			},
			{
				"family": "Vila",
				"given": "Oriol Pujol"
			},
			{
				"family": "Marca",
				"given": "Jordi Vitrià"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lucicWhyDoesMy2020",
		"type": "paper-conference",
		"abstract": "In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be \"black boxes,\" even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a \"black-box\" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372824",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 9\npublisher-place: Barcelona, Spain",
		"page": "90–98",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why does my model fail? contrastive local explanations for retail forecasting",
		"URL": "https://doi.org/10.1145/3351095.3372824",
		"author": [
			{
				"family": "Lucic",
				"given": "Ana"
			},
			{
				"family": "Haned",
				"given": "Hinda"
			},
			{
				"family": "Rijke",
				"given": "Maarten",
				"non-dropping-particle": "de"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sendakHumanBodyBlack2020",
		"type": "paper-conference",
		"abstract": "Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372827",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "99–109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"The human body is a black box\": supporting clinical decision-making with deep learning",
		"URL": "https://doi.org/10.1145/3351095.3372827",
		"author": [
			{
				"family": "Sendak",
				"given": "Mark"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			},
			{
				"family": "Gao",
				"given": "Michael"
			},
			{
				"family": "Futoma",
				"given": "Joseph"
			},
			{
				"family": "Ratliff",
				"given": "William"
			},
			{
				"family": "Nichols",
				"given": "Marshall"
			},
			{
				"family": "Bedoya",
				"given": "Armando"
			},
			{
				"family": "Balu",
				"given": "Suresh"
			},
			{
				"family": "O'Brien",
				"given": "Cara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "batesIntegratingFATECritical2020",
		"type": "paper-conference",
		"abstract": "There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372832",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "425–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?",
		"URL": "https://doi.org/10.1145/3351095.3372832",
		"author": [
			{
				"family": "Bates",
				"given": "Jo"
			},
			{
				"family": "Cameron",
				"given": "David"
			},
			{
				"family": "Checco",
				"given": "Alessandro"
			},
			{
				"family": "Clough",
				"given": "Paul"
			},
			{
				"family": "Hopfgartner",
				"given": "Frank"
			},
			{
				"family": "Mazumdar",
				"given": "Suvodeep"
			},
			{
				"family": "Sbaffi",
				"given": "Laura"
			},
			{
				"family": "Stordy",
				"given": "Peter"
			},
			{
				"family": "Vega de León",
				"given": "Antonio",
				"non-dropping-particle": "de la"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mustafarajCaseVotercenteredAudits2020",
		"type": "paper-conference",
		"abstract": "Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's \"related searches\" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372835",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "559–569",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The case for voter-centered audits of search engines during political elections",
		"URL": "https://doi.org/10.1145/3351095.3372835",
		"author": [
			{
				"family": "Mustafaraj",
				"given": "Eni"
			},
			{
				"family": "Lurie",
				"given": "Emma"
			},
			{
				"family": "Devine",
				"given": "Claire"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hancox-liRobustnessMachineLearning2020",
		"type": "paper-conference",
		"abstract": "The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372836",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 8\npublisher-place: Barcelona, Spain",
		"page": "640–647",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robustness in machine learning explanations: does it matter?",
		"URL": "https://doi.org/10.1145/3351095.3372836",
		"author": [
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lundgardMeasuringJusticeMachine2020",
		"type": "paper-conference",
		"abstract": "How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372838",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 1\npublisher-place: Barcelona, Spain",
		"page": "680",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring justice in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3372838",
		"author": [
			{
				"family": "Lundgard",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "joLessonsArchivesStrategies2020",
		"type": "paper-conference",
		"abstract": "A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372829",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "306–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lessons from archives: strategies for collecting sociocultural data in machine learning",
		"URL": "https://doi.org/10.1145/3351095.3372829",
		"author": [
			{
				"family": "Jo",
				"given": "Eun Seo"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "harrisonEmpiricalStudyPerceived2020",
		"type": "paper-conference",
		"abstract": "There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model \"unbiased\" and considering it \"fair.\" Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372831",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "392–402",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical study on the perceived fairness of realistic, imperfect machine learning models",
		"URL": "https://doi.org/10.1145/3351095.3372831",
		"author": [
			{
				"family": "Harrison",
				"given": "Galen"
			},
			{
				"family": "Hanson",
				"given": "Julia"
			},
			{
				"family": "Jacinto",
				"given": "Christine"
			},
			{
				"family": "Ramirez",
				"given": "Julio"
			},
			{
				"family": "Ur",
				"given": "Blase"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wieringaWhatAccountWhen2020",
		"type": "paper-conference",
		"abstract": "As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372833",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 18\npublisher-place: Barcelona, Spain",
		"page": "1–18",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability",
		"URL": "https://doi.org/10.1145/3351095.3372833",
		"author": [
			{
				"family": "Wieringa",
				"given": "Maranke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "toreiniRelationshipTrustAI2020",
		"type": "paper-conference",
		"abstract": "To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372834",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "272–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The relationship between trust in AI and trustworthy machine learning technologies",
		"URL": "https://doi.org/10.1145/3351095.3372834",
		"author": [
			{
				"family": "Toreini",
				"given": "Ehsan"
			},
			{
				"family": "Aitken",
				"given": "Mhairi"
			},
			{
				"family": "Coopamootoo",
				"given": "Kovila"
			},
			{
				"family": "Elliott",
				"given": "Karen"
			},
			{
				"family": "Zelaya",
				"given": "Carlos Gonzalez"
			},
			{
				"family": "Moorsel",
				"given": "Aad",
				"non-dropping-particle": "van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sweeneyReducingSentimentPolarity2020",
		"type": "paper-conference",
		"abstract": "The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372837",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "359–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning",
		"URL": "https://doi.org/10.1145/3351095.3372837",
		"author": [
			{
				"family": "Sweeney",
				"given": "Chris"
			},
			{
				"family": "Najafian",
				"given": "Maryam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "slackFairnessWarningsFairMAML2020",
		"type": "paper-conference",
		"abstract": "Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372839",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "200–209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness warnings and fair-MAML: learning fairly with minimal data",
		"URL": "https://doi.org/10.1145/3351095.3372839",
		"author": [
			{
				"family": "Slack",
				"given": "Dylan"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Givental",
				"given": "Emile"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "borradaileWhoseTweetsAre2020",
		"type": "paper-conference",
		"abstract": "Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372841",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "570–580",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files",
		"URL": "https://doi.org/10.1145/3351095.3372841",
		"author": [
			{
				"family": "Borradaile",
				"given": "Glencora"
			},
			{
				"family": "Burkhardt",
				"given": "Brett"
			},
			{
				"family": "LeClerc",
				"given": "Alexandria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "greenAlgorithmicRealismExpanding2020",
		"type": "paper-conference",
		"abstract": "Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as \"algorithmic formalism\" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking—\"algorithmic realism\"—that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372840",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "19–31",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic realism: expanding the boundaries of algorithmic thought",
		"URL": "https://doi.org/10.1145/3351095.3372840",
		"author": [
			{
				"family": "Green",
				"given": "Ben"
			},
			{
				"family": "Viljoen",
				"given": "Salomé"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "elzaynEffectsCompetitionRegulation2020",
		"type": "paper-conference",
		"abstract": "Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372842",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "669–679",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The effects of competition and regulation on error inequality in data-driven markets",
		"URL": "https://doi.org/10.1145/3351095.3372842",
		"author": [
			{
				"family": "Elzayn",
				"given": "Hadi"
			},
			{
				"family": "Fish",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "papakyriakopoulosBiasWordEmbeddings2020",
		"type": "paper-conference",
		"abstract": "Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372843",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "446–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in word embeddings",
		"URL": "https://doi.org/10.1145/3351095.3372843",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Hegelich",
				"given": "Simon"
			},
			{
				"family": "Serrano",
				"given": "Juan Carlos Medina"
			},
			{
				"family": "Marco",
				"given": "Fabienne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "hannaCriticalRaceMethodology2020",
		"type": "paper-conference",
		"abstract": "We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372826",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "501–512",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a critical race methodology in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3351095.3372826",
		"author": [
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Smith-Loud",
				"given": "Jamila"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "raghavanMitigatingBiasAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372828",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 13\npublisher-place: Barcelona, Spain",
		"page": "469–481",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating bias in algorithmic hiring: evaluating claims and practices",
		"URL": "https://doi.org/10.1145/3351095.3372828",
		"author": [
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "barocasHiddenAssumptionsCounterfactual2020",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established \"principal reason\" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.These \"feature-highlighting explanations\" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world—and the subjective choices necessary to compensate for this—must be understood before these techniques can be usefully implemented.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372830",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "80–89",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The hidden assumptions behind counterfactual explanations and principal reasons",
		"URL": "https://doi.org/10.1145/3351095.3372830",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Selbst",
				"given": "Andrew D."
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "washingtonWhoseSideAre2020",
		"type": "paper-conference",
		"abstract": "The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the \"social good\", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372844",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "230–240",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Whose side are ethics codes on? power, responsibility and the social good",
		"URL": "https://doi.org/10.1145/3351095.3372844",
		"author": [
			{
				"family": "Washington",
				"given": "Anne L."
			},
			{
				"family": "Kuo",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "donahueFairnessUtilizationAllocating2020",
		"type": "paper-conference",
		"abstract": "Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find — somewhat surprisingly — that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372847",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "658–668",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and utilization in allocating resources with uncertain demand",
		"URL": "https://doi.org/10.1145/3351095.3372847",
		"author": [
			{
				"family": "Donahue",
				"given": "Kate"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "blackFlipTestFairnessTesting2020",
		"type": "paper-conference",
		"abstract": "We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372845",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "111–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FlipTest: fairness testing via optimal transport",
		"URL": "https://doi.org/10.1145/3351095.3372845",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Yeom",
				"given": "Samuel"
			},
			{
				"family": "Fredrikson",
				"given": "Matt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lumImpactOverbookingPretrial2020",
		"type": "paper-conference",
		"abstract": "Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372846",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 10\npublisher-place: Barcelona, Spain",
		"page": "482–491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of overbooking on a pre-trial risk assessment tool",
		"URL": "https://doi.org/10.1145/3351095.3372846",
		"author": [
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Boudin",
				"given": "Chesa"
			},
			{
				"family": "Price",
				"given": "Megan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "ilventoMulticategoryFairnessSponsored2020",
		"type": "paper-conference",
		"abstract": "Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the \"platform utility\" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372848",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "348–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-category fairness in sponsored search auctions",
		"URL": "https://doi.org/10.1145/3351095.3372848",
		"author": [
			{
				"family": "Ilvento",
				"given": "Christina"
			},
			{
				"family": "Jagadeesan",
				"given": "Meena"
			},
			{
				"family": "Chawla",
				"given": "Shuchi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangEffectConfidenceExplanation2020",
		"type": "paper-conference",
		"abstract": "Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372852",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "295–305",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making",
		"URL": "https://doi.org/10.1145/3351095.3372852",
		"author": [
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Bellamy",
				"given": "Rachel K. E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "paniguttiDoctorXAIOntologybased2020",
		"type": "paper-conference",
		"abstract": "Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372855",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "629–639",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Doctor XAI: an ontology-based approach to black-box sequential data classification explanations",
		"URL": "https://doi.org/10.1145/3351095.3372855",
		"author": [
			{
				"family": "Panigutti",
				"given": "Cecilia"
			},
			{
				"family": "Perotti",
				"given": "Alan"
			},
			{
				"family": "Pedreschi",
				"given": "Dino"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "celisInterventionsRankingPresence2020",
		"type": "paper-conference",
		"abstract": "Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372858",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 12\npublisher-place: Barcelona, Spain",
		"page": "369–380",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interventions for ranking in the presence of implicit bias",
		"URL": "https://doi.org/10.1145/3351095.3372858",
		"author": [
			{
				"family": "Celis",
				"given": "L. Elisa"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sanchez-monederoWhatDoesIt2020",
		"type": "paper-conference",
		"abstract": "Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372849",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "458–468",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What does it mean to 'solve' the problem of discrimination in hiring? social, technical and legal perspectives from the UK on automated hiring systems",
		"URL": "https://doi.org/10.1145/3351095.3372849",
		"author": [
			{
				"family": "Sánchez-Monedero",
				"given": "Javier"
			},
			{
				"family": "Dencik",
				"given": "Lina"
			},
			{
				"family": "Edwards",
				"given": "Lilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mooreMoreRepresentativePolitics2020",
		"type": "paper-conference",
		"abstract": "Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls—such as co-option, whitewashing, and assumed universal values—we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples—calls to action, practitioner reflections, legislative engagement, direct action—we might allow engineers to better recognize both their diverse agencies and responsibilities.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372854",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "414–424",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a more representative politics in the ethics of computer science",
		"URL": "https://doi.org/10.1145/3351095.3372854",
		"author": [
			{
				"family": "Moore",
				"given": "Jared"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "liuDisparateEquilibriaAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification—including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.",
		"collection-title": "FAT* '20",
		"container-title": "Proceedings of the 2020 conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3351095.3372861",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6936-7",
		"note": "number-of-pages: 11\npublisher-place: Barcelona, Spain",
		"page": "381–391",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The disparate equilibria of algorithmic decision making when individuals invest rationally",
		"URL": "https://doi.org/10.1145/3351095.3372861",
		"author": [
			{
				"family": "Liu",
				"given": "Lydia T."
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			},
			{
				"family": "Haghtalab",
				"given": "Nika"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "saxonDisparitiesTexttoimageModel2023a",
		"type": "paper-conference",
		"abstract": "We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive “creative” generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594123",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1870",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparities in text-to-image model concept possession across languages",
		"URL": "https://doi.org/10.1145/3593013.3594123",
		"author": [
			{
				"family": "Saxon",
				"given": "Michael"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "andricReconcilingGovernmentalUse2023a",
		"type": "paper-conference",
		"abstract": "The societal and epistemological implications of online targeted advertising have been scrutinized by AI ethicists, legal scholars, and policymakers alike. However, the government’s use of online targeting and its consequential socio-political ramifications remain under-explored from a critical socio-technical standpoint. This paper investigates the socio-political implications of governmental online targeting, using a case study of the UK government’s application of such techniques for public policy objectives. We argue that this practice undermines democratic ideals, as it engenders three primary concerns — Transparency, Privacy, and Equality — that clash with fundamental democratic doctrines and values. To address these concerns, the paper introduces a preliminary blueprint for an AI governance framework that harmonizes governmental use of online targeting with certain democratic principles. Furthermore, we advocate for the creation of an independent, non-governmental regulatory body responsible for overseeing the process and monitoring the government’s use of online targeting, a critical measure for preserving democratic values.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594133",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1871–1881",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconciling governmental use of online targeting with democracy",
		"URL": "https://doi.org/10.1145/3593013.3594133",
		"author": [
			{
				"family": "Andrić",
				"given": "Katja"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "queerinaiQueerAICase2023",
		"type": "paper-conference",
		"abstract": "Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594134",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1882–1895",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Queer in AI: A case study in community-led participatory AI",
		"URL": "https://doi.org/10.1145/3593013.3594134",
		"author": [
			{
				"family": "Queerinai",
				"given": "Organizers Of"
			},
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Singh",
				"given": "Ashwin"
			},
			{
				"family": "Voelcker",
				"given": "Claas"
			},
			{
				"family": "Sutherland",
				"given": "Danica J."
			},
			{
				"family": "Locatelli",
				"given": "Davide"
			},
			{
				"family": "Breznik",
				"given": "Eva"
			},
			{
				"family": "Klubicka",
				"given": "Filip"
			},
			{
				"family": "Yuan",
				"given": "Hang"
			},
			{
				"family": "J",
				"given": "Hetvi"
			},
			{
				"family": "Zhang",
				"given": "Huan"
			},
			{
				"family": "Shriram",
				"given": "Jaidev"
			},
			{
				"family": "Lehman",
				"given": "Kruno"
			},
			{
				"family": "Soldaini",
				"given": "Luca"
			},
			{
				"family": "Sap",
				"given": "Maarten"
			},
			{
				"family": "Deisenroth",
				"given": "Marc Peter"
			},
			{
				"family": "Pacheco",
				"given": "Maria Leonor"
			},
			{
				"family": "Ryskina",
				"given": "Maria"
			},
			{
				"family": "Mundt",
				"given": "Martin"
			},
			{
				"family": "Agarwal",
				"given": "Milind"
			},
			{
				"family": "Mclean",
				"given": "Nyx"
			},
			{
				"family": "Xu",
				"given": "Pan"
			},
			{
				"family": "Pranav",
				"given": "A"
			},
			{
				"family": "Korpan",
				"given": "Raj"
			},
			{
				"family": "Ray",
				"given": "Ruchira"
			},
			{
				"family": "Mathew",
				"given": "Sarah"
			},
			{
				"family": "Arora",
				"given": "Sarthak"
			},
			{
				"family": "John",
				"given": "St"
			},
			{
				"family": "Anand",
				"given": "Tanvi"
			},
			{
				"family": "Agrawal",
				"given": "Vishakha"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Long",
				"given": "Yanan"
			},
			{
				"family": "Wang",
				"given": "Zijie J."
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Dennler",
				"given": "Nathaniel"
			},
			{
				"family": "Noseworthy",
				"given": "Michael"
			},
			{
				"family": "Jha",
				"given": "Sharvani"
			},
			{
				"family": "Baylor",
				"given": "Emi"
			},
			{
				"family": "Joshi",
				"given": "Aditya"
			},
			{
				"family": "Bilenko",
				"given": "Natalia Y."
			},
			{
				"family": "Mcnamara",
				"given": "Andrew"
			},
			{
				"family": "Gontijo-Lopes",
				"given": "Raphael"
			},
			{
				"family": "Markham",
				"given": "Alex"
			},
			{
				"family": "Dong",
				"given": "Evyn"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Saraswat",
				"given": "Manu"
			},
			{
				"family": "Vytla",
				"given": "Nikhil"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "vlasceanuInterdisciplinarityGenderDiversity2022",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) research plays an increasingly important role in society, impacting key aspects of human life. From face recognition algorithms aiding national security in airports, to software that advises judges in criminal cases, and medical staff in healthcare, AI research is shaping critical facets of our experience in the world. But who are the people and institutional bodies behind this influential research? What are the predictors of influence of AI researchers and research organizations? We study this question using social network analysis, in an exploration of the structural characteristics, i.e., network topology, of research organizations that shape modern AI. In a sample of 149 organizations with 9,987 affiliated authors of published papers in a major AI conference (NeurIPS) and two major conferences that specifically focus on societal impacts of AI (FAccT and AIES), we find that both industry and academic research organizations with influential authors are more interdisciplinary, have a greater fraction of women, are more hierarchical, and less clustered, even when controlling for the size of the organizations. The influence is operationalized as betweenness centrality in co-authorship networks, i.e., how often an author is on the shortest path connecting any pair of authors, acting as a bridge connecting otherwise distant (or even disconneted) members of the network, such as their own co-authors who are not each other’s co-author themselves. Using this operationalization, we also find that women have less influence in the AI community, determined as lower betweenness centrality in co-authorship networks. These results suggest that while diverse AI institutions are more influential, the individuals contributing to the increased diversity are marginalized in the AI field. We discuss these results in the context of current events with important societal implications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533069",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "1–10",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interdisciplinarity, gender diversity, and network structure predict the centrality of AI organizations",
		"URL": "https://doi.org/10.1145/3531146.3533069",
		"author": [
			{
				"family": "Vlasceanu",
				"given": "Madalina"
			},
			{
				"family": "Dudik",
				"given": "Miroslav"
			},
			{
				"family": "Momennejad",
				"given": "Ida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "benjaminFuckTheAlgorithmAlgorithmicImaginaries2022",
		"type": "paper-conference",
		"abstract": "This paper applies and extends the concept of algorithmic imaginaries in the context of political resistance to sociotechnical injustice. Focusing on the 2020 UK OfQual protests, the role of the ”fuck the algorithm” chant is examined as an imaginary of resistance to confront power in sociotechnical systems. The protest is analysed as a shift in algorithmic imaginaries amidst evolving uses of #FuckTheAlgorithm on social media as part of everyday practices of resistance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533072",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "46–57",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "#FuckTheAlgorithm: algorithmic imaginaries and political resistance",
		"URL": "https://doi.org/10.1145/3531146.3533072",
		"author": [
			{
				"family": "Benjamin",
				"given": "Garfield"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goyalFairnessIndicatorsSystematic2022",
		"type": "paper-conference",
		"abstract": "Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533074",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "70–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness indicators for systematic assessments of visual feature extractors",
		"URL": "https://doi.org/10.1145/3531146.3533074",
		"author": [
			{
				"family": "Goyal",
				"given": "Priya"
			},
			{
				"family": "Soriano",
				"given": "Adriana Romero"
			},
			{
				"family": "Hazirbas",
				"given": "Caner"
			},
			{
				"family": "Sagun",
				"given": "Levent"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "asherWhenLearningBecomes2022",
		"type": "paper-conference",
		"abstract": "We formally analyze an epistemic bias we call interpretive blindness (IB), in which under certain conditions a learner will be incapable of learning. IB is now common in our society, but it is a natural consequence of Bayesian inference and what we argue are mild assumptions about the relation between belief and evidence. IB a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a codependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic of contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533078",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "107–116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When learning becomes impossible",
		"URL": "https://doi.org/10.1145/3531146.3533078",
		"author": [
			{
				"family": "Asher",
				"given": "Nicholas"
			},
			{
				"family": "Hunter",
				"given": "Julie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "papadakiMinimaxDemographicGroup2022",
		"type": "paper-conference",
		"abstract": "Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533081",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "142–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Minimax demographic group fairness in federated learning",
		"URL": "https://doi.org/10.1145/3531146.3533081",
		"author": [
			{
				"family": "Papadaki",
				"given": "Afroditi"
			},
			{
				"family": "Martinez",
				"given": "Natalia"
			},
			{
				"family": "Bertran",
				"given": "Martin"
			},
			{
				"family": "Sapiro",
				"given": "Guillermo"
			},
			{
				"family": "Rodrigues",
				"given": "Miguel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "struppekLearningBreakDeep2022",
		"type": "paper-conference",
		"abstract": "Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533073",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "58–69",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to break deep perceptual hashing: The use case NeuralHash",
		"URL": "https://doi.org/10.1145/3531146.3533073",
		"author": [
			{
				"family": "Struppek",
				"given": "Lukas"
			},
			{
				"family": "Hintersdorf",
				"given": "Dominik"
			},
			{
				"family": "Neider",
				"given": "Daniel"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangProvidingItemsideIndividual2022",
		"type": "paper-conference",
		"abstract": "Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et&nbsp;al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533079",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "117–127",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Providing item-side individual fairness for deep recommender systems",
		"URL": "https://doi.org/10.1145/3531146.3533079",
		"author": [
			{
				"family": "Wang",
				"given": "Xiuling"
			},
			{
				"family": "Wang",
				"given": "Wendy Hui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hongDynamicPrivacyBudget2022",
		"type": "paper-conference",
		"abstract": "Protecting privacy in learning while maintaining the model performance has become increasingly critical in many applications that involve sensitive data. A popular private learning framework is differentially private learning composed of many privatized gradient iterations by noising and clipping. Under the privacy constraint, it has been shown that the dynamic policies could improve the final iterate loss, namely the quality of published models. In this talk, we will introduce these dynamic techniques for learning rate, batch size, noise magnitude and gradient clipping. Also, we discuss how the dynamic policy could change the convergence bounds which further provides insight of the impact of dynamic methods.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533070",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 25\npublisher-place: Seoul, Republic of Korea",
		"page": "11–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dynamic privacy budget allocation improves data efficiency of differentially private gradient descent",
		"URL": "https://doi.org/10.1145/3531146.3533070",
		"author": [
			{
				"family": "Hong",
				"given": "Junyuan"
			},
			{
				"family": "Wang",
				"given": "Zhangyang"
			},
			{
				"family": "Zhou",
				"given": "Jiayu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "longoniNewsGenerativeArtificial2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533077",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "97–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "News from generative artificial intelligence is believed less",
		"URL": "https://doi.org/10.1145/3531146.3533077",
		"author": [
			{
				"family": "Longoni",
				"given": "Chiara"
			},
			{
				"family": "Fradkin",
				"given": "Andrey"
			},
			{
				"family": "Cian",
				"given": "Luca"
			},
			{
				"family": "Pennycook",
				"given": "Gordon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "engelmannWhatPeopleThink2022",
		"type": "paper-conference",
		"abstract": "Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533080",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "128–141",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What people think AI should infer from faces",
		"URL": "https://doi.org/10.1145/3531146.3533080",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Ullstein",
				"given": "Chiara"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chapmanDatadrivenAnalysisInterplay2022",
		"type": "paper-conference",
		"abstract": "Previous studies have focused on the biases and feedback loops that occur in predictive policing algorithms. These studies show how systemically and institutionally biased data leads to these feedback loops when predictive policing algorithms are applied in real life. We take a step back, and show that the choice in algorithm can be embedded in a specific criminological theory, and that the choice of a model on its own even without biased data can create biased feedback loops. By synthesizing “historical” data, in which we control the relationships between crimes, location and time, we show that the current predictive policing algorithms create biased feedback loops even with completely random data. We then review the process of creation and deployment of these predictive systems, and highlight when good practices, such as fitting a model to data, “go bad” within the context of larger system development and deployment. Using best practices from previous work on assessing and mitigating the impact of new technologies, we highlight where the design of these algorithms has broken down. The study also found that multidisciplinary analysis of such systems is vital for uncovering these issues and shows that any study of equitable AI should involve a systematic and holistic analysis of their design rationalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533071",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "36–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A Data-driven analysis of the interplay between Criminological theory and predictive policing algorithms",
		"URL": "https://doi.org/10.1145/3531146.3533071",
		"author": [
			{
				"family": "Chapman",
				"given": "Adriane"
			},
			{
				"family": "Grylls",
				"given": "Philip"
			},
			{
				"family": "Ugwudike",
				"given": "Pamela"
			},
			{
				"family": "Gammack",
				"given": "David"
			},
			{
				"family": "Ayling",
				"given": "Jacqui"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "simbeckFAccTCheckAIRegulation2022",
		"type": "paper-conference",
		"abstract": "In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533076",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "89–96",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FAccT-Check on AI regulation: Systematic evaluation of AI regulation on the example of the legislation on the use of AI in the public sector in the german federal state of schleswig-holstein",
		"URL": "https://doi.org/10.1145/3531146.3533076",
		"author": [
			{
				"family": "Simbeck",
				"given": "Katharina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "singhAutomatingCareOnline2022",
		"type": "paper-conference",
		"abstract": "On March 23, 2020, the Government of India (GoI) announced one of the strictest nationwide lockdowns in the world to curb the spread of novel SARS-CoV-2, otherwise known as CoVID-19. The country came to a standstill overnight and the service industry, including small businesses and restaurants, took a massive financial hit. The unknown nature of the virus and its spread deepened anxiety among the general public, quickly turning to distrust towards any “outside” contact with goods and people. In the hopes of (re)building consumer trust, food delivery platforms Zomato and Swiggy began providing digital solutions to exhibit care towards their customers, including: (1) sharing delivery workers’ live temperatures alongside the workers’ profile inside the app; (2) mandating the use of the controversial contact tracing app Aarogya Setu for the workers; (3) monitoring workers’ usage of masks through random selfie requests; and (4) sharing specific worker vaccination details on the app for customers to view, including vaccination date and the vaccine’s serial number. Such invasive data gathering infrastructures to address public health threats have long focused on the surveillance of laborers, migrants, and the bodies of other marginalized communities. Framed as public health management, such biometric and health data gathering is treated as a necessary feature of caring for the well-being of the general public. However, such datafication practices - ones which primarily focus on the extraction of data from one specific community in order to mollify the concerns of another - normalizes the false perception that disease is transmitted unidirectionally: from worker to the consumer. By centering food delivery workers’ experiences during the pandemic and examining the normalization of such surveillance in the name of care and recovery, this paper aims to examine how new regimes of care are manufactured and legitimized using harmful and unethical datafication practices.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533082",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "160–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automating care: Online food delivery work during the CoVID-19 crisis in india",
		"URL": "https://doi.org/10.1145/3531146.3533082",
		"author": [
			{
				"family": "Singh",
				"given": "Anubha"
			},
			{
				"family": "Park",
				"given": "Tina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "weidingerTaxonomyRisksPosed2022",
		"type": "paper-conference",
		"abstract": "Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533088",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "214–229",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taxonomy of risks posed by language models",
		"URL": "https://doi.org/10.1145/3531146.3533088",
		"author": [
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Huang",
				"given": "Po-Sen"
			},
			{
				"family": "Mellor",
				"given": "John"
			},
			{
				"family": "Glaese",
				"given": "Amelia"
			},
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "Balle",
				"given": "Borja"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Biles",
				"given": "Courtney"
			},
			{
				"family": "Brown",
				"given": "Sasha"
			},
			{
				"family": "Kenton",
				"given": "Zac"
			},
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Stepleton",
				"given": "Tom"
			},
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Rimell",
				"given": "Laura"
			},
			{
				"family": "Isaac",
				"given": "William"
			},
			{
				"family": "Haas",
				"given": "Julia"
			},
			{
				"family": "Legassick",
				"given": "Sean"
			},
			{
				"family": "Irving",
				"given": "Geoffrey"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hutiriBiasAutomatedSpeaker2022",
		"type": "paper-conference",
		"abstract": "Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533089",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "230–247",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias in automated speaker recognition",
		"URL": "https://doi.org/10.1145/3531146.3533089",
		"author": [
			{
				"family": "Hutiri",
				"given": "Wiebke Toussaint"
			},
			{
				"family": "Ding",
				"given": "Aaron Yi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bellItsJustNot2022",
		"type": "paper-conference",
		"abstract": "To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533090",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "248–266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "It’s just not that simple: An empirical study of the accuracy-explainability trade-off in machine learning for public policy",
		"URL": "https://doi.org/10.1145/3531146.3533090",
		"author": [
			{
				"family": "Bell",
				"given": "Andrew"
			},
			{
				"family": "Solano-Kamaiko",
				"given": "Ian"
			},
			{
				"family": "Nov",
				"given": "Oded"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tedjopurnomoEquitablePublicBus2022",
		"type": "paper-conference",
		"abstract": "Public bus transport is a major backbone of many cities’ socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore’s public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric’s real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore’s bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533092",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "278–288",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equitable public bus network optimization for social good: A case study of singapore",
		"URL": "https://doi.org/10.1145/3531146.3533092",
		"author": [
			{
				"family": "Tedjopurnomo",
				"given": "David"
			},
			{
				"family": "Bao",
				"given": "Zhifeng"
			},
			{
				"family": "Choudhury",
				"given": "Farhana"
			},
			{
				"family": "Luo",
				"given": "Hui"
			},
			{
				"family": "Qin",
				"given": "A. K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fraserAIOpacityExplainability2022",
		"type": "paper-conference",
		"abstract": "A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533084",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "185–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI opacity and explainability in tort litigation",
		"URL": "https://doi.org/10.1145/3531146.3533084",
		"author": [
			{
				"family": "Fraser",
				"given": "Henry"
			},
			{
				"family": "Simcock",
				"given": "Rhyle"
			},
			{
				"family": "Snoswell",
				"given": "Aaron J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "luccioniFrameworkDeprecatingDatasets2022",
		"type": "paper-conference",
		"abstract": "Datasets are central to training machine learning (ML) models. The ML community has recently made significant improvements to data stewardship and documentation practices across the model development life cycle. However, the act of deprecating, or deleting, datasets has been largely overlooked, and there are currently no standardized approaches for structuring this stage of the dataset life cycle. In this paper, we study the practice of dataset deprecation in ML, identify several cases of datasets that continued to circulate despite having been deprecated, and describe the different technical, legal, ethical, and organizational issues raised by such continuations. We then propose a Dataset Deprecation Framework that includes considerations of risk, mitigation of impact, appeal mechanisms, timeline, post-deprecation protocols, and publication checks that can be adapted and implemented by the ML community. Finally, we propose creating a centralized, sustainable repository system for archiving datasets, tracking dataset modifications or deprecations, and facilitating practices of care and stewardship that can be integrated into research and publication processes.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533086",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "199–212",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for deprecating datasets: Standardizing documentation, identification, and communication",
		"URL": "https://doi.org/10.1145/3531146.3533086",
		"author": [
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Corry",
				"given": "Frances"
			},
			{
				"family": "Sridharan",
				"given": "Hamsini"
			},
			{
				"family": "Ananny",
				"given": "Mike"
			},
			{
				"family": "Schultz",
				"given": "Jason"
			},
			{
				"family": "Crawford",
				"given": "Kate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sikdarGetFairGeneralizedFairness2022",
		"type": "paper-conference",
		"abstract": "We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533094",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "289–299",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GetFair: Generalized fairness tuning of classification models",
		"URL": "https://doi.org/10.1145/3531146.3533094",
		"author": [
			{
				"family": "Sikdar",
				"given": "Sandipan"
			},
			{
				"family": "Lemmerich",
				"given": "Florian"
			},
			{
				"family": "Strohmaier",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "meyerFlippingScriptCriminal2022",
		"type": "paper-conference",
		"abstract": "In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that “flips the script.” Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were “especially lengthy.” We then use a second-stage model to predict the defendant’s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533104",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "366–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants",
		"URL": "https://doi.org/10.1145/3531146.3533104",
		"author": [
			{
				"family": "Meyer",
				"given": "Mikaela"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Marshall",
				"given": "Erica"
			},
			{
				"family": "Lum",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "jakeschHowDifferentGroups2022",
		"type": "paper-conference",
		"abstract": "Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533097",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "310–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How different groups prioritize ethical values for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533097",
		"author": [
			{
				"family": "Jakesch",
				"given": "Maurice"
			},
			{
				"family": "Buçinca",
				"given": "Zana"
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rothOutcomeTestDiscrimination2022",
		"type": "paper-conference",
		"abstract": "This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533102",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 7\npublisher-place: Seoul, Republic of Korea",
		"page": "350–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An outcome test of discrimination for ranked lists",
		"URL": "https://doi.org/10.1145/3531146.3533102",
		"author": [
			{
				"family": "Roth",
				"given": "Jonathan"
			},
			{
				"family": "Saint-Jacques",
				"given": "Guillaume"
			},
			{
				"family": "Yu",
				"given": "YinYin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "milliCausalInferenceStruggles2022",
		"type": "paper-conference",
		"abstract": "Online platforms regularly conduct randomized experiments to understand how changes to the platform causally affect various outcomes of interest. However, experimentation on online platforms has been criticized for having, among other issues, a lack of meaningful oversight and user consent. As platforms give users greater agency, it becomes possible to conduct observational studies in which users self-select into the treatment of interest as an alternative to experiments in which the platform controls whether the user receives treatment or not. In this paper, we conduct four large-scale within-study comparisons on Twitter aimed at assessing the effectiveness of observational studies derived from user self-selection on online platforms. In a within-study comparison, treatment effects from an observational study are assessed based on how effectively they replicate results from a randomized experiment with the same target population. We test the naive difference in group means estimator, exact matching, regression adjustment, and inverse probability of treatment weighting while controlling for plausible confounding variables. In all cases, all observational estimates perform poorly at recovering the ground-truth estimate from the analogous randomized experiments. In all cases except one, the observational estimates have the opposite sign of the randomized estimate. Our results suggest that observational studies derived from user self-selection are a poor alternative to randomized experimentation on online platforms. In discussing our results, we postulate a “Catch-22” that suggests that the success of causal inference in these settings may be at odds with the original motivations for providing users with greater agency.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533103",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "357–365",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal inference struggles with agency on online platforms",
		"URL": "https://doi.org/10.1145/3531146.3533103",
		"author": [
			{
				"family": "Milli",
				"given": "Smitha"
			},
			{
				"family": "Belli",
				"given": "Luca"
			},
			{
				"family": "Hardt",
				"given": "Moritz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lumDebiasingBiasMeasurement2022",
		"type": "paper-conference",
		"abstract": "When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533105",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "379–389",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "De-biasing “bias” measurement",
		"URL": "https://doi.org/10.1145/3531146.3533105",
		"author": [
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Bower",
				"given": "Amanda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "birhaneValuesEncodedMachine2022",
		"type": "paper-conference",
		"abstract": "Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533083",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "173–184",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The values encoded in machine learning research",
		"URL": "https://doi.org/10.1145/3531146.3533083",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Kalluri",
				"given": "Pratyusha"
			},
			{
				"family": "Card",
				"given": "Dallas"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Bao",
				"given": "Michelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gradwohlParetoimprovingDataSharing2022",
		"type": "paper-conference",
		"abstract": "We study the effects of data sharing between firms on prices, profits, and consumer welfare.Although indiscriminate sharing of consumer data decreases firm profits due to the subsequentincrease in competition, selective sharing can be beneficial. We show that there are data-sharing mechanisms that are strictly Pareto-improving, simultaneously increasing firm profitsand consumer welfare. Within the class of Pareto-improving mechanisms, we identify one thatmaximizes firm profits and one that maximizes consumer welfare.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533085",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 2\npublisher-place: Seoul, Republic of Korea",
		"page": "197–198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Pareto-improving Data-Sharing",
		"URL": "https://doi.org/10.1145/3531146.3533085",
		"author": [
			{
				"family": "Gradwohl",
				"given": "Ronen"
			},
			{
				"family": "Tennenholtz",
				"given": "Moshe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kallusTreatmentEffectRisk2022",
		"type": "paper-conference",
		"abstract": "Since the average treatment effect (ATE) measures the change in social welfare, even if positive, there is a risk of negative effect on, say, some 10% of the population. Assessing such risk is difficult, however, because any one individual treatment effect (ITE) is never observed so the 10% worst-affected cannot be identified, while distributional treatment effects only compare the first deciles within each treatment group, which does not correspond to any 10%-subpopulation. In this paper we consider how to nonetheless assess this important risk measure, formalized as the conditional value at risk (CVaR) of the ITE-distribution. We leverage the availability of pre-treatment covariates and characterize the tightest-possible upper and lower bounds on ITE-CVaR given by the covariate-conditional average treatment effect (CATE) function. We then proceed to study how to estimate these bounds efficiently from data and construct confidence intervals. This is challenging even in randomized experiments as it requires understanding the distribution of the unknown CATE function, which can be very complex if we use rich covariates so as to best control for heterogeneity. We develop a debiasing method that overcomes this and prove it enjoys favorable statistical properties even when CATE and other nuisances are estimated by black-box machine learning or even inconsistently. Studying a hypothetical change to French job-search counseling services, our bounds and inference demonstrate a small social benefit entails a negative impact on a substantial subpopulation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533087",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "213",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Treatment effect risk: Bounds and inference",
		"URL": "https://doi.org/10.1145/3531146.3533087",
		"author": [
			{
				"family": "Kallus",
				"given": "Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lauferFourYearsFAccT2022",
		"type": "paper-conference",
		"abstract": "Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533107",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 26\npublisher-place: Seoul, Republic of Korea",
		"page": "401–426",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Four years of FAccT: A reflexive, mixed-methods analysis of research contributions, shortcomings, and future prospects",
		"URL": "https://doi.org/10.1145/3531146.3533107",
		"author": [
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Jain",
				"given": "Sameer"
			},
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "goetzeMindGapAutonomous2022",
		"type": "paper-conference",
		"abstract": "When a computer system causes harm, who is responsible? This question has renewed significance given the proliferation of autonomous systems enabled by modern artificial intelligence techniques. At the root of this problem is a philosophical difficulty known in the literature as the responsibility gap. That is to say, because of the causal distance between the designers of autonomous systems and the eventual outcomes of those systems, the dilution of agency within the large and complex teams that design autonomous systems, and the impossibility of fully predicting how autonomous systems will behave once deployed, determining who is morally responsible for harms caused by autonomous systems is unclear at a conceptual level. I review past work on this topic, criticizing prior works for suggesting workarounds rather than philosophical answers to the conceptual problem presented by the responsibility gap. The view I develop, drawing on my earlier work on vicarious moral responsibility, explains why computing professionals are ethically required to take responsibility for the systems they design, despite not being blameworthy for the harms these systems may cause.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533106",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "390–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mind the gap: Autonomous systems, the responsibility gap, and moral entanglement",
		"URL": "https://doi.org/10.1145/3531146.3533106",
		"author": [
			{
				"family": "Goetze",
				"given": "Trystan S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "crisanInteractiveModelCards2022",
		"type": "paper-conference",
		"abstract": "Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability &amp; interpretability; sensemaking &amp; skepticism; and trust &amp; safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533108",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "427–439",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interactive model cards: A human-centered approach to model documentation",
		"URL": "https://doi.org/10.1145/3531146.3533108",
		"author": [
			{
				"family": "Crisan",
				"given": "Anamaria"
			},
			{
				"family": "Drouhard",
				"given": "Margaret"
			},
			{
				"family": "Vig",
				"given": "Jesse"
			},
			{
				"family": "Rajani",
				"given": "Nazneen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "boagTechWorkerOrganizing2022",
		"type": "paper-conference",
		"abstract": "In recent years, there has been a growing interest in the field of “AI Ethics” and related areas. This field is purposefully broad, allowing for the intersection of numerous subfields and disciplines. However, a lot of work in this area thus far has centered computational methods, leading to a narrow lens where technical tools are framed as solutions for broader sociotechnical problems. In this work, we discuss a less-explored mode of what it can mean to “do” AI Ethics: tech worker collective action. Through collective action, the employees of powerful tech companies can act as a countervailing force against strong corporate impulses to grow or make a profit to the detriment of other values. In this work, we ground these efforts in existing scholarship of social movements and labor organizing. We characterize 150 documented collective actions, and explore several case studies of successful campaigns. Looking forward, we also identify under-explored types of actions, and provide conceptual frameworks and inspiration for how to utilize worker organizing as an effective lever for change.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533111",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "452–463",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tech worker organizing for power and accountability",
		"URL": "https://doi.org/10.1145/3531146.3533111",
		"author": [
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Lepe",
				"given": "Bianca"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dengExploringHowMachine2022",
		"type": "paper-conference",
		"abstract": "Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533113",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "473–484",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring how machine learning practitioners (try to) use fairness toolkits",
		"URL": "https://doi.org/10.1145/3531146.3533113",
		"author": [
			{
				"family": "Deng",
				"given": "Wesley Hanwen"
			},
			{
				"family": "Nagireddy",
				"given": "Manish"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kongAreIntersectionallyFair2022",
		"type": "paper-conference",
		"abstract": "A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is “intersectionally fair” if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533114",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "485–494",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Are “Intersectionally fair” AI algorithms really fair to women of color? A philosophical analysis",
		"URL": "https://doi.org/10.1145/3531146.3533114",
		"author": [
			{
				"family": "Kong",
				"given": "Youjin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangAffirmativeAlgorithmsRelational2022",
		"type": "paper-conference",
		"abstract": "Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533115",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "495–507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Affirmative algorithms: Relational equality as algorithmic fairness",
		"URL": "https://doi.org/10.1145/3531146.3533115",
		"author": [
			{
				"family": "Zhang",
				"given": "Marilyn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kollnigGoodbyeTrackingImpact2022",
		"type": "paper-conference",
		"abstract": "Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules. We find that Apple’s new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users&nbsp;–&nbsp;a positive development for privacy. However, the number of tracking libraries has&nbsp;–&nbsp;on average&nbsp;–&nbsp;roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple’s policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring from its new tracking rules. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading, especially in less popular apps. Overall, our observations suggest that, while Apple’s changes make tracking individual users more difficult, they motivate a countermovement, and reinforce existing market power of gatekeeper companies with access to large troves of first-party data. Making the privacy properties of apps transparent through large-scale analysis remains a difficult target for independent researchers, and a key obstacle to meaningful, accountable and verifiable privacy protections.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533116",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "508–520",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Goodbye tracking? Impact of iOS app tracking transparency and privacy labels",
		"URL": "https://doi.org/10.1145/3531146.3533116",
		"author": [
			{
				"family": "Kollnig",
				"given": "Konrad"
			},
			{
				"family": "Shuba",
				"given": "Anastasia"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Shadbolt",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shenModelCardAuthoring2022",
		"type": "paper-conference",
		"abstract": "There have been increasing calls for centering impacted communities – both online and offline – in the design of the AI systems that will be deployed in their communities. However, the complicated nature of a community’s goals and needs, as well as the complexity of AI’s development procedures, outputs, and potential impacts, often prevents effective participation. In this paper, we present the Model Card Authoring Toolkit, a toolkit that supports community members to understand, navigate and negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values. Through a series of workshops, we conduct an empirical investigation of the initial effectiveness of our approach in two online communities – English and Dutch Wikipedia, and document how our participants collectively set the threshold for a machine learning based quality prediction system used in their communities’ content moderation applications. Our results suggest that the use of the Model Card Authoring Toolkit helps improve the understanding of the trade-offs across multiple community goals on AI design, engage community members to discuss and negotiate the trade-offs, and facilitate collective and informed decision-making in their own community contexts. Finally, we discuss the challenges for a community-centered, deliberation-driven approach for AI design as well as potential design implications.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533110",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "440–451",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The model card authoring toolkit: Toward community-centered, deliberation-driven AI design",
		"URL": "https://doi.org/10.1145/3531146.3533110",
		"author": [
			{
				"family": "Shen",
				"given": "Hong"
			},
			{
				"family": "Wang",
				"given": "Leijie"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Brusse",
				"given": "Ciell"
			},
			{
				"family": "Velgersdijk",
				"given": "Ronald"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chowdhuryEquiexplanationMapsConcise2022",
		"type": "paper-conference",
		"abstract": "We attempt to summarize the model logic of a black-box classification model in order to generate concise and informative global explanations. We propose equi-explanation maps, a new explanation data-structure that presents the region of interest as a union of equi-explanation subspaces along with their explanation vectors. We then propose E-Map, a method to generate equi-explanation maps. We demonstrate the broad utility of our approach by generating equi-explanation maps for various binary classification models (Logistic Regression, SVM, MLP, and XGBoost) on the UCI Heart disease dataset and the Pima Indians diabetes dataset. Each subspace in our generated map is the union of d-dimensional hyper-cuboids which can be compactly represented for the sake of interpretability. For each of these subspaces, we present linear explanations assigning a weight to each explanation feature. We justify the use of equi-explanation maps in comparison to other global explanation methods by evaluating in terms of interpretability, fidelity, and informativeness. A user study further corroborates the use of equi-explanation maps to generate compact and informative global explanations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533112",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 9\npublisher-place: Seoul, Republic of Korea",
		"page": "464–472",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equi-explanation maps: Concise and informative global summary explanations",
		"URL": "https://doi.org/10.1145/3531146.3533112",
		"author": [
			{
				"family": "Chowdhury",
				"given": "Tanya"
			},
			{
				"family": "Rahimi",
				"given": "Razieh"
			},
			{
				"family": "Allan",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "marklLanguageVariationAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533117",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "521–534",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition",
		"URL": "https://doi.org/10.1145/3531146.3533117",
		"author": [
			{
				"family": "Markl",
				"given": "Nina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yurritaMultistakeholderValuebasedAssessment2022",
		"type": "paper-conference",
		"abstract": "In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533118",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 29\npublisher-place: Seoul, Republic of Korea",
		"page": "535–563",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a multi-stakeholder value-based assessment framework for algorithmic systems",
		"URL": "https://doi.org/10.1145/3531146.3533118",
		"author": [
			{
				"family": "Yurrita",
				"given": "Mireia"
			},
			{
				"family": "Murray-Rust",
				"given": "Dave"
			},
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Bozzon",
				"given": "Alessandro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "midhaEthicalConcernsPerceptions2022",
		"type": "paper-conference",
		"abstract": "With rapid growth in the development of consumer neurotechnology, it is imperative to consider the ethical implications that this might have in order to minimise consumer harm. Whilst ethical and legal guidelines for commercialisation have previously been suggested, we aimed to further this discussion by investigating the ethical concerns held by potential end users of consumer neurotechnology. 19 participants who had previously experienced mental workload tracking in their daily lives were interviewed about their ethical concerns and perceptions of this type of future neurotechnology. An Interpretive Phenomenological Analysis (IPA) approach identified three superordinate themes. These related to concerns surrounding privacy, data validity and misinterpretation, and personal identity. The findings provide further validation for previous research and highlight further ethical considerations that should be factored into the commercialisation of neurotechnology.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533119",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "564–573",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical concerns and perceptions of consumer neurotechnology from lived experiences of mental workload tracking",
		"URL": "https://doi.org/10.1145/3531146.3533119",
		"author": [
			{
				"family": "Midha",
				"given": "Serena"
			},
			{
				"family": "Wilson",
				"given": "Max L."
			},
			{
				"family": "Sharples",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "doniaNormativeLogicsAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "The relevance of algorithms in contemporary life is often appreciated when they ‘fail’—either because they did not perform as expected, or because they led to outcomes that were later determined to be unacceptable. As a result, academic, policy, and public discourse has increasingly emphasized accountability as a desirable, if not elusive, feature of system design, and component of effective governance. Accountability, however, is a versatile concept that has been operationalized in a number of ways across different use-contexts, policy settings, and research disciplines. While accountability is often framed as a normative good, it is unclear exactly what kind of normative work it is expected to do, and how it is expected to do it. Informed by perspectives from critical data studies and science and technology studies, this article introduces five normative logics underpinning discussions of algorithmic accountability that appear in the academic research literature: (1) accountability as verification, (2) accountability as representation, (3) accountability as social licence, (4) accountability as fiduciary duty, and (5) accountability as legal compliance. These normative logics, and the resulting rules, codes, and practices that constitute an emerging set of algorithmic accountability regimes, are especially discussed in terms of the presumed agency of actors involved. The article suggests that implicit assumptions characterizing each of ‘algorithms’ and ‘accountability’ are highly significant for each other, and that more explicit acknowledgement of this can lead to improved understanding of the diverse knowledge claims and practical goals associated with different logics of algorithmic accountability, and relatedly, the agency of different actors to pursue it in its different forms. Link to full text: josephdonia.com/facct-2022",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533123",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Normative logics of algorithmic accountability",
		"URL": "https://doi.org/10.1145/3531146.3533123",
		"author": [
			{
				"family": "Donia",
				"given": "Joseph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mehrotraSelectionPresenceImplicit2022",
		"type": "paper-conference",
		"abstract": "In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533124",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "599–609",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Selection in the presence of implicit bias: The advantage of intersectional constraints",
		"URL": "https://doi.org/10.1145/3531146.3533124",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Pradelski",
				"given": "Bary S. R."
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schuffHumanInterpretationSaliencybased2022",
		"type": "paper-conference",
		"abstract": "While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople’s interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees’ importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533127",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 26\npublisher-place: Seoul, Republic of Korea",
		"page": "611–636",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human interpretation of saliency-based explanation over text",
		"URL": "https://doi.org/10.1145/3531146.3533127",
		"author": [
			{
				"family": "Schuff",
				"given": "Hendrik"
			},
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Adel",
				"given": "Heike"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Vu",
				"given": "Ngoc Thang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghoshSubvertingFairImage2022",
		"type": "paper-conference",
		"abstract": "In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model&nbsp;[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533128",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "637–650",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Subverting fair image search with generative adversarial perturbations",
		"URL": "https://doi.org/10.1145/3531146.3533128",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Jagielski",
				"given": "Matthew"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "salemDontLetRicci2022",
		"type": "paper-conference",
		"abstract": "Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action. In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533129",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 16\npublisher-place: Seoul, Republic of Korea",
		"page": "651–666",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Don’t let ricci v. DeStefano hold you back: A bias-aware legal solution to the hiring paradox",
		"URL": "https://doi.org/10.1145/3531146.3533129",
		"author": [
			{
				"family": "Salem",
				"given": "Jad"
			},
			{
				"family": "Desai",
				"given": "Deven"
			},
			{
				"family": "Gupta",
				"given": "Swati"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sureshIntersectionalFeministParticipatory2022",
		"type": "paper-conference",
		"abstract": "Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide&nbsp;—&nbsp;gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process&nbsp;—&nbsp;with quantitative, qualitative and participatory steps&nbsp;—&nbsp;focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533132",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "667–678",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards intersectional feminist and participatory ML: A case study in supporting feminicide counterdata collection",
		"URL": "https://doi.org/10.1145/3531146.3533132",
		"author": [
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Movva",
				"given": "Rajiv"
			},
			{
				"family": "Dogan",
				"given": "Amelia Lee"
			},
			{
				"family": "Bhargava",
				"given": "Rahul"
			},
			{
				"family": "Cruxen",
				"given": "Isadora"
			},
			{
				"family": "Cuba",
				"given": "Angeles Martinez"
			},
			{
				"family": "Taurino",
				"given": "Guilia"
			},
			{
				"family": "So",
				"given": "Wonyoung"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "niuBestVsAll2022",
		"type": "paper-conference",
		"abstract": "We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533121",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "574–586",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Best vs. All: Equity and accuracy of standardized test score reporting",
		"URL": "https://doi.org/10.1145/3531146.3533121",
		"author": [
			{
				"family": "Niu",
				"given": "Mingzi"
			},
			{
				"family": "Kannan",
				"given": "Sampath"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Vohra",
				"given": "Rakesh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "smithREALMLRecognizing2022",
		"type": "paper-conference",
		"abstract": "Transparency around limitations can improve the scientific rigor of research, help ensure appropriate interpretation of research findings, and make research claims more credible. Despite these benefits, the machine learning (ML) research community lacks well-developed norms around disclosing and discussing limitations. To address this gap, we conduct an iterative design process with 30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided activities to help ML researchers recognize, explore, and articulate the limitations of their research. Using a three-stage interview and survey study, we identify ML researchers’ perceptions of limitations, as well as the challenges they face when recognizing, exploring, and articulating limitations. We develop REAL ML to address some of these practical challenges, and highlight additional cultural challenges that will require broader shifts in community norms to address. We hope our study and REAL ML help move the ML research community toward more active and appropriate engagement with limitations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533122",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "587–597",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "REAL ML: Recognizing, exploring, and articulating limitations of machine learning research",
		"URL": "https://doi.org/10.1145/3531146.3533122",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			},
			{
				"family": "Wortman Vaughan",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fongFairnessAUCFeature2022",
		"type": "paper-conference",
		"abstract": "We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type&nbsp;I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533126",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "610",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness for AUC via feature augmentation",
		"URL": "https://doi.org/10.1145/3531146.3533126",
		"author": [
			{
				"family": "Fong",
				"given": "Hortense"
			},
			{
				"family": "Kumar",
				"given": "Vineet"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Vishnoi",
				"given": "Nisheeth K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "birhaneForgottenMarginsAI2022",
		"type": "paper-conference",
		"abstract": "How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "948–958",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The forgotten margins of AI ethics",
		"URL": "https://doi.org/10.1145/3531146.3533157",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Ruane",
				"given": "Elayne"
			},
			{
				"family": "Laurent",
				"given": "Thomas"
			},
			{
				"family": "S. Brown",
				"given": "Matthew"
			},
			{
				"family": "Flowers",
				"given": "Johnathan"
			},
			{
				"family": "Ventresque",
				"given": "Anthony"
			},
			{
				"family": "L. Dancy",
				"given": "Christopher"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "geddesDeathLegalSubject2022",
		"type": "paper-conference",
		"abstract": "This paper explores the epistemological differences between the socio-political legal subject of Western liberalism, and the algorithmic subject of informational capitalism. It argues that the increasing use of predictive algorithms in judicial decision-making is reconstructing both the nature and experience of legal subjectivity in a manner that is incompatible with law's normative commitments to individualized justice. Whereas algorithmic subjectivity derives its epistemic authority from population-level insights, legal subjectivity has historically derived credibility from its close approximation of the underlying individual, through careful evaluation of their mental and physical autonomy, prior to any assignment of legal liability. With the introduction of predictive algorithms in judicial decision-making, knowledge about the legal subject is increasingly algorithmically produced, in a manner that discounts, and effectively displaces, qualitative knowledge about the legal subject's intentions, motivations, and moral capabilities. This results in the death of the legal subject, or the emergence of new, algorithmic practices of signification that no longer require the input of the underlying individual. As algorithms increasingly guide judicial decision-making, the shifting epistemology of legal subjectivity has long-term consequences for the legitimacy of legal institutions.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533134",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "691–701",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The death of the legal subject: How predictive algorithms are (re)constructing legal subjectivity",
		"URL": "https://doi.org/10.1145/3531146.3533134",
		"author": [
			{
				"family": "Geddes",
				"given": "Katrina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangAttributePrivacyFramework2022",
		"type": "paper-conference",
		"abstract": "Ensuring the privacy of training data is a growing concern since many machine learning models are trained on confidential and potentially sensitive data. Much attention has been devoted to methods for protecting individual privacy during analyses of large datasets. However in many settings, global properties of the dataset may also be sensitive (e.g., mortality rate in a hospital rather than presence of a particular patient in the dataset). In this work, we depart from individual privacy to initiate the study of attribute privacy, where a data owner is concerned about revealing sensitive properties of a whole dataset during analysis. We propose definitions to capture attribute privacy in two relevant cases where global attributes may need to be protected: (1) properties of a specific dataset and (2) parameters of the underlying distribution from which dataset is sampled. We also provide two efficient mechanisms for specific data distributions and one general but inefficient mechanism that satisfy attribute privacy for these settings. We base our results on a novel and non-trivial use of the Pufferfish framework to account for correlations across attributes in the data, thus addressing “the challenging problem of developing Pufferfish instantiations and algorithms for general aggregate secrets” that was left open by Kifer and Machanavajjhala in 2014 [15].",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533139",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "757–766",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Attribute privacy: Framework and mechanisms",
		"URL": "https://doi.org/10.1145/3531146.3533139",
		"author": [
			{
				"family": "Zhang",
				"given": "Wanrong"
			},
			{
				"family": "Ohrimenko",
				"given": "Olga"
			},
			{
				"family": "Cummings",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "contractorBehavioralUseLicensing2022",
		"type": "paper-conference",
		"abstract": "With the growing reliance on artificial intelligence (AI) for many different applications, the sharing of code, data, and models is important to ensure the replicability and democratization of scientific knowledge. Many high-profile academic publishing venues expect code and models to be submitted and released with papers. Furthermore, developers often want to release these assets to encourage development of technology that leverages their frameworks and services. A number of organizations have expressed concerns about the inappropriate or irresponsible use of AI and have proposed ethical guidelines around the application of such systems. While such guidelines can help set norms and shape policy, they are not easily enforceable. In this paper, we advocate the use of licensing to enable legally enforceable behavioral use conditions on software and code and provide several case studies that demonstrate the feasibility of behavioral use licensing. We envision how licensing may be implemented in accordance with existing responsible AI guidelines.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533143",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "778–788",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Behavioral use licensing for responsible AI",
		"URL": "https://doi.org/10.1145/3531146.3533143",
		"author": [
			{
				"family": "Contractor",
				"given": "Danish"
			},
			{
				"family": "McDuff",
				"given": "Daniel"
			},
			{
				"family": "Haines",
				"given": "Julia Katherine"
			},
			{
				"family": "Lee",
				"given": "Jenny"
			},
			{
				"family": "Hines",
				"given": "Christopher"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			},
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Li",
				"given": "Hanlin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "harrisExploringRoleGrammar2022",
		"type": "paper-conference",
		"abstract": "Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533144",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "789–798",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring the role of grammar and word choice in bias toward african american english (AAE) in hate speech classification",
		"URL": "https://doi.org/10.1145/3531146.3533144",
		"author": [
			{
				"family": "Harris",
				"given": "Camille"
			},
			{
				"family": "Halevy",
				"given": "Matan"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			},
			{
				"family": "Bruckman",
				"given": "Amy"
			},
			{
				"family": "Yang",
				"given": "Diyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "norvalDisclosureDesignDesigning2022",
		"type": "paper-conference",
		"abstract": "There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims. This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533133",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "679–690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability",
		"URL": "https://doi.org/10.1145/3531146.3533133",
		"author": [
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cornelius",
				"given": "Kristin"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kaurSensibleAIReimagining2022",
		"type": "paper-conference",
		"abstract": "Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact—an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick’s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders’ needs—we build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533135",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "702–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sensible AI: Re-imagining interpretability and explainability using sensemaking theory",
		"URL": "https://doi.org/10.1145/3531146.3533135",
		"author": [
			{
				"family": "Kaur",
				"given": "Harmanpreet"
			},
			{
				"family": "Adar",
				"given": "Eytan"
			},
			{
				"family": "Gilbert",
				"given": "Eric"
			},
			{
				"family": "Lampe",
				"given": "Cliff"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "nandyAchievingFairnessPostprocessing2022",
		"type": "paper-conference",
		"abstract": "Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533136",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "715–725",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving fairness via post-processing in web-scale recommender Systems✱",
		"URL": "https://doi.org/10.1145/3531146.3533136",
		"author": [
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Venugopalan",
				"given": "Divya"
			},
			{
				"family": "Logan",
				"given": "Heloise"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			},
			{
				"family": "El Karoui",
				"given": "Noureddine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cooperMakingUnaccountableInternet2022",
		"type": "paper-conference",
		"abstract": "Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533137",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "726–742",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making the unaccountable internet: The changing meaning of accounting in the early ARPANET",
		"URL": "https://doi.org/10.1145/3531146.3533137",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Vidan",
				"given": "Gili"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hundtRobotsEnactMalignant2022",
		"type": "paper-conference",
		"abstract": "Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)&nbsp;[18, 80], Natural Language Processing (NLP)&nbsp;[6], or both, in the case of large image and caption models such as OpenAI CLIP&nbsp;[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533138",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "743–756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robots enact malignant stereotypes",
		"URL": "https://doi.org/10.1145/3531146.3533138",
		"author": [
			{
				"family": "Hundt",
				"given": "Andrew"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Zeng",
				"given": "Vicky"
			},
			{
				"family": "Kacianka",
				"given": "Severin"
			},
			{
				"family": "Gombolay",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "riekeImperfectInferencesPractical2022",
		"type": "paper-conference",
		"abstract": "Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533140",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "767–777",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Imperfect inferences: A practical assessment",
		"URL": "https://doi.org/10.1145/3531146.3533140",
		"author": [
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Southerland",
				"given": "Vincent"
			},
			{
				"family": "Svirsky",
				"given": "Dan"
			},
			{
				"family": "Hsu",
				"given": "Mingwei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zamfirescu-pereiraTrucksDontMean2022",
		"type": "paper-conference",
		"abstract": "Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533145",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "799–813",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trucks don’t mean trump: Diagnosing human error in image analysis",
		"URL": "https://doi.org/10.1145/3531146.3533145",
		"author": [
			{
				"family": "Zamfirescu-Pereira",
				"given": "J.D."
			},
			{
				"family": "Chen",
				"given": "Jerry"
			},
			{
				"family": "Wen",
				"given": "Emily"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiFairRepresentationClustering2022",
		"type": "paper-conference",
		"abstract": "We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ∈ X belongs to one of ℓ groups. Further, we are given fair representation parameters αj and βj for each group j ∈ [ℓ]. We say that a k-clustering C1, ⋅⋅⋅, Ck fairly represents all groups if the number of points from group j in cluster Ci is between αj|Ci| and βj|Ci| for every j ∈ [ℓ] and i ∈ [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ℓ1-objective ∑x ∈ Xd(x, ϕ(x)). We present an O(log k)-approximation algorithm that runs in time nO(ℓ). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ℓ. We also consider an important special case of the problem where and for all j ∈ [ℓ]. For this special case, we present an O(log k)-approximation algorithm that runs in time.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533146",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "814–823",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair representation clustering with several protected classes",
		"URL": "https://doi.org/10.1145/3531146.3533146",
		"author": [
			{
				"family": "Dai",
				"given": "Zhen"
			},
			{
				"family": "Makarychev",
				"given": "Yury"
			},
			{
				"family": "Vakilian",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shanmugamLearningLimitData2022",
		"type": "paper-conference",
		"abstract": "Modern machine learning systems are increasingly characterized by extensive personal data collection, despite the diminishing returns and increasing societal costs of such practices. Yet, data minimisation is one of the core data protection principles enshrined in the European Union’s General Data Protection Regulation (’GDPR’) and requires that only personal data that is adequate, relevant and limited to what is necessary is processed. However, the principle has seen limited adoption due to the lack of technical interpretation. In this work, we build on literature in machine learning and law to propose FIDO, a Framework for Inhibiting Data Overcollection. FIDO learns to limit data collection based on an interpretation of data minimization tied to system performance. Concretely, FIDO provides a data collection stopping criterion by iteratively updating an estimate of the performance curve, or the relationship between dataset size and performance, as data is acquired. FIDO estimates the performance curve via a piecewise power law technique that models distinct phases of an algorithm’s performance throughout data collection separately. Empirical experiments show that the framework produces accurate performance curves and data collection stopping criteria across datasets and feature acquisition algorithms. We further demonstrate that many other families of curves systematically overestimate the return on additional data. Results and analysis from our investigation offer deeper insights into the relevant considerations when designing a data minimization framework, including the impacts of active feature acquisition on individual users and the feasability of user-specific data minimization. We conclude with practical recommendations for the implementation of data minimization.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533148",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "839–849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to limit data collection via scaling laws: A computational interpretation for the legal principle of data minimization",
		"URL": "https://doi.org/10.1145/3531146.3533148",
		"author": [
			{
				"family": "Shanmugam",
				"given": "Divya"
			},
			{
				"family": "Diaz",
				"given": "Fernando"
			},
			{
				"family": "Shabanian",
				"given": "Samira"
			},
			{
				"family": "Finck",
				"given": "Michele"
			},
			{
				"family": "Biega",
				"given": "Asia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "blackModelMultiplicityOpportunities2022",
		"type": "paper-conference",
		"abstract": "Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533149",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "850–863",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model multiplicity: Opportunities, concerns, and solutions",
		"URL": "https://doi.org/10.1145/3531146.3533149",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Raghavan",
				"given": "Manish"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cooperAccountabilityAlgorithmicSociety2022a",
		"type": "paper-conference",
		"abstract": "In 1996, Accountability in a Computerized Society&nbsp;[95] issued a clarion call concerning the erosion of accountability in society due to the ubiquitous delegation of consequential functions to computerized systems. Nissenbaum [95] described four barriers to accountability that computerization presented, which we revisit in relation to the ascendance of data-driven algorithmic systems—i.e., machine learning or artificial intelligence—to uncover new challenges for accountability that these systems present. Nissenbaum’s original paper grounded discussion of the barriers in moral philosophy; we bring this analysis together with recent scholarship on relational accountability frameworks and discuss how the barriers present difficulties for instantiating a unified moral, relational framework in practice for data-driven algorithmic systems. We conclude by discussing ways of weakening the barriers in order to do so.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533150",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "864–876",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accountability in an algorithmic society: Relationality, responsibility, and robustness in machine learning",
		"URL": "https://doi.org/10.1145/3531146.3533150",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "fischerPromotingEthicalAwareness2022",
		"type": "paper-conference",
		"abstract": "Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533151",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "877–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Promoting ethical awareness in communication analysis: Investigating potentials and limits of visual analytics for intelligence applications",
		"URL": "https://doi.org/10.1145/3531146.3533151",
		"author": [
			{
				"family": "Fischer",
				"given": "Maximilian T."
			},
			{
				"family": "Hirsbrunner",
				"given": "Simon David"
			},
			{
				"family": "Jentner",
				"given": "Wolfgang"
			},
			{
				"family": "Miller",
				"given": "Matthias"
			},
			{
				"family": "Keim",
				"given": "Daniel A."
			},
			{
				"family": "Helm",
				"given": "Paula"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chienMultidisciplinaryFairnessConsiderations2022",
		"type": "paper-conference",
		"abstract": "While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "906–924",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-disciplinary fairness considerations in machine learning for clinical trials",
		"URL": "https://doi.org/10.1145/3531146.3533154",
		"author": [
			{
				"family": "Chien",
				"given": "Isabel"
			},
			{
				"family": "Deliu",
				"given": "Nina"
			},
			{
				"family": "Turner",
				"given": "Richard"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Villar",
				"given": "Sofia"
			},
			{
				"family": "Kilbertus",
				"given": "Niki"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangHowAreMLBased2022",
		"type": "paper-conference",
		"abstract": "Machine learning-based predictive systems are increasingly used to assist online groups and communities in various content moderation tasks. However, there are limited quantitative understandings of whether and how different groups and communities use such predictive systems differently according to their community characteristics. In this research, we conducted a field evaluation of how content moderation systems are used in 17 Wikipedia language communities. We found that 1) larger communities tend to use predictive systems to identify the most damaging edits, while smaller communities tend to use them to identify any edit that could be damaging; 2) predictive systems are used less in content areas where there are more local editing activities; 3) predictive systems have mixed effects on reducing disparate treatment between anonymous and registered editors across communities of different characteristics. Finally, we discuss the theoretical and practical implications for future human-centered moderation algorithms.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533147",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "824–838",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How are ML-Based online content moderation systems actually used? Studying community size, local activity, and disparate treatment",
		"URL": "https://doi.org/10.1145/3531146.3533147",
		"author": [
			{
				"family": "Wang",
				"given": "Leijie"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mclaughlinFairnessMachineassistedHuman2022",
		"type": "paper-conference",
		"abstract": "When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "890",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the fairness of machine-assisted human decisions",
		"URL": "https://doi.org/10.1145/3531146.3533152",
		"author": [
			{
				"family": "McLaughlin",
				"given": "Bryce"
			},
			{
				"family": "Spiess",
				"given": "Jann"
			},
			{
				"family": "Gillis",
				"given": "Talia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bordtPosthocExplanationsFail2022",
		"type": "paper-conference",
		"abstract": "Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to “explain”. Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law’s objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying “explainability” obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union’s draft Artificial Intelligence Act.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "891–905",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Post-hoc explanations fail to achieve their purpose in adversarial contexts",
		"URL": "https://doi.org/10.1145/3531146.3533153",
		"author": [
			{
				"family": "Bordt",
				"given": "Sebastian"
			},
			{
				"family": "Finck",
				"given": "Michèle"
			},
			{
				"family": "Raidl",
				"given": "Eric"
			},
			{
				"family": "Luxburg",
				"given": "Ulrike",
				"non-dropping-particle": "von"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hongPredictionExtractionDiscretion2022",
		"type": "paper-conference",
		"abstract": "I argue that data-driven predictions work primarily as instruments for systematic extraction of discretionary power – the practical capacity to make everyday decisions and define one's situation. This extractive relation reprises a long historical pattern, in which new methods of producing knowledge generate a redistribution of epistemic power: who declares what kind of truth about me, to count for what kinds of decisions? I argue that prediction as extraction of discretion is normal and fundamental to the technology, rather than isolated cases of bias or error. Synthesising critical observations across anthropology, history of technology and critical data studies, the paper demonstrates this dynamic in two contemporary domains: (1) crime and policing demonstrates how predictive systems are extractive by design. Rather than neutral models led astray by garbage data, pre-existing interests thoroughly shape how prediction conceives of its object, its measures, and most importantly, what it does not measure and in doing so devalues. (2) I then examine the prediction of productivity in the long tradition of extracting discretion as a means to extract labour power. Making human behaviour more predictable for the client of prediction (the manager, the corporation, the police officer) often means making life and work more unpredictable for the target of prediction (the employee, the applicant, the citizen).",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 10\npublisher-place: Seoul, Republic of Korea",
		"page": "925–934",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Prediction as extraction of discretion",
		"URL": "https://doi.org/10.1145/3531146.3533155",
		"author": [
			{
				"family": "Hong",
				"given": "Sun-ha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sloaneGermanAIStartups2022",
		"type": "paper-conference",
		"abstract": "The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "935–947",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "German AI start-ups and “AI ethics”: Using a social practice lens for assessing and implementing socio-technical innovation",
		"URL": "https://doi.org/10.1145/3531146.3533156",
		"author": [
			{
				"family": "Sloane",
				"given": "Mona"
			},
			{
				"family": "Zakrzewski",
				"given": "Janina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rajiFallacyAIFunctionality2022",
		"type": "paper-conference",
		"abstract": "Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, despite this reality, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on “ethical” or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we analyze a set of case studies to create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy challenge, operating as a necessary first step towards protecting affected communities from algorithmic harm.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533158",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "959–972",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The fallacy of AI functionality",
		"URL": "https://doi.org/10.1145/3531146.3533158",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Selbst",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "soFairnessReparativeAlgorithms2022",
		"type": "paper-conference",
		"abstract": "Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533160",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "988–1004",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond fairness: Reparative algorithms to address historical injustices of housing discrimination in the US",
		"URL": "https://doi.org/10.1145/3531146.3533160",
		"author": [
			{
				"family": "So",
				"given": "Wonyoung"
			},
			{
				"family": "Lohia",
				"given": "Pranay"
			},
			{
				"family": "Pimplikar",
				"given": "Rakesh"
			},
			{
				"family": "Hosoi",
				"given": "A.E."
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yewRegulatingFacialProcessing2022",
		"type": "paper-conference",
		"abstract": "Harms resulting from the development and deployment of facial processing technologies (FPT) have been met with increasing controversy. Several states and cities in the U.S. have banned the use of facial recognition by law enforcement and governments, but FPT are still being developed and used in a wide variety of contexts where they primarily are regulated by state biometric information privacy laws. Among these laws, the 2008 Illinois Biometric Information Privacy Act (BIPA) has generated a significant amount of litigation. Yet, with most BIPA lawsuits reaching settlements before there have been meaningful clarifications of relevant technical intricacies and legal definitions, there remains a great degree of uncertainty as to how exactly this law applies to FPT. What we have found through applications of BIPA in FPT litigation so far, however, points to potential disconnects between technical and legal communities. This paper analyzes what we know based on BIPA court proceedings and highlights these points of tension: areas where the technical operationalization of BIPA may create unintended and undesirable incentives for FPT development, as well as areas where BIPA litigation can bring to light the limitations of solely technical methods in achieving legal privacy values. These factors are relevant for (i) reasoning about biometric information privacy laws as a governing mechanism for FPT, (ii) assessing the potential harms of FPT, and (iii) providing incentives for the mitigation of these harms. By illuminating these considerations, we hope to empower courts and lawmakers to take a more nuanced approach to regulating FPT and developers to better understand privacy values in the current U.S. legal landscape.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533163",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1017–1027",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating facial processing technologies: Tensions between legal and technical considerations in the application of illinois BIPA",
		"URL": "https://doi.org/10.1145/3531146.3533163",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pahlFemaleWhite272022",
		"type": "paper-conference",
		"abstract": "Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533159",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 15\npublisher-place: Seoul, Republic of Korea",
		"page": "973–987",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Female, white, 27? Bias evaluation on data and algorithms for affect recognition in faces",
		"URL": "https://doi.org/10.1145/3531146.3533159",
		"author": [
			{
				"family": "Pahl",
				"given": "Jaspar"
			},
			{
				"family": "Rieger",
				"given": "Ines"
			},
			{
				"family": "Möller",
				"given": "Anna"
			},
			{
				"family": "Wittenberg",
				"given": "Thomas"
			},
			{
				"family": "Schmid",
				"given": "Ute"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "luSubvertingMachinesFluctuating2022",
		"type": "paper-conference",
		"abstract": "Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533161",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1005–1015",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Subverting machines, fluctuating identities: Re-learning human categorization",
		"URL": "https://doi.org/10.1145/3531146.3533161",
		"author": [
			{
				"family": "Lu",
				"given": "Christina"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "McKee",
				"given": "Kevin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mokanderModelsClassifyingAI2022",
		"type": "paper-conference",
		"abstract": "Organisations that design and deploy systems based on artificial intelligence (AI) increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. A major obstacle to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the practical purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533162",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "1016",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Models for classifying AI systems: the switch, the ladder, and the matrix",
		"URL": "https://doi.org/10.1145/3531146.3533162",
		"author": [
			{
				"family": "Mökander",
				"given": "Jakob"
			},
			{
				"family": "Sheth",
				"given": "Margi"
			},
			{
				"family": "Watson",
				"given": "David"
			},
			{
				"family": "Floridi",
				"given": "Luciano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "duDatadrivenSimulationNew2022",
		"type": "paper-conference",
		"abstract": "We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system’s ability to achieve it’s stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention— a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533165",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1028–1038",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A data-driven simulation of the new york state foster care system",
		"URL": "https://doi.org/10.1145/3531146.3533165",
		"author": [
			{
				"family": "Du",
				"given": "Yuhao"
			},
			{
				"family": "Ionescu",
				"given": "Stefania"
			},
			{
				"family": "Sage",
				"given": "Melanie"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "pfohlNetBenefitCalibration2022",
		"type": "paper-conference",
		"abstract": "A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533166",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 14\npublisher-place: Seoul, Republic of Korea",
		"page": "1039–1052",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare",
		"URL": "https://doi.org/10.1145/3531146.3533166",
		"author": [
			{
				"family": "Pfohl",
				"given": "Stephen"
			},
			{
				"family": "Xu",
				"given": "Yizhe"
			},
			{
				"family": "Foryciarz",
				"given": "Agata"
			},
			{
				"family": "Ignatiadis",
				"given": "Nikolaos"
			},
			{
				"family": "Genkins",
				"given": "Julian"
			},
			{
				"family": "Shah",
				"given": "Nigam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mishlerFADEFAirDouble2022",
		"type": "paper-conference",
		"abstract": "Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533167",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 1\npublisher-place: Seoul, Republic of Korea",
		"page": "1053",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FADE: FAir double ensemble learning for observable and counterfactual outcomes",
		"URL": "https://doi.org/10.1145/3531146.3533167",
		"author": [
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Kennedy",
				"given": "Edward H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "albiniCounterfactualShapleyAdditive2022",
		"type": "paper-conference",
		"abstract": "Feature attributions are a common paradigm for model explanations due to their simplicity in assigning a single numeric score for each input feature to a model. In the actionable recourse setting, wherein the goal of the explanations is to improve outcomes for model consumers, it is often unclear how feature attributions should be correctly used. With this work, we aim to strengthen and clarify the link between actionable recourse and feature attributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP (CF-SHAP), that incorporates counterfactual information to produce a background dataset for use within the marginal (a.k.a. interventional) Shapley value framework. We motivate the need within the actionable recourse setting for careful consideration of background datasets when using Shapley values for feature attributions with numerous synthetic examples. Moreover, we demonstrate the efficacy of CF-SHAP by proposing and justifying a quantitative score for feature attributions, counterfactual-ability, showing that as measured by this metric, CF-SHAP is superior to existing methods when evaluated on public datasets using tree ensembles.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533168",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 17\npublisher-place: Seoul, Republic of Korea",
		"page": "1054–1070",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual shapley additive explanations",
		"URL": "https://doi.org/10.1145/3531146.3533168",
		"author": [
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Long",
				"given": "Jason"
			},
			{
				"family": "Dervovic",
				"given": "Danial"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "buylTacklingAlgorithmicDisability2022",
		"type": "paper-conference",
		"abstract": "Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533169",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1071–1082",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tackling algorithmic disability discrimination in the hiring process: An ethical, legal and technical analysis",
		"URL": "https://doi.org/10.1145/3531146.3533169",
		"author": [
			{
				"family": "Buyl",
				"given": "Maarten"
			},
			{
				"family": "Cociancig",
				"given": "Christina"
			},
			{
				"family": "Frattone",
				"given": "Cristina"
			},
			{
				"family": "Roekens",
				"given": "Nele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "globus-harrisAlgorithmicFrameworkBias2022",
		"type": "paper-conference",
		"abstract": "We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533172",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 19\npublisher-place: Seoul, Republic of Korea",
		"page": "1106–1124",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An algorithmic framework for bias bounties",
		"URL": "https://doi.org/10.1145/3531146.3533172",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "linAuditingGerrymanderingIdentifying2022",
		"type": "paper-conference",
		"abstract": "Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533174",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1125–1135",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing for gerrymandering by identifying disenfranchised individuals",
		"URL": "https://doi.org/10.1145/3531146.3533174",
		"author": [
			{
				"family": "Lin",
				"given": "Jerry"
			},
			{
				"family": "Chen",
				"given": "Carolyn"
			},
			{
				"family": "Chmielewski",
				"given": "Marc"
			},
			{
				"family": "Zaman",
				"given": "Samia"
			},
			{
				"family": "Fain",
				"given": "Brandon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "botesBrainComputerInterfaces2022",
		"type": "paper-conference",
		"abstract": "Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533176",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 8\npublisher-place: Seoul, Republic of Korea",
		"page": "1154–1161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Brain computer interfaces and human rights: Brave new rights for a brave new world",
		"URL": "https://doi.org/10.1145/3531146.3533176",
		"author": [
			{
				"family": "Botes",
				"given": "Marietjie Wilhelmina Maria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "watsonRationalShapleyValues2022",
		"type": "paper-conference",
		"abstract": "Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce rational Shapley values, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533170",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 12\npublisher-place: Seoul, Republic of Korea",
		"page": "1083–1094",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rational shapley values",
		"URL": "https://doi.org/10.1145/3531146.3533170",
		"author": [
			{
				"family": "Watson",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "mashiatTradeoffsGroupFairness2022",
		"type": "paper-conference",
		"abstract": "We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533171",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 11\npublisher-place: Seoul, Republic of Korea",
		"page": "1095–1105",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trade-offs between group fairness metrics in societal resource allocation",
		"URL": "https://doi.org/10.1145/3531146.3533171",
		"author": [
			{
				"family": "Mashiat",
				"given": "Tasfia"
			},
			{
				"family": "Gitiaux",
				"given": "Xavier"
			},
			{
				"family": "Rangwala",
				"given": "Huzefa"
			},
			{
				"family": "Fowler",
				"given": "Patrick"
			},
			{
				"family": "Das",
				"given": "Sanmay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "lucchesiSmallsetTimelinesVisual2022",
		"type": "paper-conference",
		"abstract": "Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533175",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 18\npublisher-place: Seoul, Republic of Korea",
		"page": "1136–1153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Smallset timelines: A visual representation of data preprocessing decisions",
		"URL": "https://doi.org/10.1145/3531146.3533175",
		"author": [
			{
				"family": "Lucchesi",
				"given": "Lydia R."
			},
			{
				"family": "Kuhnert",
				"given": "Petra M."
			},
			{
				"family": "Davis",
				"given": "Jenny L."
			},
			{
				"family": "Xie",
				"given": "Lexing"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "balagopalanRoadExplainabilityPaved2022",
		"type": "paper-conference",
		"abstract": "Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533179",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 13\npublisher-place: Seoul, Republic of Korea",
		"page": "1194–1206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The road to explainability is paved with bias: Measuring the fairness of explanations",
		"URL": "https://doi.org/10.1145/3531146.3533179",
		"author": [
			{
				"family": "Balagopalan",
				"given": "Aparna"
			},
			{
				"family": "Zhang",
				"given": "Haoran"
			},
			{
				"family": "Hamidieh",
				"given": "Kimia"
			},
			{
				"family": "Hartvigsen",
				"given": "Thomas"
			},
			{
				"family": "Rudzicz",
				"given": "Frank"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "dianaMultiaccurateProxiesDownstream2022",
		"type": "paper-conference",
		"abstract": "We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time — in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner — with minimal assumptions on their prediction task — to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.",
		"collection-title": "FAccT '22",
		"container-title": "Proceedings of the 2022 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3531146.3533180",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9352-2",
		"note": "number-of-pages: 33\npublisher-place: Seoul, Republic of Korea",
		"page": "1207–1239",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multiaccurate proxies for downstream fairness",
		"URL": "https://doi.org/10.1145/3531146.3533180",
		"author": [
			{
				"family": "Diana",
				"given": "Emily"
			},
			{
				"family": "Gill",
				"given": "Wesley"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Sharifi-Malvajerdi",
				"given": "Saeed"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wangPredictiveOptimizationLegitimacy2023a",
		"type": "paper-conference",
		"abstract": "We formalize predictive optimization, a category of decision-making algorithms that use machine learning (ML) to predict future outcomes of interest about \\ individuals}. For example, pre-trial risk prediction algorithms such as COMPAS use ML to predict whether an individual will re-offend in the future. Our thesis is that predictive optimization raises a distinctive and serious set of normative concerns that cause it to fail on its own terms. To test this, we review 387 reports, articles, and web pages from academia, industry, non-profits, governments, and modeling contests, and find many real-world examples of predictive optimization. We select eight particularly consequential examples as case studies. Simultaneously, we develop a set of normative and technical critiques that challenge the claims made by the developers of these applications—in particular, claims of increased accuracy, efficiency, and fairness. Our key finding is that these critiques apply to each of the applications, are not easily evaded by redesigning the systems, and thus challenge whether these applications should be deployed. We argue that the burden of evidence for justifying why the deployment of predictive optimization is not harmful should rest with the developers of the tools. Based on our analysis, we provide a rubric of critical questions that can be used to deliberate or contest specific predictive optimization applications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",
		"DOI": "10.1145/3593013.3594030",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"page": "626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy",
		"title-short": "Against Predictive Optimization",
		"URL": "https://dl.acm.org/doi/10.1145/3593013.3594030",
		"author": [
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					3,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "divakaranBroadeningAIEthics2023a",
		"type": "paper-conference",
		"abstract": "Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction–that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the ‘Natyashastra’), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593971",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "2–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Broadening AI ethics narratives: An indic art view",
		"URL": "https://doi.org/10.1145/3593013.3593971",
		"author": [
			{
				"family": "Divakaran",
				"given": "Ajay"
			},
			{
				"family": "Sridhar",
				"given": "Aparna"
			},
			{
				"family": "Srinivasan",
				"given": "Ramya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenMachineExplanationsHuman2023a",
		"type": "paper-conference",
		"abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593970",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine explanations and human understanding",
		"URL": "https://doi.org/10.1145/3593013.3593970",
		"author": [
			{
				"family": "Chen",
				"given": "Chacha"
			},
			{
				"family": "Feng",
				"given": "Shi"
			},
			{
				"family": "Sharma",
				"given": "Amit"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "benbouzidFairnessMachineLearning2023a",
		"type": "paper-conference",
		"abstract": "We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications – are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, “trained judgement,” which is based on a reasonably partial justification from the designer of the machine – which itself comes to be politically situated as a result.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593974",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "35–43",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism",
		"URL": "https://doi.org/10.1145/3593013.3593974",
		"author": [
			{
				"family": "Benbouzid",
				"given": "Bilel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mainzTwoReasonsSubjecting2023a",
		"type": "paper-conference",
		"abstract": "This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593975",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 6\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "44–49",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Two reasons for subjecting medical AI systems to lower standards than humans",
		"URL": "https://doi.org/10.1145/3593013.3593975",
		"author": [
			{
				"family": "Mainz",
				"given": "Jakob"
			},
			{
				"family": "Munch",
				"given": "Lauritz"
			},
			{
				"family": "Bjerring",
				"given": "Jens Christian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lauferOptimizationsNeglectedNormative2023",
		"type": "paper-conference",
		"abstract": "Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593976",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "50–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Optimization’s neglected normative commitments",
		"URL": "https://doi.org/10.1145/3593013.3593976",
		"author": [
			{
				"family": "Laufer",
				"given": "Benjamin"
			},
			{
				"family": "Gilbert",
				"given": "Thomas"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouHowExplainJustify2023a",
		"type": "paper-conference",
		"abstract": "Discussion of the “right to an explanation” has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593972",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "12–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How to explain and justify almost any decision: Potential pitfalls for accountability in AI decision-making",
		"URL": "https://doi.org/10.1145/3593013.3593972",
		"author": [
			{
				"family": "Zhou",
				"given": "Joyce"
			},
			{
				"family": "Joachims",
				"given": "Thorsten"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kollnigWeAreAdults2023a",
		"type": "paper-conference",
		"abstract": "Many mobile apps are designed not just to support end-users’ needs, but also commercial aims. This can result in app designs that compromise end-user privacy, safety, and well-being. Since apps nowadays provide vital digital information and services, users often have no choice but to accept potentially harmful or manipulative app designs. What if, instead, individuals could customise their apps to make them safer and better suit their needs? This exploratory work examines this question through a multi-faceted approach; first, to understand user needs, we conducted a survey (n = 100) of changes users wanted in their apps, and of perceptions of risks in app repair. Second, to identify technical challenges, we developed a prototype that enables end-users to change their apps, and realised several modifications suggested by survey participants. Finally, we conduct a set of expert interviews (n = 8) to delve into the ethical and legal aspects of such a tool, and synthesise a framework of risks and opportunities of app repair.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593973",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "22–34",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "‘We are adults and deserve control of our phones’: Examining the risks and opportunities of a right to repair for mobile apps",
		"URL": "https://doi.org/10.1145/3593013.3593973",
		"author": [
			{
				"family": "Kollnig",
				"given": "Konrad"
			},
			{
				"family": "Datta",
				"given": "Siddhartha"
			},
			{
				"family": "Serban Von Davier",
				"given": "Thomas"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Lyngs",
				"given": "Ulrik"
			},
			{
				"family": "Shadbolt",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kimHumansAIContext2023a",
		"type": "paper-conference",
		"abstract": "Trust is an important factor in people’s interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593978",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "77–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Humans, AI, and context: Understanding end-users’ trust in a real-world computer vision application",
		"URL": "https://doi.org/10.1145/3593013.3593978",
		"author": [
			{
				"family": "Kim",
				"given": "Sunnie S. Y."
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Russakovsky",
				"given": "Olga"
			},
			{
				"family": "Fong",
				"given": "Ruth"
			},
			{
				"family": "Monroy-Hernández",
				"given": "Andrés"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "solaimanGradientGenerativeAI2023a",
		"type": "paper-conference",
		"abstract": "As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593981",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "111–122",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The gradient of generative AI release: Methods and considerations",
		"URL": "https://doi.org/10.1145/3593013.3593981",
		"author": [
			{
				"family": "Solaiman",
				"given": "Irene"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshHowBiasedAre2023a",
		"type": "paper-conference",
		"abstract": "Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier’s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier’s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier. The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593983",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "138–148",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“How biased are your features?”: Computing fairness influence functions with global sensitivity analysis",
		"URL": "https://doi.org/10.1145/3593013.3593983",
		"author": [
			{
				"family": "Ghosh",
				"given": "Bishwamittra"
			},
			{
				"family": "Basu",
				"given": "Debabrota"
			},
			{
				"family": "Meel",
				"given": "Kuldeep S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "narayananWelfaristMoralGrounding2023a",
		"type": "paper-conference",
		"abstract": "As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple – even simplistic – move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593977",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "64–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Welfarist moral grounding for transparent AI",
		"URL": "https://doi.org/10.1145/3593013.3593977",
		"author": [
			{
				"family": "Narayanan",
				"given": "Devesh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "royMultidimensionalDiscriminationLaw2023a",
		"type": "paper-conference",
		"abstract": "AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called “multi-dimensional discrimination” or “multi-dimensional fairness” problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593979",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "89–100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-dimensional discrimination in law and machine learning - a comparative overview",
		"URL": "https://doi.org/10.1145/3593013.3593979",
		"author": [
			{
				"family": "Roy",
				"given": "Arjun"
			},
			{
				"family": "Horstmann",
				"given": "Jan"
			},
			{
				"family": "Ntoutsi",
				"given": "Eirini"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rothReconcilingIndividualProbability2023a",
		"type": "paper-conference",
		"abstract": "Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data—or on how to sample from a data distribution—cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of “reconciliation” that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual probabilities are unknowable, they are contestable via a computationally and data efficient process that must lead to agreement. Thus we cannot find ourselves in a situation in which we have two equally accurate and unimprovable models that disagree substantially in their predictions—providing an answer to what is sometimes called the predictive or model multiplicity problem.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593980",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "101–110",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconciling individual probability Forecasts",
		"URL": "https://doi.org/10.1145/3593013.3593980",
		"author": [
			{
				"family": "Roth",
				"given": "Aaron"
			},
			{
				"family": "Tolbert",
				"given": "Alexander"
			},
			{
				"family": "Weinstein",
				"given": "Scott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "xiaoNameFairnessAssessing2023a",
		"type": "paper-conference",
		"abstract": "Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593982",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "123–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In the name of fairness: Assessing the bias in clinical record de-identification",
		"URL": "https://doi.org/10.1145/3593013.3593982",
		"author": [
			{
				"family": "Xiao",
				"given": "Yuxin"
			},
			{
				"family": "Lim",
				"given": "Shulammite"
			},
			{
				"family": "Pollard",
				"given": "Tom Joseph"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangPreventingDiscriminatoryDecisionmaking2023a",
		"type": "paper-conference",
		"abstract": "Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593984",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "149–159",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preventing discriminatory decision-making in evolving data streams",
		"URL": "https://doi.org/10.1145/3593013.3593984",
		"author": [
			{
				"family": "Wang",
				"given": "Zichong"
			},
			{
				"family": "Saxena",
				"given": "Nripsuta"
			},
			{
				"family": "Yu",
				"given": "Tongjia"
			},
			{
				"family": "Karki",
				"given": "Sneha"
			},
			{
				"family": "Zetty",
				"given": "Tyler"
			},
			{
				"family": "Haque",
				"given": "Israat"
			},
			{
				"family": "Zhou",
				"given": "Shan"
			},
			{
				"family": "Kc",
				"given": "Dukka"
			},
			{
				"family": "Stockwell",
				"given": "Ian"
			},
			{
				"family": "Wang",
				"given": "Xuyu"
			},
			{
				"family": "Bifet",
				"given": "Albert"
			},
			{
				"family": "Zhang",
				"given": "Wenbin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "septiandriWEIRDFAccTsHow2023a",
		"type": "paper-conference",
		"abstract": "Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world’s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems’ fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593985",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "160–171",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "WEIRD FAccTs: How western, educated, industrialized, rich, and democratic is FAccT?",
		"URL": "https://doi.org/10.1145/3593013.3593985",
		"author": [
			{
				"family": "Septiandri",
				"given": "Ali Akbar"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Tahaei",
				"given": "Mohammad"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "knowlesTrustworthyAILogics2023a",
		"type": "paper-conference",
		"abstract": "Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for ‘trustworthy AI’ and, from these, objective indicators of ‘trustworthiness’ for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly ‘trustworthy’ AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as ‘undeserving’, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI—fairness, accountability, and transparency—that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593986",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "172–182",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trustworthy AI and the logics of intersectional resistance",
		"URL": "https://doi.org/10.1145/3593013.3593986",
		"author": [
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Fledderjohann",
				"given": "Jasmine"
			},
			{
				"family": "Richards",
				"given": "John T."
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "senguptaHerShoesGendered2023a",
		"type": "paper-conference",
		"abstract": "In recent years, a proliferation of women’s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate ‘safety maps’ used by policy makers for urban design and academics for studying mobility patterns. Men and women’s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents’ data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593987",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "183–192",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In her shoes: Gendered labelling in crowdsourced safety perceptions data from india",
		"URL": "https://doi.org/10.1145/3593013.3593987",
		"author": [
			{
				"family": "Sengupta",
				"given": "Nandana"
			},
			{
				"family": "Vaidya",
				"given": "Ashwini"
			},
			{
				"family": "Evans",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "meyerDatasetMultiplicityProblem2023a",
		"type": "paper-conference",
		"abstract": "We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets’ factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593988",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "193–204",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dataset multiplicity problem: How unreliable data impacts predictions",
		"URL": "https://doi.org/10.1145/3593013.3593988",
		"author": [
			{
				"family": "Meyer",
				"given": "Anna P."
			},
			{
				"family": "Albarghouthi",
				"given": "Aws"
			},
			{
				"family": "D'Antoni",
				"given": "Loris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gadirajuWouldntSayOffensive2023",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593989",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "205–216",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"I wouldn’t say offensive but...\": Disability-centered perspectives on large language models",
		"URL": "https://doi.org/10.1145/3593013.3593989",
		"author": [
			{
				"family": "Gadiraju",
				"given": "Vinitha"
			},
			{
				"family": "Kane",
				"given": "Shaun"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Taylor",
				"given": "Alex"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Brewer",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "aliWalkingWalkAI2023a",
		"type": "paper-conference",
		"abstract": "Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate “ethics washing.” Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593990",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "217–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Walking the walk of AI ethics: Organizational challenges and the individualization of risk among ethics entrepreneurs",
		"URL": "https://doi.org/10.1145/3593013.3593990",
		"author": [
			{
				"family": "Ali",
				"given": "Sanna J."
			},
			{
				"family": "Christin",
				"given": "Angèle"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Katila",
				"given": "Riitta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lapostolpideritAlgorithmicTransparencySouth2023",
		"type": "paper-conference",
		"abstract": "This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ibáñez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593991",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "227–235",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic Transparency from the South: Examining the state of algorithmic transparency in Chile's public administration algorithms",
		"URL": "https://doi.org/10.1145/3593013.3593991",
		"author": [
			{
				"family": "Lapostol Piderit",
				"given": "José Pablo"
			},
			{
				"family": "Garrido Iglesias",
				"given": "Romina"
			},
			{
				"family": "Hermosilla Cornejo",
				"given": "María Paz"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "limaWhoShouldPay2023b",
		"type": "paper-conference",
		"abstract": "The question of who should be held responsible when machines cause harm in high-risk environments is open to debate. Empirical research examining laypeople’s opinions has been largely restricted to the moral domain and has only inspected a limited set of negative outcomes. This study collects lay perceptions of legal responsibility for a wide range of machine-caused harms. We investigated how much people (N = 572) expect users and developers of machines to pay as legal damages in 37 diverse scenarios from the book “How Humans Judge Machines” by Hidalgo et al. [37]. Our results suggest that people’s expectations of legal damages for machine-caused harms are influenced by several factors, including perceived moral wrongness and the presence of victims. The scenarios exhibited substantial variation in how they were perceived and thus in the amount of legal damages they called for. People viewed both users and developers as legally responsible and expected the latter to pay higher damages. We discuss our findings in the context of future regulations of machines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593992",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "236–246",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who should pay when machines cause harm? Laypeople’s expectations of legal damages for machine-caused harm",
		"URL": "https://doi.org/10.1145/3593013.3593992",
		"author": [
			{
				"family": "Lima",
				"given": "Gabriel"
			},
			{
				"family": "Grgic-Hlaca",
				"given": "Nina"
			},
			{
				"family": "Jeong",
				"given": "Jin Keun"
			},
			{
				"family": "Cha",
				"given": "Meeyoung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jacoviDiagnosingAIExplanation2023a",
		"type": "paper-conference",
		"abstract": "We investigate a formalism for the conditions of a successful explanation of AI. We consider “success” to depend not only on what information the explanation contains, but also on what information the human explainee understands from it. Theory of mind literature discusses the folk concepts that humans use to understand and generalize behavior. We posit that folk concepts of behavior provide us with a “language” that humans understand behavior with. We use these folk concepts as a framework of social attribution by the human explainee—the information constructs that humans are likely to comprehend from explanations—by introducing a blueprint for an explanatory narrative that explains AI behavior with these constructs. We then demonstrate that many XAI methods today can be mapped to folk concepts of behavior in a qualitative evaluation. This allows us to uncover their failure modes that prevent current methods from explaining successfully—i.e., the information constructs that are missing for any given XAI method, and whose inclusion can decrease the likelihood of misunderstanding AI behavior.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593993",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "247",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diagnosing AI explanation methods with folk concepts of behavior",
		"URL": "https://doi.org/10.1145/3593013.3593993",
		"author": [
			{
				"family": "Jacovi",
				"given": "Alon"
			},
			{
				"family": "Bastings",
				"given": "Jasmijn"
			},
			{
				"family": "Gehrmann",
				"given": "Sebastian"
			},
			{
				"family": "Goldberg",
				"given": "Yoav"
			},
			{
				"family": "Filippova",
				"given": "Katja"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "scharowskiCertificationLabelsTrustworthy2023a",
		"type": "paper-conference",
		"abstract": "Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593994",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "248–260",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Certification labels for trustworthy AI: Insights from an empirical mixed-method study",
		"URL": "https://doi.org/10.1145/3593013.3593994",
		"author": [
			{
				"family": "Scharowski",
				"given": "Nicolas"
			},
			{
				"family": "Benk",
				"given": "Michaela"
			},
			{
				"family": "Kühne",
				"given": "Swen J."
			},
			{
				"family": "Wettstein",
				"given": "Léane"
			},
			{
				"family": "Brühlmann",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hawkinsEthicalAmbiguityAI2023a",
		"type": "paper-conference",
		"abstract": "The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or ‘data enrichment’, has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593995",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "261–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices",
		"URL": "https://doi.org/10.1145/3593013.3593995",
		"author": [
			{
				"family": "Hawkins",
				"given": "Will"
			},
			{
				"family": "Mittelstadt",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "blili-hamelinMakingIntelligenceEthical2023a",
		"type": "paper-conference",
		"abstract": "In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly “technical” or “scientific” decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence—standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating value-neutral benchmarks. Finally, we outline practical recommendations for ML benchmark research ethics and ethics review.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593996",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "271–284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making intelligence: Ethical values in IQ and ML benchmarks",
		"URL": "https://doi.org/10.1145/3593013.3593996",
		"author": [
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "boggustSaliencyCardsFramework2023a",
		"type": "paper-conference",
		"abstract": "Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model’s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593997",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "285–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Saliency cards: A framework to characterize and compare saliency methods",
		"URL": "https://doi.org/10.1145/3593013.3593997",
		"author": [
			{
				"family": "Boggust",
				"given": "Angie"
			},
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Strobelt",
				"given": "Hendrik"
			},
			{
				"family": "Guttag",
				"given": "John"
			},
			{
				"family": "Satyanarayan",
				"given": "Arvind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "watson-danielsMultitargetMultiplicityFlexibility2023a",
		"type": "paper-conference",
		"abstract": "Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals’ outcomes and selection rate disparities across groups. We call this multi-target multiplicity. Along the way, we refine the study of single-target multiplicity by introducing notions of multiplicity that respect resource constraints—a feature of many real-world tasks that isn’t captured by existing notions of predictive multiplicity. We apply our methods on a healthcare dataset, and show that the level of multiplicity that stems from target variable choice can be greater than that stemming from nearly-optimal models of a single target.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593998",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "297–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-target multiplicity: Flexibility and fairness in target specification under resource constraints",
		"URL": "https://doi.org/10.1145/3593013.3593998",
		"author": [
			{
				"family": "Watson-Daniels",
				"given": "Jamelle"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Hofman",
				"given": "Jake M."
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "davisAffordancesMachineLearning2023b",
		"type": "paper-conference",
		"abstract": "The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594000",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "324–332",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "‘Affordances’ for machine learning",
		"URL": "https://doi.org/10.1145/3593013.3594000",
		"author": [
			{
				"family": "Davis",
				"given": "Jenny L"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "millerExplainableAIDead2023a",
		"type": "paper-conference",
		"abstract": "In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594001",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "333–342",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI is dead, long live explainable AI! Hypothesis-driven decision support using evaluative AI",
		"URL": "https://doi.org/10.1145/3593013.3594001",
		"author": [
			{
				"family": "Miller",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pistilliStrongerTogetherArticulation2023a",
		"type": "paper-conference",
		"abstract": "The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594002",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "343–354",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stronger together: on the articulation of ethical charters, legal tools, and technical documentation in ML",
		"URL": "https://doi.org/10.1145/3593013.3594002",
		"author": [
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Muñoz Ferrandis",
				"given": "Carlos"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bellSimplicityBiasLeads2023a",
		"type": "paper-conference",
		"abstract": "Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594003",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "355–369",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Simplicity bias leads to amplified performance disparities",
		"URL": "https://doi.org/10.1145/3593013.3594003",
		"author": [
			{
				"family": "Bell",
				"given": "Samuel James"
			},
			{
				"family": "Sagun",
				"given": "Levent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "prussGhostingMachineJudicial2023a",
		"type": "paper-conference",
		"abstract": "Recidivism risk assessment instruments are presented as an ‘evidence-based’ strategy for criminal justice reform – a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as “useless,” “worthless,” “boring,” “a waste of time,” “a non-thing,” and simply “not helpful.” I argue that this algorithm aversion cannot be accounted for by individuals’ distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument’s non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3593999",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "312–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ghosting the machine: Judicial resistance to a recidivism risk assessment instrument",
		"URL": "https://doi.org/10.1145/3593013.3593999",
		"author": [
			{
				"family": "Pruss",
				"given": "Dasha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cabelloIndependenceAssociationBias2023a",
		"type": "paper-conference",
		"abstract": "The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594004",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "370–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the independence of association bias and empirical fairness in language models",
		"URL": "https://doi.org/10.1145/3593013.3594004",
		"author": [
			{
				"family": "Cabello",
				"given": "Laura"
			},
			{
				"family": "Jørgensen",
				"given": "Anna Katrine"
			},
			{
				"family": "Søgaard",
				"given": "Anders"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "brewerEnvisioningEquitableSpeech2023a",
		"type": "paper-conference",
		"abstract": "There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594005",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "379–388",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Envisioning equitable speech technologies for black older adults",
		"URL": "https://doi.org/10.1145/3593013.3594005",
		"author": [
			{
				"family": "Brewer",
				"given": "Robin N."
			},
			{
				"family": "Harrington",
				"given": "Christina"
			},
			{
				"family": "Heldreth",
				"given": "Courtney"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "estornellGroupfairClassificationStrategic2023a",
		"type": "paper-conference",
		"abstract": "The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be “fair” under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594006",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "389–399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group-fair classification with strategic agents",
		"URL": "https://doi.org/10.1145/3593013.3594006",
		"author": [
			{
				"family": "Estornell",
				"given": "Andrew"
			},
			{
				"family": "Das",
				"given": "Sanmay"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Vorobeychik",
				"given": "Yevgeniy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "alvarezDomainAdaptiveDecision2023a",
		"type": "paper-conference",
		"abstract": "In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594008",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "423–433",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Domain adaptive decision trees: Implications for accuracy and fairness",
		"URL": "https://doi.org/10.1145/3593013.3594008",
		"author": [
			{
				"family": "Alvarez",
				"given": "Jose M."
			},
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			},
			{
				"family": "Ruggieri",
				"given": "Salvatore"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "widderItsPowerWhat2023",
		"type": "paper-conference",
		"abstract": "How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594012",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "467–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?",
		"URL": "https://doi.org/10.1145/3593013.3594012",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			},
			{
				"family": "Zhen",
				"given": "Derrick"
			},
			{
				"family": "Dabbish",
				"given": "Laura"
			},
			{
				"family": "Herbsleb",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "neumannDoesAIAssistedFactchecking2023a",
		"type": "paper-conference",
		"abstract": "In recent years, algorithms have been incorporated into fact-checking pipelines. They are used not only to flag previously fact-checked misinformation, but also to provide suggestions about which trending claims should be prioritized for fact-checking - a paradigm called ‘check-worthiness.’ While several studies have examined the accuracy of these algorithms, none have investigated how the benefits from these algorithms (via reduction in exposure to misinformation) are distributed amongst various online communities. In this paper, we investigate how diverse representation across multiple stages of the AI development pipeline affects the distribution of benefits from AI-assisted fact-checking for different online communities. We simulate information propagation through the network using our novel Topic-Aware, Community-Impacted Twitter (TACIT) simulator on a large Twitter followers network, tuned to produce realistic cascades of true and false information across multiple topics. Finally, using simulated data as a test bed, we implement numerous algorithmic fact-checking interventions that explicitly account for notions of diversity. We find that both representative and egalitarian methods for sampling and labeling check-worthiness model training data can lead to network-wide benefit concentrated in majority communities, while incorporating diversity into how fact-checkers use algorithmic recommendations can actively reduce inequalities in benefits between majority and minority communities. These findings contribute to an important conversation around the responsible implementation of AI-assisted fact-checking by social media platforms and fact-checking organizations.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594013",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "480–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI-Assisted fact-checking disproportionately benefit majority groups online?",
		"URL": "https://doi.org/10.1145/3593013.3594013",
		"author": [
			{
				"family": "Neumann",
				"given": "Terrence"
			},
			{
				"family": "Wolczynski",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rakovaAlgorithmsSocialecologicaltechnologicalSystems2023a",
		"type": "paper-conference",
		"abstract": "This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594014",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms as social-ecological-technological systems: an environmental justice lens on algorithmic audits",
		"URL": "https://doi.org/10.1145/3593013.3594014",
		"author": [
			{
				"family": "Rakova",
				"given": "Bogdana"
			},
			{
				"family": "Dobbe",
				"given": "Roel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kingPrivacybiasTradeoffData2023",
		"type": "paper-conference",
		"abstract": "An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this “privacy-bias tradeoff” has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden’s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing “equity action plans,” with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by “visual observation” when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594015",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "492–505",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The privacy-bias tradeoff: Data minimization and racial disparity assessments in U.S. government",
		"URL": "https://doi.org/10.1145/3593013.3594015",
		"author": [
			{
				"family": "King",
				"given": "Jennifer"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			},
			{
				"family": "Gupta",
				"given": "Arushi"
			},
			{
				"family": "Wu",
				"given": "Victor"
			},
			{
				"family": "Webley-Brown",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bellPossibilityFairnessRevisiting2023a",
		"type": "paper-conference",
		"abstract": "The “impossibility theorem” — which is considered foundational in algorithmic fairness literature — asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner’s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when — and to what degree — fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594007",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 23\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "400–422",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The possibility of fairness: Revisiting the impossibility theorem in practice",
		"URL": "https://doi.org/10.1145/3593013.3594007",
		"author": [
			{
				"family": "Bell",
				"given": "Andrew"
			},
			{
				"family": "Bynum",
				"given": "Lucius"
			},
			{
				"family": "Drushchak",
				"given": "Nazarii"
			},
			{
				"family": "Zakharchenko",
				"given": "Tetiana"
			},
			{
				"family": "Rosenblatt",
				"given": "Lucas"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "marianAlgorithmicTransparencyAccountability2023a",
		"type": "paper-conference",
		"abstract": "Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency. In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many “wasted choices” on students’ ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594009",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "434–443",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic transparency and accountability through crowdsourcing: A study of the NYC school admission lottery",
		"URL": "https://doi.org/10.1145/3593013.3594009",
		"author": [
			{
				"family": "Marian",
				"given": "Amelie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "eyertRethinkingTransparencyCommunicative2023a",
		"type": "paper-conference",
		"abstract": "In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594010",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "444–454",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking transparency as a communicative constellation",
		"URL": "https://doi.org/10.1145/3593013.3594010",
		"author": [
			{
				"family": "Eyert",
				"given": "Florian"
			},
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kangPraxesPoliticsAI2023a",
		"type": "paper-conference",
		"abstract": "There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594011",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "455–466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the praxes and politics of AI speech emotion recognition",
		"URL": "https://doi.org/10.1145/3593013.3594011",
		"author": [
			{
				"family": "Kang",
				"given": "Edward B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "qadriAIsRegimesRepresentation2023",
		"type": "paper-conference",
		"abstract": "This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants’ reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594016",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "506–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI’s regimes of representation: A community-centered study of text-to-image models in south asia",
		"URL": "https://doi.org/10.1145/3593013.3594016",
		"author": [
			{
				"family": "Qadri",
				"given": "Rida"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Bennett",
				"given": "Cynthia L."
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grigoryanTheoryAuditabilityAllocation2023a",
		"type": "paper-conference",
		"abstract": "In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594017",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "518",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A theory of auditability for allocation and social choice mechanisms",
		"URL": "https://doi.org/10.1145/3593013.3594017",
		"author": [
			{
				"family": "Grigoryan",
				"given": "Aram"
			},
			{
				"family": "Möller",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "franchiDetectingDisparitiesPolice2023a",
		"type": "paper-conference",
		"abstract": "Large-scale policing data is vital for detecting inequity in police behavior and policing algorithms. However, one important type of policing data remains largely unavailable within the United States: aggregated police deployment data capturing which neighborhoods have the heaviest police presences. Here we show that disparities in police deployment levels can be quantified by detecting police vehicles in dashcam images of public street scenes. Using a dataset of 24,803,854 dashcam images from rideshare drivers in New York City, we find that police vehicles can be detected with high accuracy (average precision 0.82, AUC 0.99) and identify 233,596 images which contain police vehicles. There is substantial inequality across neighborhoods in police vehicle deployment levels. The neighborhood with the highest deployment levels has almost 20 times higher levels than the neighborhood with the lowest. Two strikingly different types of areas experience high police vehicle deployments — 1) dense, higher-income, commercial areas and 2) lower-income neighborhoods with higher proportions of Black and Hispanic residents. We discuss the implications of these disparities for policing equity and for algorithms trained on policing data.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594020",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "534–544",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting disparities in police deployments using dashcam data",
		"URL": "https://doi.org/10.1145/3593013.3594020",
		"author": [
			{
				"family": "Franchi",
				"given": "Matt"
			},
			{
				"family": "Zamfirescu-Pereira",
				"given": "J.D."
			},
			{
				"family": "Ju",
				"given": "Wendy"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangDelayedIndirectImpacts2023a",
		"type": "paper-conference",
		"abstract": "The impacts of link recommendations on social networks are challenging to evaluate, due to feedback loops between algorithmic recommendations and underlying network dynamics. Observational studies have limitations in answering causal questions; naive A/B experiments often result in biased evaluations due to unaccounted network interference and finally, existing simulations primarily employ static network models that do not take into account dynamics. Departing from existing approaches, we employ simulations to study dynamic impacts of link recommendations. Specifically, we propose an extension to the Jackson-Rogers network evolution model and investigate how link recommendations affect network evolution over time. Our experiments demonstrate that link recommendations can have surprising delayed and indirect effects on the structural properties of networks. Effects of recommendations vary in the short-term and long-term, such as the immediate reduction in degree inequality but eventual increase in degree inequality through friend-of-friend recommendations. Furthermore, even after recommendations are discontinued, their impacts can persist in the network, in part by altering natural network evolution dynamics. These results provide valuable insights into the interplay between algorithmic interventions and natural network dynamics and highlight the limitations of current evaluation paradigms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594021",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "545–557",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Delayed and indirect impacts of link recommendations",
		"URL": "https://doi.org/10.1145/3593013.3594021",
		"author": [
			{
				"family": "Zhang",
				"given": "Han"
			},
			{
				"family": "Lu",
				"given": "Shangen"
			},
			{
				"family": "Wang",
				"given": "Yixin"
			},
			{
				"family": "Curmei",
				"given": "Mihaela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "theusStrivingAffirmativeAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "The social sciences have a keen eye for the complex forces that shape our diverse experiences and excel in uncovering an issue's genesis by making sense of how the past has shaped the present. What is sometimes missing though is the practical application of this knowledge. Computer science disciplines and human-computer interaction on the other hand, are very skilled at identifying existing issues and at proposing practical solutions but sometimes miss to unpack and to scrutinize a problem's history and evolution. And while a lot of valuable domain specific knowledge exists, interdisciplinary socio-technical expertise is still scarce. This paper argues that a strong connection between these fields can counter the development of algorithmic systems that lead to inequitable consequences and instead support the design of algorithmic systems that result in more just outcomes and cultivate, what I call, affirmative algorithmic futures. To that end, this paper introduces a compendium of theoretical concepts and practical measures rooted in social science scholarship that foster socio-technical algorithmic system design practices and promote knowledge mobilization between disciplines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594022",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "558–568",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Striving for affirmative algorithmic futures: How the social sciences can promote more equitable and just algorithmic system design",
		"URL": "https://doi.org/10.1145/3593013.3594022",
		"author": [
			{
				"family": "Theus",
				"given": "Anna-Lena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bergmanRepresentationAIEvaluations2023b",
		"type": "paper-conference",
		"abstract": "Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with \"representation\" or \"representativeness\" generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be \"representative\"? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594019",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "519–533",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation in AI evaluations",
		"URL": "https://doi.org/10.1145/3593013.3594019",
		"author": [
			{
				"family": "Bergman",
				"given": "A. Stevie"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Wu",
				"given": "Boxi"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Kunesch",
				"given": "Markus"
			},
			{
				"family": "Duan",
				"given": "Isabella"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Isaac",
				"given": "William"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chowdharyCanWorkersMeaningfully2023a",
		"type": "paper-conference",
		"abstract": "Sensing technologies deployed in the workplace can unobtrusively collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers’ meaningful consent raises privacy and ethical concerns. This paper unpacks challenges workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants’ expectations and capacity to consent to these technologies. We sketched possible interventions that could better support meaningful consent to workplace wellbeing technologies, by drawing on critical computing and feminist scholarship—which reframes consent from a purely individual choice to a structural condition experienced at the individual level that needs to be freely given, reversible, informed, enthusiastic, and specific (FRIES). The focus groups revealed how workers are vulnerable to “meaningless” consent—as they may be subject to power dynamics that minimize their ability to withhold consent and may thus experience an erosion of autonomy in their workplace, also undermining the value of data gathered in the name of “wellbeing.” To meaningfully consent, participants wanted changes to how the technology works and is being used, as well as to the policies and practices surrounding the technology. Our mapping of what prevents workers from meaningfully consenting to workplace wellbeing technologies (challenges) and what they require to do so (interventions) illustrates how the lack of meaningful consent is a structural problem requiring socio-technical solutions.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594023",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "569–582",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can workers meaningfully consent to workplace wellbeing technologies?",
		"URL": "https://doi.org/10.1145/3593013.3594023",
		"author": [
			{
				"family": "Chowdhary",
				"given": "Shreya"
			},
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Gray",
				"given": "Mary L"
			},
			{
				"family": "Suh",
				"given": "Jina"
			},
			{
				"family": "Olteanu",
				"given": "Alexandra"
			},
			{
				"family": "Saha",
				"given": "Koustuv"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuHonorEthicsChallenge2023a",
		"type": "paper-conference",
		"abstract": "Some researchers have recognized that privileged communities dominate the discourse on AI Ethics, and other voices need to be heard. As such, we identify the current ethics milieu as arising from WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and aim to expand the discussion to non-WEIRD global communities, who are also stakeholders in global sociotechnical systems. We argue that accounting for honor, along with its values and related concepts, would better approximate a global ethical perspective. This complex concept already underlies some of the WEIRD discourse on AI ethics, but certain cultural forms of honor also bring overlooked issues and perspectives to light. We first describe honor according to recent empirical and philosophical scholarship. We then review “consensus” principles for AI ethics framed from an honor-based perspective, grounding comparisons and contrasts via example settings such as content moderation, job hiring, and genomics databases. A better appreciation of the marginalized concept of honor could, we hope, lead to more productive AI value alignment discussions, and to AI systems that better reflect the needs and values of users around the globe.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594026",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "593–602",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Honor ethics: The challenge of globalizing value alignment in AI",
		"URL": "https://doi.org/10.1145/3593013.3594026",
		"author": [
			{
				"family": "Wu",
				"given": "Stephen Tze-Inn"
			},
			{
				"family": "Demetriou",
				"given": "Daniel"
			},
			{
				"family": "Husain",
				"given": "Rudwan Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "henzingerRuntimeMonitoringDynamic2023a",
		"type": "paper-conference",
		"abstract": "A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank’s credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator’s allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594028",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "604–614",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Runtime monitoring of dynamic fairness properties",
		"URL": "https://doi.org/10.1145/3593013.3594028",
		"author": [
			{
				"family": "Henzinger",
				"given": "Thomas"
			},
			{
				"family": "Karimi",
				"given": "Mahyar"
			},
			{
				"family": "Kueffner",
				"given": "Konstantin"
			},
			{
				"family": "Mallik",
				"given": "Kaushik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "amugongoInvigoratingUbuntuEthics2023a",
		"type": "paper-conference",
		"abstract": "The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of “I am because we are” in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594024",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "583–592",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Invigorating ubuntu ethics in AI for healthcare: Enabling equitable care",
		"URL": "https://doi.org/10.1145/3593013.3594024",
		"author": [
			{
				"family": "Amugongo",
				"given": "Lameck Mbangula"
			},
			{
				"family": "Bidwell",
				"given": "Nicola J."
			},
			{
				"family": "Corrigan",
				"given": "Caitlin C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lopezPowerResistanceTwitter2023a",
		"type": "paper-conference",
		"abstract": "In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first \"algorithmic bias bounty challenge\", inviting the general public to detect algorithmic harm in the cropping tool. Twitter’s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse? This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter’s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter’s response to the bias accusations and the way in which the company was able to effectively stabilize its position – rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594027",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "603",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Power and resistance in the twitter bias discourse",
		"URL": "https://doi.org/10.1145/3593013.3594027",
		"author": [
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rudnikCareCoordinationAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making has permeated health and care domains (e.g., automated diagnoses, fall detection, caregiver staffing). Researchers have raised concerns about how these algorithms are built and how they shape fair and ethical care practices. To investigate algorithm development and understand its impact on people who provide and coordinate care, we conducted a case study of a U.S.-based senior care network and platform. We interviewed 14 technologists, 9 paid caregivers, and 7 care coordinators to explore their interactions with the platform’s algorithms. We find that technologists draw on a multitude of moral frameworks to navigate complex and contradictory demands and expectations. Despite technologists’ espoused commitments to fairness, accountability, and transparency, the platform reassembles problematic aspects of care labor. By analyzing how technologists justify their work, the problems that they claim to solve, the solutions they present, and caregivers’ and coordinators’ experiences, we advance fairness research that focuses on agency and power asymmetries in algorithmic platforms. We (1) make an empirical contribution, revealing tensions when developing and implementing algorithms and (2) provide insight into the social processes that reproduce power asymmetries in algorithmic decision-making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594031",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "627–638",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Care and coordination in algorithmic systems: An economies of worth approach",
		"URL": "https://doi.org/10.1145/3593013.3594031",
		"author": [
			{
				"family": "Rudnik",
				"given": "John"
			},
			{
				"family": "Brewer",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "maYouSoundDepressed2023a",
		"type": "paper-conference",
		"abstract": "There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer “objective” depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user’s voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizing Sonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594032",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "639–650",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "You sound depressed: A case study on sonde health’s diagnostic use of voice analysis AI",
		"URL": "https://doi.org/10.1145/3593013.3594032",
		"author": [
			{
				"family": "Ma",
				"given": "Anna"
			},
			{
				"family": "Patitsas",
				"given": "Elizabeth"
			},
			{
				"family": "Sterne",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chanHarmsIncreasinglyAgentic2023",
		"type": "paper-conference",
		"abstract": "Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594033",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "651–666",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Harms from increasingly agentic algorithmic systems",
		"URL": "https://doi.org/10.1145/3593013.3594033",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Salganik",
				"given": "Rebecca"
			},
			{
				"family": "Markelius",
				"given": "Alva"
			},
			{
				"family": "Pang",
				"given": "Chris"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			},
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "He",
				"given": "Zhonghao"
			},
			{
				"family": "Duan",
				"given": "Yawen"
			},
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Lin",
				"given": "Michelle"
			},
			{
				"family": "Mayhew",
				"given": "Alex"
			},
			{
				"family": "Collins",
				"given": "Katherine"
			},
			{
				"family": "Molamohammadi",
				"given": "Maryam"
			},
			{
				"family": "Burden",
				"given": "John"
			},
			{
				"family": "Zhao",
				"given": "Wanru"
			},
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Voudouris",
				"given": "Konstantinos"
			},
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zuziakDataCollaborativesUse2023a",
		"type": "paper-conference",
		"abstract": "The endeavor to find appropriate data governance frameworks capable of reconciling conflicting interests in data has dramatically gained importance across disciplines and has been discussed among legal scholars, computer scientists as well as policy-makers alike. The predominant part of the current discussion is centered around the challenging task of creating a data governance framework where data is ‘as open as possible and as closed as necessary’. In this article, we elaborate on modern approaches to data governance and their limitations. It analyses how propositions evolved from property rights in data towards the creation of data access and data sharing obligations and how the corresponding debates reflect the difficulty of developing approaches that reconcile seemingly opposite objectives – such as giving individuals and businesses more control over ‘their’ data while at the same time ensuring its availability to different stakeholders. Furthermore, we propose a wider acknowledgement of data collaboratives powered by decentralised learning techniques as a possible remedy to the shortcomings of current data governance schemes. Hence, we propose a mild formalization of the set of existing technological solutions that could inform existing approaches to data governance issues. Our proposition is based on an abstractive notion of collaborative computation as well as on several principles that are essential for our definition of data collaboratives. By adopting an interdisciplinary perspective on data governance, this article highlights how innovative technological solutions can enhance control over data while at the same time ensuring its availability to other stakeholders and thereby contributing to the achievement of the policy goals of the European Strategy for Data.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594029",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "615–625",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data collaboratives with the use of decentralised learning",
		"URL": "https://doi.org/10.1145/3593013.3594029",
		"author": [
			{
				"family": "Zuziak",
				"given": "Maciej Krzysztof"
			},
			{
				"family": "Hinrichs",
				"given": "Onntje"
			},
			{
				"family": "Abdrassulova",
				"given": "Aizhan"
			},
			{
				"family": "Rinzivillo",
				"given": "Salvatore"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chengHowRedundantAre2023a",
		"type": "paper-conference",
		"abstract": "We address two emerging concerns in algorithmic fairness: (i) redundant encodings of race – the notion that machine learning models encode race with probability nearing one as the feature set grows – which is widely noted in theory, with little empirical evidence; and (ii) the lack of race and ethnicity data in many domains, where state-of-the-art remains (Naive) Bayesian Improved Surname Geocoding (BISG) that relies on name and geographic information. We leverage a novel and highly granular dataset of over 7.7 million patients’ electronic health records to provide one of the first empirical studies of redundant encodings in a realistic health care setting and examine the ability to assess health care disparities when race may be missing. First, we show that machine learning (random forest) applied to name and geographic information can improve on BISG, driven primarily by better performance in identifying minority groups. Second, contrary to theoretical concerns about redundant encodings as undercutting anti-discrimination law’s anti-classification principle, additional electronic health information provides little marginal information about race and ethnicity: race still remains measured with substantial noise. Third, we show how machine learning can enable the disaggregation of racial categories, responding to longstanding critiques of the government race reporting standard. Fourth, we show that an increasing feature set can differentially impact performance on majority and minority groups. Our findings address important questions for fairness in machine learning and algorithmic decision-making, enabling the assessment of disparities, tempering concerns about redundant encodings in one important setting, and demonstrating how bigger data can shape the accuracy of race imputations in nuanced ways.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594034",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 20\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "667–686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How redundant are redundant encodings? Blindness in the wild and racial disparity when race is unobserved",
		"URL": "https://doi.org/10.1145/3593013.3594034",
		"author": [
			{
				"family": "Cheng",
				"given": "Lingwei"
			},
			{
				"family": "Gallegos",
				"given": "Isabel O"
			},
			{
				"family": "Ouyang",
				"given": "Derek"
			},
			{
				"family": "Goldin",
				"given": "Jacob"
			},
			{
				"family": "Ho",
				"given": "Dan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lazarSitePredictiveJustice2023a",
		"type": "paper-conference",
		"abstract": "Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these ‘cheap’ predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594035",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "687",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the site of predictive justice",
		"URL": "https://doi.org/10.1145/3593013.3594035",
		"author": [
			{
				"family": "Lazar",
				"given": "Seth"
			},
			{
				"family": "Stone",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dengInvestigatingPracticesOpportunities2023a",
		"type": "paper-conference",
		"abstract": "An emerging body of research indicates that ineffective cross-functional collaboration – the interdisciplinary work done by industry practitioners across roles – represents a major barrier to addressing issues of fairness in AI design and development. In this research, we sought to better understand practitioners’ current practices and tactics to enact cross-functional collaboration for AI fairness, in order to identify opportunities to support more effective collaboration. We conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies. We found that practitioners engaged in bridging work to overcome frictions in understanding, contextualization, and evaluation around AI fairness across roles. In addition, in organizational contexts with a lack of resources and incentives for fairness work, practitioners often piggybacked on existing requirements (e.g., for privacy assessments) and AI development norms (e.g., the use of quantitative evaluation metrics), although they worry that these tactics may be fundamentally compromised. Finally, we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness. We close by discussing opportunities for both FAccT researchers and AI practitioners to better support cross-functional collaboration for fairness in the design and development of AI systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594037",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "705–716",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating practices and opportunities for cross-functional collaboration around AI fairness in industry practice",
		"URL": "https://doi.org/10.1145/3593013.3594037",
		"author": [
			{
				"family": "Deng",
				"given": "Wesley Hanwen"
			},
			{
				"family": "Yildirim",
				"given": "Nur"
			},
			{
				"family": "Chang",
				"given": "Monica"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Madaio",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "karanYourBrowsingHistory2023a",
		"type": "paper-conference",
		"abstract": "In many online markets we “shop alone” — there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents’ attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers’ pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is  0.44intheflightmarketand 0.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as  6.00forflightsand 3.00 for hotels (i.e., 15 × and 33 × the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594038",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "717–735",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Your browsing history may cost you: A framework for discovering differential pricing in non-transparent markets",
		"URL": "https://doi.org/10.1145/3593013.3594038",
		"author": [
			{
				"family": "Karan",
				"given": "Aditya"
			},
			{
				"family": "Balepur",
				"given": "Naina"
			},
			{
				"family": "Sundaram",
				"given": "Hari"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guerdanGroundlessTruthCausal2023",
		"type": "paper-conference",
		"abstract": "A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594036",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "688–704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ground(less) truth: A causal framework for proxy labels in human-algorithm decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594036",
		"author": [
			{
				"family": "Guerdan",
				"given": "Luke"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "richardsonAddremoveorrelabelPractitionerfriendlyBias2023a",
		"type": "paper-conference",
		"abstract": "Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594039",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "736–752",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Add-remove-or-relabel: Practitioner-friendly bias mitigation via influential fairness",
		"URL": "https://doi.org/10.1145/3593013.3594039",
		"author": [
			{
				"family": "Richardson",
				"given": "Brianna"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Ramamurthy",
				"given": "Karthikeyan Natesan"
			},
			{
				"family": "Varshney",
				"given": "Kush"
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Gilbert",
				"given": "Juan E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "singhFairAssignStochasticallyFair2023a",
		"type": "paper-conference",
		"abstract": "Increasing adoption of online commerce has created income opportunities for millions of delivery drivers who deliver items, from clothes and smartphones to foods and medicines, to customers. Despite their indispensability for the ecosystem, the drivers are often deprived of employment benefits and their earnings are tied to the number of successful deliveries, forcing them to go on repeated strikes to demand fair wage. In addition to low wages, there is considerable variability in driver incomes. One major component contributing to this variability is the static assignment of drivers to delivery zones as different zones likely have different workloads and hence earning opportunities. To reduce this variability, we directly engage with the gig delivery drivers to understand their perspectives on fair income distribution, and incorporate the same by proposing FairAssign for dynamic assignment of drivers to delivery zones to ensure fair distribution of earning opportunities. Specifically, we introduce a framework for stochastic pairwise fairness where, based on a similarity measure between the drivers, individual drivers are assigned to probability distributions over different zones such that similar individuals are mapped to statistically similar distributions. To realize these distributions, we develop a randomized dependent rounding based efficient sampling algorithm such that the workload constraints in each zone are satisfied, and the expected travel cost is minimized. Extensive experiments on real-world food delivery data and semi-synthetic ecommerce data show the efficacy of FairAssign over other baselines.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594040",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "753–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairAssign: Stochastically fair driver assignment in gig delivery platforms",
		"URL": "https://doi.org/10.1145/3593013.3594040",
		"author": [
			{
				"family": "Singh",
				"given": "Daman Deep"
			},
			{
				"family": "Das",
				"given": "Syamantak"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shenGenderAnimusCan2023a",
		"type": "paper-conference",
		"abstract": "This paper investigates gender discrimination and its underlying drivers on a prominent Chinese online peer-to-peer (P2P) lending platform. While existing studies on P2P lending focus on disparate treatment (DT), DT narrowly recognizes direct discrimination and overlooks indirect and proxy discrimination, providing an incomplete picture. In this work, we measure a broadened discrimination notion called disparate impact (DI), which encompasses any disparity in the loan’s funding rate that does not commensurate with the actual return rate. We develop a two-stage predictor substitution approach to estimate DI from observational data. Our findings reveal (i) female borrowers, given identical actual return rates, are 3.97% more likely to receive funding, (ii) at least of this DI favoring female is indirect or proxy discrimination, and (iii) DT indeed underestimates the overall female favoritism by . However, we also identify the overall female favoritism can be explained by one specific discrimination driver, rational statistical discrimination, wherein investors accurately predict the expected return rate from imperfect observations. Furthermore, female borrowers still require 2% higher expected return rate to secure funding, indicating another driver taste-based discrimination co-exists and is against female. These results altogether tell a cautionary tale: on one hand, P2P lending provides a valuable alternative credit market where the affirmative action to support female naturally emerges from the rational crowd; on the other hand, while the overall discrimination effect (both in terms of DI or DT) favors female, concerning taste-based discrimination can persist and can be obscured by other co-existing discrimination drivers, such as statistical discrimination.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594042",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "775–791",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender animus can still exist under favorable disparate impact: a cautionary tale from online P2P lending",
		"URL": "https://doi.org/10.1145/3593013.3594042",
		"author": [
			{
				"family": "Shen",
				"given": "Xudong"
			},
			{
				"family": "Tan",
				"given": "Tianhui"
			},
			{
				"family": "Phan",
				"given": "Tuan"
			},
			{
				"family": "Keppo",
				"given": "Jussi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "radenskyThinkYouMight2023a",
		"type": "paper-conference",
		"abstract": "With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like...” or “I think you will like...,” but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594043",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "792–804",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I think you might like this”: Exploring effects of confidence signal patterns on trust in and reliance on conversational recommender systems",
		"URL": "https://doi.org/10.1145/3593013.3594043",
		"author": [
			{
				"family": "Radensky",
				"given": "Marissa"
			},
			{
				"family": "Séguin",
				"given": "Julie Anne"
			},
			{
				"family": "Lim",
				"given": "Jang Soo"
			},
			{
				"family": "Olson",
				"given": "Kristen"
			},
			{
				"family": "Geiger",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "weertsAlgorithmicUnfairnessLens2023a",
		"type": "paper-conference",
		"abstract": "Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594044",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "805–816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic unfairness through the lens of EU non-discrimination law: Or why the law is not a decision tree",
		"URL": "https://doi.org/10.1145/3593013.3594044",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Xenidis",
				"given": "Raphaële"
			},
			{
				"family": "Tarissan",
				"given": "Fabien"
			},
			{
				"family": "Olsen",
				"given": "Henrik Palmer"
			},
			{
				"family": "Pechenizkiy",
				"given": "Mykola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "petersenAssessingFairnessRisk2023a",
		"type": "paper-conference",
		"abstract": "Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings – in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594045",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "817–829",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On (assessing) the fairness of risk score models",
		"URL": "https://doi.org/10.1145/3593013.3594045",
		"author": [
			{
				"family": "Petersen",
				"given": "Eike"
			},
			{
				"family": "Ganz",
				"given": "Melanie"
			},
			{
				"family": "Holm",
				"given": "Sune"
			},
			{
				"family": "Feragen",
				"given": "Aasa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dejongeUNFairSearchEngine2023a",
		"type": "paper-conference",
		"abstract": "Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594046",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "830–839",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "UNFair: Search engine manipulation, undetectable by amortized inequity",
		"URL": "https://doi.org/10.1145/3593013.3594046",
		"author": [
			{
				"family": "De Jonge",
				"given": "Tim"
			},
			{
				"family": "Hiemstra",
				"given": "Djoerd"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "valdiviaDataficationGenealogiesAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594047",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "840–850",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Datafication genealogies beyond algorithmic fairness: Making up racialised subjects",
		"URL": "https://doi.org/10.1145/3593013.3594047",
		"author": [
			{
				"family": "Valdivia",
				"given": "Ana"
			},
			{
				"family": "Tazzioli",
				"given": "Martina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "defranceMaximalFairness2023a",
		"type": "paper-conference",
		"abstract": "Fairness in AI has garnered quite some attention in research, and increasingly also in society. The so-called \"Impossibility Theorem\" has been one of the more striking research results with both theoretical and practical consequences, as it states that satisfying a certain combination of fairness measures is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness measures that can be simultaneously satisfied. The fairness measures used are demographic parity, equal opportunity, predictive equality, predictive parity, false omission rate parity, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness measures are possible, among which are seven combinations of two measures, and five combinations of three measures. Our work raises interesting questions regarding the practical relevance of each of these 12 maximal fairness notions in various scenarios.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594048",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 30\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "851–880",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Maximal fairness",
		"URL": "https://doi.org/10.1145/3593013.3594048",
		"author": [
			{
				"family": "Defrance",
				"given": "Marybeth"
			},
			{
				"family": "De Bie",
				"given": "Tijl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lyonsAlgorithmicDecisionsDesire2023a",
		"type": "paper-conference",
		"abstract": "In this paper, we explore why decision subjects generally express a preference for human reviewers of algorithmic decisions over algorithmic reviewers. We theorise that decision subjects desire control over the decision-making process in order to increase their chance of receiving a favourable outcome. To this end, human reviewers will be seen as easier to influence than algorithmic reviewers, thus providing more control. Using an online study we find that: (1) people who have a greater Desire for Control over their lives exhibit a stronger preference for human review; (2) interaction with a reviewer is important because it enables influence and ensures understanding; and (3) the higher the impact of a decision, the greater the incentive to influence the outcome, and the greater the preference for human review. Our qualitative results confirm that outcome favourability is a driver for reviewer preference, but so is the desire to be treated with dignity.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594041",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "764–774",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic decisions, desire for control, and the preference for human review over algorithmic review",
		"URL": "https://doi.org/10.1145/3593013.3594041",
		"author": [
			{
				"family": "Lyons",
				"given": "Henrietta"
			},
			{
				"family": "Miller",
				"given": "Tim"
			},
			{
				"family": "Velloso",
				"given": "Eduardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "papakyriakopoulosAugmentedDatasheetsSpeech2023a",
		"type": "paper-conference",
		"abstract": "Speech datasets are crucial for training Speech Language Technologies (SLT); however, the lack of diversity of the underlying training data can lead to serious limitations in building equitable and robust SLT products, especially along dimensions of language, accent, dialect, variety, and speech impairment—and the intersectionality of speech features with socioeconomic and demographic features. Furthermore, there is often a lack of oversight on the underlying training data—commonly built on massive web-crawling and/or publicly available speech—with regard to the ethics of such data collection. To encourage standardized documentation of such speech data components, we introduce an augmented datasheet for speech datasets1, which can be used in addition to “Datasheets for Datasets” [78]. We then exemplify the importance of each question in our augmented datasheet based on in-depth literature reviews of speech data used in domains such as machine learning, linguistics, and health. Finally, we encourage practitioners—ranging from dataset creators to researchers—to use our augmented datasheet to better define the scope, properties, and limits of speech datasets, while also encouraging consideration of data-subject protection and user community empowerment. Ethical dataset creation is not a one-size-fits-all process, but dataset creators can use our augmented datasheet to reflexively consider the social context of related SLT applications and data sources in order to foster more inclusive SLT products downstream.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594049",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 24\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "881–904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Augmented datasheets for speech datasets and ethical decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594049",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Choi",
				"given": "Anna Seo Gyeong"
			},
			{
				"family": "Thong",
				"given": "William"
			},
			{
				"family": "Zhao",
				"given": "Dora"
			},
			{
				"family": "Andrews",
				"given": "Jerone"
			},
			{
				"family": "Bourke",
				"given": "Rebecca"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "peysakhovichImplementingFairnessConstraints2023a",
		"type": "paper-conference",
		"abstract": "Fisher markets are those where buyers with budgets compete for scarce items, a natural model for many real world markets including online advertising. A market equilibrium is a set of prices and allocations of items such that supply meets demand. We show how market designers can use taxes or subsidies in Fisher markets to ensure that market equilibrium outcomes fall within certain constraints. We show how these taxes and subsidies can be computed even in an online setting where the market designer does not have access to private valuations. We adapt various types of fairness constraints proposed in existing literature to the market case and show who benefits and who loses from these constraints, as well as the extent to which properties of markets including Pareto optimality, envy-freeness, and incentive compatibility are preserved. We find that some prior discussed constraints have few guarantees in terms of who is made better or worse off by their imposition.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594051",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "916–930",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Implementing fairness constraints in markets using taxes and subsidies",
		"URL": "https://doi.org/10.1145/3593013.3594051",
		"author": [
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			},
			{
				"family": "Kroer",
				"given": "Christian"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hemmentAIPublicEye2023a",
		"type": "paper-conference",
		"abstract": "Recent advances in diffusion models and large language models have underpinned a new generation of powerful and accessible tools, and some of the most publicly visible applications are for artistic endeavour. Such tools, however, provide little scope for deeper understanding of AI systems, while the growing public interest in them can eclipse notice of the vibrant community of artists who have long worked with other forms of AI. We explore the potential for AI Art – particularly work in which AI is both tool and topic – to facilitate public AI literacies and consider how tactics developed before the current generative AI boom have continued relevance today. We look at the strategies of critical AI artists to scaffold public understanding of AI and enhance legibility for non-experts. This paper also investigates how collaborations between artists and AI researchers and designers can illuminate key technical and social issues relevant to the development of AI. The study entailed workshops between three professional artists who work with AI and a cross-disciplinary set of academic participants. This paper reports on these workshops and presents the intentions and strategies expressed by the artists, as well as insights of relevance to the research community on public AI literacies. We find that critical AI art can link underlying technical systems to structural issues of power and facilitate experiential learning that is situated and embodied, valuing interpretation over explanation. The findings also demonstrate the importance of transdisciplinary conversations around art, ethics and the political economy of AI technologies and how these dialogues may feed into AI design processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594052",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "931–942",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI in the public eye: Investigating public AI literacy through AI art",
		"URL": "https://doi.org/10.1145/3593013.3594052",
		"author": [
			{
				"family": "Hemment",
				"given": "Drew"
			},
			{
				"family": "Currie",
				"given": "Morgan"
			},
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Elwes",
				"given": "Jake"
			},
			{
				"family": "Ridler",
				"given": "Anna"
			},
			{
				"family": "Sinders",
				"given": "Caroline"
			},
			{
				"family": "Vidmar",
				"given": "Matjaz"
			},
			{
				"family": "Hill",
				"given": "Robin"
			},
			{
				"family": "Warner",
				"given": "Holly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "golpayeganiBeHighriskNot2023a",
		"type": "paper-conference",
		"abstract": "The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594050",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "905–915",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "To be high-risk, or not to Be—Semantic specifications and implications of the AI act’s high-risk AI applications and harmonised standards",
		"URL": "https://doi.org/10.1145/3593013.3594050",
		"author": [
			{
				"family": "Golpayegani",
				"given": "Delaram"
			},
			{
				"family": "Pandit",
				"given": "Harshvardhan J."
			},
			{
				"family": "Lewis",
				"given": "Dave"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bertrandQuestioningAbilityFeaturebased2023a",
		"type": "paper-conference",
		"abstract": "Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a “fully informed decision”. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either “dialogic”—more textual—or “graphical”—more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users’ trust in the recommendations of the robo-advisor, sometimes to the users’ detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594053",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "943–958",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594053",
		"author": [
			{
				"family": "Bertrand",
				"given": "Astrid"
			},
			{
				"family": "Eagan",
				"given": "James R."
			},
			{
				"family": "Maxwell",
				"given": "Winston"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dominguezhernandezAddressingContingencyAlgorithmic2023",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of “truth” used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies—key moments during model development that could lead to different future outcomes, uncertainty and harmful effects as these tools are deployed by social media platforms. We conclude by offering a tentative path toward reflexive and responsible development of ML tools for moderating misinformation and other harmful content online.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594055",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 1\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "971",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda",
		"URL": "https://doi.org/10.1145/3593013.3594055",
		"author": [
			{
				"family": "Domínguez Hernández",
				"given": "Andrés"
			},
			{
				"family": "Owen",
				"given": "Richard"
			},
			{
				"family": "Nielsen",
				"given": "Dan Saattrup"
			},
			{
				"family": "Mcconville",
				"given": "Ryan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wangWeTryEmpower2023a",
		"type": "paper-conference",
		"abstract": "Previous work on technology in Public Employment Services and job market chances has focused on profiling systems that are intended for tasks such as assessing and classifying jobseekers. To integrate into the local job market, migrants and refugees seek support from the Public Employment Services (PES), but also non-profit, non-governmental organizations (herein referred to as third sector organizations, or TSOs). How do design visions for technologies to support jobseekers change when developed not under bureaucratic rules but by people interacting directly and informally with jobseekers? We focus on the perspectives of TSO workers assisting migrants and refugees seeking support for their job search. Through interviews and a design fiction exercise, we investigate (1) the role of TSO workers, (2) factors beyond those used in profiling systems that they consider relevant, and (3) their ideal technology. We describe how TSO workers contextualize formal criteria used in profiling systems while prioritising jobseekers’ personal interests and strengths. Based on our findings on existing tools and methods, and imagined future technologies, we propose a software-based project that expands existing job taxonomies into a coordinated resource combining job characteristics, required competencies, and soft skills to support multiple informational tools for jobseekers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594056",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "972–983",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“We try to empower them” - exploring future technologies to support migrant jobseekers",
		"URL": "https://doi.org/10.1145/3593013.3594056",
		"author": [
			{
				"family": "Wang",
				"given": "Sonja Mei"
			},
			{
				"family": "Scott",
				"given": "Kristen M"
			},
			{
				"family": "Artemenko",
				"given": "Margarita"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ehyaeiRobustnessImpliesFairness2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594057",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "984–1001",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robustness implies fairness in causal algorithmic recourse",
		"URL": "https://doi.org/10.1145/3593013.3594057",
		"author": [
			{
				"family": "Ehyaei",
				"given": "Ahmad-Reza"
			},
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Schoelkopf",
				"given": "Bernhard"
			},
			{
				"family": "Maghsudi",
				"given": "Setareh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "schmudeImpactExplanationsUnderstanding2023a",
		"type": "paper-conference",
		"abstract": "Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by \"high-risk\" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a \"high-risk\" ADM system to participants and analyse their responses both inductively and deductively, using the \"six facets of understanding\" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the \"six facets\" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the \"dialogue\" modality as a valid explanation approach to increase participant engagement and interaction with the \"explainer\", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the \"six facets\" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594054",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "959–970",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the impact of explanations on understanding of algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3593013.3594054",
		"author": [
			{
				"family": "Schmude",
				"given": "Timothée"
			},
			{
				"family": "Koesten",
				"given": "Laura"
			},
			{
				"family": "Möller",
				"given": "Torsten"
			},
			{
				"family": "Tschiatschek",
				"given": "Sebastian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "baumannBiasDemandModelling2023a",
		"type": "paper-conference",
		"abstract": "Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people’s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594058",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1002–1013",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias on demand: A modelling framework that generates synthetic data with bias",
		"URL": "https://doi.org/10.1145/3593013.3594058",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Castelnovo",
				"given": "Alessandro"
			},
			{
				"family": "Crupi",
				"given": "Riccardo"
			},
			{
				"family": "Inverardi",
				"given": "Nicole"
			},
			{
				"family": "Regoli",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "alertubellaACROCPoLisDescriptiveFramework2023",
		"type": "paper-conference",
		"abstract": "Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594059",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1014–1025",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ACROCPoLis: A descriptive framework for making sense of fairness",
		"URL": "https://doi.org/10.1145/3593013.3594059",
		"author": [
			{
				"family": "Aler Tubella",
				"given": "Andrea"
			},
			{
				"family": "Coelho Mollo",
				"given": "Dimitri"
			},
			{
				"family": "Dahlgren Lindström",
				"given": "Adam"
			},
			{
				"family": "Devinney",
				"given": "Hannah"
			},
			{
				"family": "Dignum",
				"given": "Virginia"
			},
			{
				"family": "Ericson",
				"given": "Petter"
			},
			{
				"family": "Jonsson",
				"given": "Anna"
			},
			{
				"family": "Kampik",
				"given": "Timotheus"
			},
			{
				"family": "Lenaerts",
				"given": "Tom"
			},
			{
				"family": "Mendez",
				"given": "Julian Alfredo"
			},
			{
				"family": "Nieves",
				"given": "Juan Carlos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jingLaborTransparencySituated2023a",
		"type": "paper-conference",
		"abstract": "Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594060",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1026–1037",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards labor transparency in situated computational systems impact research",
		"URL": "https://doi.org/10.1145/3593013.3594060",
		"author": [
			{
				"family": "Jing",
				"given": "Felicia S."
			},
			{
				"family": "Berger",
				"given": "Sara E."
			},
			{
				"family": "Becerra Sandoval",
				"given": "Juana Catalina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "abbasiMeasuringMitigatingVoting2023a",
		"type": "paper-conference",
		"abstract": "Voter suppression and associated racial disparities in access to voting are long-standing civil rights concerns in the United States. A history of violent explicit discouragement has shifted to more subtle access limitations that can include long lines and wait times, long travel times to reach a polling station, and other logistical barriers to voting. Our focus in this work is on quantifying disparities in voting access pertaining to the overall time-to-vote, and how they could be remedied via a better choice of polling location or provisioning more sites where voters can cast ballots. However, appropriately calibrating access disparities is difficult because of the need to account for factors such as population density and different community expectations for reasonable travel times. In this paper, we perform one of the first large-scale studies of voter access to polling locations, using real-world voter data from Florida and North Carolina in the 2020 general election. We develop a methodology for the calibrated measurement of disparities in polling location \"load\" and distance to polling locations based on a novel normalized distance metric to model the voter experience of distance. We find that voter turnout is reduced when this normalized distance to polling locations increases, and that non-white voters had to travel further to the polls in Florida (using this normalized distance) than White voters. We also introduce algorithms, with modifications to handle scale, that can reduce these disparities by suggesting new polling locations from a given list of identified public locations (including schools and libraries). The developed voting access measurement methodology and algorithmic remediation technique demonstrates that better polling location placement is possible.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594061",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1038–1048",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring and mitigating voting access disparities: a study of race and polling locations in Florida and North Carolina",
		"URL": "https://doi.org/10.1145/3593013.3594061",
		"author": [
			{
				"family": "Abbasi",
				"given": "Mohsen"
			},
			{
				"family": "Barrett",
				"given": "Calvin"
			},
			{
				"family": "Lum",
				"given": "Kristian"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "luriaCodesignPerspectivesAlgorithm2023a",
		"type": "paper-conference",
		"abstract": "Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes’ strengths and weaknesses, and areas of exploration for future work.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594064",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1076–1087",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-design perspectives on algorithm transparency reporting: Guidelines and prototypes",
		"URL": "https://doi.org/10.1145/3593013.3594064",
		"author": [
			{
				"family": "Luria",
				"given": "Michal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "currieEmotionsDynamicAssemblages2023a",
		"type": "paper-conference",
		"abstract": "In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to. We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area. Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users’ lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users’ interactions with ADMs shapes their actions responding to these systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594066",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1101–1111",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Emotions and dynamic assemblages: A study of automated social security using qualitative longitudinal research",
		"URL": "https://doi.org/10.1145/3593013.3594066",
		"author": [
			{
				"family": "Currie",
				"given": "Morgan"
			},
			{
				"family": "Podoletz",
				"given": "Lena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hackerRegulatingChatGPTOther2023a",
		"type": "paper-conference",
		"abstract": "Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594067",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1112–1123",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating ChatGPT and other large generative AI models",
		"URL": "https://doi.org/10.1145/3593013.3594067",
		"author": [
			{
				"family": "Hacker",
				"given": "Philipp"
			},
			{
				"family": "Engel",
				"given": "Andreas"
			},
			{
				"family": "Mauer",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "holtgenRichnessCalibration2023a",
		"type": "paper-conference",
		"abstract": "Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group errors, generalising previously proposed calibration scores. Complementary to such population-level scores, we explore calibration scores at the individual level and analyse their relationship to choices of grouping. We draw on these insights to introduce and axiomatise fairness deviation measures for population-level scores. We demonstrate that with appropriate choices of grouping, these novel global fairness scores can provide notions of (sub-)group or individual fairness.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594068",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1124–1138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the richness of calibration",
		"URL": "https://doi.org/10.1145/3593013.3594068",
		"author": [
			{
				"family": "Höltgen",
				"given": "Benedikt"
			},
			{
				"family": "Williamson",
				"given": "Robert C"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "leidingerWhichStereotypesAre2023a",
		"type": "paper-conference",
		"abstract": "Warning: This paper contains content that may be offensive or upsetting.Language technologies that perpetuate stereotypes actively cement social hierarchies. This study enquires into the moderation of stereotypes in autocompletion results by Google, DuckDuckGo and Yahoo! We investigate the moderation of derogatory stereotypes for social groups, examining the content and sentiment of the autocompletions. We thereby demonstrate which categories are highly moderated (i.e., sexual orientation, religious affiliation, political groups and communities or peoples) and which less so (age and gender), both overall and per engine. We found that under-moderated categories contain results with negative sentiment and derogatory stereotypes. We also identify distinctive moderation strategies per engine, with Google and DuckDuckGo moderating greatly and Yahoo! being more permissive. The research has implications for both moderation of stereotypes in commercial autocompletion tools, as well as large language models in NLP, particularly the question of the content deserving of moderation.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594062",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1049–1061",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Which stereotypes are moderated and under-moderated in search engine autocompletion?",
		"URL": "https://doi.org/10.1145/3593013.3594062",
		"author": [
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Rogers",
				"given": "Richard"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pettiEthicalConsiderationsEarly2023a",
		"type": "paper-conference",
		"abstract": "While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594063",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1062–1075",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical considerations in the early detection of Alzheimer's disease using speech and AI",
		"URL": "https://doi.org/10.1145/3593013.3594063",
		"author": [
			{
				"family": "Petti",
				"given": "Ulla"
			},
			{
				"family": "Nyrup",
				"given": "Rune"
			},
			{
				"family": "Skopek",
				"given": "Jeffrey M."
			},
			{
				"family": "Korhonen",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lovatoMoreDataTypes2023a",
		"type": "paper-conference",
		"abstract": "Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594065",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1088–1100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More data types more problems: A temporal analysis of complexity, stability, and sensitivity in privacy policies",
		"URL": "https://doi.org/10.1145/3593013.3594065",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper"
			},
			{
				"family": "Mueller",
				"given": "Philip"
			},
			{
				"family": "Suchdev",
				"given": "Parisa"
			},
			{
				"family": "Dodds",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "paniguttiRoleExplainableAI2023a",
		"type": "paper-conference",
		"abstract": "The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594069",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1139–1150",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role of explainable AI in the context of the AI Act",
		"URL": "https://doi.org/10.1145/3593013.3594069",
		"author": [
			{
				"family": "Panigutti",
				"given": "Cecilia"
			},
			{
				"family": "Hamon",
				"given": "Ronan"
			},
			{
				"family": "Hupont",
				"given": "Isabelle"
			},
			{
				"family": "Fernandez Llorca",
				"given": "David"
			},
			{
				"family": "Fano Yela",
				"given": "Delia"
			},
			{
				"family": "Junklewitz",
				"given": "Henrik"
			},
			{
				"family": "Scalzo",
				"given": "Salvatore"
			},
			{
				"family": "Mazzini",
				"given": "Gabriele"
			},
			{
				"family": "Sanchez",
				"given": "Ignacio"
			},
			{
				"family": "Soler Garrido",
				"given": "Josep"
			},
			{
				"family": "Gomez",
				"given": "Emilia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liDimensionsDataLabor2023a",
		"type": "paper-conference",
		"abstract": "Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as “data labor”, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data’s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594070",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1151–1161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dimensions of data labor: A road map for researchers, activists, and policymakers to empower data producers",
		"URL": "https://doi.org/10.1145/3593013.3594070",
		"author": [
			{
				"family": "Li",
				"given": "Hanlin"
			},
			{
				"family": "Vincent",
				"given": "Nicholas"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Hecht",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grovesGoingPublicRole2023a",
		"type": "paper-conference",
		"abstract": "In recent years, discussions of responsible AI practices have seen growing support for ‘participatory AI’ approaches, intended to involve members of the public in the design and development of AI systems. Prior research has identified a lack of standardised methods or approaches for how to use participatory approaches in the AI development process. At present, there is a dearth of evidence on attitudes to and approaches for participation in the sites driving major AI developments: commercial AI labs. Through 12 semi-structured interviews with industry practitioners and subject-matter experts, this paper explores how commercial AI labs understand participatory AI approaches and the obstacles they have faced implementing these practices in the development of AI systems and research. We find that while interviewees view participation as a normative project that helps achieve ‘societally beneficial’ AI systems, practitioners face numerous barriers to embedding participatory approaches in their companies: participation is expensive and resource intensive, it is ‘atomised’ within companies, there is concern about exploitation, there is no incentive to be transparent about its adoption, and it is complicated by a lack of clear context. These barriers result in a piecemeal approach to participation that confers no decision-making power to participants and has little ongoing impact for AI labs. This paper’s contribution is to provide novel empirical research on the implementation of public participation in commercial AI labs, and shed light on the current challenges of using participatory approaches in this context.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594071",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1162–1173",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Going public: the role of public participation approaches in commercial AI labs",
		"URL": "https://doi.org/10.1145/3593013.3594071",
		"author": [
			{
				"family": "Groves",
				"given": "Lara"
			},
			{
				"family": "Peppin",
				"given": "Aidan"
			},
			{
				"family": "Strait",
				"given": "Andrew"
			},
			{
				"family": "Brennan",
				"given": "Jenny"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nanniniExplainabilityAIPolicies2023a",
		"type": "paper-conference",
		"abstract": "Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594074",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1198–1212",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability in AI policies: A critical review of communications, reports, regulations, and standards in the EU, US, and UK",
		"URL": "https://doi.org/10.1145/3593013.3594074",
		"author": [
			{
				"family": "Nannini",
				"given": "Luca"
			},
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Smith",
				"given": "Adam Leon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "quinonerocandelaDisentanglingOperationalizingAI2023",
		"type": "paper-conference",
		"abstract": "Operationalizing AI fairness at LinkedIn’s scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn’s AI fairness framework, shares the principles that support it, and illustrates their application through a case study. We hope this paper will encourage other big tech companies to join us in sharing their approach to operationalizing AI fairness at scale, so that together we can keep advancing this constantly evolving field.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594075",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1213–1228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling and operationalizing AI fairness at LinkedIn",
		"URL": "https://doi.org/10.1145/3593013.3594075",
		"author": [
			{
				"family": "Quiñonero Candela",
				"given": "Joaquin"
			},
			{
				"family": "Wu",
				"given": "Yuwen"
			},
			{
				"family": "Hsu",
				"given": "Brian"
			},
			{
				"family": "Jain",
				"given": "Sakshi"
			},
			{
				"family": "Ramos",
				"given": "Jennifer"
			},
			{
				"family": "Adams",
				"given": "Jon"
			},
			{
				"family": "Hallman",
				"given": "Robert"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ovalleImFullyWho2023",
		"type": "paper-conference",
		"abstract": "Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594078",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1246–1266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I’m fully who I am”: Towards centering transgender and non-binary voices to measure biases in open language generation",
		"URL": "https://doi.org/10.1145/3593013.3594078",
		"author": [
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Goyal",
				"given": "Palash"
			},
			{
				"family": "Dhamala",
				"given": "Jwala"
			},
			{
				"family": "Jaggers",
				"given": "Zachary"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			},
			{
				"family": "Galstyan",
				"given": "Aram"
			},
			{
				"family": "Zemel",
				"given": "Richard"
			},
			{
				"family": "Gupta",
				"given": "Rahul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wolfeContrastiveLanguagevisionAI2023a",
		"type": "paper-conference",
		"abstract": "Warning: The content of this paper may be upsetting or triggering.Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt; 0.80) and sadness (d &gt; 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594072",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1174–1185",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contrastive language-vision AI models pretrained on web-scraped multimodal data exhibit sexual objectification bias",
		"URL": "https://doi.org/10.1145/3593013.3594072",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Yang",
				"given": "Yiwei"
			},
			{
				"family": "Howe",
				"given": "Bill"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cobbeUnderstandingAccountabilityAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by ‘many hands’. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594073",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1186–1197",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding accountability in algorithmic supply chains",
		"URL": "https://doi.org/10.1145/3593013.3594073",
		"author": [
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Veale",
				"given": "Michael"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "calviEnhancingAIFairness2023a",
		"type": "paper-conference",
		"abstract": "How to protect people from algorithmic harms? A promising solution, although in its infancy, is algorithmic impact assessment (AIA). AIAs are iterative processes used to investigate the possible short and long terms societal impacts of AI systems before their use, but with ongoing monitoring and periodic revisiting even after their implementation. When conducted in a participatory and transparent fashion, they could create bridges across the legal, social and computer science domains, promoting the accountability of the entity performing them as well as public scrutiny. They could enable to re-attach the societal and regulatory context to the mathematical definition of fairness, thus expanding the formalistic approach thereto. Whilst the regulatory framework in the European Union currently lacks the obligation to perform such AIA, some other provisions are expected to play a role in AI development, leading the way towards more widespread adoption of AIA. These include the Data Protection Impact Assessment (DPIA) under the General Data Protection Regulation (GDPR), the risk assessment process under the Digital Services Act (DSA) and the Conformity Assessment (CA) foreseen under the AI Regulation proposal.In this paper, after briefly introducing the plurality of definitions of fairness in the legal, social and computer science domains, and explaining to which extent the current and upcoming legal framework mandates the adoption of fairness metrics, we will illustrate how AIA could create bridges between all these disciplines, allowing us to build fairer AI solutions. We will then recognise the role of DPIA, DSA risk assessment and CA by discussing the contributions they can offer towards AIA but also identify the aspects lacking therein. We will then identify how these assessment provisions could aid the overall technical discussion of introducing and assessing fairness in AI-based models and processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594076",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1229–1245",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enhancing AI fairness through impact assessment in the European Union: a legal and computer science perspective",
		"URL": "https://doi.org/10.1145/3593013.3594076",
		"author": [
			{
				"family": "Calvi",
				"given": "Alessandra"
			},
			{
				"family": "Kotzinos",
				"given": "Dimitris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "thebault-spiekerDiversePerspectivesCan2023a",
		"type": "paper-conference",
		"abstract": "In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594080",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1280–1291",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diverse perspectives can mitigate political bias in crowdsourced content moderation",
		"URL": "https://doi.org/10.1145/3593013.3594080",
		"author": [
			{
				"family": "Thebault-Spieker",
				"given": "Jacob"
			},
			{
				"family": "Venkatagiri",
				"given": "Sukrit"
			},
			{
				"family": "Mine",
				"given": "Naomi"
			},
			{
				"family": "Luther",
				"given": "Kurt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lucajAIRegulationNot2023a",
		"type": "paper-conference",
		"abstract": "The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594079",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1267–1279",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI regulation is (not) all you need",
		"URL": "https://doi.org/10.1145/3593013.3594079",
		"author": [
			{
				"family": "Lucaj",
				"given": "Laura"
			},
			{
				"family": "Smagt",
				"given": "Patrick",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Benbouzid",
				"given": "Djalel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gerchickDevilDetailsInterrogating2023a",
		"type": "paper-conference",
		"abstract": "The design decisions of developers and researchers in creating algorithmic tools — like constructing variables, performing feature selection, and binning model outputs — are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST’s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as “risky” by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes “making decisions based on as much information as possible,” even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594081",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1292–1310",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The devil is in the details: Interrogating values embedded in the allegheny family screening tool",
		"URL": "https://doi.org/10.1145/3593013.3594081",
		"author": [
			{
				"family": "Gerchick",
				"given": "Marissa"
			},
			{
				"family": "Jegede",
				"given": "Tobi"
			},
			{
				"family": "Shah",
				"given": "Tarak"
			},
			{
				"family": "Gutierrez",
				"given": "Ana"
			},
			{
				"family": "Beiers",
				"given": "Sophie"
			},
			{
				"family": "Shemtov",
				"given": "Noam"
			},
			{
				"family": "Xu",
				"given": "Kath"
			},
			{
				"family": "Samant",
				"given": "Anjana"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ajmaniSystematicReviewEthics2023a",
		"type": "paper-conference",
		"abstract": "Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples’ lives because of the area’s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving – it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or \"stickier\", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594082",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1311–1323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of ethics disclosures in predictive mental health research",
		"URL": "https://doi.org/10.1145/3593013.3594082",
		"author": [
			{
				"family": "Ajmani",
				"given": "Leah Hope"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			},
			{
				"family": "Mehta",
				"given": "Bijal"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			},
			{
				"family": "Zimmer",
				"given": "Michael"
			},
			{
				"family": "De Choudhury",
				"given": "Munmun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "abduEmpiricalAnalysisRacial2023a",
		"type": "paper-conference",
		"abstract": "Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594083",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1324–1333",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical analysis of racial categories in the algorithmic fairness literature",
		"URL": "https://doi.org/10.1145/3593013.3594083",
		"author": [
			{
				"family": "Abdu",
				"given": "Amina A."
			},
			{
				"family": "Pasquetto",
				"given": "Irene V."
			},
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hammanCanQueryingBias2023a",
		"type": "paper-conference",
		"abstract": "Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594086",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1358–1368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can querying for bias leak protected attributes? Achieving privacy with smooth sensitivity",
		"URL": "https://doi.org/10.1145/3593013.3594086",
		"author": [
			{
				"family": "Hamman",
				"given": "Faisal"
			},
			{
				"family": "Chen",
				"given": "Jiahao"
			},
			{
				"family": "Dutta",
				"given": "Sanghamitra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "laiScienceHumanAIDecision2023a",
		"type": "paper-conference",
		"abstract": "AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594087",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1369–1385",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a science of human-AI decision making: An overview of design space in empirical human-subject studies",
		"URL": "https://doi.org/10.1145/3593013.3594087",
		"author": [
			{
				"family": "Lai",
				"given": "Vivian"
			},
			{
				"family": "Chen",
				"given": "Chacha"
			},
			{
				"family": "Smith-Renner",
				"given": "Alison"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diberardinoAntiintentionalHarmsConceptual2023",
		"type": "paper-conference",
		"abstract": "‘Emotion AI’ is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options—interpretability, technical reform, and non-use—for their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594088",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1386–1395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(anti)-intentional harms: The conceptual pitfalls of emotion AI in education",
		"URL": "https://doi.org/10.1145/3593013.3594088",
		"author": [
			{
				"family": "DiBerardino",
				"given": "Nathalie"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kimOrganizationalGovernanceEmerging2023a",
		"type": "paper-conference",
		"abstract": "Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594089",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 22\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1396–1417",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Organizational governance of emerging technologies: AI adoption in healthcare",
		"URL": "https://doi.org/10.1145/3593013.3594089",
		"author": [
			{
				"family": "Kim",
				"given": "Jee Young"
			},
			{
				"family": "Boag",
				"given": "William"
			},
			{
				"family": "Gulamali",
				"given": "Freya"
			},
			{
				"family": "Hasan",
				"given": "Alifia"
			},
			{
				"family": "Hogg",
				"given": "Henry David Jeffry"
			},
			{
				"family": "Lifson",
				"given": "Mark"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre"
			},
			{
				"family": "Patel",
				"given": "Manesh"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Sehgal",
				"given": "Ajai"
			},
			{
				"family": "Shaw",
				"given": "Keo"
			},
			{
				"family": "Tobey",
				"given": "Danny"
			},
			{
				"family": "Valladares",
				"given": "Alexandra"
			},
			{
				"family": "Vidal",
				"given": "David"
			},
			{
				"family": "Balu",
				"given": "Suresh"
			},
			{
				"family": "Sendak",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "radiya-dixitSociotechnicalAuditAssessing2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594084",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1334–1346",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A sociotechnical audit: Assessing police use of facial recognition",
		"URL": "https://doi.org/10.1145/3593013.3594084",
		"author": [
			{
				"family": "Radiya-Dixit",
				"given": "Evani"
			},
			{
				"family": "Neff",
				"given": "Gina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cachelFairerTogetherMitigating2023a",
		"type": "paper-conference",
		"abstract": "In social choice, traditional Kemeny rank aggregation combines the preferences of voters, expressed as rankings, into a single consensus ranking without consideration for how this ranking may unfairly affect marginalized groups (i.e., racial or gender). Developing fair rank aggregation methods is critical due to their societal influence in applications prioritizing job applicants, funding proposals, and scheduling medical patients. In this work, we introduce the Fair Exposure Kemeny Aggregation Problem (FairExp-kap) for combining vast and diverse voter preferences into a single ranking that is not only a suitable consensus, but ensures opportunities are not withheld from marginalized groups. In formalizing FairExp-kap, we extend the fairness of exposure notion from information retrieval to the rank aggregation context and present a complimentary metric for voter preference representation. We design algorithms for solving FairExp-kap that explicitly account for position bias, a common ranking-based concern that end-users pay more attention to higher ranked candidates. epik solves FairExp-kap exactly by incorporating non-pairwise fairness of exposure into the pairwise Kemeny optimization; while the approximate epira is a candidate swapping algorithm, that guarantees ranked candidate fairness. Utilizing comprehensive synthetic simulations and six real-world datasets, we show the efficacy of our approach illustrating that we succeed in mitigating disparate group exposure unfairness in consensus rankings, while maximally representing voter preferences.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594085",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1347–1357",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairer together: Mitigating disparate exposure in kemeny rank aggregation",
		"URL": "https://doi.org/10.1145/3593013.3594085",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "norvalNavigatingAuditLandscape2023a",
		"type": "paper-conference",
		"abstract": "“Extended reality” (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount – where information revealing and enabling some reconstruction of an XR system’s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why. Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework’s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594090",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1418–1431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Navigating the audit landscape: A framework for developing transparent and auditable XR",
		"URL": "https://doi.org/10.1145/3593013.3594090",
		"author": [
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "metcalfTakingAlgorithmsCourts2023a",
		"type": "paper-conference",
		"abstract": "In widely used sociological descriptions of how accountability is structured through institutions, an “actor” (e.g., the developer) is accountable to a “forum” (e.g., regulatory agencies) empowered to pass judgements on and demand changes from the actor or enforce sanctions. However, questions about structuring accountability persist: why and how is a forum compelled to keep making demands of the actor when such demands are called for? To whom is a forum accountable in the performance of its responsibilities, and how can its practices and decisions be contested? In the context of algorithmic accountability, we contend that a robust accountability regime requires a triadic relationship, wherein the forum is also accountable to another entity: the public(s). Typically, as is the case with environmental impact assessments, public(s) make demands upon the forum's judgements and procedures through the courts, thereby establishing a minimum standard of due diligence. However, core challenges relating to: (1) lack of documentation, (2) difficulties in claiming standing, and (3) struggles around admissibility of expert evidence on and achieving consensus over the workings of algorithmic systems in adversarial proceedings prevent the public from approaching the courts when faced with algorithmic harms. In this paper, we demonstrate that the courts are the primary route—and the primary roadblock—in the pursuit of redress for algorithmic harms. Courts often find algorithmic harms non-cognizable and rarely require developers to address material claims of harm. To address the core challenges of taking algorithms to court, we develop a relational approach to algorithmic accountability that emphasizes not what the actors do nor the results of their actions, but rather how interlocking relationships of accountability are constituted in a triadic relationship between actors, forums, and public(s). As is the case in other regulatory domains, we believe that impact assessments (and similar accountability documentation) can provide the grounds for contestation between these parties, but only when that triad is structured such that the public(s) are able to cohere around shared experiences and interests, contest the outcomes of algorithmic systems that affect their lives, and make demands upon the other parties. Where courts now find algorithmic harms non-cognizable, an impact assessment regime can potentially create procedural rights to protect substantive rights of the public(s). This would require algorithmic accountability policies currently under consideration to provide the public(s) with adequate standing in courts, and opportunities to access and contest the actor's documentation and the forum's judgments.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594092",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 13\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1450–1462",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taking algorithms to courts: A relational approach to algorithmic accountability",
		"URL": "https://doi.org/10.1145/3593013.3594092",
		"author": [
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Tafesse",
				"given": "Emnet"
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nigatuCodesigningTransparencyLessons2023a",
		"type": "paper-conference",
		"abstract": "Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594093",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1463–1478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-designing for transparency: Lessons from building a document organization tool in the criminal justice domain",
		"URL": "https://doi.org/10.1145/3593013.3594093",
		"author": [
			{
				"family": "Nigatu",
				"given": "Hellina Hailu"
			},
			{
				"family": "Pickoff-White",
				"given": "Lisa"
			},
			{
				"family": "Canny",
				"given": "John"
			},
			{
				"family": "Chasins",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fieldExaminingRisksRacial2023a",
		"type": "paper-conference",
		"abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594094",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1479–1492",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Examining risks of racial biases in NLP tools for child protective services",
		"URL": "https://doi.org/10.1145/3593013.3594094",
		"author": [
			{
				"family": "Field",
				"given": "Anjalie"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Gandhi",
				"given": "Nupoor"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Putnam-Hornstein",
				"given": "Emily"
			},
			{
				"family": "Steier",
				"given": "David"
			},
			{
				"family": "Tsvetkov",
				"given": "Yulia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bianchiEasilyAccessibleTexttoimage2023a",
		"type": "paper-conference",
		"abstract": "Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594095",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1493–1504",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Easily accessible text-to-image generation amplifies demographic stereotypes at large scale",
		"URL": "https://doi.org/10.1145/3593013.3594095",
		"author": [
			{
				"family": "Bianchi",
				"given": "Federico"
			},
			{
				"family": "Kalluri",
				"given": "Pratyusha"
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Ladhak",
				"given": "Faisal"
			},
			{
				"family": "Cheng",
				"given": "Myra"
			},
			{
				"family": "Nozza",
				"given": "Debora"
			},
			{
				"family": "Hashimoto",
				"given": "Tatsunori"
			},
			{
				"family": "Jurafsky",
				"given": "Dan"
			},
			{
				"family": "Zou",
				"given": "James"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chenPersonalizedPricingGroup2023a",
		"type": "paper-conference",
		"abstract": "In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers’ features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594097",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1520–1530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Personalized pricing with group fairness constraint",
		"URL": "https://doi.org/10.1145/3593013.3594097",
		"author": [
			{
				"family": "Chen",
				"given": "Xin"
			},
			{
				"family": "Xu",
				"given": "Zexing"
			},
			{
				"family": "Zhao",
				"given": "Zishuo"
			},
			{
				"family": "Zhou",
				"given": "Yuan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kwegyir-aggreyMisuseAUCWhat2023a",
		"type": "paper-conference",
		"abstract": "When determining which machine learning model best performs some high impact risk assessment task, practitioners commonly use the Area under the Curve (AUC) to defend and validate their model choices. In this paper, we argue that the current use and understanding of AUC as a model performance metric misunderstands the way the metric was intended to be used. To this end, we characterize the misuse of AUC and illustrate how this misuse negatively manifests in the real world across several risk assessment domains. We locate this disconnect in the way the original interpretation of AUC has shifted over time to the point where issues pertaining to decision thresholds, class balance, statistical uncertainty, and protected groups remain unaddressed by AUC-based model comparisons, and where model choices that should be the purview of policymakers are hidden behind the veil of mathematical rigor. We conclude that current model validation practices involving AUC are not robust, and often invalid.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594100",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1570–1583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The misuse of AUC: What high impact risk assessment gets wrong",
		"URL": "https://doi.org/10.1145/3593013.3594100",
		"author": [
			{
				"family": "Kwegyir-Aggrey",
				"given": "Kweku"
			},
			{
				"family": "Gerchick",
				"given": "Marissa"
			},
			{
				"family": "Mohan",
				"given": "Malika"
			},
			{
				"family": "Horowitz",
				"given": "Aaron"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "liuGroupFairnessDemographics2023a",
		"type": "paper-conference",
		"abstract": "Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a “group-free\" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594091",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1432–1449",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness without demographics using social networks",
		"URL": "https://doi.org/10.1145/3593013.3594091",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Do",
				"given": "Virginie"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Nickel",
				"given": "Maximilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "mccraddenWhatsFairFair2023",
		"type": "paper-conference",
		"abstract": "The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594096",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1505–1519",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB",
		"URL": "https://doi.org/10.1145/3593013.3594096",
		"author": [
			{
				"family": "Mccradden",
				"given": "Melissa"
			},
			{
				"family": "Odusi",
				"given": "Oluwadara"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Akrout",
				"given": "Ismail"
			},
			{
				"family": "Ndlovu",
				"given": "Kagiso"
			},
			{
				"family": "Glocker",
				"given": "Ben"
			},
			{
				"family": "Maicas",
				"given": "Gabriel"
			},
			{
				"family": "Liu",
				"given": "Xiaoxuan"
			},
			{
				"family": "Mazwi",
				"given": "Mjaye"
			},
			{
				"family": "Garnett",
				"given": "Tee"
			},
			{
				"family": "Oakden-Rayner",
				"given": "Lauren"
			},
			{
				"family": "Alfred",
				"given": "Myrtede"
			},
			{
				"family": "Sihlahla",
				"given": "Irvine"
			},
			{
				"family": "Shafei",
				"given": "Oswa"
			},
			{
				"family": "Goldenberg",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "pangAuditingCrossculturalConsistency2023a",
		"type": "paper-conference",
		"abstract": "Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models—based on these labels—are applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are “consistently conceptualized” across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594098",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 22\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1531–1552",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing cross-cultural consistency of human-annotated labels for recommendation systems",
		"URL": "https://doi.org/10.1145/3593013.3594098",
		"author": [
			{
				"family": "Pang",
				"given": "Rock Yuren"
			},
			{
				"family": "Cenatempo",
				"given": "Jack"
			},
			{
				"family": "Graham",
				"given": "Franklyn"
			},
			{
				"family": "Kuehn",
				"given": "Bridgette"
			},
			{
				"family": "Whisenant",
				"given": "Maddy"
			},
			{
				"family": "Botchway",
				"given": "Portia"
			},
			{
				"family": "Stone Perez",
				"given": "Katie"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zilkaProgressionDisparitiesCriminal2023a",
		"type": "paper-conference",
		"abstract": "Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person’s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5–2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5–11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594099",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1553–1569",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The progression of disparities within the criminal justice system: Differential enforcement and risk assessment instruments",
		"URL": "https://doi.org/10.1145/3593013.3594099",
		"author": [
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Hron",
				"given": "Jiri"
			},
			{
				"family": "Butcher",
				"given": "Bradley"
			},
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "guerdanCounterfactualPredictionOutcome2023a",
		"type": "paper-conference",
		"abstract": "Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594101",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1584–1598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual prediction under outcome measurement error",
		"URL": "https://doi.org/10.1145/3593013.3594101",
		"author": [
			{
				"family": "Guerdan",
				"given": "Luke"
			},
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Wu",
				"given": "Zhiwei Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "poulainImprovingFairnessAI2023a",
		"type": "paper-conference",
		"abstract": "Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models’ overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method’s superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594102",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1599–1608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Improving fairness in AI models on electronic health records: The case for federated learning methods",
		"URL": "https://doi.org/10.1145/3593013.3594102",
		"author": [
			{
				"family": "Poulain",
				"given": "Raphael"
			},
			{
				"family": "Bin Tarek",
				"given": "Mirza Farhan"
			},
			{
				"family": "Beheshti",
				"given": "Rahmatollah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "corbettInterrogatingFAccT2023a",
		"type": "paper-conference",
		"abstract": "Fairness, accountability, and transparency are the three conceptual foundations of the FAccT conference. Transparency, however, has yet to be scrutinized to the same degree as accountability and fairness. As a result, we don't know: What does this community mean when it talks about transparency? How are we doing transparency? And to what ends? What commitments does (or should) the T in FAccT signify? This paper interrogates the T in FAccT using perspectives from critical transparency literature. Subsequently, we argue that FAccT might be better off dropping the T from its title for two reasons: (1) transparency can often be counterproductive to FAccT's primary objectives and (2) it is misleading as FAccT is mainly preoccupied with explainability rather than actual transparency. If we want to keep the T, we need to reframe how we think about and do transparency by making transparency contingent, reclaiming it from explainability, and bringing people into transparency processes.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594104",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1624–1634",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Interrogating the T in FAccT",
		"URL": "https://doi.org/10.1145/3593013.3594104",
		"author": [
			{
				"family": "Corbett",
				"given": "Eric"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bashardoustReducingAccessDisparities2023a",
		"type": "paper-conference",
		"abstract": "In social networks, a node’s position is, in and of itself, a form of social capital. Better-positioned members not only benefit from (faster) access to diverse information, but innately have more potential influence on information spread. Structural biases often arise from network formation, and can lead to significant disparities in information access based on position. Further, processes such as link recommendation can exacerbate this inequality by relying on network structure to augment connectivity. In this paper, we argue that one can understand and quantify this social capital through the lens of information flow in the network. In contrast to prior work, we consider the setting where all nodes may be sources of distinct information, and a node’s (dis)advantage takes into account its ability to access all information available on the network, not just that from a single source. We introduce three new measures of advantage (broadcast, influence, and control), which are quantified in terms of position in the network using access signatures – vectors that represent a node’s ability to share information with each other node in the network. We then consider the problem of improving equity by making interventions to increase the access of the least-advantaged nodes. Since all nodes are already sources of information in our model, we argue that edge augmentation is most appropriate for mitigating bias in the network structure, and frame a budgeted intervention problem for maximizing broadcast (minimum pairwise access) over the network. Finally, we propose heuristic strategies for selecting edge augmentations and empirically evaluate their performance on a corpus of real-world social networks. We demonstrate that a small number of interventions can not only significantly increase the broadcast measure of access for the least-advantaged nodes (over 5 times more than random), but also simultaneously improve the minimum influence. Additional analysis shows that edge augmentations targeted at improving minimum pairwise access can also dramatically shrink the gap in advantage between nodes (over ) and reduce disparities between their access signatures.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594105",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1635–1651",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reducing access disparities in networks using edge Augmentation",
		"URL": "https://doi.org/10.1145/3593013.3594105",
		"author": [
			{
				"family": "Bashardoust",
				"given": "Ashkan"
			},
			{
				"family": "Friedler",
				"given": "Sorelle"
			},
			{
				"family": "Scheidegger",
				"given": "Carlos"
			},
			{
				"family": "Sullivan",
				"given": "Blair D."
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "smithManyFacesFairness2023a",
		"type": "paper-conference",
		"abstract": "Recommender systems have a variety of stakeholders. Applying concepts of fairness in such systems requires attention to stakeholders’ complex and often-conflicting needs. Since fairness is socially constructed, there are numerous definitions, both in the social science and machine learning literatures. Still, it is rare for machine learning researchers to develop their metrics in close consideration of their social context. More often, standard definitions are adopted and assumed to be applicable across contexts and stakeholders. Our research starts with a recommendation context and then seeks to understand the breadth of the fairness considerations of associated stakeholders. In this paper, we report on the results of a semi-structured interview study with 23 employees who work for the Kiva microlending platform. We characterize the many different ways in which they enact and strive toward fairness for microlending recommendations in their own work, uncover the ways in which these different enactments of fairness are in tension with each other, and identify how stakeholders are differentially prioritized. Finally, we reflect on the implications of this study for future research and for the design of multistakeholder recommender systems.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594106",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1652–1663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The many faces of fairness: Exploring the institutional logics of multistakeholder microlending recommendation",
		"URL": "https://doi.org/10.1145/3593013.3594106",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Buhayh",
				"given": "Anas"
			},
			{
				"family": "Kathait",
				"given": "Anushka"
			},
			{
				"family": "Ragothaman",
				"given": "Pradeep"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Voida",
				"given": "Amy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gardnerCrossinstitutionalTransferLearning2023a",
		"type": "paper-conference",
		"abstract": "Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions. We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594107",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1664–1684",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Cross-institutional transfer learning for educational models: Implications for model performance, fairness, and equity",
		"URL": "https://doi.org/10.1145/3593013.3594107",
		"author": [
			{
				"family": "Gardner",
				"given": "Joshua"
			},
			{
				"family": "Yu",
				"given": "Renzhe"
			},
			{
				"family": "Nguyen",
				"given": "Quan"
			},
			{
				"family": "Brooks",
				"given": "Christopher"
			},
			{
				"family": "Kizilcec",
				"given": "Rene"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shresthaHelpHinderEvaluating2023a",
		"type": "paper-conference",
		"abstract": "For applications where multiple stakeholders provide recommendations, a fair consensus ranking must not only ensure that the preferences of rankers are well represented, but must also mitigate disadvantages among socio-demographic groups in the final result. However, there is little empirical guidance on the value or challenges of visualizing and integrating fairness metrics and algorithms into human-in-the-loop systems to aid decision-makers. In this work, we design a study to analyze the effectiveness of integrating such fairness metrics-based visualization and algorithms. We explore this through a task-based crowdsourced experiment comparing an interactive visualization system for constructing consensus rankings, ConsensusFuse, with a similar system that includes visual encodings of fairness metrics and fair-rank generation algorithms, FairFuse. We analyze the measure of fairness, agreement of rankers’ decisions, and user interactions in constructing the fair consensus ranking across these two systems. In our study with 200 participants, results suggest that providing these fairness-oriented support features nudges users to align their decision with the fairness metrics while minimizing the tedious process of manually having to amend the consensus ranking. We discuss the implications of these results for the design of next-generation fairness oriented-systems and along with emerging directions for future research.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594108",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1685–1698",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Help or hinder? Evaluating the impact of fairness metrics and algorithms in visualizations for consensus ranking",
		"URL": "https://doi.org/10.1145/3593013.3594108",
		"author": [
			{
				"family": "Shrestha",
				"given": "Hilson"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Alkhathlan",
				"given": "Mallak"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			},
			{
				"family": "Harrison",
				"given": "Lane"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "meiBias93Stigmatized2023a",
		"type": "paper-conference",
		"abstract": "Warning: The content of this paper may be upsetting or triggering.The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence \"They are people who have less than a high school education.\" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594109",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1699–1710",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias against 93 stigmatized groups in masked language models and downstream sentiment classification tasks",
		"URL": "https://doi.org/10.1145/3593013.3594109",
		"author": [
			{
				"family": "Mei",
				"given": "Katelyn"
			},
			{
				"family": "Fereidooni",
				"given": "Sonia"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kulynychArbitraryDecisionsAre2023a",
		"type": "paper-conference",
		"abstract": "Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze—both theoretically and through extensive experiments—the predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594103",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1609–1623",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Arbitrary decisions are a hidden cost of differentially private training",
		"URL": "https://doi.org/10.1145/3593013.3594103",
		"author": [
			{
				"family": "Kulynych",
				"given": "Bogdan"
			},
			{
				"family": "Hsu",
				"given": "Hsiang"
			},
			{
				"family": "Troncoso",
				"given": "Carmela"
			},
			{
				"family": "Calmon",
				"given": "Flavio P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "sampsonRepresentationSelfdeterminationRefusal2023a",
		"type": "paper-conference",
		"abstract": "Targeted online advertising systems increasingly draw scrutiny for the surveillance underpinning their collection of people’s private data, and subsequent automated categorization and inference. The experiences of LGBTQ+ people, whose identities call into question dominant assumptions about who is seen as “normal,” and deserving of privacy, autonomy, and the right to self-determination, are a fruitful site for exploring the impacts of ad targeting. We conducted semi-structured interviews with LGBTQ+ individuals (N=18) to understand their experiences with online advertising, their perceptions of ad targeting, and the interplay of these systems with their queerness and other identities. Our results reflect participants’ overall negative experiences with online ad content—they described it as stereotypical and tokenizing in its lack of diversity and nuance. But their desires for better ad content also clashed with their more fundamental distrust and rejection of the non-consensual and extractive nature of ad targeting. They voiced privacy concerns about continuous data aggregation and behavior tracking, a desire for greater control over their data and attention, and even the right to opt-out entirely. Drawing on scholarship from queer and feminist theory, we explore targeted ads’ homonormativity in their failure to represent multiply-marginalized queer people, the harms of automated inference and categorization to identity formation and self-determination, and the theory of refusal underlying participants’ queer visions for a better online experience.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594110",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1711–1722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation, self-determination, and refusal: Queer people’s experiences with targeted advertising",
		"URL": "https://doi.org/10.1145/3593013.3594110",
		"author": [
			{
				"family": "Sampson",
				"given": "Princess"
			},
			{
				"family": "Encarnacion",
				"given": "Ro"
			},
			{
				"family": "Metaxa",
				"given": "Danaë"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kellyCapturingHumansMental2023a",
		"type": "paper-conference",
		"abstract": "Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594111",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1723–1734",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Capturing humans’ mental models of AI: An item response theory approach",
		"URL": "https://doi.org/10.1145/3593013.3594111",
		"author": [
			{
				"family": "Kelly",
				"given": "Markelle"
			},
			{
				"family": "Kumar",
				"given": "Aakriti"
			},
			{
				"family": "Smyth",
				"given": "Padhraic"
			},
			{
				"family": "Steyvers",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bruns-smithUsingSupervisedLearning2023a",
		"type": "paper-conference",
		"abstract": "Household responses to income shocks are important drivers of financial fragility, the evolution of wealth inequality, and the effectiveness of fiscal and monetary policy. Traditional approaches to measuring the size and persistence of income shocks are based on restrictive econometric models that impose strong homogeneity across households and over time. In this paper, we propose a more flexible, machine learning framework for estimating income shocks that allows for variation across all observable features and time horizons. First, we propose non-parametric estimands for shocks and shock persistence. We then show how to estimate these quantities by using off-the-shelf supervised learning tools to approximate the conditional expectation of future income given present information. We solve this income prediction problem in a large Icelandic administrative dataset, and then use the estimated shocks to document several features of labor income risk in Iceland that are not captured by standard economic income models.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594113",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1747–1756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using supervised learning to estimate inequality in the size and persistence of income shocks",
		"URL": "https://doi.org/10.1145/3593013.3594113",
		"author": [
			{
				"family": "Bruns-Smith",
				"given": "David"
			},
			{
				"family": "Feller",
				"given": "Avi"
			},
			{
				"family": "Nakamura",
				"given": "Emi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "barrettSkinDeepInvestigating2023a",
		"type": "paper-conference",
		"abstract": "To investigate the well-observed racial disparities in computer vision systems that analyze images of humans, researchers have turned to skin tone as a more objective annotation than race metadata for fairness performance evaluations. However, the current state of skin tone annotation procedures is highly varied. For instance, researchers use a range of untested scales and skin tone categories, have unclear annotation procedures, and provide inadequate analyses of uncertainty. In addition, little attention is paid to the positionality of the humans involved in the annotation process—both designers and annotators alike—and the historical and sociological context of skin tone in the United States. Our work is the first to investigate the skin tone annotation process as a sociotechnical project. We surveyed recent skin tone annotation procedures and conducted annotation experiments to examine how subjective understandings of skin tone are embedded in skin tone annotation procedures. Our systematic literature review revealed the uninterrogated association between skin tone and race and the limited effort to analyze annotator uncertainty in current procedures for skin tone annotation in computer vision evaluation. Our experiments demonstrated that design decisions in the annotation procedure such as the order in which the skin tone scale is presented or additional context in the image (i.e., presence of a face) significantly affected the resulting inter-annotator agreement and individual uncertainty of skin tone annotations. We call for greater reflexivity in the design, analysis, and documentation of procedures for evaluation using skin tone.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594114",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1757–1771",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Skin deep: Investigating subjectivity in skin tone annotations for computer vision benchmark datasets",
		"URL": "https://doi.org/10.1145/3593013.3594114",
		"author": [
			{
				"family": "Barrett",
				"given": "Teanna"
			},
			{
				"family": "Chen",
				"given": "Quanze"
			},
			{
				"family": "Zhang",
				"given": "Amy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "diciccioDetectionMitigationAlgorithmic2023a",
		"type": "paper-conference",
		"abstract": "Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594117",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1801–1816",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detection and mitigation of algorithmic bias via predictive parity",
		"URL": "https://doi.org/10.1145/3593013.3594117",
		"author": [
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Hsu",
				"given": "Brian"
			},
			{
				"family": "Yu",
				"given": "Yinyin"
			},
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "wuSlowViolenceSurveillance2023a",
		"type": "paper-conference",
		"abstract": "People’s negative reactions to online behavioral advertising (OBA) are well-documented. However, past work has primarily focused on cataloguing these reactions and exploring how to change them, rather than understanding the ways these negative reactions affect people’s lived experiences. Drawing upon scholarship on socio-technical harms in human-computer interaction and computer-supported cooperative work, we investigate and categorize the different ways people report having been harmed by OBA. Through an online survey with 420 participants, we identified four key harms arising from OBA: psychological distress, loss of autonomy, constriction of user behavior, and algorithmic marginalization and traumatization. We next discuss the “slow violence” inflicted by OBA and the normalization of people’s affective discomfort with OBA, and how the two can present an opportunity for researchers to re-conceptualize OBA—and the invasive data practices it entails—as not just abstractly concerning to people, but as actively harmful.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594119",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1826–1837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The slow violence of surveillance capitalism: How online behavioral advertising harms people",
		"URL": "https://doi.org/10.1145/3593013.3594119",
		"author": [
			{
				"family": "Wu",
				"given": "Yuxi"
			},
			{
				"family": "Bice",
				"given": "Sydney"
			},
			{
				"family": "Edwards",
				"given": "W. Keith"
			},
			{
				"family": "Das",
				"given": "Sauvik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "silvaRepresentationOnlineMatters2023",
		"type": "paper-conference",
		"abstract": "As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-OR logical operator to bucketized retrieval at the retrieval stage and from greedy re-rankers to multi-objective optimization using determinantal point processes for the ranking stage, balances diversity and utility while enabling fast iterations and scalable expansion to diversification over multiple dimensions. Our experiments indicate that these approaches significantly improve diversity metrics, with a neutral to a positive impact on utility metrics and improved user satisfaction, both qualitatively and quantitatively, in production.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594112",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1735–1746",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Representation online matters: Practical end-to-end diversification in search and recommender systems",
		"URL": "https://doi.org/10.1145/3593013.3594112",
		"author": [
			{
				"family": "Silva",
				"given": "Pedro"
			},
			{
				"family": "Juneja",
				"given": "Bhawna"
			},
			{
				"family": "Desai",
				"given": "Shloka"
			},
			{
				"family": "Singh",
				"given": "Ashudeep"
			},
			{
				"family": "Fawaz",
				"given": "Nadia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nagarajraoDiscriminationImageSelection2023",
		"type": "paper-conference",
		"abstract": "Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools. In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery – through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594115",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1772–1788",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Discrimination through image selection by job advertisers on facebook",
		"URL": "https://doi.org/10.1145/3593013.3594115",
		"author": [
			{
				"family": "Nagaraj Rao",
				"given": "Varun"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ganeshImpactMachineLearning2023a",
		"type": "paper-conference",
		"abstract": "Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594116",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1789–1800",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the impact of machine learning randomness on group fairness",
		"URL": "https://doi.org/10.1145/3593013.3594116",
		"author": [
			{
				"family": "Ganesh",
				"given": "Prakhar"
			},
			{
				"family": "Chang",
				"given": "Hongyan"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Shokri",
				"given": "Reza"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangFairnessAuditingUrban2023a",
		"type": "paper-conference",
		"abstract": "Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594118",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1817–1825",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness auditing in urban decisions using LP-based data combination",
		"URL": "https://doi.org/10.1145/3593013.3594118",
		"author": [
			{
				"family": "Yang",
				"given": "Jingyi"
			},
			{
				"family": "Miller",
				"given": "Joel"
			},
			{
				"family": "Ohannessian",
				"given": "Mesrob"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grillBiasBoundaryObject2023a",
		"type": "paper-conference",
		"abstract": "Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis \"technical biases,\" like issues around measurement, rigidity, and coarseness of variables, \"emergent biases,\" such as disruptive events that change the labor market, and, finally, \"preexisting biases,\" like the use of variables that act as proxies for inequality. Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined “unbiased\" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594120",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1838–1849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bias as boundary object: Unpacking the politics of an austerity algorithm using bias frameworks",
		"URL": "https://doi.org/10.1145/3593013.3594120",
		"author": [
			{
				"family": "Grill",
				"given": "Gabriel"
			},
			{
				"family": "Fischer",
				"given": "Fabian"
			},
			{
				"family": "Cech",
				"given": "Florian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "binnsLegalTaxonomiesMachine2023a",
		"type": "paper-conference",
		"abstract": "Previous literature on ‘fair’ machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594121",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1850–1858",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Legal taxonomies of machine bias: Revisiting direct discrimination",
		"URL": "https://doi.org/10.1145/3593013.3594121",
		"author": [
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Adams-Prassl",
				"given": "Jeremias"
			},
			{
				"family": "Kelly-Lyth",
				"given": "Aislinn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "laugelAchievingDiversityCounterfactual2023a",
		"type": "paper-conference",
		"abstract": "In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.",
		"collection-title": "FAccT '23",
		"container-title": "Proceedings of the 2023 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3593013.3594122",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0192-4",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Chicago¡/city¿, ¡state¿IL¡/state¿, ¡country¿USA¡/country¿, ¡/conf-loc¿",
		"page": "1859–1869",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving diversity in counterfactual explanations: a review and discussion",
		"URL": "https://doi.org/10.1145/3593013.3594122",
		"author": [
			{
				"family": "Laugel",
				"given": "Thibault"
			},
			{
				"family": "Jeyasothy",
				"given": "Adulam"
			},
			{
				"family": "Lesot",
				"given": "Marie-Jeanne"
			},
			{
				"family": "Marsala",
				"given": "Christophe"
			},
			{
				"family": "Detyniecki",
				"given": "Marcin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "freszClassificationMetricsImage2024",
		"type": "paper-conference",
		"abstract": "Decision processes of computer vision models—especially deep neural networks—are opaque in nature, meaning that these decisions cannot be understood by humans. Thus, over the last years, many methods to provide human-understandable explanations have been proposed. For image classification, the most common group are saliency methods, which provide (super-)pixelwise feature attribution scores for input images. But their evaluation still poses a problem, as their results cannot be simply compared to the unknown ground truth. To overcome this, a slew of different proxy metrics have been defined, which are—as the explainability methods themselves—often built on intuition and thus, are possibly unreliable. In this paper, new evaluation metrics for saliency methods are developed and common saliency methods are benchmarked on ImageNet. In addition, a scheme for reliability evaluation of such metrics is proposed that is based on concepts from psychometric testing.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658537",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Classification metrics for image explanations: Towards building reliable XAI-evaluations",
		"URL": "https://doi.org/10.1145/3630106.3658537",
		"author": [
			{
				"family": "Fresz",
				"given": "Benjamin"
			},
			{
				"family": "Lörcher",
				"given": "Lena"
			},
			{
				"family": "Huber",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rateikeDesigningLongtermGroup2024",
		"type": "paper-conference",
		"abstract": "Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term—even if fairness considerations were taken into account in the policy design process. In this paper, we propose a novel framework for studying long-term group fairness in dynamical systems, in which current decisions may affect an individual’s features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long-term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov Chain Convergence Theorem to ensure unique convergence. Our framework enables the utilization of historical temporal data to tackle challenges associated with delayed feedback when learning long-term fair policies in practice. Importantly, our framework shows that interventions on the data distribution (e.g., subsidies) can be used to achieve policy learning that is both short- and long-term fair. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. In semi-synthetic simulations based on real-world datasets, we show how our approach facilitates identifying effective interventions for long-term fairness.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658538",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 31\npublisher-place: Rio de Janeiro, Brazil",
		"page": "20–50",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing long-term group fair policies in dynamical systems",
		"URL": "https://doi.org/10.1145/3630106.3658538",
		"author": [
			{
				"family": "Rateike",
				"given": "Miriam"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			},
			{
				"family": "Forré",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "blandinLearningFairnessDemonstrations2024",
		"type": "paper-conference",
		"abstract": "Defining fairness in algorithmic contexts is challenging, particularly when adapting to new domains. Our research introduces a novel method for learning and applying group fairness preferences across different classification domains, without the need for manual fine-tuning. Utilizing concepts from inverse reinforcement learning (IRL), our approach enables the extraction and application of fairness preferences from human experts or established algorithms. We propose the first technique for using IRL to recover and adapt group fairness preferences to new domains, offering a low-touch solution for implementing fair classifiers in settings where expert-established fairness tradeoffs are not yet defined.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658539",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "51–61",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fairness from demonstrations via inverse reinforcement learning",
		"URL": "https://doi.org/10.1145/3630106.3658539",
		"author": [
			{
				"family": "Blandin",
				"given": "Jack"
			},
			{
				"family": "Kash",
				"given": "Ian A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "finocchiaroUsingPropertyElicitation2024",
		"type": "paper-conference",
		"abstract": "Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658540",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "62–73",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using property elicitation to understand the impacts of fairness regularizers",
		"URL": "https://doi.org/10.1145/3630106.3658540",
		"author": [
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bogiatzis-gibbonsIndividualAccountabilityReasserting2024",
		"type": "paper-conference",
		"abstract": "AI control mechanisms like accountability procedures or technical standards are usually subpolitical: decisions are primarily debated and made within circumscribed subsystems of experts or interest groups, like the professional community of data scientists. However, AI systems are more deeply intertwined with a wider sense of politics than these mechanisms contemplate. In Winner’s dual senses, they are incidentally political as they settle disputes within political communities through their design, invention, and arrangement, and inherently political as they reciprocally contribute to and are sustained by patterning of economic, social, and political orders. This work, therefore, draws upon political theory to argue for democratically controlled AI beyond individual notions of accountability. In its weaker form, it demands substantive, rule-bound oversight of state actors’ use of AI systems, seeking to remedy historical tendencies toward extra-legal surveillance and strengthen accountability beyond individuals. Conversely, the stronger form advocates for comprehensive democratic control over all facets of AI, even by questioning the permissibility of AI within particular socio-economic spheres, as these systems are becoming fundamental parts of our collective life. I sketch the necessary institutional frameworks to operationalize these two forms of democratic control: first, for the \"weak\" form through the concept of a \"control\" power separate from the executive from Sun Yat-Sen’s political thought, and second, participatory institutions such as citizens’ assemblies. Finally, I discuss actions data scientists can take without legal frameworks for control: furthering new social imaginaries of AI that foreground the possibility of control and involving affected communities in decision-making around AI systems. The concept of democratic control is then both a measuring stick for existing standards and legislation and a clarion call for future advocacy.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658541",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "74–84",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond individual accountability: (Re-)asserting democratic control of AI",
		"URL": "https://doi.org/10.1145/3630106.3658541",
		"author": [
			{
				"family": "Bogiatzis-Gibbons",
				"given": "Daniel James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "luccioniPowerHungryProcessing2024",
		"type": "paper-conference",
		"abstract": "Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of “generality” comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ‘general-purpose’ models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658542",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "85–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Power hungry processing: Watts driving the cost of AI deployment?",
		"URL": "https://doi.org/10.1145/3630106.3658542",
		"author": [
			{
				"family": "Luccioni",
				"given": "Sasha"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Strubell",
				"given": "Emma"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kleinDataFeminismAI2024",
		"type": "paper-conference",
		"abstract": "This paper presents a set of intersectional feminist principles for conducting equitable, ethical, and sustainable AI research. In Data Feminism (2020), we offered seven principles for examining and challenging unequal power in data science. Here, we present a rationale for why feminism remains deeply relevant for AI research, rearticulate the original principles of data feminism with respect to AI, and introduce two potential new principles related to environmental impact and consent. Together, these principles help to 1) account for the unequal, undemocratic, extractive, and exclusionary forces at work in AI research, development, and deployment; 2) identify and mitigate predictable harms in advance of unsafe, discriminatory, or otherwise oppressive systems being released into the world; and 3) inspire creative, joyful, and collective ways to work towards a more equitable, sustainable world in which all of us can thrive.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658543",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "100–112",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data feminism for AI",
		"URL": "https://doi.org/10.1145/3630106.3658543",
		"author": [
			{
				"family": "Klein",
				"given": "Lauren"
			},
			{
				"family": "D'Ignazio",
				"given": "Catherine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "razReliabilityGapsGroups2024",
		"type": "paper-conference",
		"abstract": "This paper investigates the inter-rater reliability of risk assessment instruments (RAIs). The main question is whether different, socially salient groups are affected differently by a lack of inter-rater reliability of RAIs, that is, whether mistakes with respect to different groups affects them differently. The question is investigated with a simulation study of the COMPAS dataset. A controlled degree of noise is injected into the input data of a predictive model; the noise can be interpreted as a synthetic rater that makes mistakes. The main finding is that there are systematic differences in output reliability between groups in the COMPAS dataset. The sign of the difference depends on the kind of inter-rater statistic that is used (Cohen’s Kappa, Byrt’s PABAK, ICC), and in particular whether or not the statistic corrects for prediction prevalences of the groups.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658544",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "113–126",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reliability gaps between groups in COMPAS dataset",
		"URL": "https://doi.org/10.1145/3630106.3658544",
		"author": [
			{
				"family": "Räz",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gornetMappingAIEthics2024",
		"type": "paper-conference",
		"abstract": "The recent years have seen a surge of initiatives with the goal of defining what “ethical” artificial intelligence would or should entail, resulting in the publication of various charters and manifestos discussing AI ethics; these documents originate from academia, AI industry companies, non-profits, regulatory institutions, and the civil society. The contents of such documents vary wildly, from short, vague position statements to verbatims of democratic debates or impact assessment studies. As such, they are a marker of the social world of artificial intelligence, outlining the tenets of different actors, the consensus and dissensus on important goals, and so on. Multiple meta-analyses have focused on qualitatively identifying recurring themes in these documents, highlighting the high polysemy of themes such as transparency or trust, among others. The broad term of “AI ethics” and its guiding principles hide multiple disparities, shaped by our collective imaginations, economic and regulatory incentives, and the pre-existing social and structural power asymmetries; through quantitative analyses, we validate and infirm previous qualitative results. In this paper, we create and present a corpus of charters and manifestos discussing AI ethics through the process of collection and its quantitative analysis using text analysis to shed light on common and distinct vocabularies. Through frequency analysis, hierarchical topic clustering and semantic graph modelling, we show that the charters and manifestos discuss AI ethics along three broad axes: technical documents, regulatory ones, and innovation and business ones. We use our quantitative analysis to back up and nuance previous qualitative results, showing how some themes remain specific while others have fully permeated the space of AI ethics. We document and release our corpus, comprising of 436 documents, charters and manifestos discussing AI ethics. We release the corpus, its datasheet and our analysis, to open the way to further studies and discussions around vocabulary, principles and their evolution, as well as interactions among actors of AI ethics, in order to foster further studies on the topic.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658545",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "127–140",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping AI ethics: a meso-scale analysis of its charters and manifestos",
		"URL": "https://doi.org/10.1145/3630106.3658545",
		"author": [
			{
				"family": "Gornet",
				"given": "Mélanie"
			},
			{
				"family": "Delarue",
				"given": "Simon"
			},
			{
				"family": "Boritchev",
				"given": "Maria"
			},
			{
				"family": "Viard",
				"given": "Tiphaine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "nigatuSearchedReligiousSong2024",
		"type": "paper-conference",
		"abstract": "Online social media platforms such as YouTube have a wide, global reach. However, little is known about the experience of low-resourced language speakers on such platforms; especially in how they experience and navigate harmful content. To better understand this, we (1) conducted semi-structured interviews (n=15) and (2) analyzed search results (n=9313), recommendations (n=3336), channels (n=120) and comments (n=406) of policy-violating sexual content on YouTube focusing on the Amharic language. Our findings reveal that – although Amharic-speaking YouTube users find the platform crucial for several aspects of their lives – participants reported unplanned exposure to policy-violating sexual content when searching for benign, popular queries. Furthermore, malicious content creators seem to exploit under-performing language technologies and content moderation to further target vulnerable groups of speakers, including migrant domestic workers, diaspora, and local Ethiopians. Overall, our study sheds light on how failures in low-resourced language technology may lead to exposure to harmful content and suggests implications for stakeholders in minimizing harm. Content Warning: This paper includes discussions of NSFW topics and harmful content (hate, abuse, sexual harassment, self-harm, misinformation). The authors do not support the creation or distribution of harmful content.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658546",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "141–160",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“I searched for a religious song in amharic and got sexual content instead’’: Investigating online harm in low-resourced languages on YouTube.",
		"URL": "https://doi.org/10.1145/3630106.3658546",
		"author": [
			{
				"family": "Nigatu",
				"given": "Hellina Hailu"
			},
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "quWhyProblemsPredictive2024",
		"type": "paper-conference",
		"abstract": "Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word “problems” is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N = 300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658547",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "161–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why is \"problems\" predictive of positive sentiment? A case study of explaining unintuitive features in sentiment classification",
		"URL": "https://doi.org/10.1145/3630106.3658547",
		"author": [
			{
				"family": "Qu",
				"given": "Jiaming"
			},
			{
				"family": "Arguello",
				"given": "Jaime"
			},
			{
				"family": "Wang",
				"given": "Yue"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kieslichRegulatingAIbasedRemote2024",
		"type": "paper-conference",
		"abstract": "AI is increasingly being used in the public sector, including public security. In this context, the use of AI-powered remote biometric identification (RBI) systems is a much-discussed technology. RBI systems are used to identify criminal activity in public spaces, but at the same time they are criticised for inheriting biases and violating fundamental human rights. As a result, the use of RBI poses risks to society. It is therefore important to ensure that such systems are developed in the public interest, which means that any technology that is deployed for public use needs to be scrutinised. While there is a broad consensus among business leaders, policymakers and scientists that AI must be developed in an ethical and trustworthy manner, scholars have argued that ethical guidelines do not guarantee ethical AI, but rather prevent stronger regulation of AI for the Common Good. As a possible counterweight, public opinion can have a decisive influence on policymakers (e.g. through voter demands) to establish boundaries and conditions under which AI systems should be used – if at all. However, we know little about the conditions that lead to regulatory demand for AI systems. In this study, we focus on the role of trust in AI as well as trust in law enforcement as potential factors that may lead to demands for regulation of AI technology. In addition, we explore the mediating effects of discrimination perceptions regarding RBI. We test the effects on four different use cases of RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose of use (persecution of criminals vs. safeguarding public events) in a survey among German citizens. We found that German citizens do not differentiate between the different modes of application in terms of their demand for RBI regulation. Furthermore, we show that perceptions of discrimination lead to a demand for stronger regulation, while trust in AI and trust in law enforcement lead to opposite effects in terms of demand for a ban on RBI systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658548",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "173–185",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating AI-based remote biometric identification. Investigating the public demand for bans, audits, and public database registrations",
		"URL": "https://doi.org/10.1145/3630106.3658548",
		"author": [
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Lünich",
				"given": "Marco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "goetzeAIArtTheft2024",
		"type": "paper-conference",
		"abstract": "Since the launch of applications such as dall•e, Midjourney, and Stable Diffusion, generative artificial intelligence has been controversial as a tool for creating artwork. Some writers have presented worries about these technologies as harbingers of fully automated futures to come, but more pressing is the impact of generative AI on creative labour in the present. Already, business leaders have begun replacing human artistic labour with AI-generated images. In response, the artistic community has launched a protest movement, which argues that AI image generation is a kind of theft. This paper analyzes, substantiates, and critiques these arguments, concluding that AI image generators involve an unethical kind of labour theft. If correct, many other AI applications also rely upon theft.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658898",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "186–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI art is theft: Labour, extraction, and exploitation: Or, on the dangers of stochastic pollocks",
		"URL": "https://doi.org/10.1145/3630106.3658898",
		"author": [
			{
				"family": "Goetze",
				"given": "Trystan S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "jainAlgorithmicPluralismStructural2024",
		"type": "paper-conference",
		"abstract": "We present a structural approach toward achieving equal opportunity in systems of algorithmic decision-making called algorithmic pluralism. Algorithmic pluralism describes a state of affairs in which no set of algorithms severely limits access to opportunity, allowing individuals the freedom to pursue a diverse range of life paths. To argue for algorithmic pluralism, we adopt Joseph Fishkin’s theory of bottlenecks, which focuses on the structure of decision-points that determine how opportunities are allocated. The theory contends that each decision-point or “bottleneck’’ limits access to opportunities with some degree of severity and legitimacy. We extend Fishkin’s structural viewpoint and use it to reframe existing systemic concerns about equal opportunity in algorithmic decision-making, such as patterned inequality and algorithmic monoculture. In proposing algorithmic pluralism, we argue for the urgent priority of alleviating severe bottlenecks in algorithmic-decision-making. We contend that there must be a pluralism of opportunity available to many different individuals in order to promote equal opportunity in a systemic way. We further show how this framework has several implications for system design and regulation through current debates about equal opportunity in algorithmic hiring.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658899",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "197–206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic pluralism: a structural approach to equal opportunity",
		"URL": "https://doi.org/10.1145/3630106.3658899",
		"author": [
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Suriyakumar",
				"given": "Vinith"
			},
			{
				"family": "Creel",
				"given": "Kathleen"
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gausenFrameworkExploringConsequences2024",
		"type": "paper-conference",
		"abstract": "Organisations generate vast amounts of information, which has resulted in a long-term research effort into knowledge access systems for enterprise settings. Recent developments in artificial intelligence, in relation to large language models, are poised to have significant impact on knowledge access. This has the potential to shape the workplace and knowledge in new and unanticipated ways. Many risks can arise from the deployment of these types of AI systems, due to interactions between the technical system and organisational power dynamics. This paper presents the Consequences-Mechanisms-Risks framework to identify risks to workers from AI-mediated enterprise knowledge access systems. We have drawn on wide-ranging literature detailing risks to workers, and categorised risks as being to worker value, power, and wellbeing. The contribution of our framework is to additionally consider (i) the consequences of these systems that are of moral import: commodification, appropriation, concentration of power, and marginalisation, and (ii) the mechanisms, which represent how these consequences may take effect in the system. The mechanisms are a means of contextualising risk within specific system processes, which is critical for mitigation. This framework is aimed at helping practitioners involved in the design and deployment of AI-mediated knowledge access systems to consider the risks introduced to workers, identify the precise system mechanisms that introduce those risks, and begin to approach mitigation. Future work could apply this framework to other technological systems to promote the protection of workers and other groups.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658900",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "207–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for exploring the consequences of AI-mediated enterprise knowledge access and identifying risks to workers",
		"URL": "https://doi.org/10.1145/3630106.3658900",
		"author": [
			{
				"family": "Gausen",
				"given": "Anna"
			},
			{
				"family": "Mitra",
				"given": "Bhaskar"
			},
			{
				"family": "Lindley",
				"given": "Siân"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "guoDecisionTheoreticFramework2024",
		"type": "paper-conference",
		"abstract": "Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI’s recommendation from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies from literature, we demonstrate how our framework can be used to separate the loss due to mis-reliance from the loss due to not accurately differentiating the signals. We evaluate these losses by comparing to a baseline and a benchmark for complementary performance defined by the expected payoff achieved by a rational decision-maker facing the same decision task as the behavioral decision-makers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658901",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "221–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A decision theoretic framework for measuring AI reliance",
		"URL": "https://doi.org/10.1145/3630106.3658901",
		"author": [
			{
				"family": "Guo",
				"given": "Ziyang"
			},
			{
				"family": "Wu",
				"given": "Yifan"
			},
			{
				"family": "Hartline",
				"given": "Jason D."
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "jaimeEthnicClassificationsAlgorithmic2024",
		"type": "paper-conference",
		"abstract": "We address the challenges and implications of ensuring fairness in algorithmic decision-making (ADM) practices related to ethnicity. Expanding beyond the U.S.-centric approach to race, we provide an overview of ethnic classification schemes in European countries and emphasize how the distinct approaches to ethnicity in Europe can impact fairness assessments in ADM. Drawing on large-scale German survey data, we highlight differences in ethnic disadvantage across subpopulations defined by different measures of ethnicity. We build prediction models in the labor market, health, and finance domain and investigate the fairness implications of different ethnic classification schemes across multiple prediction tasks and fairness metrics. Our results show considerable variation in fairness scores across ethnic classifications, where error disparities for the same model can be twice as large when using different operationalizations of ethnicity. We argue that ethnic classifications differ in their ability to identify ethnic disadvantage across ADM domains and advocate for context-sensitive operationalizations of ethnicity and its transparent reporting in fair machine learning (ML) applications.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658902",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "237–253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethnic classifications in algorithmic fairness: Concepts, measures and implications in practice",
		"URL": "https://doi.org/10.1145/3630106.3658902",
		"author": [
			{
				"family": "Jaime",
				"given": "Sofia"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "fledderjohannAlgorithmicReproductiveJustice2024",
		"type": "paper-conference",
		"abstract": "Reproductive justice is an intersectional feminist framework and movement which argues all people have the right to have a child, to not have a child, to parent in safe and healthy environments, and to own their bodies and control their futures. We identify increasing surveillance, assessing worth, datafication and monetisation, and decimating planetary health as forms of structural violence associated with emerging digital technologies. These trends are implicated in the (re)production of inequities, creating barriers to the realisation of reproductive justice. We call for algorithmic reproductive justice, and highlight the potential for both acts of resistance and industry reform to advance that aim.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658903",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "254–266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic reproductive justice",
		"URL": "https://doi.org/10.1145/3630106.3658903",
		"author": [
			{
				"family": "Fledderjohann",
				"given": "Jasmine"
			},
			{
				"family": "Knowles",
				"given": "Bran"
			},
			{
				"family": "Miller",
				"given": "Esmorie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "reyes-cruzRearrangingDeckChairs2024",
		"type": "paper-conference",
		"abstract": "Radical and disruptive interventions are needed to reach \"Net Zero\" by 2050 to avert the climate catastrophe. Although governments, companies, cities, and institutions have pledged to take action and reduce their carbon emissions, the idea of personal carbon allowances or budgets for individuals has also been proposed as a potential national policy in the UK. In this paper, we employ a Research through Design approach to explore the notion of a carbon budget. We present combined results from two studies: firstly a workshop with members of environmental organisations (industry, charity, and policymaking) discussing the concept of a Citizen Carbon Budget (CCB) and app, from the wide perspective of societal desirability drawn from Responsible Research and Innovation (RRI); and secondly, a one-month deployment of a CCB mobile app with twelve members of the public based in the UK. Key findings from the combination of these approaches showed that the CCB app was fruitful in supporting awareness of personal carbon emissions and reflections about people’s lifestyles. However, several concerns were raised, including the unfairness of treating all people equally in environmental policy, regardless of their background and context. We provide considerations for policymaking and design, including intertwined perspectives drawn from the differing approaches of individual and collective action.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658904",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "267–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"Like rearranging deck chairs on the titanic\"? Feasibility, fairness, and ethical concerns of a citizen carbon budget for reducing CO2 emissions",
		"URL": "https://doi.org/10.1145/3630106.3658904",
		"author": [
			{
				"family": "Reyes-Cruz",
				"given": "Gisela"
			},
			{
				"family": "Craigon",
				"given": "Peter"
			},
			{
				"family": "Piskopani",
				"given": "Anna-Maria"
			},
			{
				"family": "Dowthwaite",
				"given": "Liz"
			},
			{
				"family": "Lu",
				"given": "Yang"
			},
			{
				"family": "Lisinska",
				"given": "Justyna"
			},
			{
				"family": "Shafipour",
				"given": "Elnaz"
			},
			{
				"family": "Stein",
				"given": "Sebastian"
			},
			{
				"family": "Fischer",
				"given": "Joel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pelegrinaPreprocessingShapleyValuebased2024",
		"type": "paper-conference",
		"abstract": "Decision support systems became ubiquitous in every aspect of human lives. Their reliance on increasingly complex and opaque machine learning models raises transparency and fairness concerns with respect to unprivileged groups of people. This motivated several efforts to estimate importance of features towards the models’ performance and to detect unfair/disparate decisions. The latter is often dealt with by means of fairness metrics that rely on performance metrics with respect to predefined features that are considered protected (salient features such as age, gender, ethnicity, etc.) and/or sensitive (such as education, /occupation, banking information). However, such an approach is subjective (as fairness metrics depend on the choice features), there may be other features that lead to unfair (disparate) decisions and that may ask for suitable interpretations. In this paper we focus on the latter issues and propose a statistical preprocessing approach that is inspired by both the Hilbert-Schmidt independence criterion and Shapley values to estimate feature importance and to detect disparity prone features. Unlike traditional Shapley value-based approaches, we do not require trained models to measure feature importance or detect disparate results. Instead, it focuses on data and statistical criteria to measure the dependence of feature distributions. Our empirical results show that features with the highest dependence degrees with the label vector are also the ones with the highest impact on the model performance. Moreover, our empirical results indicate that this relation enables the detection of disparity prone features.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658905",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "279–289",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A preprocessing Shapley value-based approach to detect relevant and disparity prone features in machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658905",
		"author": [
			{
				"family": "Pelegrina",
				"given": "Guilherme Dean"
			},
			{
				"family": "Couceiro",
				"given": "Miguel"
			},
			{
				"family": "Duarte",
				"given": "Leonardo Tomazeli"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moreauFailingOurYoungest2024",
		"type": "paper-conference",
		"abstract": "In recent years, Danish child protective services have experienced increasing pressure, prompting the adoption of a decision-support algorithm to aid caseworkers in identifying children at heightened risk of maltreatment, named Decision Support. Despite its critical role, this algorithm has not undergone formal evaluation. Through a freedom of information request, we were able to partially access the algorithm and conduct an audit. We find that the algorithm has significant methodological flaws, suffers from information leakage, relies on inappropriate proxy values for maltreatment assessment, generates inconsistent risk scores, and exhibits age-based discrimination. Given these serious issues, we strongly advise against the use of this kind of algorithms in local government, municipal, and child protection settings, and we call for rigorous evaluation of such tools before implementation and for continual monitoring post-deployment by listing a series of specific recommendations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658906",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "290–300",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Failing our youngest: On the biases, pitfalls, and risks in a decision support algorithm used for child protection",
		"URL": "https://doi.org/10.1145/3630106.3658906",
		"author": [
			{
				"family": "Moreau",
				"given": "Therese"
			},
			{
				"family": "Sinatra",
				"given": "Roberta"
			},
			{
				"family": "Sekara",
				"given": "Vedran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "maywormMisgenderedModerationHow2024",
		"type": "paper-conference",
		"abstract": "Transgender and nonbinary social media users experience disproportionate content removals on social media platforms, even when content does not violate platforms’ guidelines. In 2022, the Oversight Board, which oversees Meta platforms’ content moderation decisions, invited public feedback on Instagram’s removal of two trans users’ posts featuring their bare chests, introducing a unique opportunity to hear trans users’ feedback on how nudity and sexual activity policies impacted them. We conducted a qualitative analysis of 83 comments made public during the Oversight Board’s public comment process. Commenters criticized Meta’s nudity policies as enforcing a cisnormative view of gender while making it unclear how images of trans users’ bodies are moderated, enabling the disproportionate removal of trans content and limiting trans users’ ability to use Meta’s platforms. Yet there was significant divergence among commenters about how to address cisnormative moderation. Some commenters suggested that Meta clarify nudity guidelines, while others suggested that Meta overhaul them entirely, removing gendered distinctions or fundamentally reconfiguring the platform’s relationship to sexual content. We then discuss how the Oversight Board’s public comment process demonstrates the value of incorporating trans people’s feedback while developing policies related to gender and nudity, while arguing that Meta must go beyond only revising policy language by reevaluating how cisnormative values are encoded in all aspects of its content moderation systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658907",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "301–312",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Misgendered during moderation: How transgender bodies make visible cisnormative content moderation policies and enforcement in a meta oversight board case",
		"URL": "https://doi.org/10.1145/3630106.3658907",
		"author": [
			{
				"family": "Mayworm",
				"given": "Samuel"
			},
			{
				"family": "Albert",
				"given": "Kendra"
			},
			{
				"family": "Haimson",
				"given": "Oliver L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "herlihyStructuredRegressionApproach2024",
		"type": "paper-conference",
		"abstract": "Disaggregated evaluation is a central task in AI fairness assessment, where the goal is to measure an AI system’s performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are included in analysis. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and demonstrate how goodness-of-fit testing helps identify the key factors that drive differences in performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658908",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "313–325",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A structured regression approach for evaluating model performance across intersectional subgroups",
		"URL": "https://doi.org/10.1145/3630106.3658908",
		"author": [
			{
				"family": "Herlihy",
				"given": "Christine"
			},
			{
				"family": "Truong",
				"given": "Kimberly"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Dudı́k",
				"given": "Miroslav"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "thachTranscenteredModerationTrans2024",
		"type": "paper-conference",
		"abstract": "Mainstream platforms’ content moderation systems typically employ generalized “one-size-fits-all” approaches, intended to serve both general and marginalized users. Thus, transgender people must often create their own technologies and moderation systems to meet their specific needs. In our interview study of transgender technology creators (n=115), we found that creators face issues of transphobic abuse and disproportionate content moderation. Trans tech creators address these issues by carefully moderating and vetting their userbases, centering trans contexts in content moderation systems, and employing collective governance and community models. Based on these findings, we argue that trans tech creators’ approaches to moderation offer important insights into how to better design for trans users, and ultimately, marginalized users in the larger platform ecology. We introduce the concept of trans-centered moderation – content moderation that reviews and successfully vets transphobic users, appoints trans moderators to effectively moderate trans contexts, considers the limitations and constraints of technology for addressing social challenges, and employs collective governance and community models. Trans-centered moderation can help to improve platform design for trans users while reducing the harm faced by trans people and marginalized users more broadly.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658909",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "326–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trans-centered moderation: Trans technology creators and centering transness in platform and community governance",
		"URL": "https://doi.org/10.1145/3630106.3658909",
		"author": [
			{
				"family": "Thach",
				"given": "Hibby"
			},
			{
				"family": "Mayworm",
				"given": "Samuel"
			},
			{
				"family": "Thomas",
				"given": "Michaelanne"
			},
			{
				"family": "Haimson",
				"given": "Oliver L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "johnsonFallAlgorithmCharacterizing2024",
		"type": "paper-conference",
		"abstract": "As more algorithmic systems have come under scrutiny for their potential to inflict societal harms, an increasing number of organizations that hold power over harmful algorithms have chosen, or were required under the law, to abandon them. While social movements and calls to abandon harmful algorithms have emerged across application domains, little academic attention has been paid to studying abandonment as a means to mitigate algorithmic harms. In this paper, we take a first step towards conceptualizing “algorithm abandonment” as an organization’s decision to stop designing, developing, or using an algorithmic system due to its (potential) harms. We conduct a thematic analysis of real-world cases of algorithm abandonment to characterize the dynamics leading to this outcome. Our analysis of 40 cases reveals that campaigns to abandon an algorithm follow a common process of six iterative phases: discovery, diagnosis, dissemination, dialogue, decision, and death, which we term the 6 D’s of abandonment. In addition, we highlight key factors that facilitate (or prohibit) abandonment, which include characteristics of both the technical and social systems that the algorithm is embedded within. We discuss implications for several stakeholders, including proprietors and technologists who have the power to influence an algorithm’s (dis)continued use, FAccT researchers, and policymakers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658910",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 22\npublisher-place: Rio de Janeiro, Brazil",
		"page": "337–358",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The fall of an algorithm: Characterizing the dynamics toward abandonment",
		"URL": "https://doi.org/10.1145/3630106.3658910",
		"author": [
			{
				"family": "Johnson",
				"given": "Nari"
			},
			{
				"family": "Moharana",
				"given": "Sanika"
			},
			{
				"family": "Harrington",
				"given": "Christina"
			},
			{
				"family": "Andalibi",
				"given": "Nazanin"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hutiriNotMyVoice2024",
		"type": "paper-conference",
		"abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens’ homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658911",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "359–376",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Not my voice! A taxonomy of ethical and safety harms of speech generators",
		"URL": "https://doi.org/10.1145/3630106.3658911",
		"author": [
			{
				"family": "Hutiri",
				"given": "Wiebke"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gillisOperationalizingSearchLess2024",
		"type": "paper-conference",
		"abstract": "The Less Discriminatory Alternative is a key provision of the disparate impact doctrine in the United States. In fair lending, this provision mandates that lenders must adopt models that reduce discrimination when they do not compromise their business interests. In this paper, we develop practical methods to audit for less discriminatory alternatives. Our approach is designed to verify the existence of less discriminatory machine learning models – by returning an alternative model that can reduce discrimination without compromising performance (discovery) or by certifying that an alternative model does not exist (refutation). We develop a method to fit the least discriminatory linear classification model in a specific lending task – by minimizing an exact measure of disparity (e.g., the maximum gap in group FNR) and enforcing hard performance constraints for business necessity (e.g., on FNR and FPR). We apply our method to study the prevalence of less discriminatory alternatives on real-world datasets from consumer finance applications. Our results highlight how models may inadvertently lead to unnecessary discrimination across common deployment regimes, and demonstrate how our approach can support lenders, regulators, and plaintiffs by reliably detecting less discriminatory alternatives in such instances.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658912",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "377–387",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Operationalizing the search for less discriminatory alternatives in fair lending",
		"URL": "https://doi.org/10.1145/3630106.3658912",
		"author": [
			{
				"family": "Gillis",
				"given": "Talia B"
			},
			{
				"family": "Meursault",
				"given": "Vitaly"
			},
			{
				"family": "Ustun",
				"given": "Berk"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "quayeAdversarialNibblerOpen2024",
		"type": "paper-conference",
		"abstract": "With text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on “implicitly adversarial” prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. We present an in-depth account of our methodology, a systematic study of novel attack strategies and safety failures, and a visualization tool for easy exploration of the dataset. The first challenge round resulted in over 10k prompt-image pairs with machine annotations for safety. A subset of 1.5k samples contains rich human annotations of harm types and attack styles. Our findings emphasize the necessity of continual auditing and adaptation as new vulnerabilities emerge. This work will enable proactive, iterative safety assessments and promote responsible development of T2I models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658913",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "388–406",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adversarial nibbler: An open red-teaming method for identifying diverse harms in text-to-image generation",
		"URL": "https://doi.org/10.1145/3630106.3658913",
		"author": [
			{
				"family": "Quaye",
				"given": "Jessica"
			},
			{
				"family": "Parrish",
				"given": "Alicia"
			},
			{
				"family": "Inel",
				"given": "Oana"
			},
			{
				"family": "Rastogi",
				"given": "Charvi"
			},
			{
				"family": "Kirk",
				"given": "Hannah Rose"
			},
			{
				"family": "Kahng",
				"given": "Minsuk"
			},
			{
				"family": "Van Liemt",
				"given": "Erin"
			},
			{
				"family": "Bartolo",
				"given": "Max"
			},
			{
				"family": "Tsang",
				"given": "Jess"
			},
			{
				"family": "White",
				"given": "Justin"
			},
			{
				"family": "Clement",
				"given": "Nathan"
			},
			{
				"family": "Mosquera",
				"given": "Rafael"
			},
			{
				"family": "Ciro",
				"given": "Juan"
			},
			{
				"family": "Janapa Reddi",
				"given": "Vijay"
			},
			{
				"family": "Aroyo",
				"given": "Lora"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "frohlichInsightsInsuranceFair2024",
		"type": "paper-conference",
		"abstract": "We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658914",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "407–421",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Insights from insurance for fair machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658914",
		"author": [
			{
				"family": "Fröhlich",
				"given": "Christian"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "nedzhvetskayaNoSimpleFix2024",
		"type": "paper-conference",
		"abstract": "The introduction of AI into working processes has resulted in workers increasingly being subject to AI-related harms. By analyzing incidents of worker-related AI harms between 2008 and 2023 in the AI Incident Database, we find that harms get addressed under considerably restricted scenarios. Results from a Qualitative Comparative Analysis (QCA) show that workers with more power resources, either in the form of expertise or labor market power, have a greater likelihood of seeing harms fixed, all else equal. By contrast, workers lacking expertise or labor market power, have lower success rates and must resort to legal or regulatory mechanisms to get fixes through. These findings suggest that the workplace is another arena in which AI has the potential to reproduce existing inequalities among workers and that stronger legal frameworks and regulations can empower more vulnerable worker populations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658915",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "422–432",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No simple fix: How AI harms reflect power and jurisdiction in the workplace",
		"URL": "https://doi.org/10.1145/3630106.3658915",
		"author": [
			{
				"family": "Nedzhvetskaya",
				"given": "Nataliya"
			},
			{
				"family": "Tan",
				"given": "JS"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "perreaultAlgorithmicMisjudgementGoogle2024",
		"type": "paper-conference",
		"abstract": "Google Search is an important way that people seek information about politics [8], and Google states that it is “committed to providing timely and authoritative information on Google Search to help voters understand, navigate, and participate in democratic processes.”1 This paper studies the extent to which government-maintained web domains are represented in the online electoral information environment, as captured through 3.45 Google Search result pages collected during the 2022 US midterm elections for 786 locations across the United States. Focusing on state, county, and local government domains that provide locality-specific information, we study not only the extent to which these sources appear in organic search results, but also the extent to which these sources are correctly targeted to their respective constituents. We label misalignment between the geographic area that non-federal domains serve and the locations for which they appear in search results as algorithmic mistargeting, a subtype of algorithmic misjudgement in which the search algorithm targets locality-specific information to users in different (incorrect) locations. In the context of the 2022 US midterm elections, we find that 71% of all occurrences of state, county, and local government sources were mistargeted, with some domains appearing disproportionately often among organic results despite providing locality-specific information that may not be relevant to all voters. However, we also find that mistargeting often occurs in low ranks. We conclude by considering the potential consequences of extensive mistargeting of non-federal government sources and argue that ensuring the correct targeting of these sources to their respective constituents is a critical part of Google’s role in facilitating access to authoritative and locally-relevant electoral information.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658916",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "433–443",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic misjudgement in google search results: Evidence from auditing the US online electoral information environment",
		"URL": "https://doi.org/10.1145/3630106.3658916",
		"author": [
			{
				"family": "Perreault",
				"given": "Brooke"
			},
			{
				"family": "Lee",
				"given": "Johanna Hoonsun"
			},
			{
				"family": "Shava",
				"given": "Ropafadzo"
			},
			{
				"family": "Mustafaraj",
				"given": "Eni"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "srinivasanSeeNotSee2024",
		"type": "paper-conference",
		"abstract": "Algorithmic recommendation is one of the most popular applications of machine learning (ML) systems. While the implication of algorithmic recommendation has been studied in the context of high-stakes domains such as finance and healthcare, there has been very little focus in understanding its impacts with respect to the arts domain. Given that ML is increasingly finding place in the arts domain such as in generative arts and content analysis, in this paper, we examine the tensions of algorithmic curation in the context of visual arts. Through case studies, we describe how curatorial algorithms that are oblivious of broader socio-cultural contexts could potentially result in ethical concerns such as over-representation and misattribution, to name a few. Towards addressing some of these concerns, the paper offers design guidelines. Specifically, the paper outlines repair strategies that suggest ways 1) to engage with cultural stakeholders in building visual art curatorial algorithms, 2) to unlearn biases embedded in digital artworks and their meta-data, and 3) emphasize the need to establish regulatory norms specific to the use of ML in visual art curation. Taking cue from the process employed by artwork curators, the paper also describes how authenticity can be prioritized by re-calibrating visual art curatorial algorithms. The paper also suggest ways through which the potential of state-of-the-art ML curatorial algorithms can be re-imagined towards empowering the audience of artworks. We hope the insights presented in the paper spark interdisciplinary discussions and pave way for fostering reformation in algorithmic curation of visual arts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658917",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "444–455",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "To see or not to see: Understanding the tensions of algorithmic curation for visual arts",
		"URL": "https://doi.org/10.1145/3630106.3658917",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Ramya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "scheuermanWalledGardenChallenges2024",
		"type": "paper-conference",
		"abstract": "Research on technology companies and their workers can externalize otherwise invisible and tacit workplace approaches, identify organizational constraints to creating more ethical AI systems, help ground interventions in real-world organizational realities, and result in the co-creation of better business practices for organizations. However, getting access to technology companies is difficult for external researchers. In this paper, I draw from insights gained by conducting research on and with industry professionals. I present four challenges when conducting industry-focused research on responsible AI. I also present methods I used to navigate each challenge. Finally, I highlight opportunities for the tech industry to lower the barriers to external research. This work aims to share ways of navigating methodological challenges and encourage better transparency in the tech industry.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658918",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "456–466",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "In the walled garden: Challenges and opportunities for research on the practices of the AI tech industry",
		"URL": "https://doi.org/10.1145/3630106.3658918",
		"author": [
			{
				"family": "Scheuerman",
				"given": "Morgan Klaus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "neriniValueEyeBeholder2024",
		"type": "paper-conference",
		"abstract": "Proprietary data is a valuable asset used to develop predictive algorithms that benefit a wide range of users, including customers, business owners, and decision-makers. Consequently, there is a growing interest in developing safe and robust techniques for sharing, learning models, and distributing predictions across a wide spectrum of potential stakeholders. However, a structured process to assess the value of data assets, and thus enabling collaborations among stakeholders, remains largely unexplored. This is particularly challenging when the data to be shared has a networked structure, where increasing the shared data samples potentially connects information observed by different data owners, providing new knowledge that is unavailable to any data owner individually. Here, we propose E-GraDE, a framework that assists organizations in assessing the value of their networked data to better address graph machine learning tasks. This framework includes a step-by-step analysis of the requirements of different stakeholders, such as the accuracy or fairness requisites of the models, ensuring a fair evaluation process and stronger alignment in the development of a data federation consortium. Additionally, we propose an approach to estimate the value of networked data to be shared while disclosing only a small fraction of the original information. We support our approach with extensive computational experiments, analysing each part of it through simulated use cases.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658919",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "467–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value is in the eye of the beholder: a framework for an equitable graph data evaluation",
		"URL": "https://doi.org/10.1145/3630106.3658919",
		"author": [
			{
				"family": "Nerini",
				"given": "Francesco Paolo"
			},
			{
				"family": "Bajardi",
				"given": "Paolo"
			},
			{
				"family": "Panisson",
				"given": "André"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rileyOverridingInjusticePretrial2024",
		"type": "paper-conference",
		"abstract": "A small but growing number of empirical studies have attempted to measure the impacts of algorithmic pretrial risk assessments on discrete policy goals such as decarceration, racial equity, and public safety. A separate but related body of work explores frontline worker resistance and discretion related to sociotechnical systems in criminal legal contexts. I build on work that aims to bridge the gaps between these literatures by offering an ethnographic account of pretrial risk assessment administration across the United States. I draw on semi-structured interviews with 74 pretrial actors and site observations across 8 jurisdictions. I highlight the process of risk assessment administration and the frontline workers who perform that labor. Like judges, pretrial officers have the autonomy to override risk assessment recommendations, unlike judges however, their decisions are made outside the courtroom and far removed from public scrutiny. This paper makes three contributions. First, it provides a detailed account of the personal, professional, and organizational dynamics that lead pretrial officers to override risk assessment recommendations. Second, it presents a taxonomy of override behavior among pretrial officers in an effort to promote more effective policy decisions. Lastly, it provides further empirical evidence that pretrial risk assessments are unlikely to guarantee racial or economic equity or decarceration in the long term.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658920",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "480–488",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Overriding (in)justice: pretrial risk assessment administration on the frontlines",
		"URL": "https://doi.org/10.1145/3630106.3658920",
		"author": [
			{
				"family": "Riley",
				"given": "Sarah"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "laszkiewiczBenchmarkingFairnessImage2024",
		"type": "paper-conference",
		"abstract": "Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics—inspired by their supervised fairness counterparts—to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to dataset imbalances. Alarmingly, we find that none of the considered methods produces statistically fair and diverse results. All experiments can be reproduced using our provided repository.1",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658921",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 29\npublisher-place: Rio de Janeiro, Brazil",
		"page": "489–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Benchmarking the fairness of image upsampling methods",
		"URL": "https://doi.org/10.1145/3630106.3658921",
		"author": [
			{
				"family": "Laszkiewicz",
				"given": "Mike"
			},
			{
				"family": "Daunhawer",
				"given": "Imant"
			},
			{
				"family": "Vogt",
				"given": "Julia E."
			},
			{
				"family": "Fischer",
				"given": "Asja"
			},
			{
				"family": "Lederer",
				"given": "Johannes"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "yehAnalyzingRelationshipDifference2024",
		"type": "paper-conference",
		"abstract": "In research studying the fairness of machine learning algorithms and models, fairness often means that a metric is the same when computed for two different groups of people. For example, one might define fairness to mean that the false positive rate of a classifier is the same for people of different genders, ages, or races. However, it is usually not possible to make this metric identical for all groups. Instead, algorithms ensure that the metric is similar—for example, that the false positive rates are similar. Researchers usually measure this similarity or dissimilarity using either the difference or ratio between the metric values for different groups of people. Although these two approaches are known to be different, there has been little work analyzing their differences and respective benefits. In this paper we examine this relationship analytically and empirically, and conclude that unless there are application-specific reasons to prefer the difference approach, the ratio approach should be preferred.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658922",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "518–528",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing the relationship between difference and ratio-based fairness metrics",
		"URL": "https://doi.org/10.1145/3630106.3658922",
		"author": [
			{
				"family": "Yeh",
				"given": "Min-Hsuan"
			},
			{
				"family": "Metevier",
				"given": "Blossom"
			},
			{
				"family": "Hoag",
				"given": "Austin"
			},
			{
				"family": "Thomas",
				"given": "Philip"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "globus-harrisDiversifiedEnsemblingExperiment2024",
		"type": "paper-conference",
		"abstract": "Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In [12], the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants’ efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams’ approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658923",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "529–545",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversified ensembling: An experiment in crowdsourced machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658923",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Harrison",
				"given": "Declan"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Perona",
				"given": "Pietro"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pareekTrustDevelopmentRepair2024",
		"type": "paper-conference",
		"abstract": "Leveraging Artificial Intelligence to support human decision-makers requires harnessing the unique strengths of both entities, where human expertise often complements AI capabilities. However, human decision-makers must accurately discern when to trust the AI. In situations with complementary Human-AI expertise, identifying AI inaccuracies becomes challenging for humans, hindering their ability to rely on the AI only when warranted. Even when AI performance improves post-errors, this inability to assess accuracy can hinder trust recovery. Through two experimental tasks, we investigate trust development, erosion, and recovery during AI-assisted decision-making, examining explicit Trust Repair Strategies (TRSs) – Apology, Denial, Promise, and Model Update. Our participants classified familiar and unfamiliar stimuli with an AI with varying accuracy. We find that participants leveraged AI accuracy in familiar tasks as a heuristic to dynamically calibrate their trust during unfamiliar tasks. Further, once trust in the AI was eroded, trust restored through Model Update surpassed initial trust values, followed by Apology, Promise, and the baseline (no repair), with Denial being least effective. We empirically demonstrate how trust calibration occurs during complementary expertise, highlighting factors influencing the different effectiveness of TRSs despite identical AI accuracy, and offering implications for effectively restoring trust in Human-AI collaborations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658924",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "546–561",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trust development and repair in AI-assisted decision-making during complementary expertise",
		"URL": "https://doi.org/10.1145/3630106.3658924",
		"author": [
			{
				"family": "Pareek",
				"given": "Saumya"
			},
			{
				"family": "Velloso",
				"given": "Eduardo"
			},
			{
				"family": "Goncalves",
				"given": "Jorge"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bellaTacklingLanguageModelling2024",
		"type": "paper-conference",
		"abstract": "Current AI-based language technologies—language models, machine translation systems, multilingual dictionaries and corpora—are known to focus on the world’s 2–3% most widely spoken languages. Research efforts of the past decade have attempted to expand this coverage to ‘under-resourced languages.’ The goal of our paper is to bring attention to a corollary phenomenon that we call language modelling bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. We define language modelling bias as uneven per-language performance under similar test conditions. We show that bias stems not only from technology but also from ethically problematic research and development methodologies that disregard the needs of language communities. Moving towards diversity-aware alternatives, we present an initiative that aims at reducing language modelling bias within lexical resources through both technology design and methodology, based on an eye-level collaboration with local communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658925",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "562–572",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tackling language modelling bias in support of linguistic diversity",
		"URL": "https://doi.org/10.1145/3630106.3658925",
		"author": [
			{
				"family": "Bella",
				"given": "Gábor"
			},
			{
				"family": "Helm",
				"given": "Paula"
			},
			{
				"family": "Koch",
				"given": "Gertraud"
			},
			{
				"family": "Giunchiglia",
				"given": "Fausto"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vrijenhoekDiversityWhatDifferent2024",
		"type": "paper-conference",
		"abstract": "Diversity is a commonly known principle in the design of recommender systems, but also ambiguous in its conceptualization. Through semi-structured interviews we explore how practitioners at three different public service media organizations in the Netherlands conceptualize diversity within the scope of their recommender systems. We provide an overview of the goals that they have with diversity in their systems, which aspects are relevant, and how recommendations should be diversified. We show that even within this limited domain, conceptualization of diversity greatly varies, and argue that it is unlikely that a standardized conceptualization will be achieved. Instead, we should focus on effective communication of what diversity in this particular system means, thus allowing for operationalizations of diversity that are capable of expressing the nuances and requirements of that particular domain.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658926",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "573–584",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity of what? On the different conceptualizations of diversity in recommender systems",
		"URL": "https://doi.org/10.1145/3630106.3658926",
		"author": [
			{
				"family": "Vrijenhoek",
				"given": "Sanne"
			},
			{
				"family": "Daniil",
				"given": "Savvina"
			},
			{
				"family": "Sandel",
				"given": "Jorden"
			},
			{
				"family": "Hollink",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hallGeographicInclusionEvaluation2024",
		"type": "paper-conference",
		"abstract": "Rapid progress in text-to-image generative models coupled with their deployment for visual content creation has magnified the importance of thoroughly evaluating their performance and identifying potential biases. In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics to facilitate scalable and cost-effective performance profiling. However, commonly-used metrics often fail to account for the full diversity of human preference; often even in-depth human evaluations face challenges with subjectivity, especially as interpretations of evaluation criteria vary across regions and cultures. In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images from state-of-the art public APIs. We collect over 65,000 image annotations and 20 survey responses. We contrast human annotations with common automated metrics, finding that human preferences vary notably across geographic location and that current metrics do not fully account for this diversity. For example, annotators in different locations often disagree on whether exaggerated, stereotypical depictions of a region are considered geographically representative. In addition, the utility of automatic evaluations is dependent on assumptions about their set-up, such as the alignment of feature extractors with human perception of object similarity or the definition of “appeal” captured in reference datasets used to ground evaluations. We recommend steps for improved automatic and human evaluations. This includes collecting annotations from people located inside and outside the region of interest, instructing annotators on whether they should follow specific definitions of evaluation criteria or utilize their own interpretation, and reporting assumptions underlying automatic evaluations.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658927",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "585–601",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards geographic inclusion in the evaluation of text-to-image models",
		"URL": "https://doi.org/10.1145/3630106.3658927",
		"author": [
			{
				"family": "Hall",
				"given": "Melissa"
			},
			{
				"family": "Bell",
				"given": "Samuel J."
			},
			{
				"family": "Ross",
				"given": "Candace"
			},
			{
				"family": "Williams",
				"given": "Adina"
			},
			{
				"family": "Drozdzal",
				"given": "Michal"
			},
			{
				"family": "Soriano",
				"given": "Adriana Romero"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "blackDhacking2024",
		"type": "paper-conference",
		"abstract": "Recent regulatory efforts, including Executive Order 14110 and the AI Bill of Rights, have focused on mitigating discrimination in AI systems through novel and traditional application of anti-discrimination laws. While these initiatives rightly emphasize fairness testing and mitigation, we argue that they pay insufficient attention to robust bias measurement and mitigation—and that without doing so, the frameworks cannot effectively achieve the goal of reducing discrimination in deployed AI models. This oversight is particularly concerning given the instability and brittleness of current algorithmic bias mitigation and fairness optimization methods, as highlighted by growing evidence in the algorithmic fairness literature. This instability heightens the risk of what we term discrimination-hacking or d-hacking, a scenario where, inadvertently or deliberately, the selection of models based on favorable fairness metrics within specific samples could lead to misleading or non-generalizable fairness performance. We term this effect d-hacking because systematically selecting among numerous models to find the least discriminatory one parallels the concept of p-hacking in social science research of selectively reporting outcomes that appear statistically significant resulting in misleading conclusions. In light of these challenges, we argue that AI fairness regulation should not only call for fairness measurement and bias mitigation, but also specify methods to ensure robust solutions to discrimination in AI systems. Towards the goal of arguing for robust fairness assessment and bias mitigation in AI regulation, this paper (1) synthesizes evidence of d-hacking in the computer science literature and provides experimental demonstrations of d-hacking, (2) analyzes current legal frameworks to understand the treatment of robust fairness and non-discriminatory behavior, both in recent AI regulation proposals and traditional U.S. discrimination law, and (3) outlines policy recommendations for preventing d-hacking in high-stakes domains.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658928",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "602–615",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "D-hacking",
		"URL": "https://doi.org/10.1145/3630106.3658928",
		"author": [
			{
				"family": "Black",
				"given": "Emily"
			},
			{
				"family": "Gillis",
				"given": "Talia"
			},
			{
				"family": "Hall",
				"given": "Zara Yasmine"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "somerstepAlgorithmicFairnessPerformative2024",
		"type": "paper-conference",
		"abstract": "In many prediction problems, the predictive model affects the distribution of the prediction target. This phenomenon is known as performativity and is often caused by the behavior of individuals with vested interests in the outcome of the predictive model. Although performativity is generally problematic because it manifests as distribution shifts, we develop algorithmic fairness practices that leverage performativity to achieve stronger group fairness guarantees in social classification problems (compared to what is achievable in non-performative settings). In particular, we leverage the policymaker’s ability to steer the population to remedy inequities in the long term. A crucial benefit of this approach is that it is possible to resolve the incompatibilities between conflicting group fairness definitions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658929",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "616–630",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness in performative policy learning: Escaping the impossibility of group fairness",
		"URL": "https://doi.org/10.1145/3630106.3658929",
		"author": [
			{
				"family": "Somerstep",
				"given": "Seamus"
			},
			{
				"family": "Ritov",
				"given": "Ya'acov"
			},
			{
				"family": "Sun",
				"given": "Yuekai"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ajmaniDataAgencyTheory2024",
		"type": "paper-conference",
		"abstract": "Data collection methods for AI applications have been heavily scrutinized by researchers, policymakers, and the general public. In this paper, we propose data agency theory (DAT), a precise theory of justice to evaluate and improve current consent procedures used in AI applications. We argue that data agency is systematically defined by consent policies. Therefore, data agency is a matter of justice. DAT claims data agency ought to be afforded in a way that minimizes the oppression of data contributors by data collectors. We then apply DAT to two salient consent procedures in AI applications: Reddit’s Terms of Service agreement and the United States’s IRB protocols. Through these cases, we demonstrate how our theory helps evaluate justice and generate ideas for improvement. Finally, we discuss the implications of using justice as an evaluation metric, comparing consent procedures, and adopting DAT in future research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658930",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "631–641",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data agency theory: a precise theory of justice for AI applications",
		"URL": "https://doi.org/10.1145/3630106.3658930",
		"author": [
			{
				"family": "Ajmani",
				"given": "Leah"
			},
			{
				"family": "Stapleton",
				"given": "Logan"
			},
			{
				"family": "Houtti",
				"given": "Mo"
			},
			{
				"family": "Chancellor",
				"given": "Stevie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "simsonLazyDataPractices2024",
		"type": "paper-conference",
		"abstract": "Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a lack of representation for certain protected attributes in both data and evaluations; (2) the widespread exclusion of minorities during data preprocessing; and (3) opaque data processing threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658931",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "642–659",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Lazy data practices harm fairness research",
		"URL": "https://doi.org/10.1145/3630106.3658931",
		"author": [
			{
				"family": "Simson",
				"given": "Jan"
			},
			{
				"family": "Fabris",
				"given": "Alessandro"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mahomedAuditingGptsContent2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly appearing in consumer-facing products. To prevent problematic use, the organizations behind these systems have put content moderation guardrails in place that prevent the models from generating content they consider harmful. However, most of these enforcement standards and processes are opaque. Although they play a major role in the user experience of these tools, automated content moderation tools have received relatively less attention than other aspects of the models. This study undertakes an algorithm audit of OpenAI’s ChatGPT with the goal of better understanding its content moderation guardrails and their potential biases. To evaluate performance on a broad cultural range of content, we generate a dataset of 100 popular United States television shows with one to three synopses for each episode in the first season of each show (3,309 total synopses). We probe GPT’s content moderation endpoint (ME) to identify violating content both in the synopses themselves, and in GPT’s own outputs when asked to generate a script based on each synopsis, also comparing with ME outputs on 81 real scripts from the same TV shows (269,578 total ME outputs). Our findings show that a large number of GPT-generated and real scripts flag as content violations (about 18% of GPT scripts and 69% of real ones). Using metadata, we find that TV maturity ratings, as well as certain genres (Animation, Crime, Fantasy, and others) are statistically significantly related to a script’s likelihood of flagging. We conclude by discussing the implications of LLM self-censorship and directions for future research on their moderation procedures.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658932",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 27\npublisher-place: Rio de Janeiro, Brazil",
		"page": "660–686",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing gpt's content moderation guardrails: Can ChatGPT write your favorite TV show?",
		"URL": "https://doi.org/10.1145/3630106.3658932",
		"author": [
			{
				"family": "Mahomed",
				"given": "Yaaseen"
			},
			{
				"family": "Crawford",
				"given": "Charlie M."
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Friedler",
				"given": "Sorelle A."
			},
			{
				"family": "Metaxa",
				"given": "Danaë"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "glazkoIdentifyingImprovingDisability2024",
		"type": "paper-conference",
		"abstract": "As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability-related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658933",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "687–700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Identifying and improving disability bias in GPT-based resume screening",
		"URL": "https://doi.org/10.1145/3630106.3658933",
		"author": [
			{
				"family": "Glazko",
				"given": "Kate"
			},
			{
				"family": "Mohammed",
				"given": "Yusuf"
			},
			{
				"family": "Kosa",
				"given": "Ben"
			},
			{
				"family": "Potluri",
				"given": "Venkatesh"
			},
			{
				"family": "Mankoff",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "naudtsDigitalFacesOppression2024",
		"type": "paper-conference",
		"abstract": "Drawing from Iris Marion Young's politics of difference and democratic theory, this contribution formulates a relational and egalitarian account of digital justice to understand and help counter, the social and technical conditions under which data-driven decision-making systems are liable to reinforce and introduce social injustice. To do so, this contribution is structured alongside three axes. First, I present data-driven decision-making systems as socio-technical systems that both take meaning from and co-shape people's relationships and the social structures they are part of. Due to this relational push and pull, I argue, data-driven systems have the potential to restructure society and, consequently, the conditions that govern people's exposure to, and experience of, injustice therein. Second, I transpose Young's ideation of oppression and domination onto the digital ecosystem. Both notions are used to locate within complex, dynamic and automated environments, a series of social and technological conditions that unjustifiably limit people's actions and behaviours. Third, I build on Young's model for an inclusive democracy to propose a series of institutional and procedural practices to ensure that, within the digital ecosystem, each person has the effective opportunity to pursue the life projects they value and to communicate their needs, concerns and experiences in ways that are heard and recognized by others.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658934",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "701–712",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The digital faces of oppression and domination: a relational and egalitarian perspective on the data-driven society and its regulation",
		"URL": "https://doi.org/10.1145/3630106.3658934",
		"author": [
			{
				"family": "Naudts",
				"given": "Laurens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "tangAIFailureCards2024",
		"type": "paper-conference",
		"abstract": "AI-based decision support tools have been used in a wide range of high-stakes settings. However, many of them have failed. Past literature in FAccT contributes important insights into how to detect and mitigate AI failures from a technical perspective. Recently, there are growing calls to understand AI failures as socio-technical and to support community-centered, grassroots-based mitigations to AI failures, in addition to top-down approaches. In this paper, we present AI Failure Cards, a novel method for both improving communities’ understanding of AI failures and for eliciting their current practices and desired strategies for mitigation, with a goal to better support those efforts in the future. Through a series of workshops with unhoused individuals, frontline workers and service providers, as well as local policy advocates, we conducted an empirical investigation of our method in the context of a locally deployed predictive housing allocation algorithm. Our results suggest that the use of the method helped impacted communities better understand these AI failures. It also surfaced a wide range of existing grassroots practices and desired mitigation strategies. Finally, we discuss both the challenges and opportunities for supporting grassroots efforts in mitigating AI failures.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658935",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "713–732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI failure cards: Understanding and supporting grassroots efforts to mitigate AI failures in homeless services",
		"URL": "https://doi.org/10.1145/3630106.3658935",
		"author": [
			{
				"family": "Tang",
				"given": "Ningjing"
			},
			{
				"family": "Zhi",
				"given": "Jiayin"
			},
			{
				"family": "Kuo",
				"given": "Tzu-Sheng"
			},
			{
				"family": "Kainaroi",
				"given": "Calla"
			},
			{
				"family": "Northup",
				"given": "Jeremy J."
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Shen",
				"given": "Hong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "stauferSilencingRiskNot2024",
		"type": "paper-conference",
		"abstract": "Whistleblowing is essential for ensuring transparency and accountability in both public and private sectors. However, (potential) whistleblowers often fear or face retaliation, even when reporting anonymously. The specific content of their disclosures and their distinct writing style may re-identify them as the source. Legal measures, such as the EU Whistleblower Directive, are limited in their scope and effectiveness. Therefore, computational methods to prevent re-identification are important complementary tools for encouraging whistleblowers to come forward. However, current text sanitization tools follow a one-size-fits-all approach and take an overly limited view of anonymity. They aim to mitigate identification risk by replacing typical high-risk words (such as person names and other labels of named entities) and combinations thereof with placeholders. Such an approach, however, is inadequate for the whistleblowing scenario since it neglects further re-identification potential in textual features, including the whistleblower’s writing style. Therefore, we propose, implement, and evaluate a novel classification and mitigation strategy for rewriting texts that involves the whistleblower in the assessment of the risk and utility. Our prototypical tool semi-automatically evaluates risk at the word/term level and applies risk-adapted anonymization techniques to produce a grammatically disjointed yet appropriately sanitized text. We then use a Large Language Model (LLM) that we fine-tuned for paraphrasing to render this text coherent and style-neutral. We evaluate our tool’s effectiveness using court cases from the European Court of Human Rights (ECHR) and excerpts from a real-world whistleblower testimony and measure the protection against authorship attribution attacks and utility loss statistically using the popular IMDb62 movie reviews dataset, which consists of 62 individuals. Our method can significantly reduce authorship attribution accuracy from 98.81% to 31.22%, while preserving up to 73.1% of the original content’s semantics, as measured by the established cosine similarity of sentence embeddings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658936",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "733–745",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Silencing the risk, not the whistle: a semi-automated text sanitization tool for mitigating the risk of whistleblower re-identification",
		"URL": "https://doi.org/10.1145/3630106.3658936",
		"author": [
			{
				"family": "Staufer",
				"given": "Dimitri"
			},
			{
				"family": "Pallas",
				"given": "Frank"
			},
			{
				"family": "Berendt",
				"given": "Bettina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "benattiGenderBiasDetection2024",
		"type": "paper-conference",
		"abstract": "Data derived from the realm of the social sciences is often produced in digital text form, which motivates its use as a source for natural language processing methods. Researchers and practitioners have developed and relied on artificial intelligence techniques to collect, process, and analyze documents in the legal field, especially for tasks such as text summarization and classification. While increasing procedural efficiency is often the primary motivation behind natural language processing in the field, several works have proposed solutions for human rights-related issues, such as assessment of public policy and institutional social settings. One such issue is the presence of gender biases in court decisions, which has been largely studied in social sciences fields; biased institutional responses to gender-based violence are a violation of international human rights dispositions since they prevent gender minorities from accessing rights and hamper their dignity. Natural language processing-based approaches can help detect these biases on a larger scale. Still, the development and use of such tools require researchers and practitioners to be mindful of legal and ethical aspects concerning data sharing and use, reproducibility, domain expertise, and value-charged choices. In this work, we (a) present an experimental framework developed to automatically detect gender biases in court decisions issued in Brazilian Portuguese and (b) describe and elaborate on features we identify to be critical in such a technology, given its proposed use as a support tool for research and assessment of court activity.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658937",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "746–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias detection in court decisions: a brazilian case study",
		"URL": "https://doi.org/10.1145/3630106.3658937",
		"author": [
			{
				"family": "Benatti",
				"given": "Raysa"
			},
			{
				"family": "Severi",
				"given": "Fabiana"
			},
			{
				"family": "Avila",
				"given": "Sandra"
			},
			{
				"family": "Colombini",
				"given": "Esther Luna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "watkinsFourfifthsRuleNot2024",
		"type": "paper-conference",
		"abstract": "Computer scientists are trained in the art of creating abstractions that simplify and generalize. However, a premature abstraction that omits crucial contextual details creates the risk of epistemic trespassing, by falsely asserting its relevance into other contexts. We study how the field of responsible AI has created an imperfect synecdoche by abstracting the four-fifths rule (a.k.a. the ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 1¡/AltText¿¡File name=\"facct24-53-inline1\" type=\"svg\"/¿¡/Formula¿ rule or 80% rule), a single part of disparate impact discrimination law, into the disparate impact metric. This metric incorrectly introduces a new deontic nuance and new potentials for ethical harms that were absent in the original ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 2¡/AltText¿¡File name=\"facct24-53-inline2\" type=\"svg\"/¿¡/Formula¿ rule. We also survey how the field has amplified the potential for harm in codifying the ¡Formula format=\"inline\"¿¡TexMath¿¡?TeX nicefrac45?¿¡/TexMath¿¡AltText¿Math 3¡/AltText¿¡File name=\"facct24-53-inline3\" type=\"svg\"/¿¡/Formula¿ rule into popular AI fairness software toolkits. The harmful erasure of legal nuances is a wake-up call for computer scientists to self-critically re-evaluate the abstractions they create and use, particularly in the interdisciplinary field of AI ethics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658938",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "764–775",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3630106.3658938",
		"author": [
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Chen",
				"given": "Jiahao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dominguezhernandezMappingIndividualSocial2024",
		"type": "paper-conference",
		"abstract": "Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658939",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "776–796",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping the individual, social and biospheric impacts of Foundation Models",
		"URL": "https://doi.org/10.1145/3630106.3658939",
		"author": [
			{
				"family": "Domı́nguez Hernández",
				"given": "Andrés"
			},
			{
				"family": "Krishna",
				"given": "Shyam"
			},
			{
				"family": "Perini",
				"given": "Antonella Maia"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Borda",
				"given": "Ann"
			},
			{
				"family": "Hashem",
				"given": "Youmna"
			},
			{
				"family": "Hadjiloizou",
				"given": "Semeli"
			},
			{
				"family": "Mahomed",
				"given": "Sabeehah"
			},
			{
				"family": "Jayadeva",
				"given": "Smera"
			},
			{
				"family": "Aitken",
				"given": "Mhairi"
			},
			{
				"family": "Leslie",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "srinivasanGeneralizedPeopleDiversity2024",
		"type": "paper-conference",
		"abstract": "Capturing the diversity of people in images is challenging: recent literature tends to focus on diversifying one or two attributes, requiring expensive attribute labels or building classifiers. We introduce a diverse people image ranking method which more flexibly aligns with human notions of people diversity in a less prescriptive, label-free manner. The Perception-Aligned Text-derived Human representation Space (PATHS) aims to capture all or many relevant features of people-related diversity, and, when used as the representation space in the standard Maximal Marginal Relevance (MMR) ranking algorithm [7], is better able to surface a range of types of people-related diversity (e.g. disability, cultural attire). PATHS is created in two stages. First, a text-guided approach is used to extract a person-diversity representation from a pre-trained image-text model. Then this representation is fine-tuned on perception judgments from human annotators so that it captures the aspects of people-related similarity that humans find most salient. Empirical results show that the PATHS method achieves diversity better than baseline methods, according to side-by-side ratings from human annotators.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658940",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 25\npublisher-place: Rio de Janeiro, Brazil",
		"page": "797–821",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Generalized people diversity: Learning a human perception-aligned diversity representation for people images",
		"URL": "https://doi.org/10.1145/3630106.3658940",
		"author": [
			{
				"family": "Srinivasan",
				"given": "Hansa"
			},
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "Sinha",
				"given": "Aradhana"
			},
			{
				"family": "Madras",
				"given": "David"
			},
			{
				"family": "Olanubi",
				"given": "Gbolahan Oluwafemi"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Ricco",
				"given": "Susanna"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kimImNotSure2024",
		"type": "paper-conference",
		"abstract": "Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658941",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "822–835",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"I'm not sure, but...\": Examining the impact of large language models' uncertainty expression on user reliance and trust",
		"URL": "https://doi.org/10.1145/3630106.3658941",
		"author": [
			{
				"family": "Kim",
				"given": "Sunnie S. Y."
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Vorvoreanu",
				"given": "Mihaela"
			},
			{
				"family": "Ballard",
				"given": "Stephanie"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "riveraEscalationRisksLanguage2024",
		"type": "paper-conference",
		"abstract": "Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658942",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 63\npublisher-place: Rio de Janeiro, Brazil",
		"page": "836–898",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Escalation risks from language models in military and diplomatic decision-making",
		"URL": "https://doi.org/10.1145/3630106.3658942",
		"author": [
			{
				"family": "Rivera",
				"given": "Juan-Pablo"
			},
			{
				"family": "Mukobi",
				"given": "Gabriel"
			},
			{
				"family": "Reuel",
				"given": "Anka"
			},
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Smith",
				"given": "Chandler"
			},
			{
				"family": "Schneider",
				"given": "Jacquelyn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sobeyHarmfulFetishisationReductive2024",
		"type": "paper-conference",
		"abstract": "Personal tracking and the quantified self have grown increasingly popular as technological capabilities for individual insights have grown. Incorporated into many of these systems is the capacity to monitor metrics over time to offer visualisations of attributes such as health, fitness and nutrition. However, many such systems rely on single, simplified measures to represent these complex phenomena, and due to tracking and visualisations, they add value judgements, such as success and failure, to the users' information. This paper, therefore, aims to shed light on the challenges of reductive measures through the case of the BMI (Body Mass Index). The BMI is a clear example of a reductive measure that is used to offer insight into health in both formal and informal healthcare, despite a substantial body of literature that demonstrates other more accurate factors of health that are easily measured. Through a historical consideration of the origin and narratives around the BMI, we demonstrate the fallacy of its use and offer a broader critique of reductive metrics. This understanding of the BMI allows us to highlight the potential harms arising from personalised ‘health’ tracking technologies and the values encoded into such systems: we use established frameworks of digital harm to demonstrate that using the BMI is harmful for not only well-documented health reasons, but that this harm is exacerbated when it is incorporated into digital technology. Our paper offers a challenge to traditional health thinking and, more broadly, the fetishisation of reductive metrics in data systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658943",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "899–908",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The harmful fetishisation of reductive personal tracking metrics in digital systems",
		"URL": "https://doi.org/10.1145/3630106.3658943",
		"author": [
			{
				"family": "Sobey",
				"given": "Aisha"
			},
			{
				"family": "Carter",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lopezMoreSumIts2024",
		"type": "paper-conference",
		"abstract": "Algorithmic systems are increasingly being applied in contexts of state action to, in some capacity, mediate the relations between state and individual. Disadvantageous effects, such as potential discriminatory outcomes brought forth by different kinds of biases, have been the locus of severe critique by academic scholarship and political activism. There has been scholarly work conceptualizing biases and types of biases, as well as types of harm. Drawing from Elizabeth Anderson’s conceptualization of relational equality, this paper emphasizes the relationality of the encounters between state and individual. This paper introduces \"susceptibility to algorithmic disadvantage\" as a conceptual framework to address the relational constellation at play. Susceptibility to algorithmic disadvantage has a vertical dimension that addresses the relation between a state actor and an individual and a horizontal dimension that is characterized by intersectional inequalities that prevail in societal contexts. Intersectional feminist scholarship has established that interlocking systems of oppression amount to more than the sum of their single-axis parts. Paralleling this argument, this paper argues that susceptibility to algorithmic disadvantage amounts to more than the sum of the vertical and the horizontal dimension: the dimensions co-constitute and reinforce each other. The proposed framework is applied to four international case studies situated in crucial areas of state action: facial recognition in law enforcement in the USA, biometric identification in social welfare in India, dialect recognition in the asylum system in Germany, and grade prediction in the education system in the UK. Viewed through the lens of the proposed framework, heterogeneous use cases in different locations and areas of state action emerge as similar considering the inquiry into questions of justice, rendering the proposed framework a useful tool for analysis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658944",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "909–919",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More than the sum of its parts: Susceptibility to algorithmic disadvantage as a conceptual framework",
		"URL": "https://doi.org/10.1145/3630106.3658944",
		"author": [
			{
				"family": "Lopez",
				"given": "Paola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "awumeySystematicReviewBiometric2024",
		"type": "paper-conference",
		"abstract": "Modern advances in AI have increased employer interest in tracking workers’ biometric signals — e.g., their brainwaves and facial expressions — to evaluate and make predictions about their performance and productivity. These technologies afford managers information about internal emotional and physiological states that were previously accessible only to individual workers, raising new concerns around worker privacy and autonomy. Yet, the research literature on the impact of AI-powered biometric work monitoring (AI-BWM) technologies on workers remains fragmented across disciplines and industry sectors, limiting our understanding of its impacts on workers at large. In this paper, we sytematically review 129 papers, spanning varied disciplines and industry sectors, that discuss and analyze the impact of AI-powered biometric monitoring technologies in occupational settings. We situate this literature across a process model that spans the development, deployment, and usage phases of these technologies. We further draw on Shelby et al.’s Taxonomy of Socio-technical Harms in AI systems to systematize the harms experienced by workers across the three phases of our process model. We find that the development, deployment, and sustained use of AI-powered biometric work monitoring technologies put workers at risk of a number of the socio-technical harms specified by Shelby et al.: e.g., by forcing workers to exert additional emotional labor to avoid flagging unreliable affect monitoring systems, or through the use of these data to make inferences about productivity. Our research contributes to the field of critical AI studies by highlighting the potential for a cascade of harms to occur when the impact of these technologies on workers is not considered at all phases of our process model.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658945",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "920–932",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of biometric monitoring in the workplace: Analyzing socio-technical harms in development, deployment and use",
		"URL": "https://doi.org/10.1145/3630106.3658945",
		"author": [
			{
				"family": "Awumey",
				"given": "Ezra"
			},
			{
				"family": "Das",
				"given": "Sauvik"
			},
			{
				"family": "Forlizzi",
				"given": "Jodi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chienBehavioristRepresentationalHarms2024",
		"type": "paper-conference",
		"abstract": "Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, examining current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement and mitigation praxis.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658946",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "933–946",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond behaviorist representational harms: a plan for measurement and mitigation",
		"URL": "https://doi.org/10.1145/3630106.3658946",
		"author": [
			{
				"family": "Chien",
				"given": "Jennifer"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "pessachGenderRepresentationOnline2024",
		"type": "paper-conference",
		"abstract": "We present a broad characterization of gender representation in a large heterogeneous sample of retail products. In particular, we study online product textual information, such as titles and descriptions. Our goal is to understand from a semantic perspective, differences and similarities in how girls (women) and boys (men) are represented. We perform a comparative analysis of the language used in gendered products (i.e., products that mention exclusively either of these two genders), and additionally compare it to products that are explicitly gender neutral or inclusive. We found that the adjectives, skills, occupations, and values described in gendered products tended to reinforce stereotypes. Some of these stereotypes are aligned with historical findings from research on traditional off-line retail stores, and others are new owing to the up-to-date product dataset our research is based on. By leveraging additional existing resources we were able to gain insight into how certain product descriptions reflect stereotypes that are related to soft-skills and hierarchical occupational information. Conversely, we found that a large segment of products present explicitly as gender neutral or inclusive. We explore whether the language used by gender-inclusive products can be useful to improve stereotypes reflected in gendered product text. Specifically, we study its effect in word embedding fairness through debiasing techniques.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658947",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "947–957",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender representation across online retail products",
		"URL": "https://doi.org/10.1145/3630106.3658947",
		"author": [
			{
				"family": "Pessach",
				"given": "Dana"
			},
			{
				"family": "Poblete",
				"given": "Barbara"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chanVisibilityAIAgents2024",
		"type": "paper-conference",
		"abstract": "Increased delegation of commercial, scientific, governmental, and personal activities to AI agents—systems capable of pursuing complex goals with limited supervision—may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as visibility, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: agent identifiers, real-time monitoring, and activity logging. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for various actors in the supply chain including hardware and software service providers. Finally, we discuss the implications of our measures for privacy and concentration of power. Further work into understanding the measures and mitigating their negative impacts can help to build a foundation for the governance of AI agents.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658948",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "958–973",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Visibility into AI agents",
		"URL": "https://doi.org/10.1145/3630106.3658948",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Kaufmann",
				"given": "Max"
			},
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Hammond",
				"given": "Lewis"
			},
			{
				"family": "Bradley",
				"given": "Herbie"
			},
			{
				"family": "Bluemke",
				"given": "Emma"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hancockTensionsDataSharing2024",
		"type": "paper-conference",
		"abstract": "There are calls for greater data sharing to address human rights issues. Advocates claim this will provide an evidence-base to increase transparency, improve accountability, enhance decision-making, identify abuses, and offer remedies for rights violations. However, these well-intentioned efforts have been found to sometimes enable harms against the people they seek to protect. This paper shows issues relating to fairness, accountability, or transparency (FAccT) in and around data sharing can produce such ‘ironic’ consequences. It does so using an empirical case study: efforts to tackle modern slavery and human trafficking in the UK. We draw on a qualitative analysis of expert interviews, workshops, ecosystem mapping exercises, and a desk-based review. The findings show how, in the UK, a large ecosystem of data providers, hubs, and users emerged to process and exchange data from across the country. We identify how issues including legal uncertainties, non-transparent sharing procedures, and limited accountability regarding downstream uses of data may undermine efforts to tackle modern slavery and place victims of abuses at risk of further harms. Our findings help explain why data sharing activities can have negative consequences for human rights, even within human rights initiatives. Moreover, our analysis offers a window into how FAccT principles for technology relate to the human rights implications of data sharing. Finally, we discuss why these tensions may be echoed in other areas where data sharing is pursued for human rights concerns, identifying common features which may lead to similar results, especially where sensitive data is shared to achieve social goods or policy objectives.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658949",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "974–987",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The tensions of data sharing for human rights: A modern slavery case study",
		"URL": "https://doi.org/10.1145/3630106.3658949",
		"author": [
			{
				"family": "Hancock",
				"given": "Jamie"
			},
			{
				"family": "Mahesh",
				"given": "Sarada"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Mazumder",
				"given": "Anjali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hancockTroubleSeaData2024",
		"type": "paper-conference",
		"abstract": "Recent years have revealed the severity and scale of human rights abuses at sea. Yet maritime human rights investigations remain challenging due to an array of difficulties, including physical inaccessibility and a complex legal environment. Improving the availability of data has been framed as a solution that will enhance transparency in marine-related activities and improve accountability for rights violations. Such enthusiasm has fuelled the development of technological solutions promising to identify abuses and safeguard vulnerable individuals. However, these efforts clash with concerns over the use of data and technology in human rights practice. In the context of such tensions, this paper studies how data and technology have been integrated within investigations into rights abuses at sea. We examine the challenges posed for transparency, accountability, and fairness regarding communities affected by rights violations. We ask: do data and digital technologies offer effective means for helping to expose rights abuses and hold malicious actors accountable? Or do they introduce new threats to autonomy, privacy, and dignity? We present empirical research based on qualitative engagements with expert practitioners. We find: 1) an increased availability of datasets did not necessarily prevent harm or improve safeguarding for vulnerable people; 2) many tech solutions were detached from affected individuals’ lived experiences and appeared not to meet communities’ needs; 3) uses of data and technology could introduce or aggravate risks to fairness and accountability within human rights investigations. We contribute a much-needed reflection on the actual implications of the use of data and technological tools for communities affected by human rights violations. Regarding maritime human rights, we argue that prioritising large-scale, top-down monitoring to collect larger datasets or market more tech solutions is not the best way for data and technology to contribute to transparency and accountability. Instead, we advocate for deeper engagement with affected communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658950",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "988–1001",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trouble at Sea: Data and digital technology challenges for maritime human rights concerns",
		"URL": "https://doi.org/10.1145/3630106.3658950",
		"author": [
			{
				"family": "Hancock",
				"given": "Jamie"
			},
			{
				"family": "Hui",
				"given": "Ruoyun"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Mazumder",
				"given": "Anjali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kouModelPerformanceClaim2024",
		"type": "paper-conference",
		"abstract": "Two goals – improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the responsibility gap – holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability’s advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658951",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1002–1013",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From model performance to claim: How a change of focus in machine learning replicability can help bridge the responsibility gap",
		"URL": "https://doi.org/10.1145/3630106.3658951",
		"author": [
			{
				"family": "Kou",
				"given": "Tianqi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "zhangStructuralInterventionsDynamics2024",
		"type": "paper-conference",
		"abstract": "Recent conversations in the algorithmic fairness literature have raised several concerns with standard conceptions of fairness. First, constraining predictive algorithms to satisfy fairness benchmarks may sometimes lead to non-optimal outcomes for disadvantaged groups. Second, technical interventions are often ineffective by themselves, especially when divorced from an understanding of structural processes that generate social inequality. Inspired by both these critiques, we construct a common decision-making model, using mortgage loans as a running example. We show that under some conditions, any choice of decision threshold will inevitably perpetuate existing disparities in financial stability unless one deviates from the Pareto optimal policy. This confirms the intuition that technical interventions, such as fairness constraints, often do not sufficiently address persistent underlying inequities. Then, we model the effects of three different types of interventions: (1) policy changes in the algorithm’s decision threshold, and external changes to parameters that govern the downstream effects of late payment for (2) the whole population or (3) disadvantaged subgroups. We show how different interventions are recommended depending on the difficulty of enacting structural change upon external parameters and depending on the policymaker’s preferences for equity or efficiency. Counterintuitively, we demonstrate that preferences for efficiency over equity may sometimes lead to recommendations for interventions that target the under-resourced group alone. Finally, we simulate the effects of interventions on a dataset that combines HMDA and Fannie Mae loan data. This research highlights the ways that structural inequality can be perpetuated by seemingly unbiased decision mechanisms, and it shows that in many situations, technical solutions must be paired with external, context-aware interventions to enact social change.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658952",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1014–1030",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Structural interventions and the dynamics of inequality",
		"URL": "https://doi.org/10.1145/3630106.3658952",
		"author": [
			{
				"family": "Zhang",
				"given": "Aurora"
			},
			{
				"family": "Hosoi",
				"given": "Anette"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lunichExplainableArtificialIntelligence2024",
		"type": "paper-conference",
		"abstract": "The rising adoption of learning analytics and academic performance prediction technologies in higher education highlights the urgent need for transparency and explainability. This demand, rooted in ethical concerns and fairness considerations, converges with Explainable Artificial Intelligence (XAI) principles. Despite the recognized importance of transparency and fairness in learning analytics, empirical studies examining student fairness perceptions, particularly within academic performance prediction, remain limited. We conducted a pre-registered factorial survey experiment involving 1,047 German students to investigate how decision tree features (simplicity and accuracy) influence perceived distributive and informational fairness, mediated by causability (i.e., the self-assessed understandability of a machine learning model’s cause-effect linkages). Additionally, we examined the moderating role of institutional trust in these relationships. Our results indicate that decision tree simplicity positively affects fairness perceptions, mediated by causability. In contrast, prediction accuracy neither directly nor indirectly influences these perceptions. Even if the hypothesized effects of interest are either minor or non-existent, results show that the medium positive effect of causability on the distributive fairness assessment depends on institutional trust. These findings substantially impact the crafting of transparent machine learning models in educational settings. We discuss important implications for fairness and transparency in implementing academic performance prediction systems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658953",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1031–1042",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable artificial intelligence for academic performance prediction. An experimental study on the impact of accuracy and simplicity of decision trees on causability and fairness perceptions",
		"URL": "https://doi.org/10.1145/3630106.3658953",
		"author": [
			{
				"family": "Lünich",
				"given": "Marco"
			},
			{
				"family": "Keller",
				"given": "Birte"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "selialiaMitigatingGroupBias2024",
		"type": "paper-conference",
		"abstract": "Federated learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature, i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients. It produces biased global models, i.e., models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity. Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy. Our main idea is to leverage average conditional probabilities to compute a cross-domain group importance weights derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while ensuring through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of image classification benchmarks assesses the fair decision-making of our framework in real-world settings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658954",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1043–1054",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating group bias in federated learning for heterogeneous devices",
		"URL": "https://doi.org/10.1145/3630106.3658954",
		"author": [
			{
				"family": "Selialia",
				"given": "Khotso"
			},
			{
				"family": "Chandio",
				"given": "Yasra"
			},
			{
				"family": "Anwar",
				"given": "Fatima M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "bhardwajMachineLearningData2024",
		"type": "paper-conference",
		"abstract": "Studies of dataset development in machine learning call for greater attention to the data practices that make model development possible and shape its outcomes. Many argue that the adoption of theory and practices from archives and data curation fields can support greater fairness, accountability, transparency, and more ethical machine learning. In response, this paper examines data practices in machine learning dataset development through the lens of data curation. We evaluate data practices in machine learning as data curation practices. To do so, we develop a framework for evaluating machine learning datasets using data curation concepts and principles through a rubric. Through a mixed-methods analysis of evaluation results for 25 ML datasets, we study the feasibility of data curation principles to be adopted for machine learning data work in practice and explore how data curation is currently performed. We find that researchers in machine learning, which often emphasizes model development, struggle to apply standard data curation principles. Our findings illustrate difficulties at the intersection of these fields, such as evaluating dimensions that have shared terms in both fields but non-shared meanings, a high degree of interpretative flexibility in adapting concepts without prescriptive restrictions, obstacles in limiting the depth of data curation expertise needed to apply the rubric, and challenges in scoping the extent of documentation dataset creators are responsible for. We propose ways to address these challenges and develop an overall framework for evaluation that outlines how data curation concepts and methods can inform machine learning data practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658955",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1055–1067",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning data practices through a data curation lens: An evaluation framework",
		"URL": "https://doi.org/10.1145/3630106.3658955",
		"author": [
			{
				"family": "Bhardwaj",
				"given": "Eshta"
			},
			{
				"family": "Gujral",
				"given": "Harshit"
			},
			{
				"family": "Wu",
				"given": "Siyi"
			},
			{
				"family": "Zogheib",
				"given": "Ciara"
			},
			{
				"family": "Maharaj",
				"given": "Tegan"
			},
			{
				"family": "Becker",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "maedaWhenHumanAIInteractions2024",
		"type": "paper-conference",
		"abstract": "With the continuous improvement of large language models (LLMs), chatbots can produce coherent and continuous word sequences that mirror natural human language. While the use of natural language and human-like conversation styles enables the use of chatbots within a range of everyday settings, these usability-enhancing features can also have unintended consequences, such as making fallible information seem trustworthy by emphasizing friendliness and closeness. This can have serious implications for information retrieval tasks performed with chatbots. In this paper, we provide an overview of the literature on parasociality, social affordance, and trust to bridge these concepts within human-AI interactions. We critically examine how chatbot “roleplaying” and user role projection co-produce a pseudo-interactive, technologically-mediated space with imbalanced dynamics between users and chatbots. Based on the review of the literature, we develop a conceptual framework of parasociality in chatbots that describes interactions between humans and anthropomorphized chatbots. We dissect how chatbots use personal pronouns, conversational conventions, affirmations, and similar strategies to position the chatbots as users’ companions or assistants, and how these tactics induce trust-forming behaviors in users. Finally, based on the conceptual framework, we outline a set of ethical concerns that emerge from parasociality, including illusions of reciprocal engagement, task misalignment, and leaks of sensitive information. This paper argues that these possible consequences arise from a positive feedback cycle wherein anthropomorphized chatbot features encourage users to fill in the context around predictive outcomes.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658956",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1068–1077",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When human-AI interactions become parasocial: Agency and anthropomorphism in affective design",
		"URL": "https://doi.org/10.1145/3630106.3658956",
		"author": [
			{
				"family": "Maeda",
				"given": "Takuya"
			},
			{
				"family": "Quan-Haase",
				"given": "Anabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lamFrameworkAssuranceAudits2024",
		"type": "paper-conference",
		"abstract": "An increasing number of regulations propose ‘AI audits’ as a mechanism for achieving transparency and accountability for artificial intelligence (AI) systems. Despite some converging norms around various forms of AI auditing, auditing for the purpose of compliance and assurance currently lacks agreed-upon practices, procedures, taxonomies, and standards. We propose the ‘criterion audit’ as an operationalizable compliance and assurance external audit framework. We model elements of this approach after financial auditing practices, and argue that AI audits should similarly provide assurance to their stakeholders about AI organizations’ ability to govern their algorithms in ways that mitigate harms and uphold human values. We discuss the necessary conditions for the criterion audit and provide a procedural blueprint for performing an audit engagement in practice. We illustrate how this framework can be adapted to current regulations by deriving the criteria on which ‘bias audits’ can be performed for in-scope hiring algorithms, as required by the recently effective New York City Local Law 144 of 2021. We conclude by offering a critical discussion on the benefits, inherent limitations, and implementation challenges of applying practices of the more mature financial auditing industry to AI auditing where robust guardrails against quality assurance issues are only starting to emerge. Our discussion—informed by experiences in performing these audits in practice—highlights the critical role that an audit ecosystem plays in ensuring the effectiveness of audits.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658957",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1078–1092",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for assurance audits of algorithmic systems",
		"URL": "https://doi.org/10.1145/3630106.3658957",
		"author": [
			{
				"family": "Lam",
				"given": "Khoa"
			},
			{
				"family": "Lange",
				"given": "Benjamin"
			},
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Davidovic",
				"given": "Jovana"
			},
			{
				"family": "Brown",
				"given": "Shea"
			},
			{
				"family": "Hasan",
				"given": "Ali"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "devrioBuildingShiftingEmploying2024",
		"type": "paper-conference",
		"abstract": "A large body of research has attempted to ensure that algorithmic systems adhere to notions of fairness and transparency. Increasingly, researchers have highlighted that mitigating algorithmic harms requires explicitly taking power structures into account. Those with power over algorithmic systems often fail to sufficiently address algorithmic harms and rarely consult those directly harmed by algorithmic systems. Left to their own devices, people respond to algorithmic harms they encounter in a wide variety of ways, but we lack broader, overarching understandings of these responses. In this work, we synthesize documented, historical cases into a taxonomy of responses “from below” to algorithmic harm. Our taxonomy connects different types of responses to existing theorizations of power from fields including anthropology, human-computer interaction, and communication, centering how people employ, shift, and build power in their responses to algorithmic harm. Based on our taxonomy, we highlight an opportunity space for the FAccT community to engage with and support such action from below.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658958",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1093–1106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building, shifting, & employing power: a taxonomy of responses from below to algorithmic harm",
		"URL": "https://doi.org/10.1145/3630106.3658958",
		"author": [
			{
				"family": "DeVrio",
				"given": "Alicia"
			},
			{
				"family": "Eslami",
				"given": "Motahhare"
			},
			{
				"family": "Holstein",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "grovesAuditingWorkExploring2024",
		"type": "paper-conference",
		"abstract": "In July 2023, New York City (NYC) implemented the first attempt to create an algorithm auditing regime for commercial machine-learning systems. Local Law 144 (LL 144), requires NYC-based employers using automated employment decision-making tools (AEDTs) in hiring to be subject to annual bias audits by an independent auditor. In this paper, we analyse what lessons can be learned from LL 144 for other national attempts to create algorithm auditing regimes. Using qualitative interviews with 17 experts and practitioners working within the regime, we find LL 144 has failed to create an effective auditing regime: the law fails to clearly define key aspects like AEDTs and what constitutes an independent auditor, leaving auditors, vendors who create AEDTs, and companies using AEDTs to define the law’s practical implementation in ways that failed to protect job applicants. Several factors contribute to this: first, the law was premised on a faulty transparency-driven theory of change that fails to stop biased AEDTs from being used by employers. Second, industry lobbying led to the definition of what constitutes an AEDT being narrowed to the point where most companies considered their tools exempt. Third, we find auditors face enormous practical and cultural challenges gaining access to data from employers and vendors building these tools. Fourth, we find wide disagreement over what constitutes a legitimate auditor and identify four different kinds of ‘auditor roles’ that serve different functions and offer different kinds of services. We conclude with four recommendations for policymakers seeking to create similar bias auditing regimes that use clearer definitions and metrics and more accountability. By exploring LL 144 through the lens of auditors, our paper advances the evidence base around audit as an accountability mechanism, and can provide guidance for policymakers seeking to create similar regimes.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658959",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1107–1120",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing work: Exploring the new york city algorithmic bias audit regime",
		"URL": "https://doi.org/10.1145/3630106.3658959",
		"author": [
			{
				"family": "Groves",
				"given": "Lara"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Kennedy",
				"given": "Alayna"
			},
			{
				"family": "Vecchione",
				"given": "Briana"
			},
			{
				"family": "Strait",
				"given": "Andrew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kaushalAutomatedTransparencyLegal2024",
		"type": "paper-conference",
		"abstract": "The Digital Services Act (DSA) is a much awaited platforms liability reform in the European Union that was adopted on 1 November 2022 with the ambition to set a global example in terms of accountability and transparency. Among other obligations, the DSA emphasizes the need for online platforms to report on their content moderation decisions (‘statements of reasons’ - SoRs), which is a novel transparency mechanism we refer to as automated transparency in this study. SoRs are currently made available in the DSA Transparency Database, launched by the European Commission in September 2023. The DSA Transparency Database marks a historical achievement in platform governance, and allows investigations about the actual transparency gains, both at structure level as well as at the level of platform compliance. This study aims to understand whether the Transparency Database helps the DSA to live up to its transparency promises. We use legal and empirical arguments to show that while there are some transparency gains, compliance remains problematic, as the current database structure allows for a lot of discretion from platforms in terms of transparency practices. In our empirical study, we analyze a representative sample of the Transparency Database (131m SoRs) submitted in November 2023, to characterise and evaluate platform content moderation practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658960",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1121–1132",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automated transparency: a legal and empirical analysis of the digital services act transparency database",
		"URL": "https://doi.org/10.1145/3630106.3658960",
		"author": [
			{
				"family": "Kaushal",
				"given": "Rishabh"
			},
			{
				"family": "Van De Kerkhof",
				"given": "Jacob"
			},
			{
				"family": "Goanta",
				"given": "Catalina"
			},
			{
				"family": "Spanakis",
				"given": "Gerasimos"
			},
			{
				"family": "Iamnitchi",
				"given": "Adriana"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "cachelPreFAIRCombiningPartial2024",
		"type": "paper-conference",
		"abstract": "Preference aggregation mechanisms help decision-makers combine diverse preference rankings produced by multiple voters into a single consensus ranking. Prior work has developed methods for aggregating multiple rankings into a fair consensus over the same set of candidates. Yet few real-world problems present themselves as such precisely formulated aggregation tasks with each voter fully ranking all candidates. Instead, preferences are often expressed as rankings over partial and even disjoint subsets of candidates. For instance, hiring committee members typically opt to rank their top choices instead of exhaustively ordering every single job applicant. However, the existing literature does not offer a framework for characterizing nor ensuring group fairness in such partial preference aggregation tasks. Unlike fully ranked settings, partial preferences imply both a selection decision of whom to rank plus an ordering decision of how to rank the selected candidates. Our work fills this gap by conceptualizing the open problem of fair partial preference aggregation. We introduce an impossibility result for fair selection from partial preferences and design a computational framework showing how we can navigate this obstacle. Inspired by Single Transferable Voting, our proposed solution PreFair produces consensus rankings that are fair in the selection of candidates and also in their relative ordering. Our experimental study demonstrates that PreFair achieves the best performance in this dual fairness objective compared to state-of-the-art alternatives adapted to this new problem while still satisfying voter preferences.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658961",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1133–1149",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "PreFAIR: Combining partial preferences for fair consensus decision-making",
		"URL": "https://doi.org/10.1145/3630106.3658961",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "abduAlgorithmicTransparencyParticipation2024",
		"type": "paper-conference",
		"abstract": "Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust. But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded. Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts. We adopt two theoretical frames, Mulligan and Nissenbaum’s handoff model and Star and Griesemer’s boundary objects, to reveal such shifts during the U.S. Census Bureau’s adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census. This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process. Using publicly available documents concerning the Census’ implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout. We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658962",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1150–1162",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic transparency and participation through the handoff lens: Lessons learned from the U.S. census bureau’s adoption of differential privacy",
		"URL": "https://doi.org/10.1145/3630106.3658962",
		"author": [
			{
				"family": "Abdu",
				"given": "Amina A."
			},
			{
				"family": "Chambers",
				"given": "Lauren M."
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			},
			{
				"family": "Jacobs",
				"given": "Abigail Z."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leuAuditingImagebasedNSFW2024",
		"type": "paper-conference",
		"abstract": "This paper examines NSFW (Not Safe For Work) image classifiers for content filtering. Through an audit of three prevalent NSFW classifiers, we analyze the relationship between NSFW predictions and three demographic factors: gender, skin-tone, and age. Our study reveals that women are disproportionately more frequently misclassified as NSFW compared to men, even when they appear conducting common daily-life activities. Additionally, we find that NSFW classifiers tend to mispredict images of people with lighter skin-tones and images depicting younger people. We explore the causes of such mispredictions by analyzing the explanatory pixel maps, which reveal some of the reasons behind the misclassifications. Overall, the implications of our findings become particularly salient when considering the application of filters based on NSFW classifiers, which we identified to have a direct impact on image datasets, computer vision models, generative AI, user experience, and artistic creativity. In summary, we hope our study brings attention to the inherent biases within NSFW classifiers and underscores the importance of addressing these issues to ensure fair and equitable outcomes in content filtering.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658963",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1163–1173",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing image-based NSFW classifiers for content filtering",
		"URL": "https://doi.org/10.1145/3630106.3658963",
		"author": [
			{
				"family": "Leu",
				"given": "Warren"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "manziniShouldUsersTrust2024",
		"type": "paper-conference",
		"abstract": "As AI assistants become increasingly sophisticated and deeply integrated into our lives, questions of trust rise to the forefront. In this paper, we build on philosophical studies of trust to investigate when user trust in AI assistants is justified. By moving beyond a focus on the technical artefact in isolation, we consider the broader societal system in which AI assistants are developed and deployed. We conceptualise user trust in AI assistants as encompassing two main targets, namely AI assistants and their developers. We argue that – as AI assistants become more human like and exhibit increased agency – discerning when user trust is justified requires consideration not only of competence, on the part of AI assistants and their developers, but also alignment between the competing interests, values or incentives of AI assistants, developers and users. To help users understand if and when their trust in the competence and alignment of AI assistants and developers is justified, we propose a sociotechnical approach that requires evidence to be collected at three levels: AI assistant design, organisational practices and third-party governance. Taken together, these measures can help harness the transformative potential of AI assistants while also ensuring their operation is ethical and value aligned.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658964",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1174–1186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should users trust advanced AI assistants? Justified trust as a function of competence and alignment",
		"URL": "https://doi.org/10.1145/3630106.3658964",
		"author": [
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Keeling",
				"given": "Geoff"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vengroffImpactChartsTool2024",
		"type": "paper-conference",
		"abstract": "We introduce impact charts and apply them to problems of systematic bias encoded in three different data sets. Impact charts are highly visual, making the effects they find easy to understand by both domain experts and non-experts. Impact charts are nonlinear and non-parametric, so they are able to identify structural biases whose functional forms are not a priori well understood. Impact charts are based on SHAP, an interpretability method initially designed to interpret predictions made by Machine Learning (ML) models, which is in turn based on Shapley values, an approach to assigning responsibility for economic outcomes to different factors. Although impact charts use techniques from the ML community, they are intended for use in general settings, whether ML is present or not. Impact charts provide valuable insights even when generated from aggregate data sets. Aggregate data sets typically provide the individuals whose data they are derived from an additional level of privacy as compared to the original unaggregated data. In this work, we relied predominantly on aggregate data from the U.S. Census Bureau, which is known to have a robust privacy program. We introduce and evaluate impact charts using three examples of their use. Our first example uses impact charts to identify racial and ethnic bias in eviction rates. Our second example uses U.S. Census data to identify racial and ethnic bias in housing prices. Our third example assesses the impact of several factors on local access to supermarkets. All three examples not only correct for the effects of income, but also clearly demonstrate the relative impact of income as compared to racial and ethnic features. For example, we demonstrate that in some areas like DeKalb County, GA, the fraction of the population that is Black impacts eviction rates three times more than income does. In addition to the impact charts specifically discussed herein, we have produced thousands of geographically localized impact charts for the data sets mentioned above. There is wide variation in the shape and structure of impact plots built using data from different local areas. We hypothesize that in the future work we will be able to categorize these and identify local policy decisions, whether de jure or de facto that cause the differences from one area to the next.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658965",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1187–1198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impact charts: a tool for identifying systematic bias in social systems and data",
		"URL": "https://doi.org/10.1145/3630106.3658965",
		"author": [
			{
				"family": "Vengroff",
				"given": "Darren Erik"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wolfeLaboratoryscaleAIOpenweight2024",
		"type": "paper-conference",
		"abstract": "The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear. We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658966",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1199–1210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Laboratory-scale AI: Open-weight models are competitive with ChatGPT even in low-resource settings",
		"URL": "https://doi.org/10.1145/3630106.3658966",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Slaughter",
				"given": "Isaac"
			},
			{
				"family": "Han",
				"given": "Bin"
			},
			{
				"family": "Wen",
				"given": "Bingbing"
			},
			{
				"family": "Yang",
				"given": "Yiwei"
			},
			{
				"family": "Rosenblatt",
				"given": "Lucas"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Brown",
				"given": "Eva"
			},
			{
				"family": "Qu",
				"given": "Zening"
			},
			{
				"family": "Weber",
				"given": "Nic"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moayeriWorldBenchQuantifyingGeographic2024",
		"type": "paper-conference",
		"abstract": "As large language models (LLMs) continue to improve and gain popularity, some may use the models to recall facts, despite well documented limitations with LLM factuality. Towards ensuring that models work reliably for all, we seek to uncover if geographic disparities emerge when asking an LLM the same question about different countries. To this end, we present WorldBench, a dynamic and flexible benchmark composed of per-country data from the World Bank. In extensive experiments on state of the art open and closed source models, including GPT-4, Gemini, Llama-2, and Vicuna, to name a few, we find significant biases based on region and income level. For example, error rates are 1.5 times higher for countries from Sub-Saharan Africa compared to North American countries. We observe these disparities to be consistent over 20 LLMs and 11 individual World Bank indicators (i.e. specific statistics, such as population or CO2 emissions). WorldBench also enables automatic detection of citation hallucination, where models cite the World Bank itself while providing false statistics, and a manner to assess when an LLM’s stored facts begin to go out of date. We hope our benchmark will draw attention to geographic disparities in existing LLMs and facilitate the remedying of these biases.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658967",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1211–1228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "WorldBench: Quantifying geographic disparities in LLM factual recall",
		"URL": "https://doi.org/10.1145/3630106.3658967",
		"author": [
			{
				"family": "Moayeri",
				"given": "Mazda"
			},
			{
				"family": "Tabassi",
				"given": "Elham"
			},
			{
				"family": "Feizi",
				"given": "Soheil"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "birhaneDarkSideDataset2024",
		"type": "paper-conference",
		"abstract": "‘Scale the model, scale the data, scale the GPU farms’ is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts on model performance remain under-explored. This is particularly important in the context of multimodal datasets whose main source is the World Wide Web, condensed and packaged as the Common Crawl dump, which is known to exhibit numerous drawbacks. In this paper, we evaluate the downstream impact of dataset scaling on 14 visio-linguistic models (VLMs) trained on the LAION400-M and LAION-2B datasets by measuring racial and gender bias using the Chicago Face Dataset (CFD) as the probe. Our results show that as the training data increased, the probability of a pre-trained CLIP model misclassifying human images as offensive non-human classes such as chimpanzee, gorilla, and orangutan decreased, but misclassifying the same images as human offensive classes such as criminal increased. Furthermore, of the 14 Vision Transformer-based VLMs we evaluated, the probability of predicting an image of a Black man and a Latino man as criminal increases by 65% and 69%, respectively, when the dataset is scaled from 400M to 2B samples for the larger ViT-L models. Conversely, for the smaller base ViT-B models, the probability of predicting an image of a Black man and a Latino man as criminal decreases by 20% and 47%, respectively, when the dataset is scaled from 400M to 2B samples. We ground the model audit results in a qualitative and historical analysis, reflect on our findings and their implications for dataset curation practice, and close with a summary of mitigation mechanisms and ways forward. All the meta-datasets curated in this endeavor and the code used are shared at: https://github.com/SepehrDehdashtian/the-dark-side-of-dataset-scaling. Content warning: This article contains racially dehumanising and offensive descriptions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658968",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1229–1244",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dark side of dataset scaling: Evaluating racial classification in multimodal models",
		"URL": "https://doi.org/10.1145/3630106.3658968",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Dehdashtian",
				"given": "Sepehr"
			},
			{
				"family": "Prabhu",
				"given": "Vinay"
			},
			{
				"family": "Boddeti",
				"given": "Vishnu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "prinosSpeakingAccentContent2024",
		"type": "paper-conference",
		"abstract": "Automatic speech recognition (ASR) researchers are working to address the differing transcription performance of ASR by accent or dialect. However, research often has a limited view of accent in ways that reproduce discrimination and limit the scope of potential solutions. In this paper we present a content analysis of 22 papers published in 2022 in top conferences and journals on the topic of accent and ASR. We report on how accent is sometimes mistakenly viewed as something some people don’t have; as having a default; and being an attribute only of the speaker, and not of the listener. We discuss the implications on research and provide recommendations to researchers who hope to reduce ASR biases by accent.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658969",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1245–1254",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Speaking of accent: A content analysis of accent misconceptions in ASR research",
		"URL": "https://doi.org/10.1145/3630106.3658969",
		"author": [
			{
				"family": "Prinos",
				"given": "Kerri"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Power",
				"given": "Cathleen A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "terzisLawEmergingPolitical2024",
		"type": "paper-conference",
		"abstract": "For almost a decade now, scholarship in and beyond the ACM FAccT community has been focusing on novel and innovative ways and methodologies to audit the functioning of algorithmic systems. Over the years, this research idea and technical project has matured enough to become a regulatory mandate. Today, the Digital Services Act (DSA) and the Online Safety Act (OSA) have established the framework within which technology corporations and (traditional) auditors will develop the ‘practice’ of algorithmic auditing thereby presaging how this ‘ecosystem’ will develop. In this paper, we systematically review the auditing provisions in the DSA and the OSA in light of observations from the emerging industry of algorithmic auditing. Who is likely to occupy this space? What are some political and ethical tensions that are likely to arise? How are the mandates of ‘independent auditing’ or ‘the evaluation of the societal context of an algorithmic function’ likely to play out in practice? By shaping the picture of the emerging political economy of algorithmic auditing, we draw attention to strategies and cultures of traditional auditors that risk eroding important regulatory pillars of the DSA and the OSA. Importantly, we warn that ambitious research ideas and technical projects of/for algorithmic auditing may end up crashed by the standardising grip of traditional auditors and/or diluted within a complex web of (sub-)contractual arrangements, diverse portfolios, and tight timelines.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658970",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1255–1267",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Law and the emerging political economy of algorithmic audits",
		"URL": "https://doi.org/10.1145/3630106.3658970",
		"author": [
			{
				"family": "Terzis",
				"given": "Petros"
			},
			{
				"family": "Veale",
				"given": "Michael"
			},
			{
				"family": "Gaumann",
				"given": "Noëlle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "schorMeaningfulTransparencyClinicians2024",
		"type": "paper-conference",
		"abstract": "AI systems can bring great benefits to our healthcare systems, e.g. by improving patient outcomes. Yet implementing them into clinical practice remains challenging. To bridge the gap between academic research and design implementation, we argue clinicians need transparency about such systems that is meaningful—i.e. contextually appropriate—to them. Towards this, we explore recent HCXAI recommendations for building transparent AI systems for users in a specific domain: gynaecology. By better understanding clinicians’ perspectives on meaningful transparency, our aim is to complement and help operationalise such recommendations. We conduct a co-design workshop and interviews with n=15 gynaecologists in the UK and the Netherlands. We show that HCXAI must better account for clinical teams with different types of gynaecologist users, and that the timeliness and relevance of the information provided about the AI-based tool throughout its design lifecycle—in particular before a tool is implemented into clinical practice—is crucial for transparency to become meaningful. Our contributions include: i) testing recommendations from the latest HCXAI literature with a prospective, real-life AI application in a relatively less-studied clinical domain; ii) describing and visualising gynaecologists’ understanding of meaningful transparency for clinicians; iii) outlining four design recommendations towards realising meaningful transparency for clinicians and opportunities for research; and iv) expanding HCI and AI research in women’s health by directly engaging with gynaecologists as users and co-designers. Exploring such issues is key to facilitate the implementation of AI systems that meet clinicians’ information needs and that they can trust.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658971",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1268–1281",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meaningful transparency for clinicians: Operationalising HCXAI research with gynaecologists",
		"URL": "https://doi.org/10.1145/3630106.3658971",
		"author": [
			{
				"family": "Schor",
				"given": "Bianca Giulia Sarah"
			},
			{
				"family": "Kallina",
				"given": "Emma"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			},
			{
				"family": "Blackwell",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mhasawadeCausalPerspectiveLabel2024",
		"type": "paper-conference",
		"abstract": "Predictive models developed with machine learning techniques are commonly used to inform decision making and resource allocation in high-stakes contexts, such as healthcare and public health. One means through which this practice may propagate equity-related harms is when the data used for model development or evaluation exhibits label bias. In such cases, the target of prediction is a proxy label of a construct of interest that may be difficult or impossible to measure, while the relationship between the proxy and the construct of interest differs systematically across subgroups. Label bias can be especially challenging to identify and mitigate in practice because consequential fairness violations are masked when the model is evaluated with respect to the proxy label. In this work, we aim to develop further formal understanding of label bias to inform the development of approaches for the identification and mitigation of it. To do so, we present desiderata for unbiased and biased proxy labels, introduce candidate causal graphical criteria for label bias, and consider the extent to which proxy labels can be used to reason about fairness with respect to a true construct of interest. We validate our findings with a simulation study and experiments with synthetic health insurance data used in the context of a care management system.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658972",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1282–1294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A causal perspective on label bias",
		"URL": "https://doi.org/10.1145/3630106.3658972",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "D'Amour",
				"given": "Alexander"
			},
			{
				"family": "Pfohl",
				"given": "Stephen R"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "widderEpistemicPowerAI2024",
		"type": "paper-conference",
		"abstract": "What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate? Based on 75 interviews with technologists including researchers, developers, open source contributors, and activists, this paper explores the various epistemic bases from which AI ethics is discussed and practiced. In the context of outside attacks on AI ethics as an impediment to “progress,” I show how some AI ethics practices have reached toward authority from automation and quantification, and achieved some legitimacy as a result, while those based on richly embodied and situated lived experience have not. This paper draws together the work of feminist Anthropology and Science and Technology Studies scholars Diana Forsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices. By entrenching the epistemic power of quantification, dominant AI ethics practices—employing Model Cards and similar interventions—risk legitimizing AI ethics as a project in equal and opposite measure to which they marginalize lived experience as a legitimate part of the same project. In response, I propose humble technical practices: quantified or technical practices which specifically seek to make their epistemic limits clear in order to flatten hierarchies of epistemic power.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658973",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1295–1304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic power in AI ethics labor: Legitimizing located complaints",
		"URL": "https://doi.org/10.1145/3630106.3658973",
		"author": [
			{
				"family": "Widder",
				"given": "David Gray"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "simsonOneModelMany2024",
		"type": "paper-conference",
		"abstract": "A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. The downstream effects of ADM systems critically depend on the decisions made during a systems’ design, implementation, and evaluation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these decisions are made implicitly, without knowing exactly how they will influence the final system. To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit decisions during design and evaluation into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible “universes” of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can investigate the variability and robustness of fairness scores and see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand fairness implications of design and evaluation decisions using an exemplary case study of predicting public health care coverage for vulnerable populations. Our results highlight how decisions regarding the evaluation of a system can lead to vastly different fairness metrics for the same model. This is problematic, as a nefarious actor could optimise or “hack” a fairness metric to portray a discriminating model as fair merely by changing how it is evaluated. We illustrate how a multiverse analysis can help to address this issue.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658974",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1305–1320",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One model many scores: Using multiverse analysis to prevent fairness hacking and evaluate the influence of model design decisions",
		"URL": "https://doi.org/10.1145/3630106.3658974",
		"author": [
			{
				"family": "Simson",
				"given": "Jan"
			},
			{
				"family": "Pfisterer",
				"given": "Florian"
			},
			{
				"family": "Kern",
				"given": "Christoph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leeLargeLanguageModels2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658975",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1321–1340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Large language models portray socially subordinate groups as more homogeneous, consistent with a bias observed in humans",
		"URL": "https://doi.org/10.1145/3630106.3658975",
		"author": [
			{
				"family": "Lee",
				"given": "Messi H.J."
			},
			{
				"family": "Montgomery",
				"given": "Jacob M."
			},
			{
				"family": "Lai",
				"given": "Calvin K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "alphertsPerceptiveVisualUrban2024",
		"type": "paper-conference",
		"abstract": "The use of Computer Vision, through a Perceptive Visual Urban Analytics (VUA) paradigm, has been proposed as a way for municipalities to more easily monitor their cities. However, prior studies fall short of actually investigating whether Perceptive VUA is ready for municipal use. In this paper we take a critical look at this paradigm by comparing key methods and evaluating them on usability and trustworthiness with municipal experts as well as Responsible AI and Computer Vision researchers. Based on on this evaluation we find that Perceptive VUA is not (yet) ready for municipal use as they do not incorporate domain knowledge and overly rely on spurious correlations. We conclude by providing recommendations for how to progress Perceptive VUA such that it may actually contribute to improving the liveability and quality of urban environments.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658976",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1341–1354",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptive visual urban analytics is not (yet) suitable for municipalities",
		"URL": "https://doi.org/10.1145/3630106.3658976",
		"author": [
			{
				"family": "Alpherts",
				"given": "Tim"
			},
			{
				"family": "Ghebreab",
				"given": "Sennay"
			},
			{
				"family": "Hsu",
				"given": "Yen-Chia"
			},
			{
				"family": "Van Noord",
				"given": "Nanne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "akpinarImpactDifferentialFeature2024",
		"type": "paper-conference",
		"abstract": "Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such “differential feature under-reporting” as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, little is known about the setting of data missingness absent indicators (i.e. differential feature under-reporting). In this work, we study an analytically tractable model of differential feature under-reporting to characterizethe impact of under-report on algorithmic fairness. We demonstrate how standard missing data methods typically fail to mitigate bias in this setting, and propose a new set of augmented loss and imputation methods. Our results show that, in real world data settings, under-reporting typically exacerbates disparities. The proposed solution methods show some success in mitigating disparities attributable to feature under-reporting.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658977",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 28\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1355–1382",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of differential feature under-reporting on algorithmic fairness",
		"URL": "https://doi.org/10.1145/3630106.3658977",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mashiatEvictionPredictionLeveraging2024",
		"type": "paper-conference",
		"abstract": "There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts – informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with a recent history of evictions. We also discuss the importance of neighborhood and ownership features in both risk prediction and targeted outreach.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658978",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1383–1394",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond eviction prediction: Leveraging local spatiotemporal public records to inform action",
		"URL": "https://doi.org/10.1145/3630106.3658978",
		"author": [
			{
				"family": "Mashiat",
				"given": "Tasfia"
			},
			{
				"family": "DiChristofano",
				"given": "Alex"
			},
			{
				"family": "Fowler",
				"given": "Patrick J."
			},
			{
				"family": "Das",
				"given": "Sanmay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "huangCollectiveConstitutionalAI2024",
		"type": "paper-conference",
		"abstract": "There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs—from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658979",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 23\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1395–1417",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Collective constitutional AI: Aligning a language model with public input",
		"URL": "https://doi.org/10.1145/3630106.3658979",
		"author": [
			{
				"family": "Huang",
				"given": "Saffron"
			},
			{
				"family": "Siddarth",
				"given": "Divya"
			},
			{
				"family": "Lovitt",
				"given": "Liane"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Durmus",
				"given": "Esin"
			},
			{
				"family": "Tamkin",
				"given": "Alex"
			},
			{
				"family": "Ganguli",
				"given": "Deep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "baumannFairnessOnlineAd2024",
		"type": "paper-conference",
		"abstract": "Advertising funds a number of services that play a major role in our everyday online experiences, from social networking, to maps, search, and news. As the power and reach of advertising platforms grow, so do the concerns about the potential for discrimination associated with targeted advertising. However, despite our ever-improving ability to measure and describe instances of unfair distribution of high-stakes ads—such as employment, housing, or credit—we lack the tools to model and predict the extent to which alternative systems could address such problems. In this paper, we simulate an ad distribution system to model the effects that enforcing popularly proposed fairness approaches would have on the utility of the advertising platforms and their users. We show that in many realistic scenarios, achieving statistical parity would come at a much higher utility cost to platforms than enforcing predictive parity or equality of opportunity. Additionally, we identify a tradeoff between different notions of fairness, i.e., enforcing one criterion leads to worse outcomes with respect to other criteria. We further describe how pursuing fairness in situations where one group of users is more expensive to advertise to is likely to result in “leveling down” effects, i.e., not benefiting any group of users. We show that these negative effects can be prevented by ensuring that it is the platforms that carry the cost of fairness rather than passing it on to their users or advertisers. Overall, our findings contribute to ongoing discussions on fair ad delivery. We show that fairness is not satisfied by default, that limiting targeting options is not sufficient to address potential discrimination and bias in online ad delivery, and that choices made by regulators and platforms may backfire if potential side-effects are not properly considered.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658980",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1418–1432",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in online ad delivery",
		"URL": "https://doi.org/10.1145/3630106.3658980",
		"author": [
			{
				"family": "Baumann",
				"given": "Joachim"
			},
			{
				"family": "Sapiezynski",
				"given": "Piotr"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Hannak",
				"given": "Aniko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kraftKnowledgeenhancedLanguageModels2024",
		"type": "paper-conference",
		"abstract": "The factual inaccuracies (\"hallucinations\") of large language models have recently inspired more research on knowledge-enhanced language modeling approaches. These are often assumed to enhance the overall trustworthiness and objectivity of language models. Meanwhile, the issue of bias is usually only mentioned as a limitation of statistical representations. This dissociation of knowledge-enhancement and bias is in line with previous research on AI engineers’ assumptions about knowledge, which indicate that knowledge is commonly understood as objective and value-neutral by this community. We argue that claims and practices by actors of the field still reflect this underlying conception of knowledge. We contrast this assumption with literature from social and, in particular, feminist epistemology, which argues that the idea of a universal disembodied knower is blind to the reality of knowledge practices and seriously challenges claims of \"objective\" or \"neutral\" knowledge. Knowledge enhancement techniques commonly use Wikidata and Wikipedia as their sources for knowledge, due to their large scales, public accessibility, and assumed trustworthiness. In this work, they serve as a case study for the influence of the social setting and the identity of knowers on epistemic processes. Indeed, the communities behind Wikidata and Wikipedia are known to be male-dominated and many instances of hostile behavior have been reported in the past decade. In effect, the contents of these knowledge bases are highly biased. It is therefore doubtful that these knowledge bases would contribute to bias reduction. In fact, our empirical evaluations of RoBERTa, KEPLER, and CoLAKE, demonstrate that knowledge enhancement may not live up to the hopes of increased objectivity. In our study, the average probability for stereotypical associations was preserved on two out of three metrics and performance-related gender gaps on knowledge-driven task were also preserved. We build on these results and critical literature to argue that the label of \"knowledge\" and the commonly held beliefs about it can obscure the harm that is still done to marginalized groups. Knowledge enhancement is at risk of perpetuating epistemic injustice, and AI engineers’ understanding of knowledge as objective per se conceals this injustice. Finally, to get closer to trustworthy language models, we need to rethink knowledge in AI and aim for an agenda of diversification and scrutiny from outgroup members.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658981",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1433–1445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Knowledge-enhanced language models are not bias-proof: Situated knowledge and epistemic injustice in AI",
		"URL": "https://doi.org/10.1145/3630106.3658981",
		"author": [
			{
				"family": "Kraft",
				"given": "Angelie"
			},
			{
				"family": "Soulier",
				"given": "Eloı̈se"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "antoniakNLPMaternalHealthcare2024",
		"type": "paper-conference",
		"abstract": "Ethical frameworks for the use of natural language processing (NLP) are urgently needed to shape how large language models (LLMs) and similar tools are used for healthcare applications. Healthcare faces existing challenges including the balance of power in clinician-patient relationships, systemic health disparities, historical injustices, and economic constraints. Drawing directly from the voices of those most affected, and focusing on a case study of a specific healthcare setting, we propose a set of guiding principles for the use of NLP in maternal healthcare. We led an interactive session centered on an LLM-based chatbot demonstration during a full-day workshop with 39 participants, and additionally surveyed 30 healthcare workers and 30 birthing people about their values, needs, and perceptions of NLP tools in the context of maternal health. We conducted quantitative and qualitative analyses of the survey results and interactive discussions to consolidate our findings into a set of guiding principles. We propose nine principles for ethical use of NLP for maternal healthcare, grouped into three themes: (i) recognizing contextual significance (ii) holistic measurements, and (iii) who/what is valued. For each principle, we describe its underlying rationale and provide practical advice. This set of principles can provide a methodological pattern for other researchers and serve as a resource to practitioners working on maternal health and other healthcare fields to emphasize the importance of technical nuance, historical context, and inclusive design when developing NLP technologies for clinical use.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658982",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1446–1463",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "NLP for maternal healthcare: Perspectives and guiding principles in the age of llms",
		"URL": "https://doi.org/10.1145/3630106.3658982",
		"author": [
			{
				"family": "Antoniak",
				"given": "Maria"
			},
			{
				"family": "Naik",
				"given": "Aakanksha"
			},
			{
				"family": "Alvarado",
				"given": "Carla S."
			},
			{
				"family": "Wang",
				"given": "Lucy Lu"
			},
			{
				"family": "Chen",
				"given": "Irene Y."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kinahanAchievingReproducibilityEEGbased2024",
		"type": "paper-conference",
		"abstract": "Despite the inherent complexity of electroencephalogram (EEG) data characterized by its high dimensionality, artifactual noise, and biological variability, many machine learning (ML) studies claim impressive performance in decoding or classifying EEG signals. Recently, several studies have highlighted that flawed data analysis is a prevalent issue in the literature, leading to irreproducible results and exaggerated claims. To address this issue, we propose a framework that addresses three primary obstacles in EEG ML research: data leakage, data scarcity, and flawed model selection. We introduce the EEG ML Model Card, a standardized and transparent EEG ML model documentation tool that aims to directly address these pitfalls and enhance reproducibility and trustworthiness in EEG ML research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658983",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1464–1474",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achieving reproducibility in EEG-based machine learning",
		"URL": "https://doi.org/10.1145/3630106.3658983",
		"author": [
			{
				"family": "Kinahan",
				"given": "Sean"
			},
			{
				"family": "Saidi",
				"given": "Pouria"
			},
			{
				"family": "Daliri",
				"given": "Ayoub"
			},
			{
				"family": "Liss",
				"given": "Julie"
			},
			{
				"family": "Berisha",
				"given": "Visar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wangInvestigatingDesigningTrust2024",
		"type": "paper-conference",
		"abstract": "Trust is a crucial factor for the adoption and responsible usage of generative AI tools in complex tasks such as software engineering. However, we have a limited understanding of how software developers evaluate the trustworthiness of AI-powered code generation tools in real-world settings. To address this gap, we conducted Study 1, an interview study with 17 developers who use AI-powered code generation tools in professional or personal settings. We found that developers’ trust is rooted in the AI tool’s perceived ability, integrity, and benevolence, and is situational, varying according to the context of usage. Existing AI code generation tools lack the affordances for developers to efficiently and effectively evaluate the trustworthiness of AI-powered code generation tools. To explore designs that can augment the existing interface of AI-powered code generation tools, we explored three sets of design concepts (suggestion quality indicators, usage stats, and control mechanisms) that derived from Study 1 findings. In Study 2, a design probe study with 12 developers, we investigated the potential of these design concepts to help developers make effective trust judgments. We discuss the implication of our findings on the design of AI-powered code generation tools and future research on trust in AI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658984",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1475–1493",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating and designing for trust in AI-powered code generation tools",
		"URL": "https://doi.org/10.1145/3630106.3658984",
		"author": [
			{
				"family": "Wang",
				"given": "Ruotong"
			},
			{
				"family": "Cheng",
				"given": "Ruijia"
			},
			{
				"family": "Ford",
				"given": "Denae"
			},
			{
				"family": "Zimmermann",
				"given": "Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "turriTransparencyWildNavigating2024",
		"type": "paper-conference",
		"abstract": "Transparency is a critical component when building artificial intelligence (AI) decision-support tools, especially for contexts in which AI outputs impact people or policy. Effectively identifying and addressing user transparency needs in practice remains a challenge. While a number of guidelines and processes for identifying transparency needs have emerged, existing methods tend to approach need-finding with a limited focus that centers around a narrow set of stakeholders and transparency techniques. To broaden this perspective, we employ numerous need-finding methods to investigate transparency mechanisms in a widely deployed AI-decision support tool developed by a wildlife conservation non-profit. Throughout our 5-month case study, we conducted need-finding through semi-structured interviews with end-users, analysis of the tool’s community forum, experiments with their ML model, and analysis of training documents created by end-users. We also held regular meetings with the tool’s product and machine learning teams. By approaching transparency need-finding from a broad lens, we uncover insights into end-users’ transparency needs as well as unexpected uses and challenges with current transparency mechanisms. Our study is one of the first to incorporate such diverse perspectives to reveal an unbiased and rich view of transparency needs. Lastly, we offer the FAccT community recommendations on broadening transparency need-finding approaches, contributing to the evolving field of transparency research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658985",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1494–1514",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency in the wild: Navigating transparency in a deployed AI system to broaden need-finding approaches",
		"URL": "https://doi.org/10.1145/3630106.3658985",
		"author": [
			{
				"family": "Turri",
				"given": "Violet"
			},
			{
				"family": "Morrison",
				"given": "Katelyn"
			},
			{
				"family": "Robinson",
				"given": "Katherine-Marie"
			},
			{
				"family": "Abidi",
				"given": "Collin"
			},
			{
				"family": "Perer",
				"given": "Adam"
			},
			{
				"family": "Forlizzi",
				"given": "Jodi"
			},
			{
				"family": "Dzombak",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "loeffladHowTypesConsequences2024",
		"type": "paper-conference",
		"abstract": "In the context of the rise of algorithmic decision-making (ADM) systems, social scoring systems are particularly controversial. They aim to encourage socially desirable behaviors by rewarding people with a good score in various decision-making contexts. In this paper, we report the results of a survey following a social scoring experiment, to predominantly understand the impact of the scoring outcome and the decision importance on people’s perceptions and behavioral intentions within an abstract social scoring system. We find that the outcome was pivotal for creating opinion differences regarding people’s perceptions, and behavioral reactions. In contrast, the decision importance did not exert a systematic impact on people’s perceptions and behavioral reactions, but exacerbated existing opinion differences in terms of perceived effectiveness. Specifically, the outcome strongly shaped the structural relationship between people’s experiences, perceptions, and behavioral reactions, creating a substantial outcome favorability bias for people with a bad outcome. Although people with a bad outcome reported an intention to adapt their behaviors, their intention to engage in desired behaviors could not be attributed to a perceived legitimacy of the system. For those with a good outcome, perceptions of procedural justice and legitimacy were weakened by the privacy-invading character of the social scoring system. Our work shows that the outcome people receive might create a pivotal disparate impact on people’s overall attitudes towards social scoring, shape their behavioral reactions, and create divergent behavioral motives, suggesting that very distinct societal dynamics may arise.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658986",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1515–1530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How the types of consequences in social scoring systems shape people's perceptions and behavioral reactions",
		"URL": "https://doi.org/10.1145/3630106.3658986",
		"author": [
			{
				"family": "Loefflad",
				"given": "Carmen"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wolfeImpactOpportunitiesGenerative2024",
		"type": "paper-conference",
		"abstract": "Generative AI appears poised to transform white collar professions, with more than 90% of Fortune 500 companies using OpenAI’s flagship GPT models, which have been characterized as “general purpose technologies” capable of effecting epochal changes in the economy. But how will such technologies impact organizations whose job is to verify and report factual information, and to ensure the health of the information ecosystem? To investigate this question, we conducted 30 interviews with N=38 participants working at 29 fact-checking organizations across six continents, asking about how they use generative AI and the opportunities and challenges they see in the technology. We found that uses of generative AI envisioned by fact-checkers differ based on organizational infrastructure, with applications for quality assurance in Editing, for trend analysis in Investigation, and for information literacy in Advocacy. We used the TOE framework to describe participant concerns ranging from the Technological (lack of transparency), to the Organizational (resource constraints), to the Environmental (uncertain and evolving policy). Building on the insights of our participants, we describe value tensions between fact-checking and generative AI, and propose a novel Verification dimension to the design space of generative models for information verification work. Finally, we outline an agenda for fairness, accountability, and transparency research to support the responsible use of generative AI in fact-checking. Throughout, we highlight the importance of human infrastructure and labor in producing verified information in collaboration with AI. We expect that this work will inform not only the scientific literature on fact-checking, but also contribute to understanding of organizational adaptation to a powerful but unreliable new technology.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658987",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1531–1543",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact and opportunities of generative AI in fact-checking",
		"URL": "https://doi.org/10.1145/3630106.3658987",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Mitra",
				"given": "Tanushree"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "madaioLearningResponsibleAI2024",
		"type": "paper-conference",
		"abstract": "Prior work has developed responsible AI (RAI) toolkits and studied how AI practitioners use such resources when practicing RAI. However, AI practitioners may not have the relevant skills or knowledge to effectively use RAI resources—particularly as pre-trained AI models have enabled more people to develop AI-based applications. In this paper, we explore current practices and aspirations for learning about RAI on-the-job, by interviewing 16 AI practitioners and 24 RAI educators across 16 organizations. We identify AI practitioners’ learning pathways for RAI, including information foraging and interpersonal learning; the orientations of RAI learning resources towards computational and procedural approaches to RAI; and aspirations for RAI learning, including desires for more sociotechnical approaches to understand potential harms of AI systems—aspirations that can be in tension with organizational priorities. We contribute empirical evidence of what and how AI practitioners are learning about RAI, and we suggest opportunities for the field to better support sociotechnical approaches to learning about RAI on-the-job.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658988",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1544–1558",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning about responsible AI on-the-job: Learning pathways, orientations, and aspirations",
		"URL": "https://doi.org/10.1145/3630106.3658988",
		"author": [
			{
				"family": "Madaio",
				"given": "Michael"
			},
			{
				"family": "Kapania",
				"given": "Shivani"
			},
			{
				"family": "Qadri",
				"given": "Rida"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Zaldivar",
				"given": "Andrew"
			},
			{
				"family": "Denton",
				"given": "Remi"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "smallEqualisedOddsNot2024",
		"type": "paper-conference",
		"abstract": "Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different – a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving the chances of a positive outcome available to individuals from another sub-population, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model’s predictive power, individual fairness and robustness while ensuring group fairness.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658989",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1559–1578",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalised odds is not equal individual odds: Post-processing for group and individual fairness",
		"URL": "https://doi.org/10.1145/3630106.3658989",
		"author": [
			{
				"family": "Small",
				"given": "Edward"
			},
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Manning",
				"given": "Daniel"
			},
			{
				"family": "Salim",
				"given": "Flora D."
			},
			{
				"family": "Chan",
				"given": "Jeffrey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "deckCriticalSurveyFairness2024",
		"type": "paper-conference",
		"abstract": "In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 scientific articles on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. Importantly, we notice that claims are often (i) vague and simplistic, (ii) lacking normative grounding, or (iii) poorly aligned with the actual capabilities of XAI. We suggest to conceive XAI not as an ethical panacea but as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness. Moreover, when making a claim about XAI and fairness, we emphasize the need to be more specific about what kind of XAI method is used, which fairness desideratum it refers to, how exactly it enables fairness, and who is the stakeholder that benefits from XAI.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658990",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 17\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1579–1595",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A critical survey on fairness benefits of explainable AI",
		"URL": "https://doi.org/10.1145/3630106.3658990",
		"author": [
			{
				"family": "Deck",
				"given": "Luca"
			},
			{
				"family": "Schoeffer",
				"given": "Jakob"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Kühl",
				"given": "Niklas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ziosiEvidenceWhatWhom2024",
		"type": "paper-conference",
		"abstract": "This paper presents a critical, qualitative study of the social role of algorithmic bias in the context of the Chicago crime prediction algorithm, a predictive policing tool that forecasts when and where in the city crime is most likely to occur. Through interviews with 18 Chicago-area community organizations, academic researchers, and public sector actors, we show that stakeholders from different groups articulate diverse problem diagnoses of the tool’s algorithmic bias, strategically using it as evidence to advance criminal justice interventions that align with stakeholders’ positionality and political ends. Drawing inspiration from Catherine D’Ignazio’s taxonomy of “refusing and using” data, we find that stakeholders use evidence of algorithmic bias to reform the policies around police patrol allocation; reject algorithm-based policing interventions; reframe crime as a structural rather than interpersonal problem; reveal data on authority figures in an effort to subvert their power; repair and heal families and communities; and, in the case of more powerful actors, to reaffirm their own authority or existing power structures. We identify the implicit assumptions and scope of these varied uses of algorithmic bias as evidence, showing that they require different (and sometimes conflicting) values about policing and AI. This divergence reflects long-standing tensions in the criminal justice reform landscape between the values of liberation and healing often centered by system-impacted communities and the values of surveillance and deterrence often instantiated in data-driven reform measures. We advocate for centering the interests and experiential knowledge of communities impacted by incarceration to ensure that evidence of algorithmic bias can serve as a device to challenge the status quo.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658991",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1596–1608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evidence of what, for whom? The socially contested role of algorithmic bias in a predictive policing tool",
		"URL": "https://doi.org/10.1145/3630106.3658991",
		"author": [
			{
				"family": "Ziosi",
				"given": "Marta"
			},
			{
				"family": "Pruss",
				"given": "Dasha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sureshParticipationAgeFoundation2024",
		"type": "paper-conference",
		"abstract": "Growing interest and investment in the capabilities of foundation models has positioned such systems to impact a wide array of services, from banking to healthcare. Alongside these opportunities is the risk that these systems reify existing power imbalances and cause disproportionate harm to historically marginalized groups. The larger scale and domain-agnostic manner in which these models operate further heightens the stakes: any errors or harms are liable to reoccur across use cases. In AI & ML more broadly, participatory approaches hold promise to lend agency and decision-making power to marginalized stakeholders, leading to systems that better benefit justice through equitable and distributed governance. But existing approaches in participatory AI/ML are typically grounded in a specific application and set of relevant stakeholders, and it is not straightforward how to apply these lessons to the context of foundation models. Our paper aims to fill this gap. First, we examine existing attempts at incorporating participation into foundation models. We highlight the tension between participation and scale, demonstrating that it is intractable for impacted communities to meaningfully shape a foundation model that is intended to be universally applicable. In response, we develop a blueprint for participatory foundation models that identifies more local, application-oriented opportunities for meaningful participation. In addition to the “foundation” layer, our framework proposes the “subfloor” layer, in which stakeholders develop shared technical infrastructure, norms and governance for a grounded domain such as clinical care, journalism, or finance, and the “surface” (or application) layer, in which affected communities shape the use of a foundation model for a specific downstream task. The intermediate “subfloor” layer scopes the range of potential harms to consider, and affords communities more concrete avenues for deliberation and intervention. At the same time, it avoids duplicative effort by scaling input across relevant use cases. Through three case studies in clinical care, financial services, and journalism, we illustrate how this multi-layer model can create more meaningful opportunities for participation than solely intervening at the foundation layer.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658992",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1609–1621",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participation in the age of foundation models",
		"URL": "https://doi.org/10.1145/3630106.3658992",
		"author": [
			{
				"family": "Suresh",
				"given": "Harini"
			},
			{
				"family": "Tseng",
				"given": "Emily"
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Gray",
				"given": "Mary"
			},
			{
				"family": "Pierson",
				"given": "Emma"
			},
			{
				"family": "Levy",
				"given": "Karen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mirowskiRobotWalksBar2024",
		"type": "paper-conference",
		"abstract": "We interviewed twenty professional comedians who perform live shows in front of audiences and who use artificial intelligence in their artistic process as part of 3-hour workshops on “AI x Comedy” conducted at the Edinburgh Festival Fringe in August 2023 and online. The workshop consisted of a comedy writing session with large language models (LLMs), a human-computer interaction questionnaire to assess the Creativity Support Index of AI as a writing tool, and a focus group interrogating the comedians’ motivations for and processes of using AI, as well as their ethical concerns about bias, censorship and copyright. Participants noted that existing moderation strategies used in safety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by erasing minority groups and their perspectives, and qualified this as a form of censorship. At the same time, most participants felt the LLMs did not succeed as a creativity support tool, by producing bland and biased comedy tropes, akin to “cruise ship comedy material from the 1950s, but a bit less racist”. Our work extends scholarship about the subtle difference between, one the one hand, harmful speech, and on the other hand, “offensive” language as a practice of resistance, satire and “punching up”. We also interrogate the global value alignment behind such language models, and discuss the importance of community-based value alignment and data ownership to build AI tools that better suit artists’ needs. Warning: this study may contain offensive language and discusses self-harm.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658993",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1622–1636",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A robot walks into a bar: Can language models serve as creativity SupportTools for comedy? An evaluation of llms’ humour alignment with comedians",
		"URL": "https://doi.org/10.1145/3630106.3658993",
		"author": [
			{
				"family": "Mirowski",
				"given": "Piotr"
			},
			{
				"family": "Love",
				"given": "Juliette"
			},
			{
				"family": "Mathewson",
				"given": "Kory"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "shiraliParticipatoryObjectiveDesign2024",
		"type": "paper-conference",
		"abstract": "In standard resource allocation problems, the designer sets the objective function, which captures the central allocation goal, in a top-down manner. The agents primarily participate in the allocation mechanism by reporting their preferences over the items; they cannot influence the objective once the designer sets it. Implicitly, this approach presumes that standard ways of eliciting the agents’ preferences adequately represent their true preferences—an assumption which does not hold if agents have preferences not just over the items they receive but also over the objective being optimized. For instance, agents may also have social preferences, such as inequality-aversion, altruism, or similar other-regarding behavior. We cannot express such preferences through standard cardinal utilities or ordinal rankings over the items the designer would typically elicit from the agents. This work examines how we can use this bottom-up preference elicitation stage to enable participants to express preferences over the objectives. We present a versatile framework that elicits agents’ preferences over a possible set of objectives and then minimally alters the underlying optimization problem to solve for a new objective that combines both the standard benchmark objective and the agents’ preferences for other objectives. We show how to evaluate this new participatory approach against the standard approach, using our notions of loss and gain in social welfare as well as individual tradeoffs. We illustrate the potency of this framework using a well-studied fair division problem where the designer aims to allocate m divisible items to n agents. In the standard setting, the designer optimizes for utilitarian social welfare, i.e., the sum of the agents’ cardinal utilities. We assume that some agents are also inequality-averse and may, therefore, have preferences for objectives that minimize inequality. Using the popular Fehr and Schmidt [31] model, we demonstrate how to map this fair division question to our framework, where the participatory approach optimizes both the standard utilitarian social welfare objective and the agents’ heterogeneous preferences over the level of inequality. We examine this problem theoretically to show that there can be large gains in social welfare if the designer uses this participatory approach. Further, we show that the loss in social welfare is linear in the level of inequality aversion and independent of the number of agents. We present a tighter bound in both cases under further natural assumptions on the preferences. We also examine the worst-case cost an individual agent might incur. Our results indicate that the loss in social welfare (measured by the standard objective) and gain in social welfare (measured by the participatory one) can favor the participatory approach in several natural settings. Throughout the work, we highlight various promising avenues for examining this participatory approach in the specific case study tackled in this paper and a broader range of resource allocation problems.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658994",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 26\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1637–1662",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participatory objective design via preference elicitation",
		"URL": "https://doi.org/10.1145/3630106.3658994",
		"author": [
			{
				"family": "Shirali",
				"given": "Ali"
			},
			{
				"family": "Finocchiaro",
				"given": "Jessie"
			},
			{
				"family": "Abebe",
				"given": "Rediet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "starkAnimationArtificialIntelligence2024",
		"type": "paper-conference",
		"abstract": "Animation as genre is broadly used across many forms of digital media. In this paper, I argue ChatGPT and similar chatbots powered by Large Language Models (LLMs) can be best understood as animated characters. More than just cartooning, puppetry, or CGI, animation is a paradigm involving the projection of qualities perceived as human such as power, agency, will, and personality outside of the self and onto objects in the environment. Characteristics of animation—including reliance on stereotypes, obfuscation of human labor, and manipulation of an audience's emotions—can help us both analyze and respond appropriately to interactive AI technologies and the hyperbolic claims of their promoters.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658995",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1663–1671",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Animation and artificial intelligence",
		"URL": "https://doi.org/10.1145/3630106.3658995",
		"author": [
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "koeneckeCarelessWhisperSpeechtotext2024",
		"type": "paper-conference",
		"abstract": "Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI’s Whisper, a state-of-the-art automated speech recognition service outperforming industry competitors, as of 2023. While many of Whisper’s transcriptions were highly accurate, we find that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as perpetuating violence, making up inaccurate associations, or implying false authority. We then study why hallucinations occur by observing the disparities in hallucination rates between speakers with aphasia (who have a lowered ability to express themselves using speech and voice) and a control group. We find that hallucinations disproportionately occur for individuals who speak with longer shares of non-vocal durations—a common symptom of aphasia. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases amplified by hallucinations in downstream applications of speech-to-text models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658996",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1672–1681",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Careless whisper: Speech-to-text hallucination harms",
		"URL": "https://doi.org/10.1145/3630106.3658996",
		"author": [
			{
				"family": "Koenecke",
				"given": "Allison"
			},
			{
				"family": "Choi",
				"given": "Anna Seo Gyeong"
			},
			{
				"family": "Mei",
				"given": "Katelyn X."
			},
			{
				"family": "Schellmann",
				"given": "Hilke"
			},
			{
				"family": "Sloane",
				"given": "Mona"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vannostrandActionableRecourseAutomated2024",
		"type": "paper-conference",
		"abstract": "Automated decision-making systems are increasingly deployed in domains such as hiring and credit approval where negative outcomes can have substantial ramifications for decision subjects. Thus, recent research has focused on providing explanations that help decision subjects understand the decision system and enable them to take actionable recourse to change their outcome. Popular counterfactual explanation techniques aim to achieve this by describing alterations to an instance that would transform a negative outcome to a positive one. Unfortunately, little user evaluation has been performed to assess which of the many counterfactual approaches best achieve this goal. In this work, we conduct a crowd-sourced between-subjects user study (N = 252) to examine the effects of counterfactual explanation type and presentation on lay decision subjects’ understandings of automated decision systems. We find that the region-based counterfactual type significantly increases objective understanding, subjective understanding, and response confidence as compared to the point-based type. We also find that counterfactual presentation significantly effects response time and moderates the effect of counterfactual type for response confidence, but not understanding. A qualitative analysis reveals how decision subjects interact with different explanation configurations and highlights unmet needs for explanation justification. Our results provide valuable insights and recommendations for the development of counterfactual explanation techniques towards achieving practical actionable recourse and empowering lay users to seek justice and opportunity in automated decision workflows.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658997",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1682–1700",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable recourse for automated decisions: Examining the effects of counterfactual explanation type and presentation on lay user understanding",
		"URL": "https://doi.org/10.1145/3630106.3658997",
		"author": [
			{
				"family": "VanNostrand",
				"given": "Peter M."
			},
			{
				"family": "Hofmann",
				"given": "Dennis M."
			},
			{
				"family": "Ma",
				"given": "Lei"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wrightNullComplianceNYC2024",
		"type": "paper-conference",
		"abstract": "In July 2023, New York City became the first jurisdiction globally to mandate bias audits for commercial algorithmic systems, specifically for automated employment decisions systems (AEDTs) used in hiring and promotion. Local Law 144 (LL 144) requires AEDTs to be independently audited annually for race and gender bias, and the audit report must be publicly posted. Additionally, employers are obligated to post a transparency notice with the job listing. In this study, 155 student investigators recorded 391 employers’ compliance with LL 144 and the user experience for prospective job applicants. Among these employers, 18 posted audit reports and 13 posted transparency notices. These rates could potentially be explained by a significant limitation in the accountability mechanisms enacted by LL 144. Since the law grants employers substantial discretion over whether their system is in scope of the law, a null result cannot be said to indicate non-compliance, a condition we call \"null compliance.\" Employer discretion may also explain our finding that nearly all audits reported an impact factor over 0.8, a rule of thumb often used in employment discrimination cases. We also find that the benefit of LL 144 to ordinary job seekers is limited due to shortcomings in accessibility and usability. Our findings offer important lessons for policy-makers as they consider regulating algorithmic systems, particularly the degree of discretion to grant to regulated parties and the limitations of relying on transparency and end-user accountability.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658998",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1701–1713",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Null compliance: NYC local law 144 and the challenges of algorithm accountability",
		"URL": "https://doi.org/10.1145/3630106.3658998",
		"author": [
			{
				"family": "Wright",
				"given": "Lucas"
			},
			{
				"family": "Muenster",
				"given": "Roxana Mika"
			},
			{
				"family": "Vecchione",
				"given": "Briana"
			},
			{
				"family": "Qu",
				"given": "Tianyao"
			},
			{
				"family": "Cai",
				"given": "Pika (Senhuang)"
			},
			{
				"family": "Smith",
				"given": "Alan"
			},
			{
				"family": "Investigators",
				"given": "Comm 2450 Student"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Matias",
				"given": "J. Nathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sullivanSIDEsSeparatingIdealization2024",
		"type": "paper-conference",
		"abstract": "Explainable AI (xAI) methods are important for establishing trust in using black-box models. However, recent criticism has mounted against current xAI methods that they disagree, are necessarily false, and can be manipulated, which has started to undermine the deployment of black-box models. Rudin (2019) goes so far as to say that we should stop using black-box models altogether in high-stakes cases because xAI explanations ‘must be wrong’. However, strict fidelity to the truth is historically not a desideratum in science. Idealizations–the intentional distortions introduced to scientific theories and models–are commonplace in the natural sciences and are seen as a successful scientific tool. Thus, it is not falsehood qua falsehood that is the issue. In this paper, I outline the need for xAI research to engage in idealization evaluation. Drawing on the use of idealizations in the natural sciences and philosophy of science, I introduce a novel framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods, and the distortions that they introduce, can be part of a successful idealization or are indeed deceptive distortions as critics suggest. I discuss the role that existing research can play in idealization evaluation and where innovation is necessary. Through a qualitative analysis we find that leading feature importance methods and counterfactual explanations are subject to idealization failure and suggest remedies for ameliorating idealization failure.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3658999",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1714–1724",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SIDEs: Separating idealization from deceptive 'explanations' in xAI",
		"URL": "https://doi.org/10.1145/3630106.3658999",
		"author": [
			{
				"family": "Sullivan",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "diberardinoAlgorithmicHarmsAlgorithmic2024",
		"type": "paper-conference",
		"abstract": "New artificial intelligence (AI) systems grounded in machine learning are being integrated into our lives at a rapid rate, but not without consequence: scholars across domains have increasingly pointed out issues related to privacy, transparency, bias, discrimination, exploitation, and exclusion associated with algorithmic systems in both public and private sector contexts. Concerns surrounding the adverse impacts of these technologies have spurred discussion on the topics of algorithmic harm. However, the overwhelming majority of articles on said harms offer no definition as to what constitutes ‘harm’ in these contexts. This paper aims to address this omission by introducing one criterion for a suitable account of algorithmic harm. More specifically, we follow Joel Feinberg in understanding harms as distinct from wrongs, where only the latter necessarily carry a normative dimension. This distinction highlights issues in the current scholarship surrounding the conflation of algorithmic harms and wrongs. In response to these issues, we put forth two requirements for upholding the harms/wrongs distinction when analyzing the increasingly far-reaching impacts of these technologies and suggest how this distinction can be useful in design, engineering, and policymaking.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659001",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 8\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1725–1732",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic harms and algorithmic wrongs",
		"URL": "https://doi.org/10.1145/3630106.3659001",
		"author": [
			{
				"family": "Diberardino",
				"given": "Nathalie"
			},
			{
				"family": "Baleshta",
				"given": "Clair"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "whitneyRealRisksFake2024",
		"type": "paper-conference",
		"abstract": "Machine learning systems require representations of the real world for training and testing - they require data, and lots of it. Collecting data at scale has logistical and ethical challenges, and synthetic data promises a solution to these challenges. Instead of needing to collect photos of real people’s faces to train a facial recognition system, a model creator could create and use photo-realistic, synthetic faces. The comparative ease of generating this synthetic data rather than relying on collecting data has made it a common practice. We present two key risks of using synthetic data in model development. First, we detail the high risk of false confidence when using synthetic data to increase dataset diversity and representation. We base this in the examination of a real world use-case of synthetic data, where synthetic datasets were generated for an evaluation of facial recognition technology. Second, we examine how using synthetic data risks circumventing consent for data usage. We illustrate this by considering the importance of consent to the U.S. Federal Trade Commission’s regulation of data collection and affected models. Finally, we discuss how these two risks exemplify how synthetic data complicates existing governance and ethical practice; by decoupling data from those it impacts, synthetic data is prone to consolidating power away those most impacted by algorithmically-mediated harm.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659002",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1733–1744",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Real risks of fake data: Synthetic data, diversity-washing and consent circumvention",
		"URL": "https://doi.org/10.1145/3630106.3659002",
		"author": [
			{
				"family": "Whitney",
				"given": "Cedric Deslandes"
			},
			{
				"family": "Norman",
				"given": "Justin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "majumdarCARMAPracticalFramework2024",
		"type": "paper-conference",
		"abstract": "Algorithms are increasingly used to automate large-scale decision-making processes, e.g., online platforms that make instant decisions in lending, hiring, and education. When such automated systems yield unfavorable decisions, it is imperative to allow for recourse by accompanying the instantaneous negative decisions with recommendations that can help affected individuals to overturn them. However, the practical challenges of providing algorithmic recourse in large-scale settings are not negligible: giving recourse recommendations that are actionable requires not only causal knowledge of the relationships between applicant features but also solving a complex combinatorial optimization problem for each rejected applicant. In this work, we introduce CARMA, a novel framework to generate causal recourse recommendations at scale. For practical settings with limited causal information, CARMA leverages pre-trained state-of-the-art causal generative models to find recourse recommendations. More importantly, CARMA addresses the scalability of finding these recommendations by casting the complex recourse optimization problem as a prediction task. By training a novel neural-network-based framework, CARMA efficiently solves the prediction task without requiring supervision for optimal recourse actions. Our extensive evaluations show that post-training, running inference on CARMA reliably amortizes causal recourse, generating optimal and instantaneous recommendations. CARMA exhibits flexibility, as its optimization is versatile with respect to the algorithmic decision-making and pre-trained causal generative models, provided their differentiability is ensured. Furthermore, we showcase CARMA in a case study, illustrating its ability to tailor causal recourse recommendations by readily incorporating population-level feature preferences based on factors such as difficulty or time needed.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659003",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1745–1762",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CARMA: A practical framework to generate recommendations for causal algorithmic recourse at scale",
		"URL": "https://doi.org/10.1145/3630106.3659003",
		"author": [
			{
				"family": "Majumdar",
				"given": "Ayan"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "agarwalSystem2RecommendersDisentangling2024",
		"type": "paper-conference",
		"abstract": "Recommender systems are an important part of the modern human experience whose influence ranges from the food we eat to the news we read. Yet, there is still debate as to what extent online recommendation platforms are aligned with the goals of their users. A core issue fueling this debate is the challenge of inferring a user’s utility based on their engagement signals such as likes, shares, watch time etc., which are often the primary metric used by platforms to optimize content. This is because users’ utility-driven decision-processes (which we refer to as System-2), e.g., reading news that are accurate and relevant for them, are often confounded by their impulsive or unconscious decision-processes (which we refer to as System-1), e.g., spend time on click-bait news articles. As a result, it is difficult to infer whether an observed engagement is utility-driven or impulse-driven. In this paper we explore a new approach to recommender systems where we infer user’s utility based on their return probability to the platform rather than engagement signals. This approach is based on the intuition that users tend to return to a platform in the long run if it creates utility for them, while pure engagement-driven interactions, i.e., interactions that do not add meaningful utility, may affect user return in the short term but will not have a lasting effect. For this purpose, we propose a generative model in which past content interactions impact the arrival rates of users based on a self-exciting Hawkes process. These arrival rates to the platform are a combination of both System-1 and System-2 decision processes. The System-2 arrival intensity depends on the utility drawn from past content interactions and has a long lasting effect on return probability. In contrast, System-1 arrival intensity depends on the instantaneous gratification or moreishness and tends to vanish rapidly in time. We show analytically that given samples from this model it is provably possible to disentangle the System-1 and System-2 decision-processes and thus infer user’s utility, thereby allowing us to optimize content based on it. We conduct experiments on synthetic data to demonstrate the effectiveness of our approach over engagement optimization.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659004",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1763–1773",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "System-2 recommenders: Disentangling utility and engagement in recommendation systems via temporal point-processes",
		"URL": "https://doi.org/10.1145/3630106.3659004",
		"author": [
			{
				"family": "Agarwal",
				"given": "Arpit"
			},
			{
				"family": "Usunier",
				"given": "Nicolas"
			},
			{
				"family": "Lazaric",
				"given": "Alessandro"
			},
			{
				"family": "Nickel",
				"given": "Maximilian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "liesenfeldRethinkingOpenSource2024",
		"type": "paper-conference",
		"abstract": "The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here we use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), we find that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. We argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659005",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1774–1787",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking open source generative AI: open washing and the EU AI Act",
		"URL": "https://doi.org/10.1145/3630106.3659005",
		"author": [
			{
				"family": "Liesenfeld",
				"given": "Andreas"
			},
			{
				"family": "Dingemanse",
				"given": "Mark"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "chanGroupFairnessGroup2024",
		"type": "paper-conference",
		"abstract": "Ensuring equitable impact of machine learning models across different societal groups is of utmost importance for real-world machine learning applications. Prior research in fairness has predominantly focused on adjusting model outputs through pre-processing, in-processing, or post-processing techniques. These techniques focus on correcting bias in either the data or the model. However, we argue that the bias in the data and model should be addressed in conjunction. To achieve this, we propose an algorithm called GroupDebias to reduce unfairness in the data in a model-guided fashion, thereby enabling models to exhibit more equitable behavior. Even though it is model-aware, the core idea of GroupDebias is independent of the model architecture, making it a versatile and effective approach that can be broadly applied across various domains and model types. Our method focuses on systematically addressing biases present in the training data itself by adaptively dropping samples that increase the biases in the model. Theoretically, the proposed approach enjoys a guaranteed improvement in demographic parity at the expense of a bounded reduction in balanced accuracy. A comprehensive evaluation of the GroupDebias algorithm through extensive experiments on diverse datasets and machine learning models demonstrates that GroupDebias consistently and significantly outperforms existing fairness enhancement techniques, achieving a more substantial reduction in unfairness with minimal impact on model performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659006",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1788–1808",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Group fairness via group consensus",
		"URL": "https://doi.org/10.1145/3630106.3659006",
		"author": [
			{
				"family": "Chan",
				"given": "Eunice"
			},
			{
				"family": "Liu",
				"given": "Zhining"
			},
			{
				"family": "Qiu",
				"given": "Ruizhong"
			},
			{
				"family": "Zhang",
				"given": "Yuheng"
			},
			{
				"family": "Maciejewski",
				"given": "Ross"
			},
			{
				"family": "Tong",
				"given": "Hanghang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kearneyUsecasesParticipatoryApproach2024",
		"type": "paper-conference",
		"abstract": "With a rising number of law enforcement agencies facing budgetary cuts, many turn to data science in an attempt to maintain service quality with fewer resources. A number of thus adopted solutions–including facial recognition, predictive policing, and risk assessments–have been contested by researchers and journalists alike. Yet comparatively little research is done at the strategy level, which determines where data science will be deployed in the first place. In this study, we interview 40 practitioners from Police Scotland, investigating what they believe to be crucial to successfully incorporate data science in their ways of working. Bucking the external trend, the participants distanced themselves from tools like facial recognition and risk assessment. Instead of focusing on individual use-cases, their primary concerns for the future were around (i) systemic issues around data is collection and use, (ii) goal misalignment between leadership and operational levels, (iii) the fear that datafication may undervalue important aspects of policing, and (iv) appropriate ways of interaction between data science teams and operational officers. Alongside the insights particular to Police Scotland, our work reaffirms how participatory approaches can go beyond the technical, and uncover structural and political barriers to success.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659007",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1809–1826",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond use-cases: a participatory approach to envisioning data science in law enforcement",
		"URL": "https://doi.org/10.1145/3630106.3659007",
		"author": [
			{
				"family": "Kearney",
				"given": "Caitlin"
			},
			{
				"family": "Hron",
				"given": "Jiri"
			},
			{
				"family": "Kosc",
				"given": "Helen"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "balepurInterveningIncreaseCommunity2024",
		"type": "paper-conference",
		"abstract": "Refugees or immigrants who arrive in new countries often feel isolated. In this work, we examine how a resource-bounded public entity can make recommendations to increase integration of these new arrivals into a community. The community is made up of agents who engage in a strategic network formation process; agents join periodically — new arrivals are the refugees. The public entity meanwhile makes a limited number of edge-formation recommendations (according to its resource constraint) per iteration in order to increase integration of refugees. This work investigates the relationship between community trust and network fairness. First, we show that increasing the public entity’s resource allocation will not compensate for low trust in the community. Then, we introduce two trust-increasing interventions by the public entity: a targeted advertising campaign, and an announcement to increase transparency. We find that diverting a fraction (20%) of the public entity’s resources to a targeted advertising campaign can increase trust and fairness in the community, especially in low trust scenarios. We find that personalized, local announcements are more effective at increasing fairness than global announcements in low trust scenarios; they almost double our fairness metric in some cases. Importantly, the transparent announcement requires no extra resource expenditure on the part of the public entity. Our work underscores the importance of community trust — low trust cannot be compensated for with resources. This work provides theoretical support for these trust-increasing interventions, which we show can lead to increased integration of refugees in communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659008",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1827–1837",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Intervening to increase community trust for fair network outcomes",
		"URL": "https://doi.org/10.1145/3630106.3659008",
		"author": [
			{
				"family": "Balepur",
				"given": "Naina"
			},
			{
				"family": "Sundaram",
				"given": "Hari"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "grillConstructingCapabilitiesPolitics2024",
		"type": "paper-conference",
		"abstract": "The advertised and perceived capabilities of generative AI products like ChatGPT have recently stimulated considerable investments and discourse surrounding their potential to aid and replace work. The prominence of these systems, and their promise to be general-purpose, has resulted in an avalanche of tests to discover and certify their capabilities. This new testing regime is concerned with creating ever-more tasks for generative AI products instead of testing a model for one specialized task. Beyond efforts to understand products’ capabilities, the construction of tasks and corresponding tests are also performative enactments meant to convince others and thus to gain attention, scientific legitimacy, and investment. The current market concentration of a few big AI companies points to a concerning conflict of interest: those with a vested interest in the success of the technology also have control over globalized testing infrastructures and thereby the exclusive means to create extensive knowledge claims about these systems. In this paper, I theorize capabilities as contested constructions and situated accomplishments shaped by power imbalances. I further unpack the globalized testing infrastructures involved in the construction and stabilization of generative AI products’ capabilities. Furthermore, I discuss how the testing of these AI models and products is externalized, extracting value from the unpaid or under-paid labor of researcher and developer communities, content creators, subcontractors, and users. Lastly, I discuss a reflexive and critical approach to testing that challenges depoliticization and seeks to produce lasting critiques that serve more emancipatory goals.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659009",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1838–1849",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Constructing capabilities: The politics of testing infrastructures for generative AI",
		"URL": "https://doi.org/10.1145/3630106.3659009",
		"author": [
			{
				"family": "Grill",
				"given": "Gabriel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "weertsUnlawfulProxyDiscrimination2024",
		"type": "paper-conference",
		"abstract": "Emerging scholarship suggests that the EU legal concept of direct discrimination - where a person is given different treatment on grounds of a protected characteristic - may apply to various algorithmic decision-making contexts. This has important implications: unlike indirect discrimination, there is generally no ‘objective justification’ stage in the direct discrimination framework, which means that the deployment of directly discriminatory algorithms will usually be unlawful per se. In this paper, we focus on the most likely candidate for direct discrimination in the algorithmic context, termed inherent direct discrimination, where a proxy is inextricably linked to a protected characteristic. We draw on computer science literature to suggest that, in the algorithmic context, ‘treatment on the grounds of’ needs to be understood in terms of two steps: proxy capacity and proxy use. Only where both elements can be made out can direct discrimination be said to be ‘on grounds of’ a protected characteristic. We analyse the legal conditions of our proposed proxy capacity and proxy use tests. Based on this analysis, we discuss technical approaches and metrics that could be developed or applied to identify inherent direct discrimination in algorithmic decision-making.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659010",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1850–1860",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unlawful proxy discrimination: a framework for challenging inherently discriminatory algorithms",
		"URL": "https://doi.org/10.1145/3630106.3659010",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Kelly-Lyth",
				"given": "Aislinn"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Adams-Prassl",
				"given": "Jeremias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "guoMiMICRIDomaincenteredCounterfactual2024",
		"type": "paper-conference",
		"abstract": "The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659011",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1861–1874",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "MiMICRI: Towards domain-centered counterfactual explanations of cardiovascular image classification models",
		"URL": "https://doi.org/10.1145/3630106.3659011",
		"author": [
			{
				"family": "Guo",
				"given": "Grace"
			},
			{
				"family": "Deng",
				"given": "Lifu"
			},
			{
				"family": "Tandon",
				"given": "Animesh"
			},
			{
				"family": "Endert",
				"given": "Alex"
			},
			{
				"family": "Kwon",
				"given": "Bum Chul"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "orrAISportCompetitive2024",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) systems are evaluated using competitive methods that rely on benchmark datasets to determine performance. These benchmark datasets, however, are often constructed through arbitrary processes that fall short in encapsulating the depth and breadth of the tasks they are intended to measure. In this paper, we interrogate the naturalization of benchmark datasets as veracious metrics by examining the historical development of benchmarking as an epistemic practice in AI research. Specifically, we highlight three key case studies that were crucial in establishing the existing reliance on benchmark datasets for evaluating the capabilities of AI systems: (1) the sharing of Highleyman’s OCR dataset in the 1960s, which solidified a community of knowledge production around a shared benchmark dataset, (2) the Common Task Framework (CTF) of the 1980s, a state-led project to standardize benchmark datasets as legitimate indicators of technical progress; and (3) the Netflix Prize which further solidified benchmarking as a competitive goal within the ML research community. This genealogy highlights how contemporary dynamics and limitations of benchmarking developed from a longer history of collaboration, standardization, and competition. We end with reflections on how this history informs our understanding of benchmarking in the current era of generative artificial intelligence.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659012",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1875–1884",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI as a sport: On the competitive epistemologies of benchmarking",
		"URL": "https://doi.org/10.1145/3630106.3659012",
		"author": [
			{
				"family": "Orr",
				"given": "Will"
			},
			{
				"family": "Kang",
				"given": "Edward B."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "eversTalkingEachOther2024",
		"type": "paper-conference",
		"abstract": "This study examines the convergence of the European Commission (EC) and Big Tech companies’ (Google and Microsoft) discourse on ‘ethical’ AI through critical discourse analysis and the concept of hegemonic discourse. The paper answers the question to what extent there is a hegemonic discourse on ethical AI between EU policymakers and Big Tech companies and whether this is impacted by the prospect of legally binding legislation, considering the possible impact of the 2021 AI Act Proposal of the European Commission. This analysis is relevant at an inflection point where previous literature notes superficial convergence between the approaches of public and private actors, indicating policy consensus. The scope of analysis however is limited to non-legally binding regulation and lacks regional focus. In the EU, the advent of legally binding AI regulation with the 2021 AI Act (AIA) Proposal marks a critical juncture: with agreement on the AIA in December 2023, ethics standards become part of market entry requirements to the EU Single Market and the underlying differences in approaching Ethical AI will have important ramifications on policy preferences, compliance, enforcement and thought leadership in the domain more broadly. I find that the European discourse on ‘ethical’ AI by the EC and Big Tech companies such as Google and Microsoft is largely hegemonic and depoliticised in non-legally binding settings from 2018-2021 due to shared assumptions on ‘ethical’ AI and absence of significant underlying social and political conflict. It evolves to non-hegemonic and repoliticised discourse through dislocation by the prospect of legally binding regulation, which pushes actors to reveal their genuine policy preferences that bear political and social conflictuality whilst both actor types take an instrumental approach to ethics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659013",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1885–1896",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Talking past each other? Navigating discourse on ethical AI: Comparing the discourse on ethical AI policy by Big Tech companies and the European Commission",
		"URL": "https://doi.org/10.1145/3630106.3659013",
		"author": [
			{
				"family": "Evers",
				"given": "Cornelia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "niFairnessSensitiveAttributes2024",
		"type": "paper-conference",
		"abstract": "While model fairness improvement has been explored previously, existing methods invariably rely on adjusting explicit sensitive attribute values in order to improve model fairness in downstream tasks. However, we observe a trend in which sensitive demographic information becomes inaccessible as public concerns around data privacy grow. In this paper, we propose a confidence-based hierarchical classifier structure called “Reckoner” for reliable fair model learning under the assumption of missing sensitive attributes. We first present results showing that if the dataset contains biased labels or other hidden biases, classifiers significantly increase the bias gap across different demographic groups in the subset with higher prediction confidence. Inspired by these findings, we devised a dual-model system in which a version of the model initialised with a high-confidence data subset learns from a version of the model initialised with a low-confidence data subset, enabling it to avoid biased predictions. Our experimental results show that Reckoner consistently outperforms state-of-the-art baselines in COMPAS dataset and New Adult dataset, considering both accuracy and fairness metrics.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659014",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1897–1906",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness without sensitive attributes via knowledge sharing",
		"URL": "https://doi.org/10.1145/3630106.3659014",
		"author": [
			{
				"family": "Ni",
				"given": "Hongliang"
			},
			{
				"family": "Han",
				"given": "Lei"
			},
			{
				"family": "Chen",
				"given": "Tong"
			},
			{
				"family": "Sadiq",
				"given": "Shazia"
			},
			{
				"family": "Demartini",
				"given": "Gianluca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "poeConflictAlgorithmicFairness2024",
		"type": "paper-conference",
		"abstract": "AI-based automated hiring systems cover a wide range of tools of varying complexity, from resume parsing tools to candidate selection models. Their close interference in economic and social life faces raising demands and investigations aiming to reduce the potential discrimination they may cause. This article covers the intersection of EU non-discrimination law and algorithmic fairness in the context of automated hiring systems. The paper analyzes the balance between equality of opportunity (formal and substantive) and equality of outcome, critiques the focus on non-conservative group fairness in machine learning, and discusses the legal implications of automated hiring systems under EU law. Additionally, it highlights often committed fallacies in relation to the process of de-biasing and advocates for a broader understanding of fairness in machine learning that aligns with EU legal standards and societal values.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659015",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1907–1916",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The conflict between algorithmic fairness and non-discrimination: An analysis of fair automated hiring",
		"URL": "https://doi.org/10.1145/3630106.3659015",
		"author": [
			{
				"family": "Poe",
				"given": "Robert Lee"
			},
			{
				"family": "El Mestari",
				"given": "Soumia Zohra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "binkyteBaBEEnhancingFairness2024",
		"type": "paper-conference",
		"abstract": "We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each group. The decision can then be based directly on the estimated E. We show, by experiments on synthetic and real data sets, that our approach provides a good level of fairness as well as high accuracy.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659016",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1917–1925",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "BaBE: Enhancing fairness via estimation of explaining variables",
		"URL": "https://doi.org/10.1145/3630106.3659016",
		"author": [
			{
				"family": "Binkyte",
				"given": "Ruta"
			},
			{
				"family": "Gorla",
				"given": "Daniele"
			},
			{
				"family": "Palamidessi",
				"given": "Catuscia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "hadaAkalBadiYa2024",
		"type": "paper-conference",
		"abstract": "Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659017",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1926–1939",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Akal badi ya bias: An exploratory study of gender bias in hindi language technology",
		"URL": "https://doi.org/10.1145/3630106.3659017",
		"author": [
			{
				"family": "Hada",
				"given": "Rishav"
			},
			{
				"family": "Husain",
				"given": "Safiya"
			},
			{
				"family": "Gumma",
				"given": "Varun"
			},
			{
				"family": "Diddee",
				"given": "Harshita"
			},
			{
				"family": "Yadavalli",
				"given": "Aditya"
			},
			{
				"family": "Seth",
				"given": "Agrima"
			},
			{
				"family": "Kulkarni",
				"given": "Nidhi"
			},
			{
				"family": "Gadiraju",
				"given": "Ujwal"
			},
			{
				"family": "Vashistha",
				"given": "Aditya"
			},
			{
				"family": "Seshadri",
				"given": "Vivek"
			},
			{
				"family": "Bali",
				"given": "Kalika"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "alkhathlanBalancingActEvaluating2024",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making using rankings— prevalent in areas from hiring and bail to university admissions— raises concerns of potential bias. In this paper, we explore the alignment between people’s perceptions of fairness and two popular fairness metrics designed for rankings. In a crowdsourced experiment with 480 participants, people rated the perceived fairness of a hypothetical scholarship distribution scenario. Results suggest a strong inclination towards relying on explicit score values. There is also evidence of people’s preference for one fairness metric, NDKL, over the other metric, ARP. Qualitative results paint a more complex picture: some participants endorse meritocratic award schemes and express concerns about fairness metrics being used to modify rankings; while other participants acknowledge socio-economic factors in score-based rankings as justification for adjusting rankings. In summary, we find that operationalizing algorithmic fairness in practice is a balancing act between mitigating harms towards marginalized groups and societal conventions of leveraging traditional performance scores such as grades in decision-making contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659018",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 31\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1940–1970",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing act: Evaluating people’s perceptions of fair ranking metrics",
		"URL": "https://doi.org/10.1145/3630106.3659018",
		"author": [
			{
				"family": "Alkhathlan",
				"given": "Mallak"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Shrestha",
				"given": "Hilson"
			},
			{
				"family": "Harrison",
				"given": "Lane"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "choksiEmergingArtifactsCentralized2024",
		"type": "paper-conference",
		"abstract": "In 2022, generative model based coding assistants became widely available with the public release of GitHub Copilot. Approaches to generative coding are often critiqued within the context of advances in machine learning. We argue that tools such as Copilot are better understood when contextualized against technologies derived from the same communities and datasets. Our work traces the historical and ideological origins of free and open source code and characterizes the process of centralization. We examine three case studies —Dependabot, Crater, and Copilot— to compare the engineering, social, and legal qualities of technical artifacts derived from shared community-based labor. Our analysis focuses on the implications these artifacts create for infrastructural dependencies, community adoption, and intellectual property. Reframing generative coding assistants through a set of peer technologies broadens considerations for academics and policymakers beyond machine learning, to include the ways technical artifacts are derived from communities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659019",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1971–1983",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The emerging artifacts of centralized open-code",
		"URL": "https://doi.org/10.1145/3630106.3659019",
		"author": [
			{
				"family": "Choksi",
				"given": "Madiha Zahrah"
			},
			{
				"family": "Mandel",
				"given": "Ilan"
			},
			{
				"family": "Widder",
				"given": "David"
			},
			{
				"family": "Shvartzshnaider",
				"given": "Yan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "zezulkaFairDistributionPredictions2024",
		"type": "paper-conference",
		"abstract": "Deploying an algorithmically informed policy is a significant intervention in society. Prominent methods for algorithmic fairness focus on the distribution of predictions at the time of training, rather than the distribution of social goods that arises after deploying the algorithm in a specific social context. However, requiring a ‘fair’ distribution of predictions may undermine efforts at establishing a fair distribution of social goods. First, we argue that addressing this problem requires a notion of prospective fairness that anticipates the change in the distribution of social goods after deployment. Second, we provide formal conditions under which this change is identified from pre-deployment data. That requires accounting for different kinds of performative effects. Here, we focus on the way predictions change policy decisions and, consequently, the causally downstream distribution of social goods. Throughout, we are guided by an application from public administration: the use of algorithms to predict who among the recently unemployed will remain unemployed in the long term and to target them with labor market programs. Third, using administrative data from the Swiss public employment service, we simulate how such algorithmically informed policies would affect gender inequalities in long-term unemployment. When risk predictions are required to be ‘fair’ according to statistical parity and equality of opportunity, targeting decisions are less effective, undermining efforts to both lower overall levels of long-term unemployment and to close the gender gap in long-term unemployment.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659020",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 23\npublisher-place: Rio de Janeiro, Brazil",
		"page": "1984–2006",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From the fair distribution of predictions to the fair distribution of social goods: Evaluating the impact of fair machine learning on long-term unemployment",
		"URL": "https://doi.org/10.1145/3630106.3659020",
		"author": [
			{
				"family": "Zezulka",
				"given": "Sebastian"
			},
			{
				"family": "Genin",
				"given": "Konstantin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "davaniDisentanglingPerceptionsOffensiveness2024",
		"type": "paper-conference",
		"abstract": "Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers’ socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators’ judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants’ moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659021",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2007–2021",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disentangling perceptions of offensiveness: Cultural and moral correlates",
		"URL": "https://doi.org/10.1145/3630106.3659021",
		"author": [
			{
				"family": "Davani",
				"given": "Aida"
			},
			{
				"family": "Dı́az",
				"given": "Mark"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "luPerceptionsPolicingSurveillance2024",
		"type": "paper-conference",
		"abstract": "In Detroit, the largest Black-majority city in the United States, municipal authorities have deployed an array of surveillance technologies with the promise of containing crime and improving community safety. This article draws from a cross-sectional survey of over two thousand Detroit residents and multi-year community-based fieldwork in Detroit’s Eastside to examine local perceptions of policing surveillance technologies. Our survey reveals that respondents, notably those in more vulnerable positions, report higher perceived safety levels with policing surveillance cameras in their neighborhoods. However, when triangulating these results with insights from our fieldwork, we argue that these survey findings should not be taken as public support for surveillance. Alongside this seeming buy-in is a widely shared “better than nothing” imaginary among residents from impacted communities. “Better than nothing,” for the residents, is a pragmatic compromise and maneuver between being aware of the inherent flaws of surveillance technologies and settling for any available resource or hope. This notion of “better than nothing” unveils residents’ prolonged wait for digital justice and institutional accountability, which we show is where racialized infrastructural harm and exploitation are enacted along the temporal dimension. Our findings offer practical insights for counter-surveillance advocacy efforts.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659022",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2022–2032",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptions of Policing Surveillance Technologies in Detroit: Moving Beyond \"Better than Nothing\"",
		"URL": "https://doi.org/10.1145/3630106.3659022",
		"author": [
			{
				"family": "Lu",
				"given": "Alex Jiahong"
			},
			{
				"family": "Moy",
				"given": "Cameron"
			},
			{
				"family": "Ackerman",
				"given": "Mark S."
			},
			{
				"family": "Morenoff",
				"given": "Jeffrey"
			},
			{
				"family": "Dillahunt",
				"given": "Tawanna R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dotanResponsibleAdoptionGenerative2024",
		"type": "paper-conference",
		"abstract": "This paper proposes an approach to the responsible adoption of generative AI in higher education, employing a “points to consider” approach that is sensitive to the goals, values, and structural features of higher education. Higher education's ethos of collaborative faculty governance, pedagogical and research goals, and embrace of academic freedom conflict, the paper argues, with centralized top-down approaches to governing AI that are common in the private sector. The paper is based on a semester-long effort at the University of Pittsburgh which gathered and organized perspectives on generative AI in higher education through a collaborative, iterative, interdisciplinary process that included recurring group discussions, three standalone focus groups, and an informal survey. The paper presents insights drawn from this effort—that give rise to the “points to consider” approach the paper develops. These insights include the benefits and risks of potential uses of generative AI In higher education, as well as barriers to its adoption, and culminate in the six normative points to consider when adopting and governing generative AI in institutions of higher education.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659023",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2033–2046",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Responsible adoption of generative AI in higher education: Developing a “points to consider” approach based on faculty perspectives",
		"URL": "https://doi.org/10.1145/3630106.3659023",
		"author": [
			{
				"family": "Dotan",
				"given": "Ravit"
			},
			{
				"family": "Parker",
				"given": "Lisa S."
			},
			{
				"family": "Radzilowicz",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "calviUnfairSidePrivacy2024",
		"type": "paper-conference",
		"abstract": "Data sharing in the European Union (EU) has gained new momentum, among others for machine learning (ML) and artificial intelligence (AI) training purposes. By enabling models’ training whilst preserving the privacy of data, Privacy Enhancing Technologies (PETs) have therefore gained popularity, especially among policymakers. So far, computer science research has focused on advancing state-of-the-art privacy engineering and exploring trade-offs between privacy and accuracy. Meanwhile, legal scholarship began investigating the challenges arising therefrom. Yet, few works have delved into the fairness implications of PETs. Further research is essential to both prevent the propagation of bias and discrimination and to limit the accumulation of market power within very few economic entities suitable to undermine fair competition and consumer rights. In our work, we will address this knowledge gap by adopting a legal and computer science point of view. After scoping our understanding of possible unfair sides of PETs based on technical and socio-legal understandings of fairness (Section 2), we provide an overview of PETs mostly relevant for ML and AI training (Section 3). We then discuss fairness-related challenges arising from their use (Section 4) and we suggest possible technical and regulatory (e.g., impact assessment, new rights) solutions to address the shortcomings identified (Section 5). We finally provide conclusions and ideas for future research (Section 6).",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659024",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2047–2059",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The unfair side of Privacy Enhancing Technologies: addressing the trade-offs between PETs and fairness",
		"URL": "https://doi.org/10.1145/3630106.3659024",
		"author": [
			{
				"family": "Calvi",
				"given": "Alessandra"
			},
			{
				"family": "Malgieri",
				"given": "Gianclaudio"
			},
			{
				"family": "Kotzinos",
				"given": "Dimitris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "weertsNeutralityFallacyWhen2024",
		"type": "paper-conference",
		"abstract": "Various metrics and interventions have been developed to identify and mitigate unfair outputs of machine learning systems. While individuals and organizations have an obligation to avoid discrimination, the use of fairness-aware machine learning interventions has also been described as amounting to ‘algorithmic positive action’ under European Union (EU) non-discrimination law. As the Court of Justice of the European Union has been strict when it comes to assessing the lawfulness of positive action, this would impose a significant legal burden on those wishing to implement fair-ml interventions. In this paper, we propose that algorithmic fairness interventions often should be interpreted as a means to prevent discrimination, rather than a measure of positive action. Specifically, we suggest that this category mistake can often be attributed to neutrality fallacies: faulty assumptions regarding the neutrality of (fairness-aware) algorithmic decision-making. Our findings raise the question of whether a negative obligation to refrain from discrimination is sufficient in the context of algorithmic decision-making. Consequently, we suggest moving away from a duty to ‘not do harm’ towards a positive obligation to actively ‘do no harm’ as a more adequate framework for algorithmic decision-making and fair ml-interventions.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659025",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2060–2070",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The neutrality fallacy: When algorithmic fairness interventions are (not) positive action",
		"URL": "https://doi.org/10.1145/3630106.3659025",
		"author": [
			{
				"family": "Weerts",
				"given": "Hilde"
			},
			{
				"family": "Xenidis",
				"given": "Raphaële"
			},
			{
				"family": "Tarissan",
				"given": "Fabien"
			},
			{
				"family": "Olsen",
				"given": "Henrik Palmer"
			},
			{
				"family": "Pechenizkiy",
				"given": "Mykola"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "kieslichMyFutureMy2024",
		"type": "paper-conference",
		"abstract": "As a general purpose technology without a concrete pre-defined purpose, personal chatbots can be used for a whole range of objectives, depending on the personal needs, contexts, and tasks of an individual, and so potentially impact a variety of values, people, and social contexts. Traditional methods of risk assessment are confronted with several challenges: the lack of a clearly defined technology purpose, the lack of clearly defined values to orient on, the heterogeneity of uses, and the difficulty of actively engaging citizens themselves in anticipating impacts from the perspective of their individual lived realities. In this article, we leverage scenario writing at scale as a method for anticipating AI impact that is responsive to these challenges. The advantages of the scenario method are its ability to engage individual users and stimulate them to consider how chatbots are likely to affect their reality and so collect different impact scenarios depending on the cultural and societal embedding of a heterogeneous citizenship. Empirically, we tasked 106 US-based participants to write short fictional stories about the future impact (whether desirable or undesirable) of AI-based personal chatbots on individuals and society and, in addition, ask respondents to explain why these impacts are important and how they relate to their values. In the analysis process, we map those impacts and analyze them in relation to socio-demographic as well as AI-related attitudes of the scenario writers. We show that our method is effective in (1) identifying and mapping desirable and undesirable impacts of AI-based personal chatbots, (2) setting these impacts in relation to values that are important for individuals, and (3) detecting socio-demographic and AI-attitude related differences of impact anticipation.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659026",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2071–2085",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "My future with my chatbot: a scenario-driven, user-centric approach to anticipating AI impacts",
		"URL": "https://doi.org/10.1145/3630106.3659026",
		"author": [
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Helberger",
				"given": "Natali"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "slaughterImpactIBuyingMore2024",
		"type": "paper-conference",
		"abstract": "Instant buyers (iBuyers)—companies that buy and sell homes based on automated valuation models (AVMs)—now hold more than 5% market share in some USA cities. In this work, we investigate the fairness of iBuyers by constructing a dataset that links racial demographics from voter records with detailed property information on over 50,000 real estate transactions. Using Bayesian hierarchical modeling we find that: 1. iBuyers Decrease the Racial Sales Price Gap Between Black and White Home Sellers. Controlling for over 50 property features we find that iBuyers reduce the racial price gap that otherwise exists between homes sold by Black and White homeowners. This is not, however, a result of equity achieved through proprietary AVMs, but rather a result of both Black and White homeowners being similarly disadvantaged by iBuyers’ low purchase prices; and, 2. iBuyers Increase Property Conversion Rates from Individual to Institutional Ownership. We trace iBuyers’ purchases as well as their follow-on sales of homes in Mecklenburg County. In doing so, we show that iBuyers increase the rate at which properties are converted from being individually owned to institutionally owned. The eventual purchasers of iBuyer homes include national and international rental companies that have been tied to high eviction rates and poor property management. As with sale prices, we find that rather than reapportioning this social harm more equitably, iBuyers are simply increasing the rate at which homes bought from White homeowners are converted to institutional ownership. Ultimately, our analysis suggests that iBuyers are Equalizing Housing Outcomes by Extending Real Estate Harms Typically Isolated to Black Homeowners to White homeowners as Well.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659027",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2086–2100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The impact of iBuying is about more than just racial disparities: Evidence from mecklenburg county, NC",
		"URL": "https://doi.org/10.1145/3630106.3659027",
		"author": [
			{
				"family": "Slaughter",
				"given": "Isaac"
			},
			{
				"family": "Brown",
				"given": "Eva Maxfield"
			},
			{
				"family": "Weber",
				"given": "Nic"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "naharRegulatingExplainabilityMachine2024",
		"type": "paper-conference",
		"abstract": "With the rise of artificial intelligence (AI), concerns about AI applications causing unforeseen harms to safety, privacy, security, and fairness are intensifying. While attempts to create regulations are underway, with initiatives such as the EU AI Act and the 2023 White House executive order, skepticism abounds as to the efficacy of such regulations. This paper explores an interdisciplinary approach to designing policy for the explainability of AI applications, as the widely discussed \"right to explanation\" associated with the EU General Data Protection Regulation is ambiguous. To develop practical guidance for explainability, we conducted an experimental study that involved continuous collaboration among a team of researchers with AI and policy backgrounds over the course of ten weeks. The objective was to determine whether, through interdisciplinary effort, we can reach consensus on a policy for explainability in AI–one that is clearer, and more actionable and enforceable than current guidelines. We share nine observations, derived from an iterative policy design process, which included drafting the policy, attempting to comply with it (or circumvent it), and collectively evaluating its effectiveness on a weekly basis. Key observations include: iterative and continuous feedback was useful to improve policy drafts over time, discussing evidence of compliance was necessary during policy design, and human-subject studies were found to be an important form of evidence. We conclude with a note of optimism, arguing that meaningful policies can be achieved within a moderate time frame and with limited experience in policy design, as demonstrated by our student researchers on the team. This holds promising implications for policymakers, signaling that practical and effective regulation for AI applications is attainable.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659028",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2101–2112",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating explainability in machine learning applications – observations from a policy design experiment",
		"URL": "https://doi.org/10.1145/3630106.3659028",
		"author": [
			{
				"family": "Nahar",
				"given": "Nadia"
			},
			{
				"family": "Rowlett",
				"given": "Jenny"
			},
			{
				"family": "Bray",
				"given": "Matthew"
			},
			{
				"family": "Omar",
				"given": "Zahra Abba"
			},
			{
				"family": "Papademetris",
				"given": "Xenophon"
			},
			{
				"family": "Menon",
				"given": "Alka"
			},
			{
				"family": "Kästner",
				"given": "Christian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "wyllieFairnessFeedbackLoops2024",
		"type": "paper-conference",
		"abstract": "Model-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. This is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. When a model induces a distribution shift, it also encodes its mistakes, biases, and unfairnesses into the ground truth of its data ecosystem. We introduce a framework that allows us to track multiple MIDS over many generations, finding that they can lead to loss in performance, fairness, and minoritized group representation, even in initially unbiased datasets. Despite these negative consequences, we identify how models might be used for positive, intentional, interventions in their data ecosystems, providing redress for historical discrimination through a framework called algorithmic reparation (AR). We simulate AR interventions by curating representative training batches for stochastic gradient descent to demonstrate how AR can improve upon the unfairnesses of models and data ecosystems subject to other MIDS. Our work takes an important step towards identifying, mitigating, and taking accountability for the unfair feedback loops enabled by the idea that ML systems are inherently neutral and objective.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659029",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 35\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2113–2147",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness feedback loops: Training on synthetic data amplifies bias",
		"URL": "https://doi.org/10.1145/3630106.3659029",
		"author": [
			{
				"family": "Wyllie",
				"given": "Sierra"
			},
			{
				"family": "Shumailov",
				"given": "Ilia"
			},
			{
				"family": "Papernot",
				"given": "Nicolas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "rifatDataAnnotationMeaningmaking2024",
		"type": "paper-conference",
		"abstract": "Data annotation is a process of meaning-making and is inherently political. The literature on ethics in data-driven technologies explores these political aspects, primarily focusing on questions of bias and power. This paper argues that the politics of annotation often overemphasize secular and modern values and overlooks faith-based, religious, and spiritual aspects (FRS) in data annotation. This oversight particularly affects the postcolonial regions of the Global South, where FRS are intertwined with people’s everyday experiences and ethics. We conducted a focus group discussion and contextual inquiries with six annotators who annotated a faith-related “violence” dataset from South Asian YouTube content. Our analysis reveals that FRS blindness in data annotation manifests through the politics of achieving objectivity and the “scientific” process of meaning-making. Due to these goals, which are predominantly shaped by Western values, FRS sensitivities are overlooked from the initial stages of data curation through annotation, ultimately leading to a context collapse within the annotation process. Finally, we advocate for the adaptation of FRS sensitivities into the annotation process and data infrastructure, particularly when the dataset clearly pertains to FRS, to promote greater cultural and contextual inclusivity in annotation practices.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659030",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 9\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2148–2156",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data, annotation, and meaning-making: The politics of categorization in annotating a dataset of faith-based communal violence",
		"URL": "https://doi.org/10.1145/3630106.3659030",
		"author": [
			{
				"family": "Rifat",
				"given": "Mohammad Rashidujjaman"
			},
			{
				"family": "Safir",
				"given": "Abdullah Hasan"
			},
			{
				"family": "Saha",
				"given": "Sourav"
			},
			{
				"family": "Junaed",
				"given": "Jahedul Alam"
			},
			{
				"family": "Saleki",
				"given": "Maryam"
			},
			{
				"family": "Amin",
				"given": "Mohammad Ruhul"
			},
			{
				"family": "Ahmed",
				"given": "Syed Ishtiaque"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "schmittRoleExplainabilityCollaborative2024",
		"type": "paper-conference",
		"abstract": "Manual verification has become very challenging based on the increasing volume of information shared online and the role of generative Artificial Intelligence (AI). Thus, AI systems are used to identify disinformation and deep fakes online. Previous research has shown that superior performance can be observed when combining AI and human expertise. Moreover, according to the EU AI Act, human oversight is inevitable when using AI systems in a domain where fundamental human rights, such as the right to free expression, might be affected. Thus, AI systems need to be transparent and offer sufficient explanations to be comprehensible. Much research has been done on integrating eXplainability (XAI) features to increase the transparency of AI systems; however, they lack human-centered evaluation. Additionally, the meaningfulness of explanations varies depending on users’ background knowledge and individual factors. Thus, this research implements a human-centered evaluation schema to evaluate different XAI features for the collaborative human-AI disinformation detection task. Hereby, objective and subjective evaluation dimensions, such as performance, perceived usefulness, understandability, and trust in the AI system, are used to evaluate different XAI features. A user study was conducted with an overall total of 433 participants, whereas 406 crowdworkers and 27 journalists participated as experts in detecting disinformation. The results show that free-text explanations contribute to improving non-expert performance but do not influence the performance of experts. The XAI features increase the perceived usefulness, understandability, and trust in the AI system, but they can also lead crowdworkers to blindly trust the AI system when its predictions are wrong.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659031",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 18\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2157–2174",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role of explainability in collaborative human-AI disinformation detection",
		"URL": "https://doi.org/10.1145/3630106.3659031",
		"author": [
			{
				"family": "Schmitt",
				"given": "Vera"
			},
			{
				"family": "Villa-Arenas",
				"given": "Luis-Felipe"
			},
			{
				"family": "Feldhus",
				"given": "Nils"
			},
			{
				"family": "Meyer",
				"given": "Joachim"
			},
			{
				"family": "Spang",
				"given": "Robert P."
			},
			{
				"family": "Möller",
				"given": "Sebastian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sannemanInformationBottleneckCharacterization2024",
		"type": "paper-conference",
		"abstract": "Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject experiments. This empirical link between human factors and information-theoretic concepts provides an important mathematical characterization of the workload-understanding tradeoff which enables user-tailored XAI design.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659032",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 24\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2175–2198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An information bottleneck characterization of the understanding-workload tradeoff in human-centered explainable AI",
		"URL": "https://doi.org/10.1145/3630106.3659032",
		"author": [
			{
				"family": "Sanneman",
				"given": "Lindsay"
			},
			{
				"family": "Tucker",
				"given": "Mycal"
			},
			{
				"family": "Shah",
				"given": "Julie A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "baackCriticalAnalysisLargest2024",
		"type": "paper-conference",
		"abstract": "Common Crawl is the largest freely available collection of web crawl data and one of the most important sources of pre-training data for large language models (LLMs). It is used so frequently and makes up such large proportions of the overall pre-training data in many cases that it arguably has become a foundational building block for LLM development, and subsequently generative AI products built on top of LLMs. Despite its pivotal role, Common Crawl itself is not widely understood, nor is there much reflection evident among LLM builders about the implications of using Common Crawl's data. This paper discusses what Common Crawl's popularity for LLM development means for fairness, accountability, and transparency in generative AI by highlighting the organization's values and practices, as well as how it views its own role within the AI ecosystem. Our qualitative analysis is based on in-depth interviews with Common Crawl staffers and relevant online documents.After discussing Common Crawl's role in generative AI and how LLM builders have typically used its data for pre-training LLMs, we review Common Crawl's self-defined values and priorities and highlight the limitations and biases of its crawling process. We find that Common Crawl's popularity has contributed to making generative AI more transparent to scrutiny in many ways, and that it has enabled more LLM research and development to take place beyond well-resourced leading AI companies. At the same time, many LLM builders have used Common Crawl as a source for training data in ways that are problematic: for instance, with lack of care and transparency for how Common Crawl's massive crawl data was filtered for harmful content before the pre-training, often by relying on rudimentary automated filtering techniques. We offer recommendations for Common Crawl and LLM builders on how to improve fairness, accountability, and transparency in LLM research and development.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659033",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2199–2208",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A critical analysis of the largest source for generative AI training data: Common crawl",
		"URL": "https://doi.org/10.1145/3630106.3659033",
		"author": [
			{
				"family": "Baack",
				"given": "Stefan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "santiniSeeingOpacityLimitations2024",
		"type": "paper-conference",
		"abstract": "Digital platforms provide a deregulated and opaque environment suited to the maintenance of their business model, in which ads are efficiently served by opaque algorithms to meticulously profiled users based on their behavioral data. The advertising infrastructure provided by these platforms made advertising more segmented and scalable, creating new opportunities and allowing for a profit-oriented influence industry to develop worldwide. Some platforms have invested in transparency measures for digital advertising, but there is still a gap between what is applied in the Global South and the Global North. In Brazil, despite evidence of an online ecosystem of suspicious, inauthentic, scam, and other types of fraudulent ads, regulatory proposals have faced a hard opposition from tech companies. Against this backdrop, there is a need to evaluate advertising transparency archives currently offered by online platforms in Brazil as a means to measure the quality of libraries and the available data.Thus, the main objective of this work is to account for transparency measures and means of accessing data of some of the largest online platforms and search engines in the country, in order to establish a general comparative diagnosis of ad transparency in Brazil. Based on the platforms’ public documentation, policies and terms of use for the Brazilian market, we perform a comparative analysis of six companies: Meta, Google, Twitter/X, Telegram, TikTok, and Spotify. Particular consideration is given to whether these companies do or do not have ads repositories, or a means to assess the disseminated advertisements. In an environment of low transparency and difficulty in accessing data, we found that the Meta Ad Library, although providing very limited data, is the most reliable source for systematic investigations of the digital advertising ecosystem. Even though Google offers an advertisement repository in Brazil, it lags considerably behind that offered by Meta and imposes greater difficulty in carrying out systematic analyses. On the other hand, Telegram, TikTok, Twitter/X and Spotify do not present any advertising repository or transparency center in order to analyse the Brazilian scenario. Although the scenario in the Global South can be characterized by a lack of transparency from platforms and by difficulties in accessing data, recent measures implemented elsewhere have demonstrated that this condition is reversible.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659034",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2209–2221",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Seeing through opacity: The limitations of digital ad transparency in Brazil",
		"URL": "https://doi.org/10.1145/3630106.3659034",
		"author": [
			{
				"family": "Santini",
				"given": "Rose Marie"
			},
			{
				"family": "Salles",
				"given": "Débora"
			},
			{
				"family": "Martins",
				"given": "Bruno Maurı́cio"
			},
			{
				"family": "Moreira",
				"given": "Alékis"
			},
			{
				"family": "Haddad",
				"given": "João Gabriel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "toneyTrustIssuesDiscrepancies2024",
		"type": "paper-conference",
		"abstract": "How governments, practitioners, and researchers define artificial intelligence (AI) ethics significantly impacts the AI models and systems designed and deployed. Thus, the convergence of policy goals and technical approaches is necessary for international norms and standards on trustworthy AI. Defining, much less achieving trustworthy AI characteristics, however, entails clear communication through consensus on the meaning of field-specific terms. This paper presents an analysis of over 322,000 scientific research papers and the national documents from five countries (Australia, Canada, Japan, the United Kingdom, and the United States) on trustworthy AI in order to provide an in-depth review and comprehensive understanding of the similarities and differences between governments’ and researchers’ definitions and frameworks. While we identified substantive and relevant differences among policy documents and scientific research, the differences do not represent substantial disagreements among the common principles for trustworthy AI terms. Overall we found broad agreement across documents’ trustworthy AI term use, suggesting that nuanced differences could be overcome in an effort to create more global policies and aligned research.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659035",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2222–2233",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trust issues: Discrepancies in trustworthy AI keywords use in policy and research",
		"URL": "https://doi.org/10.1145/3630106.3659035",
		"author": [
			{
				"family": "Toney",
				"given": "Autumn"
			},
			{
				"family": "Curlee",
				"given": "Kathleen"
			},
			{
				"family": "Probasco",
				"given": "Emelia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "gomezAlgorithmicArbitrarinessContent2024",
		"type": "paper-conference",
		"abstract": "Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML model, but can nevertheless change what the model gets wrong. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as “toxic,” leading to arbitrary restrictions on speech. We use the principles set by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice to interpret the effects of these findings in terms of Human Rights. We analyze (i) the extent of predictive multiplicity among popular state-of-the-art LLMs used for detecting “toxic” content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) the magnitude of model multiplicity on content that is unanimously recognized as toxic by human annotators. Our findings indicate that the up-scaled algorithmic moderation risks legitimizing an “algorithmic leviathan”, where an algorithm disproportionately manages human rights. To mitigate such risks, our study underscores the need to identify and increase the transparency of arbitrariness in content moderation applications. Our findings have implications to content moderation and intermediary liability laws being discussed and passed in many countries, such as the Digital Services Act in the European Union, the Online Safety Act in the United Kingdom, and the recent TSE resolutions in Brazil.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659036",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2234–2253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic arbitrariness in content moderation",
		"URL": "https://doi.org/10.1145/3630106.3659036",
		"author": [
			{
				"family": "Gomez",
				"given": "Juan Felipe"
			},
			{
				"family": "Machado",
				"given": "Caio"
			},
			{
				"family": "Paes",
				"given": "Lucas Monteiro"
			},
			{
				"family": "Calmon",
				"given": "Flavio"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "casperBlackboxAccessInsufficient2024",
		"type": "paper-conference",
		"abstract": "External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system’s inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to training and deployment information (e.g., methodology, code, documentation, data, deployment details, findings from internal evaluations) allows auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659037",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 19\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2254–2272",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Black-box access is insufficient for rigorous AI audits",
		"URL": "https://doi.org/10.1145/3630106.3659037",
		"author": [
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Siegmann",
				"given": "Charlotte"
			},
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Curtis",
				"given": "Taylor Lynn"
			},
			{
				"family": "Bucknall",
				"given": "Benjamin"
			},
			{
				"family": "Haupt",
				"given": "Andreas"
			},
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Hobbhahn",
				"given": "Marius"
			},
			{
				"family": "Sharkey",
				"given": "Lee"
			},
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Von Hagen",
				"given": "Marvin"
			},
			{
				"family": "Alberti",
				"given": "Silas"
			},
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Sun",
				"given": "Qinyi"
			},
			{
				"family": "Gerovitch",
				"given": "Michael"
			},
			{
				"family": "Bau",
				"given": "David"
			},
			{
				"family": "Tegmark",
				"given": "Max"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "ullsteinAttitudesFacialAnalysis2024",
		"type": "paper-conference",
		"abstract": "Computer vision AI systems present one of the most radical technical transformations of our time. Such systems are given unparalleled epistemic power to impose meaning on visual data, despite their inherent semantic ambiguity. This epistemic power is particularly evident in computer vision AI that interprets the meaning of human faces. The goal of this work is to empirically document laypeople’s perceptions of the epistemic and ethical complexity of computer vision AI through a large-scale qualitative study with participants in Argentina, Japan, Kenya, and the USA (N=4,468). We developed a vignette scenario about a fictitious company that analyzes people’s portraits using computer vision AI to make a variety of inferences about people based on their faces. For each inference that the fictitious company draws (e.g., age, skin color, intelligence), we ask participants from all countries to reason about how they evaluate computer vision AI inference-making. In a series of workshops, we collaborated as a multinational research team to develop a codebook that captures people’s different justifications of facial analysis AI inferences to create a comprehensive justification portfolio. Our study reveals similarities in justification patterns, but also significant intra-country and inter-country diversity in response to different facial inferences. For example, participants from Argentina, Japan, Kenya, and the USA vastly disagree over the reasonableness of AI classifications such as beautiful or skin color. They tend to agree in their opposition to AI-drawn inferences intelligence and trustworthiness. Adding much-needed non-Western perspectives to debates on computer vision ethics, our results suggest that, contrary to popular justifications for facial classification technologies, there is no such thing as a “common sense” facial classification that accords simply with a general, homogeneous “human intuition.”",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659038",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 29\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2273–2301",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Attitudes toward facial analysis AI: a cross-national study comparing argentina, kenya, japan, and the USA",
		"URL": "https://doi.org/10.1145/3630106.3659038",
		"author": [
			{
				"family": "Ullstein",
				"given": "Chiara"
			},
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Ikkatai",
				"given": "Yuko"
			},
			{
				"family": "Arnez-Jordan",
				"given": "Naira Paola"
			},
			{
				"family": "Caleno",
				"given": "Rose"
			},
			{
				"family": "Mboya",
				"given": "Brian"
			},
			{
				"family": "Higuma",
				"given": "Shuichiro"
			},
			{
				"family": "Hartwig",
				"given": "Tilman"
			},
			{
				"family": "Yokoyama",
				"given": "Hiromi"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "moayeriEmbracingDiversityInterpretable2024",
		"type": "paper-conference",
		"abstract": "Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today’s best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms — from diced to whole, on a table or in a bowl — yet standard VLM classifiers map all instances of a class to a single vector based on the class label. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity—leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659039",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 20\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2302–2321",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Embracing diversity: Interpretable zero-shot classification beyond one vector per class",
		"URL": "https://doi.org/10.1145/3630106.3659039",
		"author": [
			{
				"family": "Moayeri",
				"given": "Mazda"
			},
			{
				"family": "Rabbat",
				"given": "Michael"
			},
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "inieAIProbabilisticAutomation2024",
		"type": "paper-conference",
		"abstract": "In this paper we investigate how people’s level of trust (as reported through self-assessment) in so-called “AI” (artificial intelligence) is influenced by anthropomorphizing language in system descriptions. Building on prior work, we define four categories of anthropomorphization (1. Properties of a cognizer, 2. Agency, 3. Biological metaphors, and 4. Properties of a communicator). We use a survey-based approach (n=954) to investigate whether participants are likely to trust one of two (fictitious) “AI” systems by randomly assigning people to see either an anthropomorphized or a de-anthropomorphized description of the systems. We find that participants are no more likely to trust anthropomorphized over de-anthropmorphized product descriptions overall. The type of product or system in combination with different anthropomorphic categories appears to exert greater influence on trust than anthropomorphizing language alone, and age is the only demographic factor that significantly correlates with people’s preference for anthropomorphized or de-anthropomorphized descriptions. When elaborating on their choices, participants highlight factors such as lesser of two evils, lower or higher stakes contexts, and human favoritism as driving motivations when choosing between product A and B, irrespective of whether they saw an anthropomorphized or a de-anthropomorphized description of the product. Our results suggest that “anthropomorphism” in “AI” descriptions is an aggregate concept that may influence different groups differently, and provide nuance to the discussion of whether anthropomorphization leads to higher trust and over-reliance by the general public in systems sold as “AI”.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659040",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 26\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2322–2347",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From \"AI\" to probabilistic automation: How does anthropomorphization of technical systems descriptions influence trust?",
		"URL": "https://doi.org/10.1145/3630106.3659040",
		"author": [
			{
				"family": "Inie",
				"given": "Nanna"
			},
			{
				"family": "Druga",
				"given": "Stefania"
			},
			{
				"family": "Zukerman",
				"given": "Peter"
			},
			{
				"family": "Bender",
				"given": "Emily M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "imanaAuditingRacialDiscrimination2024",
		"type": "paper-conference",
		"abstract": "Digital ads on social-media platforms play an important role in shaping access to economic opportunities. Our work proposes and implements a new third-party auditing method that can evaluate racial bias in the delivery of ads for education opportunities. Third-party auditing is important because it allows external parties to demonstrate presence or absence of bias in social-media algorithms. Education is a domain with legal protections against discrimination and concerns of racial-targeting, but bias induced by ad delivery algorithms has not been previously explored in this domain. Prior audits demonstrated discrimination in platforms’ delivery of ads to users for housing and employment ads. These audit findings supported legal action that prompted Meta to change their ad-delivery algorithms to reduce bias, but only in the domains of housing, employment, and credit. In this work, we propose a new methodology that allows us to measure racial discrimination in a platform’s ad delivery algorithms for education ads. We apply our method to Meta using ads for real schools and observe the results of delivery. We find evidence of racial discrimination in Meta’s algorithmic delivery of ads for education opportunities, posing legal and ethical concerns. Our results extend evidence of algorithmic discrimination to the education domain, showing that current bias mitigation mechanisms are narrow in scope, and suggesting a broader role for third-party auditing of social media in areas where ensuring non-discrimination is important.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659041",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2348–2361",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing for racial discrimination in the delivery of education ads",
		"URL": "https://doi.org/10.1145/3630106.3659041",
		"author": [
			{
				"family": "Imana",
				"given": "Basileal"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			},
			{
				"family": "Heidemann",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "lamparthAnalyzingEditingInner2024",
		"type": "paper-conference",
		"abstract": "Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor robustness of large language models by locally constraining individual modules during fine-tuning on potentially poisonous data sets. Trigger warning: Offensive language.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659042",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 12\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2362–2373",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analyzing and editing inner mechanisms of backdoored language models",
		"URL": "https://doi.org/10.1145/3630106.3659042",
		"author": [
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Reuel",
				"given": "Anka"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mhasawadeUnderstandingDisparitiesPost2024",
		"type": "paper-conference",
		"abstract": "Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across “race” and “gender” as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network models that are better able to capture the underlying functional form in comparison to linear models. We also observe consistent findings regarding the effect of concept shift and omitted variable bias on explanation disparities in the Adult income dataset. Overall, results indicate that disparities in model explanations can also depend on data and model properties. Based on this systematic investigation, we provide recommendations for the design of explanation methods that mitigate undesirable disparities.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659043",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 15\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2374–2388",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding disparities in post hoc machine learning explanation",
		"URL": "https://doi.org/10.1145/3630106.3659043",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Rahman",
				"given": "Salman"
			},
			{
				"family": "Haskell-Craig",
				"given": "Zoé"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "smithRecommendMeDesigning2024",
		"type": "paper-conference",
		"abstract": "Fairness metrics have become a useful tool to measure how fair or unfair a machine learning system may be for its stakeholders. In the context of recommender systems, previous research has explored how various stakeholders experience algorithmic fairness or unfairness, but it is also important to capture these experiences in the design of fairness metrics. Therefore, we conducted four focus groups with providers (those whose items, content, or profiles are being recommended) of two different domains: content creators and dating app users. We explored how our participants experience unfairness on their associated platforms, and worked with them to co-design fairness goals, definitions, and metrics that might capture these experiences. This work represents an important step towards designing fairness metrics with the stakeholders who will be impacted by their operationalizations. We analyze the efficacy and challenges of enacting these metrics in practice and explore how future work might benefit from this methodology.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659044",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2389–2399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Recommend me? Designing fairness metrics with providers",
		"URL": "https://doi.org/10.1145/3630106.3659044",
		"author": [
			{
				"family": "Smith",
				"given": "Jessie J."
			},
			{
				"family": "Satwani",
				"given": "Aishwarya"
			},
			{
				"family": "Burke",
				"given": "Robin"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "engelmannVisionsDisciplineAnalyzing2024",
		"type": "paper-conference",
		"abstract": "Education plays an indispensable role in fostering societal well-being and is widely regarded as one of the most influential factors in shaping the future of generations to come. As artificial intelligence (AI) becomes more deeply integrated into our daily lives and the workforce, educational institutions at all levels are directing their focus on resources that cater to AI education. Yet, informal education, including online learning on social media platforms like YouTube, plays an increasingly significant role for both students and the general public. Offering greater accessibility compared to formal education, millions of individuals use YouTube for educational resources on AI today. Due to the substantial societal impact of AI, it is crucial for introductory AI courses to meaningfully address the ethical implications associated with AI. Our work investigates the current landscape of introductory AI courses on YouTube, and the potential for introducing ethics in this context. We qualitatively analyze the 20 most watched introductory AI courses on YouTube, coding a total of 92.2 hours of educational content viewed by close to 50 million people. We find that these introductory AI courses do not meaningfully engage with ethical or societal challenges of AI (RQ1). When defining and framing AI, introductory AI courses foreground excitement around AI’s transformative role in society, over-exaggerate AI’s current and future abilities, and anthropomorphize AI (RQ2). In teaching AI, we see a widespread reliance on corporate AI tools and frameworks as well as a prioritization on a hands-on approach to learning rather than on conceptual foundations (RQ3). In promoting key AI practices, introductory AI courses abstract away entirely the socio-technical nature of AI classification and prediction, for example by favoring data quantity over data quality (RQ4). Given the power of openly available introductory courses to shape enduring beliefs around AI and its field at the onset of a learning journey, we extend our analysis with recommendations that aim to integrate ethical reflections into introductory AI courses. We recommend that introductory AI courses should (1) highlight ethical challenges of AI to present a more balanced perspective, (2) raise ethical issues explicitly relevant to the technical concepts discussed and (3) nurture a sense of accountability in future AI developers.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659045",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 21\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2400–2420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Visions of a discipline: Analyzing introductory AI courses on YouTube",
		"URL": "https://doi.org/10.1145/3630106.3659045",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Choksi",
				"given": "Madiha Zahrah"
			},
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "babaeiDriversPersuasiveStrategies2024",
		"type": "paper-conference",
		"abstract": "The proliferation of e-commerce, game, and social networking sites, has brought to light the use of \"dark patterns\" or, more generally, manipulative designs (MDs), which exploit psychological effects and cognitive biases of users to channel their behavior toward outcomes that benefit the company or owner of the site, against the users’ best interests. Previous research has categorized MDs, assessed their impact on users, gauged their prevalence, and attempted automated detection using computer vision and natural language processing techniques. However, limited attention has been given to understanding how to warn and educate users about MDs, guiding them to recognize and resist such manipulative tactics. To address this gap, we carried out a controlled study with n=134 participants, using a survey based on the Protection Motivation Theory (PMT) to better understand the motivations of people to learn about MDs. We also explored the effectiveness of two persuasive strategies, based on Cialdini’s principles of influence (social influence and authority), to trigger attention towards MDs and intention to learn more about MDs and to avoid them. For this, we created a simulated application in a mobile app distribution platform modeled like Google Play Store containing a visual signal, a warning based on one of the two strategies, and simulated reviews from other users. The results indicate that two of the five PMT constructs - a higher Perceived Severity of MDs and a lower Perceived Response Cost of learning about MDs - have the most significant influence on the Intention to learn more about MDs. The participants in the experimental group, exposed to the two persuasive strategies exhibited a larger increase in their intention to seek information about MDs than the participants in the control group. Our study showcases the potential of a persuasive intervention, illustrating how mobile app distribution platforms can enhance user protection against MD exploitation. By implementing such interventions, these platforms can boost accountability and transparency of applications existing on their platform, and MD awareness among their users.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659046",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2421–2431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Drivers and persuasive strategies to influence user intention to learn about manipulative design",
		"URL": "https://doi.org/10.1145/3630106.3659046",
		"author": [
			{
				"family": "Babaei",
				"given": "Pooria"
			},
			{
				"family": "Vassileva",
				"given": "Julita"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "eyubogluModelChangeListsCharacterizing2024",
		"type": "paper-conference",
		"abstract": "Updates to Machine Learning as a Service (MLaaS) APIs may affect downstream systems that depend on their predictions. However, performance changes introduced by these updates are poorly documented by providers and seldom studied in the literature. As a result, API producers and consumers are left wondering: do model updates introduce performance changes that could adversely affect users’ system? Ideally, producers and consumers would have access to a detailed ChangeList specifying the slices of data where model performance has improved and degraded since the update. But, producing a ChangeList is challenging because it requires (1) discovering slices in the absence of detailed annotations or metadata, (2) accurately attributing coherent concepts to the discovered slices, and (3) communicating them to the user in a digestable manner. In this work, we demonstrate, discuss, and critique one approach for building, verifying, and releasing ChangeLists that aims to address these challenges. Using this approach, we analyze six real-world MLaaS API updates including GPT-3 and Google Cloud Vision. We produce a prototype ChangeList for each, identifying over 100 coherent data slices on which the model’s performance changed significantly. Notably, we find 63 instances where an update improves performance globally, but hurts performance on a coherent slice – a phenomenon not previously documented at scale in the literature. Finally, with diverse participants from industry, we conduct a think-aloud user study that explores the importance of releasing ChangeLists and highlights the strengths and weaknesses of our approach. This serves to validate some parts of our approach and uncover important areas for future work.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659047",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 22\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2432–2453",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model ChangeLists: Characterizing updates to ML models",
		"URL": "https://doi.org/10.1145/3630106.3659047",
		"author": [
			{
				"family": "Eyuboglu",
				"given": "Sabri"
			},
			{
				"family": "Goel",
				"given": "Karan"
			},
			{
				"family": "Desai",
				"given": "Arjun"
			},
			{
				"family": "Chen",
				"given": "Lingjiao"
			},
			{
				"family": "Monfort",
				"given": "Mathew"
			},
			{
				"family": "Ré",
				"given": "Chris"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "cheongAIAmNot2024",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore when and why LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (“cases”) allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts’ recommendations for LLM response strategies, which center around helping users identify ‘right questions to ask’ and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659048",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 16\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2454–2469",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(A)I am not a lawyer, but...: Engaging legal experts towards responsible LLM policies for legal advice",
		"URL": "https://doi.org/10.1145/3630106.3659048",
		"author": [
			{
				"family": "Cheong",
				"given": "Inyoung"
			},
			{
				"family": "Xia",
				"given": "King"
			},
			{
				"family": "Feng",
				"given": "K. J. Kevin"
			},
			{
				"family": "Chen",
				"given": "Quan Ze"
			},
			{
				"family": "Zhang",
				"given": "Amy X."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "vanbovenTransformingDutchDebiasing2024",
		"type": "paper-conference",
		"abstract": "Gender-neutral pronouns are increasingly being introduced across Western languages. Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals. This paper examines a Dutch coreference resolution system’s performance on gender-neutral pronouns, specifically hen and die. In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English. We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation. Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like lea, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns. Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts. Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns. We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used. This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659049",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2470–2483",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transforming dutch: Debiasing dutch coreference resolution systems for non-binary pronouns",
		"URL": "https://doi.org/10.1145/3630106.3659049",
		"author": [
			{
				"family": "Boven",
				"given": "Goya",
				"non-dropping-particle": "van"
			},
			{
				"family": "Du",
				"given": "Yupei"
			},
			{
				"family": "Nguyen",
				"given": "Dong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "mickelRacialEthnicCategories2024",
		"type": "paper-conference",
		"abstract": "Racial diversity has become increasingly discussed within the AI and algorithmic fairness literature, yet little attention is focused on justifying the choices of racial categories and understanding how people are racialized into these chosen racial categories. Even less attention is given to how racial categories shift and how the racialization process changes depending on the context of a dataset or model. An unclear understanding of who comprises the racial categories chosen and how people are racialized into these categories can lead to varying interpretations of these categories. These varying interpretations can lead to harm when the understanding of racial categories and the racialization process is misaligned from the actual racialization process and racial categories used. Harm can also arise if the racialization process and racial categories used are irrelevant or do not exist in the context they are applied. In this paper, we make two contributions. First, we demonstrate how racial categories with unclear assumptions and little justification can lead to varying datasets that poorly represent groups obfuscated or unrepresented by the given racial categories and models that perform poorly on these groups. Second, we develop a framework, CIRCSheets, for documenting the choices and assumptions in choosing racial categories and the process of racialization into these categories to facilitate transparency in understanding the processes and assumptions made by dataset or model developers when selecting or using these racial categories.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659050",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 11\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2484–2494",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial/ethnic categories in AI and algorithmic fairness: Why they matter and what they represent",
		"URL": "https://doi.org/10.1145/3630106.3659050",
		"author": [
			{
				"family": "Mickel",
				"given": "Jennifer"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "sterzQuestEffectivenessHuman2024",
		"type": "paper-conference",
		"abstract": "Human oversight is currently discussed as a potential safeguard to counter some of the negative aspects of high-risk AI applications. This prompts a critical examination of the role and conditions necessary for what is prominently termed effective or meaningful human oversight of these systems. This paper investigates effective human oversight by synthesizing insights from psychological, legal, philosophical, and technical domains. Based on the claim that the main objective of human oversight is risk mitigation, we propose a viable understanding of effectiveness in human oversight: for human oversight to be effective, the oversight person has to have (a) sufficient causal power with regard to the system and its effects, (b) suitable epistemic access to relevant aspects of the situation, (c) self-control, and (d) fitting intentions for their role. Furthermore, we argue that this is equivalent to saying that an oversight person is effective if and only if they are morally responsible and have fitting intentions. Against this backdrop, we suggest facilitators and inhibitors of effectiveness in human oversight when striving for practical applicability. We discuss factors in three domains, namely, the technical design of the system, individual factors of oversight persons, and the environmental circumstances in which they operate. Finally, this paper scrutinizes the upcoming AI Act of the European Union – in particular Article 14 on Human Oversight – as an exemplary regulatory framework in which we study the practicality of our understanding of effective human oversight. By analyzing the provisions and implications of the European AI Act proposal, we pinpoint how far that proposal aligns with our analyses regarding effective human oversight as well as how it might get enriched by our conceptual understanding of effectiveness in human oversight.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3659051",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 13\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2495–2507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the quest for effectiveness in human oversight: Interdisciplinary perspectives",
		"URL": "https://doi.org/10.1145/3630106.3659051",
		"author": [
			{
				"family": "Sterz",
				"given": "Sarah"
			},
			{
				"family": "Baum",
				"given": "Kevin"
			},
			{
				"family": "Biewer",
				"given": "Sebastian"
			},
			{
				"family": "Hermanns",
				"given": "Holger"
			},
			{
				"family": "Lauber-Rönsberg",
				"given": "Anne"
			},
			{
				"family": "Meinel",
				"given": "Philip"
			},
			{
				"family": "Langer",
				"given": "Markus"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "dinhLearningFairRanking2024",
		"type": "paper-conference",
		"abstract": "Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3661932",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 10\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2508–2517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fair ranking policies via differentiable optimization of ordered weighted averages",
		"URL": "https://doi.org/10.1145/3630106.3661932",
		"author": [
			{
				"family": "Dinh",
				"given": "My H"
			},
			{
				"family": "Kotary",
				"given": "James"
			},
			{
				"family": "Fioretto",
				"given": "Ferdinando"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "leeOneVsMany2024",
		"type": "paper-conference",
		"abstract": "As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study (N = 252) in which participants were given one or more LLM-generated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants’ perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.",
		"collection-title": "FAccT '24",
		"container-title": "Proceedings of the 2024 ACM conference on fairness, accountability, and transparency",
		"DOI": "10.1145/3630106.3662681",
		"event-place": "New York, NY, USA",
		"ISBN": "979-8-4007-0450-5",
		"note": "number-of-pages: 14\npublisher-place: Rio de Janeiro, Brazil",
		"page": "2518–2531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "One vs. many: Comprehending accurate information from multiple erroneous and inconsistent AI generations",
		"URL": "https://doi.org/10.1145/3630106.3662681",
		"author": [
			{
				"family": "Lee",
				"given": "Yoonjoo"
			},
			{
				"family": "Son",
				"given": "Kihoon"
			},
			{
				"family": "Kim",
				"given": "Tae Soo"
			},
			{
				"family": "Kim",
				"given": "Jisu"
			},
			{
				"family": "Chung",
				"given": "John Joon Young"
			},
			{
				"family": "Adar",
				"given": "Eytan"
			},
			{
				"family": "Kim",
				"given": "Juho"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	}
]