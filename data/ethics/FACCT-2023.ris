TY  - CONF
TI  - Potential for Discrimination in Online Targeted Advertising
AU  - Speicher, Till
AU  - Ali, Muhammad
AU  - Venkatadri, Giridhari
AU  - Ribeiro, Filipe Nunes
AU  - Arvanitakis, George
AU  - Benevenuto, Fabrício
AU  - Gummadi, Krishna P.
AU  - Loiseau, Patrick
AU  - Mislove, Alan
T2  - Conference on Fairness, Accountability and Transparency
AB  - Recently, online targeted advertising platforms like Facebook have been criticized for allowing advertisers to discriminate against users belonging to sensitive groups, i.e., to exclude users belonging to a certain race or gender from receiving their ads. Such criticisms have led, for instance, Facebook to disallow the use of attributes such as ethnic affinity from being used by advertisers when targeting ads related to housing or employment or financial services. In this paper, we show that such measures are far from sufficient and that the problem of discrimination in targeted advertising is much more pernicious.  We argue that discrimination measures should be based on the targeted population and not on the attributes used for targeting. We systematically investigate the different targeting methods offered by Facebook for their ability to enable discriminatory advertising.  We show that a malicious advertiser can create highly discriminatory ads without using sensitive attributes. Our findings call for exploring fundamentally new methods for mitigating discrimination in online targeted advertising.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 5
EP  - 19
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/speicher18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf
ER  - 

TY  - CONF
TI  - Discrimination in Online Advertising: A Multidisciplinary Inquiry
AU  - Datta, Amit
AU  - Datta, Anupam
AU  - Makagon, Jael
AU  - Mulligan, Deirdre K.
AU  - Tschantz, Michael Carl
T2  - Conference on Fairness, Accountability and Transparency
AB  - We explore ways in which discrimination may arise in the targeting of job-related advertising, noting the potential for multiple parties to contribute to its occurrence.  We then examine the statutes and case law interpreting the prohibition on advertisements that indicate a preference based on protected class, and consider its application to online advertising.  We focus on its interaction with Section 230 of the Communications Decency Act, which provides interactive computer services with immunity for providing access to  information created by a third party.  We argue that such services can lose that immunity if they target ads toward or away from protected classes without explicit instructions from advertisers to do so.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 20
EP  - 34
LA  - en
PB  - PMLR
ST  - Discrimination in Online Advertising
UR  - https://proceedings.mlr.press/v81/datta18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/datta18a/datta18a.pdf
L1  - http://proceedings.mlr.press/v81/datta18a/datta18a-supp.pdf
ER  - 

TY  - CONF
TI  - Privacy for All: Ensuring Fair and Equitable Privacy Protections
AU  - Ekstrand, Michael D.
AU  - Joshaghani, Rezvan
AU  - Mehrpouyan, Hoda
T2  - Conference on Fairness, Accountability and Transparency
AB  - In this position paper, we argue for applying recent research on ensuring sociotechnical systems are fair and non-discriminatory to the privacy protections those systems may provide. Privacy literature seldom considers whether a proposed privacy scheme protects all persons uniformly, irrespective of membership in protected classes or particular risk in the face of privacy failure. Just as algorithmic decision-making systems may have discriminatory outcomes even without explicit or deliberate discrimination, so also privacy regimes may disproportionately fail to protect vulnerable members of their target population, resulting in disparate impact with respect to the effectiveness of privacy protections.We propose a research agenda that will illuminate this issue, along with related issues in the intersection of fairness and privacy, and present case studies that show how the outcomes of this research may change existing privacy and fairness research. We believe it is important to ensure that technologies and policies intended to protect the users and subjects of information systems provide such protection in an equitable fashion.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 35
EP  - 47
LA  - en
PB  - PMLR
ST  - Privacy for All
UR  - https://proceedings.mlr.press/v81/ekstrand18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/ekstrand18a/ekstrand18a.pdf
ER  - 

TY  - CONF
TI  - “Meaningful Information” and the Right to Explanation
AU  - Selbst, Andrew
AU  - Powles, Julia
T2  - Conference on Fairness, Accountability and Transparency
AB  - There is no single, neat statutory provision labeled the “right to explanation” in Europe’s new General Data Protection Regulation (GDPR). But nor is such a right illusory. Responding to two prominent papers that, in turn, conjure and critique the right to explanation in the context of automated decision-making, we advocate a return to the text of the GDPR. Articles 13–15 provide rights to “meaningful information about the logic involved” in automated decisions. This is a right to explanation, whether one uses the phrase or not. The right to explanation should be interpreted functionally, flexibly, and should, at a minimum, enable a data subject to exercise his or her rights under the GDPR and human rights law.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 48
EP  - 48
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/selbst18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/selbst18a/selbst18a.pdf
ER  - 

TY  - CONF
TI  - Interpretable Active Learning
AU  - Phillips, Richard
AU  - Chang, Kyu Hyun
AU  - Friedler, Sorelle A.
T2  - Conference on Fairness, Accountability and Transparency
AB  - Active learning has long been a topic of study in machine learning. However, as increasingly complex and opaque models have become standard practice, the process of active learning, too, has become more opaque. There has been little investigation into interpreting what specific trends and patterns an active learning strategy may be exploring. This work expands on the Local Interpretable Model-agnostic Explanations framework (LIME) to provide explanations for active learning recommendations. We demonstrate how LIME can be used to generate locally faithful explanations for an active learning strategy, and how these explanations can be used to understand how different models and datasets explore a problem space over time. These explanations can also be used to generate batches based on common sources of uncertainty. These regions of common uncertainty can be useful for understanding a model’s current weaknesses.  In order to quantify the per-subgroup differences in how an active learning strategy queries spatial regions, we introduce a notion of uncertainty bias (based on disparate impact) to measure the discrepancy in the confidence for a model’s predictions between one subgroup and another.  Using the uncertainty bias measure, we show that our query explanations accurately reflect the subgroup focus of the active learning queries, allowing for an interpretable explanation of what is being learned as points with similar sources of uncertainty have their uncertainty bias resolved. We demonstrate that this technique can be applied to track uncertainty bias over user-defined clusters or automatically generated clusters based on the source of uncertainty. We also measure how the choice of initial labeled examples effects groups over time.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 49
EP  - 61
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/phillips18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/phillips18a/phillips18a.pdf
ER  - 

TY  - CONF
TI  - Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment
AU  - Barabas, Chelsea
AU  - Virza, Madars
AU  - Dinakar, Karthik
AU  - Ito, Joichi
AU  - Zittrain, Jonathan
T2  - Conference on Fairness, Accountability and Transparency
AB  - Actuarial risk assessments are frequently touted as a neutral way to counteract implicit bias and increase the fairness of decisions made at almost every juncture of the criminal justice system, from pretrial release to sentencing, parole and probation. In recent times these assessments have come under increased scrutiny, as critics claim that the statistical techniques underlying them might reproduce existing patterns of discrimination and historical biases that are reflected in the data. Much of this debate is centered around competing notions of fairness and predictive accuracy, which seek to problematize the use of variables that act as “proxies” for protected classes, such as race and gender. However, these debates fail to address the core ethical issue at hand - that current risk assessments are ill-equipped to support ethical punishment and rehabilitation practices in the criminal justice system, because they offer only a limited insight into the underlying drivers of criminal behavior. In this paper, we examine the prevailing paradigms of fairness currently under debate and propose an alternative methodology for identifying the underlying social and structural factors that drive criminal behavior. We argue that the core ethical debate surrounding the use of regression in risk assessments is not one of bias or accuracy. Rather, it’s one of purpose. If machine learning is operationalized merely in the service of predicting future crime, then it becomes difficult to break cycles of criminalization that are driven by the iatrogenic effects of the criminal justice system itself. We posit that machine learning should not be used for prediction, rather it should be used to surface covariates that are fed into a causal model for understanding the social, structural and psychological drivers of crime. We propose an alternative application of machine learning and causal inference away from predicting risk scores to risk mitigation.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 62
EP  - 76
LA  - en
PB  - PMLR
ST  - Interventions over Predictions
UR  - https://proceedings.mlr.press/v81/barabas18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/barabas18a/barabas18a.pdf
ER  - 

TY  - CONF
TI  - Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification
AU  - Buolamwini, Joy
AU  - Gebru, Timnit
T2  - Conference on Fairness, Accountability and Transparency
AB  - Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 77
EP  - 91
LA  - en
PB  - PMLR
ST  - Gender Shades
UR  - https://proceedings.mlr.press/v81/buolamwini18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf
L1  - http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a-supp.pdf
ER  - 

TY  - CONF
TI  - Analyze, Detect and Remove Gender Stereotyping from Bollywood Movies
AU  - Madaan, Nishtha
AU  - Mehta, Sameep
AU  - Agrawaal, Taneea
AU  - Malhotra, Vrinda
AU  - Aggarwal, Aditi
AU  - Gupta, Yatin
AU  - Saxena, Mayank
T2  - Conference on Fairness, Accountability and Transparency
AB  - The presence of gender stereotypes in many aspects of society is a well-known phenomenon. In this paper, we focus on studying such stereotypes and bias in Hindi movie industry (\it Bollywood) and propose an algorithm to remove these stereotypes from text. We analyze movie plots and posters for all movies released since 1970. The gender bias is detected by semantic modeling of plots at sentence and intra-sentence level. Different features like occupation, introductions, associated actions and descriptions are captured to show the pervasiveness of gender bias and stereotype in movies. Using the derived semantic graph, we compute centrality of each character and observe similar bias there. We also show that such bias is not applicable for movie posters where females get equal importance even though their character has little or no impact on the movie plot. The silver lining is that our system was able to identify 30 movies over last 3 years where such stereotypes were broken. The next step, is to generate debiased stories. The proposed debiasing algorithm extracts gender biased graphs from unstructured piece of text in stories from movies and de-bias these graphs to generate plausible unbiased stories.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 92
EP  - 105
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/madaan18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/madaan18a/madaan18a.pdf
ER  - 

TY  - CONF
TI  - Mixed Messages? The Limits of Automated Social Media Content Analysis
AU  - Duarte, Natasha
AU  - Llanso, Emma
AU  - Loup, Anna
T2  - Conference on Fairness, Accountability and Transparency
AB  - Governments and companies are turning to automated tools to make sense of what people post on social media. Policymakers routinely call for social media companies to identify and take down hate speech, terrorist propaganda, harassment, “fake news” or disinformation. Other policy proposals have focused on mining social media to inform law enforcement and immigration decisions. But these proposals wrongly assume that automated technology can accomplish on a large scale the kind of nuanced analysis that humans can do on a small scale. Today’s tools for analyzing social media text have limited ability to parse the meaning of human communication or detect the intent of the speaker.  A knowledge gap exists between data scientists studying natural language processing (NLP) and policymakers advocating for wide adoption of automated social media analysis and moderation. Policymakers must understand the capabilities and limits of NLP before endorsing or adopting automated content analysis tools, particularly for making decisions that affect fundamental rights or access to government benefits. Without proper safeguards, these tools can facilitate overbroad censorship and biased enforcement of laws or terms of service.  This paper draws on existing research to explain the capabilities and limitations of text classifiers for social media posts and other online content. It is aimed at helping researchers and technical experts address the gaps in policymakers’ knowledge about what is possible with automated text analysis.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 106
EP  - 106
LA  - en
PB  - PMLR
ST  - Mixed Messages?
UR  - https://proceedings.mlr.press/v81/duarte18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/duarte18a/duarte18a.pdf
ER  - 

TY  - CONF
TI  - The cost of fairness in binary classification
AU  - Menon, Aditya Krishna
AU  - Williamson, Robert C.
T2  - Conference on Fairness, Accountability and Transparency
AB  - Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 107
EP  - 118
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/menon18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/menon18a/menon18a.pdf
L1  - http://proceedings.mlr.press/v81/menon18a/menon18a-supp.pdf
ER  - 

TY  - CONF
TI  - Decoupled Classifiers for Group-Fair and Efficient Machine Learning
AU  - Dwork, Cynthia
AU  - Immorlica, Nicole
AU  - Kalai, Adam Tauman
AU  - Leiserson, Max
T2  - Conference on Fairness, Accountability and Transparency
AB  - When it is ethical and legal to use a sensitive attribute (such as gender or race) in machine learning systems, the question remains how to do so. We show that the naive application of machine learning algorithms using sensitive attributes leads to an inherent tradeoff in accuracy between groups. We provide a simple and efficient decoupling technique, that can be added on top of any black-box machine learning algorithm, to learn different classifiers for different groups. Transfer learning is used to mitigate the problem of having too little data on any one group.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 119
EP  - 133
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/dwork18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/dwork18a/dwork18a.pdf
ER  - 

TY  - CONF
TI  - A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions
AU  - Chouldechova, Alexandra
AU  - Benavides-Prado, Diana
AU  - Fialko, Oleksandr
AU  - Vaithianathan, Rhema
T2  - Conference on Fairness, Accountability and Transparency
AB  - Every year there are more than 3.6 million referrals made to child protection agencies across the US. The practice of screening calls is left to each jurisdiction to follow local practices and policies, potentially leading to large variation in the way in which referrals are treated across the country. Whilst increasing access to linked administrative data is available, it is difficult for welfare workers to make systematic use of historical information about all the children and adults on a single referral call. Risk prediction models that use routinely collected administrative data can help call workers to better identify cases that are likely to result in adverse outcomes. However, the use of predictive analytics in the area of child welfare is contentious. There is a possibility that some communities—such as those in poverty or from particular racial and ethnic groups—will be disadvantaged by the reliance on government administrative data. On the other hand, these analytics tools can augment or replace human judgments, which themselves are biased and imperfect. In this paper we describe our work on developing, validating, fairness auditing, and deploying a risk prediction model in Allegheny County, Pennsylvania, USA. We discuss the results of our analysis to-date, and also highlight key problems and data bias issues that present challenges for model evaluation and deployment.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 134
EP  - 148
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/chouldechova18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a.pdf
L1  - http://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a-supp.pdf
ER  - 

TY  - CONF
TI  - Fairness in Machine Learning: Lessons from Political Philosophy
AU  - Binns, Reuben
T2  - Conference on Fairness, Accountability and Transparency
AB  - What does it mean for a machine learning model to be ‘fair’, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise ‘fairness’ in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 149
EP  - 159
LA  - en
PB  - PMLR
ST  - Fairness in Machine Learning
UR  - https://proceedings.mlr.press/v81/binns18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/binns18a/binns18a.pdf
ER  - 

TY  - CONF
TI  - Runaway Feedback Loops in Predictive Policing
AU  - Ensign, Danielle
AU  - Friedler, Sorelle A.
AU  - Neville, Scott
AU  - Scheidegger, Carlos
AU  - Venkatasubramanian, Suresh
T2  - Conference on Fairness, Accountability and Transparency
AB  - Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been shown susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.  In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.   Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which reported incidents of crime (those reported by residents) and discovered incidents of crime (i.e those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 160
EP  - 171
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/ensign18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/ensign18a/ensign18a.pdf
ER  - 

TY  - CONF
TI  - All The Cool Kids, How Do They Fit In?: Popularity and Demographic Biases in Recommender Evaluation and Effectiveness
AU  - Ekstrand, Michael D.
AU  - Tian, Mucun
AU  - Azpiazu, Ion Madrazo
AU  - Ekstrand, Jennifer D.
AU  - Anuyah, Oghenemaro
AU  - McNeill, David
AU  - Pera, Maria Soledad
T2  - Conference on Fairness, Accountability and Transparency
AB  - In the research literature, evaluations of recommender system effectiveness typically report results over a given data set, providing an aggregate measure of effectiveness over each instance (e.g. user) in the data set. Recent advances in information retrieval evaluation, however, demonstrate the importance of considering the distribution of effectiveness across diverse groups of varying sizes. For example, do users of different ages or genders obtain similar utility from the system, particularly if their group is a relatively small subset of the user base? We apply this consideration to recommender systems, using offline evaluation and a utility-based metric of recommendation effectiveness to explore whether different user demographic groups experience similar recommendation accuracy. We find demographic differences in measured recommender effectiveness across two data sets containing different types of feedback in different domains; these differences sometimes, but not always, correlate with the size of the user group in question. Demographic effects also have a complex—and likely detrimental—interaction with popularity bias, a known deficiency of recommender evaluation. These results demonstrate the need for recommender system evaluation protocols that explicitly quantify the degree to which the system is meeting the information needs of all its users, as well as the need for researchers and operators to move beyond naïve evaluations that favor the needs of larger subsets of the user population while ignoring smaller subsets.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 172
EP  - 186
LA  - en
PB  - PMLR
ST  - All The Cool Kids, How Do They Fit In?
UR  - https://proceedings.mlr.press/v81/ekstrand18b.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/ekstrand18b/ekstrand18b.pdf
ER  - 

TY  - CONF
TI  - Recommendation Independence
AU  - Kamishima, Toshihiro
AU  - Akaho, Shotaro
AU  - Asoh, Hideki
AU  - Sakuma, Jun
T2  - Conference on Fairness, Accountability and Transparency
AB  - This paper studies a recommendation algorithm whose outcomes are not influenced by specified information. It is useful in contexts potentially unfair decision should be avoided, such as job-applicant recommendations that are not influenced by socially sensitive information. An algorithm that could exclude the influence of sensitive information would thus be useful for job-matching with fairness. We call the condition between a recommendation outcome and a sensitive feature Recommendation Independence, which is formally defined as statistical independence between the outcome and the feature. Our previous independence-enhanced algorithms simply matched the means of predictions between sub-datasets consisting of the same sensitive value. However, this approach could not remove the sensitive information represented by the second or higher moments of distributions. In this paper, we develop new methods that can deal with the second moment, i.e., variance, of recommendation outcomes without increasing the computational complexity. These methods can more strictly remove the sensitive information, and experimental results demonstrate that our new algorithms can more effectively eliminate the factors that undermine fairness. Additionally, we explore potential applications for independence-enhanced recommendation, and discuss its relation to other concepts, such as recommendation diversity.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 187
EP  - 201
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/kamishima18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/kamishima18a/kamishima18a.pdf
L1  - http://proceedings.mlr.press/v81/kamishima18a/kamishima18a-supp.pdf
ER  - 

TY  - CONF
TI  - Balanced Neighborhoods for Multi-sided Fairness in Recommendation
AU  - Burke, Robin
AU  - Sonboli, Nasim
AU  - Ordonez-Gauger, Aldo
T2  - Conference on Fairness, Accountability and Transparency
AB  - Fairness has emerged as an important category of analysis for machine learning systems in some application areas. In extending the concept of fairness to recommender systems, there is an essential tension between the goals of fairness and those of personalization. However, there are contexts in which  equity across recommendation outcomes is a desirable goal. It is also the case that in some applications fairness may be a multisided concept, in which the impacts on multiple groups of individuals must be considered. In this paper, we examine two different cases of fairness-aware recommender systems: consumer-centered and provider-centered. We  explore the concept of a balanced neighborhood as a mechanism to preserve personalization in recommendation while enhancing the fairness of recommendation outcomes. We show that a modified version of the Sparse Linear Method (SLIM) can be used to improve the balance of user and item neighborhoods, with the result of achieving greater outcome fairness in real-world datasets with minimal loss in ranking performance.
C3  - Proceedings of the 1st Conference on Fairness, Accountability and Transparency
DA  - 2018/01/21/
PY  - 2018
DP  - proceedings.mlr.press
SP  - 202
EP  - 214
LA  - en
PB  - PMLR
UR  - https://proceedings.mlr.press/v81/burke18a.html
Y2  - 2024/03/20/13:15:13
L1  - http://proceedings.mlr.press/v81/burke18a/burke18a.pdf
ER  - 

TY  - CONF
TI  - Controlling polarization in personalization: An algorithmic framework
AU  - Celis, L. Elisa
AU  - Kapoor, Sayash
AU  - Salehi, Farnood
AU  - Vishnoi, Nisheeth
T3  - FAT* '19
AB  - Personalization is pervasive in the online space as it leads to higher efficiency for the user and higher revenue for the platform by individualizing the most relevant content for each user. However, recent studies suggest that such personalization can learn and propagate systemic biases and polarize opinions; this has led to calls for regulatory mechanisms and algorithms that are constrained to combat bias and the resulting echo-chamber effect. We propose a versatile framework that allows for the possibility to reduce polarization in personalized systems by allowing the user to constrain the distribution from which content is selected. We then present a scalable algorithm with provable guarantees that satisfies the given constraints on the types of the content that can be displayed to a user, but – subject to these constraints – will continue to learn and personalize the content in order to maximize utility. We illustrate this framework on a curated dataset of online news articles that are conservative or liberal, show that it can control polarization, and examine the trade-off between decreasing polarization and the resulting loss to revenue. We further exhibit the flexibility and scalability of our approach by framing the problem in terms of the more general diverse content selection problem and test it empirically on both a News dataset and the MovieLens dataset.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287601
SP  - 160
EP  - 169
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287601
KW  - recommender systems
KW  - Personalization
KW  - bandit optimization
KW  - diversification
KW  - group fairness
KW  - polarization
ER  - 

TY  - CONF
TI  - 50 years of test (un)fairness: Lessons for machine learning
AU  - Hutchinson, Ben
AU  - Mitchell, Margaret
T3  - FAT* '19
AB  - Quantitative definitions of what is unfair and what is fair have been introduced in multiple disciplines for well over 50 years, including in education, hiring, and machine learning. We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287600
SP  - 49
EP  - 58
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287600
KW  - fairness
KW  - history
KW  - ML fairness
KW  - psychometrics
KW  - test fairness
ER  - 

TY  - CONF
TI  - From fair decision making to social equality
AU  - Mouzannar, Hussein
AU  - Ohannessian, Mesrob I.
AU  - Srebro, Nathan
T3  - FAT* '19
AB  - The study of fairness in intelligent decision systems has mostly ignored long-term influence on the underlying population. Yet fairness considerations (e.g. affirmative action) have often the implicit goal of achieving balance among groups within the population. The most basic notion of balance is eventual equality between the qualifications of the groups. How can we incorporate influence dynamics in decision making? How well do dynamics-oblivious fairness policies fare in terms of reaching equality? In this paper, we propose a simple yet revealing model that encompasses (1) a selection process where an institution chooses from multiple groups according to their qualifications so as to maximize an institutional utility and (2) dynamics that govern the evolution of the groups' qualifications according to the imposed policies. We focus on demographic parity as the formalism of affirmative action.We first give conditions under which an unconstrained policy reaches equality on its own. In this case, surprisingly, imposing demographic parity may break equality. When it doesn't, one would expect the additional constraint to reduce utility, however, we show that utility may in fact increase. In real world scenarios, unconstrained policies do not lead to equality. In such cases, we show that although imposing demographic parity may remedy it, there is a danger that groups settle at a worse set of qualifications. As a silver lining, we also identify when the constraint not only leads to equality, but also improves all groups. These cases and trade-offs are instrumental in determining when and how imposing demographic parity can be beneficial in selection processes, both for the institution and for society on the long run.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287599
SP  - 359
EP  - 368
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287599
KW  - fairness
KW  - dynamics
KW  - affirmative action
KW  - demographic parity
KW  - influence on society
KW  - selection processes
KW  - social equality
ER  - 

TY  - CONF
TI  - Fairness and abstraction in sociotechnical systems
AU  - Selbst, Andrew D.
AU  - Boyd, Danah
AU  - Friedler, Sorelle A.
AU  - Venkatasubramanian, Suresh
AU  - Vertesi, Janet
T3  - FAT* '19
AB  - A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science—such as abstraction and modular design—are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287598
SP  - 59
EP  - 68
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287598
KW  - Fairness-aware Machine Learning
KW  - Interdisciplinary
KW  - Sociotechnical Systems
ER  - 

TY  - CONF
TI  - The disparate effects of strategic manipulation
AU  - Hu, Lily
AU  - Immorlica, Nicole
AU  - Vaughan, Jennifer Wortman
T3  - FAT* '19
AB  - When consequential decisions are informed by algorithmic input, individuals may feel compelled to alter their behavior in order to gain a system's approval. Models of agent responsiveness, termed "strategic manipulation," analyze the interaction between a learner and agents in a world where all agents are equally able to manipulate their features in an attempt to "trick" a published classifier. In cases of real world classification, however, an agent's ability to adapt to an algorithm is not simply a function of her personal interest in receiving a positive classification, but is bound up in a complex web of social factors that affect her ability to pursue certain action responses. In this paper, we adapt models of strategic manipulation to capture dynamics that may arise in a setting of social inequality wherein candidate groups face different costs to manipulation. We find that whenever one group's costs are higher than the other's, the learner's equilibrium strategy exhibits an inequality-reinforcing phenomenon wherein the learner erroneously admits some members of the advantaged group, while erroneously excluding some members of the disadvantaged group. We also consider the effects of interventions in which a learner subsidizes members of the disadvantaged group, lowering their costs in order to improve her own classification performance. Here we encounter a paradoxical result: there exist cases in which providing a subsidy improves only the learner's utility while actually making both candidate groups worse-off—even the group receiving the subsidy. Our results reveal the potentially adverse social ramifications of deploying tools that attempt to evaluate an individual's "quality" when agents' capacities to adaptively respond differ.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287597
SP  - 259
EP  - 268
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287597
KW  - fairness in machine learning
KW  - strategic classification
ER  - 

TY  - CONF
TI  - Model cards for model reporting
AU  - Mitchell, Margaret
AU  - Wu, Simone
AU  - Zaldivar, Andrew
AU  - Barnes, Parker
AU  - Vasserman, Lucy
AU  - Hutchinson, Ben
AU  - Spitzer, Elena
AU  - Raji, Inioluwa Deborah
AU  - Gebru, Timnit
T3  - FAT* '19
AB  - Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287596
SP  - 220
EP  - 229
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287596
KW  - documentation
KW  - datasheets
KW  - disaggregated evaluation
KW  - ethical considerations
KW  - fairness evaluation
KW  - ML model evaluation
KW  - model cards
ER  - 

TY  - CONF
TI  - Deep weighted averaging classifiers
AU  - Card, Dallas
AU  - Zhang, Michael
AU  - Smith, Noah A.
T3  - FAT* '19
AB  - Recent advances in deep learning have achieved impressive gains in classification accuracy on a variety of types of data, including images and text. Despite these gains, however, concerns have been raised about the calibration, robustness, and interpretability of these models. In this paper we propose a simple way to modify any conventional deep architecture to automatically provide more transparent explanations for classification decisions, as well as an intuitive notion of the credibility of each prediction. Specifically, we draw on ideas from nonparametric kernel regression, and propose to predict labels based on a weighted sum of training instances, where the weights are determined by distance in a learned instance-embedding space. Working within the framework of conformal methods, we propose a new measure of nonconformity suggested by our model, and experimentally validate the accompanying theoretical expectations, demonstrating improved transparency, controlled error rates, and robustness to out-of-domain data, without compromising on accuracy or calibration.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287595
SP  - 369
EP  - 378
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287595
KW  - conformal methods
KW  - interpretability credibility
ER  - 

TY  - CONF
TI  - Fairness under unawareness: Assessing disparity when protected class is unobserved
AU  - Chen, Jiahao
AU  - Kallus, Nathan
AU  - Mao, Xiaojie
AU  - Svacha, Geoffry
AU  - Udell, Madeleine
T3  - FAT* '19
AB  - Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classification, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confirm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287594
SP  - 339
EP  - 348
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287594
KW  - disparate impact
KW  - Bayesian Improved Surname Geocoding
KW  - fair lending
KW  - probablistic proxy model
KW  - protected class
KW  - race imputation
KW  - racial discrimination
ER  - 

TY  - CONF
TI  - Dissecting racial bias in an algorithm that guides health decisions for 70 million people
AU  - Obermeyer, Ziad
AU  - Mullainathan, Sendhil
T3  - FAT* '19
AB  - A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care.To determine which patients will have complex future health care needs, and thus benefit from program enrollment, many systems rely on an algorithmically generated commercial risk score. In this paper, we exploit a rich dataset to study racial bias in a commercial algorithm that is deployed nationwide today in many of the US's most prominent Accountable Care Organizations (ACOs).We document significant racial bias in this widely used algorithm, using data on primary care patients at a large hospital. Blacks and whites with the same algorithmic risk scores have very different realized health. For example, the highest-risk black patients (those at the threshold where patients are auto-enrolled in the program), have significantly more chronic illnesses than white enrollees with the same risk score. We use detailed physiological data to show the pervasiveness of the bias: across a range of biomarkers, from HbA1c levels for diabetics to blood pressure control for hypertensives, we find significant racial health gaps conditional on risk score. This bias has significant material consequences for patients: it effectively means that white patients with the same health as black patients are far more likely be enrolled in the care management program, and benefit from its resources. If we simulated a world without this gap in predictions, blacks would be auto-enrolled into the program at more than double the current rate.An unusual aspect of our dataset is that we observe not just the risk scores but also the input data and objective function used to construct it. This provides a unique window into the mechanisms by which bias arises. The algorithm is given a data frame with (1) Yit (label), total medical expenditures ('costs') in year t; and (2) Xi,t–1 (features), fine-grained care utilization data in year t – 1 (e.g., visits to cardiologists, number of x-rays, etc.). The algorithm's predicted risk of developing complex health needs is thus in fact predicted costs. And by this metric, one could easily call the algorithm unbiased: costs are very similar for black and white patients with the same risk scores. So far, this is inconsistent with algorithmic bias: conditional on risk score, predictions do not favor whites or blacks.The fundamental problem we uncover is that when thinking about 'health care needs,' hospitals and insurers focus on costs. They use an algorithm whose specific objective is cost prediction, and from this perspective, predictions are accurate and unbiased. Yet from the social perspective, actual health – not just costs – also matters. This is where the problem arises: costs are not the same as health. While costs are a reasonable proxy for health (the sick do cost more, on average), they are an imperfect one: factors other than health can drive cost – for example, race.We find that blacks cost more than whites on average; but this gap can be decomposed into two countervailing effects. First, blacks bear a different and larger burden of disease, making them costlier. But this difference in illness is offset by a second factor: blacks cost less, holding constant their exact chronic conditions, a force that dramatically reduces the overall cost gap. Perversely, the fact that blacks cost less than whites conditional on health means an algorithm that predicts costs accurately across racial groups will necessarily also generate biased predictions on health.The root cause of this bias is not in the procedure for prediction, or the underlying data, but the algorithm's objective function itself. This bias is akin to, but distinct from, 'mis-measured labels': it arises here from the choice of labels, not their measurement, which is in turn a consequence of the differing objective functions of private actors in the health sector and society. From the private perspective, the variable they focus on – cost – is being appropriately optimized. But our results hint at how algorithms may amplify a fundamental problem in health care as a whole: externalities produced when health care providers focus too narrowly on financial motives, optimizing on costs to the detriment of health. In this sense, our results suggest that a pervasive problem in health care – incentives that induce health systems to focus on dollars rather than health – also has consequences for the way algorithms are built and monitored.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287593
SP  - 89
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287593
KW  - medicine
KW  - health policy
KW  - algorithms
KW  - bias
KW  - racial disparities
ER  - 

TY  - CONF
TI  - An empirical study of rich subgroup fairness for machine learning
AU  - Kearns, Michael
AU  - Neel, Seth
AU  - Roth, Aaron
AU  - Wu, Zhiwei Steven
T3  - FAT* '19
AB  - Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287592
SP  - 100
EP  - 109
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287592
KW  - Algorithmic Bias
KW  - Fair Classification
KW  - Fairness Auditing
KW  - Subgroup Fairness
ER  - 

TY  - CONF
TI  - Robot eyes wide shut: Understanding dishonest anthropomorphism
AU  - Leong, Brenda
AU  - Selinger, Evan
T3  - FAT* '19
AB  - The goal of this paper is to advance design, policy, and ethics scholarship on how engineers and regulators can protect consumers from deceptive robots and artificial intelligences that exhibit the problem of dishonest anthropomorphism. The analysis expands upon ideas surrounding the principle of honest anthropomorphism originally formulated by Margot Kaminsky, Mathew Ruben, William D. Smart, and Cindy M. Grimm in their groundbreaking Maryland Law Review article, "Averting Robot Eyes." Applying boundary management theory and philosophical insights into prediction and perception, we create a new taxonomy that identifies fundamental types of dishonest anthropomorphism and pinpoints harms that they can cause. To demonstrate how the taxonomy can be applied as well as clarify the scope of the problems that it can cover, we critically consider a representative series of ethical issues, proposals, and questions concerning whether the principle of honest anthropomorphism has been violated.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287591
SP  - 299
EP  - 308
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287591
KW  - Artificial Intelligence
KW  - Robots
KW  - Machine Learning
KW  - ethics
KW  - anthropomorphism
ER  - 

TY  - CONF
TI  - On human predictions with explanations and predictions of machine learning models: A case study on deception detection
AU  - Lai, Vivian
AU  - Tan, Chenhao
T3  - FAT* '19
AB  - Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (&gt;20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287590
SP  - 29
EP  - 38
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287590
KW  - explanations
KW  - human agency
KW  - human performance
KW  - predictions
ER  - 

TY  - CONF
TI  - A comparative study of fairness-enhancing interventions in machine learning
AU  - Friedler, Sorelle A.
AU  - Scheidegger, Carlos
AU  - Venkatasubramanian, Suresh
AU  - Choudhary, Sonam
AU  - Hamilton, Evan P.
AU  - Roth, Derek
T3  - FAT* '19
AB  - Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption.We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287589
SP  - 329
EP  - 338
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287589
KW  - benchmarks
KW  - Fairness-aware machine learning
ER  - 

TY  - CONF
TI  - Fairness-aware programming
AU  - Albarghouthi, Aws
AU  - Vinitsky, Samuel
T3  - FAT* '19
AB  - Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.We present a rich and general specification language that allows a programmer to specify a range of fairness definitions from the literature, as well as others. As the decision-making program executes, the runtime maintains statistics on the decisions made and incrementally checks whether the fairness definitions have been violated, reporting such violations to the developer. The advantages of this approach are two fold: (i) Enabling declarative mathematical specifications of fairness in the programming language simplifies the process of checking fairness, as the programmer does not have to write ad hoc code for maintaining statistics. (ii) Compared to existing techniques for checking and ensuring fairness, our approach monitors a decision-making program in the wild, which may be running on a distribution that is unlike the dataset on which a classifier was trained and tested.We describe an implementation of our proposed methodology as a library in the Python programming language and illustrate its use on case studies from the algorithmic fairness literature.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287588
SP  - 211
EP  - 219
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287588
KW  - Fairness
KW  - Assertion languages
KW  - Probabilistic specifications
KW  - Runtime monitoring
KW  - Runtime verification
ER  - 

TY  - CONF
TI  - A taxonomy of ethical tensions in inferring mental health states from social media
AU  - Chancellor, Stevie
AU  - Birnbaum, Michael L.
AU  - Caine, Eric D.
AU  - Silenzio, Vincent M. B.
AU  - De Choudhury, Munmun
T3  - FAT* '19
AB  - Powered by machine learning techniques, social media provides an unobtrusive lens into individual behaviors, emotions, and psychological states. Recent research has successfully employed social media data to predict mental health states of individuals, ranging from the presence and severity of mental disorders like depression to the risk of suicide. These algorithmic inferences hold great potential in supporting early detection and treatment of mental disorders and in the design of interventions. At the same time, the outcomes of this research can pose great risks to individuals, such as issues of incorrect, opaque algorithmic predictions, involvement of bad or unaccountable actors, and potential biases from intentional or inadvertent misuse of insights. Amplifying these tensions, there are also divergent and sometimes inconsistent methodological gaps and under-explored ethics and privacy dimensions. This paper presents a taxonomy of these concerns and ethical challenges, drawing from existing literature, and poses questions to be resolved as this research gains traction. We identify three areas of tension: ethics committees and the gap of social media research; questions of validity, data, and machine learning; and implications of this research for key stakeholders. We conclude with calls to action to begin resolving these interdisciplinary dilemmas.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287587
SP  - 79
EP  - 88
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287587
KW  - machine learning
KW  - ethics
KW  - mental health
KW  - social media
KW  - algorithms
ER  - 

TY  - CONF
TI  - Classification with fairness constraints: A meta-algorithm with provable guarantees
AU  - Celis, L. Elisa
AU  - Huang, Lingxiao
AU  - Keswani, Vijay
AU  - Vishnoi, Nisheeth K.
T3  - FAT* '19
AB  - Developing classification algorithms that are fair with respect to sensitive attributes of the data is an important problem due to the increased deployment of classification algorithms in societal contexts. Several recent works have focused on studying classification with respect to specific fairness metrics, modeled the corresponding fair classification problem as constrained optimization problems, and developed tailored algorithms to solve them. Despite this, there still remain important metrics for which there are no fair classifiers with theoretical guarantees; primarily because the resulting optimization problem is non-convex. The main contribution of this paper is a meta-algorithm for classification that can take as input a general class of fairness constraints with respect to multiple non-disjoint and multi-valued sensitive attributes, and which comes with provable guarantees. In particular, our algorithm can handle non-convex "linear fractional" constraints (which includes fairness constraints such as predictive parity) for which no prior algorithm was known. Key to our results is an algorithm for a family of classification problems with convex constraints along with a reduction from classification problems with linear fractional constraints to this family. Empirically, we observe that our algorithm is fast, can achieve near-perfect fairness with respect to various fairness metrics, and the loss in accuracy due to the imposed fairness constraints is often small.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287586
SP  - 319
EP  - 328
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287586
KW  - Classification
KW  - Algorithmic Fairness
ER  - 

TY  - CONF
TI  - Clear sanctions, vague rewards: How china's social credit system currently defines "Good" and "Bad" behavior
AU  - Engelmann, Severin
AU  - Chen, Mo
AU  - Fischer, Felix
AU  - Kao, Ching-yu
AU  - Grossklags, Jens
T3  - FAT* '19
AB  - China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, "good" behavior can result in material rewards and reputational gain while "bad" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines "good" and "bad" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform "Credit China", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on "good" behavior) and the Blacklist (information on "bad" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287585
SP  - 69
EP  - 78
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287585
KW  - Transparency
KW  - Behavioral Engineering
KW  - Social Credit System
KW  - Socio-Technical Systems
ER  - 

TY  - CONF
TI  - A moral framework for understanding fair ML through economic models of equality of opportunity
AU  - Heidari, Hoda
AU  - Loi, Michele
AU  - Gummadi, Krishna P.
AU  - Krause, Andreas
T3  - FAT* '19
AB  - We map the recently proposed notions of algorithmic fairness to economic models of Equality of opportunity (EOP)—an extensively studied ideal of fairness in political philosophy. We formally show that through our conceptual mapping, many existing definition of algorithmic fairness, such as predictive value parity and equality of odds, can be interpreted as special cases of EOP. In this respect, our work serves as a unifying moral framework for understanding existing notions of algorithmic fairness. Most importantly, this framework allows us to explicitly spell out the moral assumptions underlying each notion of fairness, and interpret recent fairness impossibility results in a new light. Last but not least and inspired by luck egalitarian models of EOP, we propose a new family of measures for algorithmic fairness. We illustrate our proposal empirically and show that employing a measure of algorithmic (un)fairness when its underlying moral assumptions are not satisfied, can have devastating consequences for the disadvantaged group's welfare.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287584
SP  - 181
EP  - 190
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287584
KW  - Equality of Odds
KW  - Equality of Opportunity (EOP)
KW  - Fairness for Machine Learning
KW  - Predictive Value Parity
KW  - Rawlsian and Luck Egalitarian EOP
KW  - Statistical Parity
ER  - 

TY  - CONF
TI  - SIREN: A simulation framework for understanding the effects of recommender systems in online news environments
AU  - Bountouridis, Dimitrios
AU  - Harambam, Jaron
AU  - Makhortykh, Mykola
AU  - Marrero, Mónica
AU  - Tintarev, Nava
AU  - Hauff, Claudia
T3  - FAT* '19
AB  - The growing volume of digital data stimulates the adoption of recommender systems in different socioeconomic domains, including news industries. While news recommenders help consumers deal with information overload and increase their engagement, their use also raises an increasing number of societal concerns, such as "Matthew effects", "filter bubbles", and the overall lack of transparency. We argue that focusing on transparency for content-providers is an under-explored avenue. As such, we designed a simulation framework called SIREN1 (SImulating Recommender Effects in online News environments), that allows content providers to (i) select and parameterize different recommenders and (ii) analyze and visualize their effects with respect to two diversity metrics. Taking the U.S. news media as a case study, we present an analysis on the recommender effects with respect to long-tail novelty and unexpectedness using SIREN. Our analysis offers a number of interesting findings, such as the similar potential of certain algorithmically simple (item-based k-Nearest Neighbour) and sophisticated strategies (based on Bayesian Personalized Ranking) to increase diversity over time. Overall, we argue that simulating the effects of recommender systems can help content providers to make more informed decisions when choosing algorithmic recommenders, and as such can help mitigate the aforementioned societal concerns.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287583
SP  - 150
EP  - 159
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287583
KW  - recommender systems
KW  - simulation
KW  - news media
KW  - diversity
ER  - 

TY  - CONF
TI  - Fair allocation through competitive equilibrium from generic incomes
AU  - Babaioff, Moshe
AU  - Nisan, Noam
AU  - Talgam-Cohen, Inbal
T3  - FAT* '19
AB  - Two food banks catering to populations of different sizes with different needs must divide among themselves a donation of food items. What constitutes a "fair" allocation of the items among them?Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods among equally entitled agents [Foley 1967, Varian 1974]. Every agent (foodbank) receives an equal endowment of artificial currency with which to "purchase" bundles of goods (food items). Prices for the goods are set high enough such that the agents can simultaneously get their favorite within-budget bundle, and low enough such that all goods are allocated (no waste). A CEEI satisfies mathematical notions of fairness like fair-share, and also has built-in transparency – prices can be published so the agents can verify they're being treated equally. However, a CEEI is not guaranteed to exist when the items are indivisible.We study competitive equilibrium from generic incomes (CEGI), which is based on the idea of slightly perturbed endowments, and enjoys similar fairness, efficiency and transparency properties as CEEI. We show that when the two agents have almost equal endowments and additive preferences for the items, a CEGI always exists. We then consider agents who are a priori non-equal (like different-sized foodbanks); we formulate a new notion of fair allocation among non-equals satisfied by CEGI, and show existence in cases of interest (like when the agents have identical preferences). Experiments on simulated and Spliddit data (a popular fair division website) indicate more general existence. Our results open opportunities for future research on fairness through generic endowments, and on fair treatment of non-equals.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287582
SP  - 180
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287582
KW  - fairness
KW  - additive preferences
KW  - Fisher markets
KW  - Market equilibrium
KW  - unequal entitlements
ER  - 

TY  - CONF
TI  - Analyzing biases in perception of truth in news stories and their implications for fact checking
AU  - Babaei, Mahmoudreza
AU  - Chakraborty, Abhijnan
AU  - Kulshrestha, Juhi
AU  - Redmiles, Elissa M.
AU  - Cha, Meeyoung
AU  - Gummadi, Krishna P.
T3  - FAT* '19
AB  - Recently, social media sites like Facebook and Twitter have been severely criticized by policy makers, and media watchdog groups for allowing fake news stories to spread unchecked on their platforms. In response, these sites are encouraging their users to report any news story they encounter on the site, which they perceive as fake. Stories that are reported as fake by a large number of users are prioritized for fact checking by (human) experts at fact checking organizations like Snopes and PolitiFact. Thus, social media sites today are relying on their users' perceptions of the truthfulness of news stories to select stories to fact check.However, few studies have focused on understanding how users perceive truth in news stories, or how biases in their perceptions might affect current strategies to detect and label fake news stories. To this end, we present an in-depth analysis on users' perceptions of truth in news stories. Specifically, we analyze users' truth perception biases for 150 stories fact checked by Snopes. Based on their ground truth and the truth value perceived by users, we can classify the stories into four categories – (i) C1: false stories perceived as false by most users, (ii) C2: true stories perceived as false by most users, (iii) C3: false stories perceived as true by most users, and (iv) C4: true stories perceived as true by most users.The stories that are likely to be reported (flagged) for fact checking are from the two classes C1 and C2 that have the lowest perceived truth levels. We argue that there is little to be gained by fact checking stories from C1 whose truth value is correctly perceived by most users. Although stories in C2 reveal the cynicality of users about true stories, social media sites presently do not explicitly mark them as true to resolve the confusion.On the contrary, stories in C3 are false stories, yet perceived as true by most users. Arguably, these stories are more damaging than C1 because the truth values of the the story in former situation is incorrectly perceived while truth values of the latter is correctly perceived. Nevertheless, the stories in C1 is likely to be fact checked with greater priority than the stories in C3! In fact, in today's social media sites, the higher the gullibility of users towards believing a false story, the less likely it is to be reported for fact checking.In summary, we make the following contributions in this work.1. Methodological: We develop a novel method for assessing users' truth perceptions of news stories. We design a test for users to rapidly assess (i.e., at the rate of a few seconds per story) how truthful or untruthful the claims in a news story are. We then conduct our truth perception tests on-line and gather truth perceptions of 100 US-based Amazon Mechanical Turk workers for each story.2. Empirical: Our exploratory analysis of users' truth perceptions reveal several interesting insights. For instance, (i) for many stories, the collective wisdom of the crowd (average truth rating) differs significantly from the actual truth of the story, i.e., wisdom of crowds is inaccurate, (ii) across different stories, we find evidence for both false positive perception bias (i.e., a gullible user perceiving the story to be more true than it is in reality) and false negative perception bias (i.e., a cynical user perceiving a story to be more false than it is in reality), and (iii) users' political ideologies influence their truth perceptions for the most controversial stories, it is frequently the result of users' political ideologies influencing their truth perceptions.3. Practical: Based on our observations, we call for prioritizing stories to fact check in order to achieve the following three important goals: (i) Remove false news stories from circulation, (ii) Correct the misperception of the users, and (iii) Decrease the disagreement between different users' perceptions of truth.Finally, we provide strategies which utilize users' truth perceptions (and predictive analysis of their biases) to achieve the three goals stated above while prioritizing stories for fact checking. The full paper is available at: https://bit.ly/2T7raFO
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287581
SP  - 139
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287581
KW  - Fact Checking
KW  - False News
KW  - Truth Perception Bias
ER  - 

TY  - CONF
TI  - On microtargeting socially divisive ads: A case study of russia-linked ad campaigns on facebook
AU  - Ribeiro, Filipe N.
AU  - Saha, Koustuv
AU  - Babaei, Mahmoudreza
AU  - Henrique, Lucas
AU  - Messias, Johnnatan
AU  - Benevenuto, Fabricio
AU  - Goga, Oana
AU  - Gummadi, Krishna P.
AU  - Redmiles, Elissa M.
T3  - FAT* '19
AB  - Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are "divisive": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287580
SP  - 140
EP  - 149
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287580
KW  - social media
KW  - advertisements
KW  - news media
KW  - perception bias
KW  - social divisiveness
KW  - targeting
ER  - 

TY  - CONF
TI  - Access to population-level signaling as a source of inequality
AU  - Immorlica, Nicole
AU  - Ligett, Katrina
AU  - Ziani, Juba
T3  - FAT* '19
AB  - We identify and explore differential access to population-level signaling (also known as information design) as a source of unequal access to opportunity. A population-level signaler has potentially noisy observations of a binary type for each member of a population and, based on this, produces a signal about each member. A decision-maker infers types from signals and accepts those individuals whose type is high in expectation. We assume the signaler of the disadvantaged population reveals her observations to the decision-maker, whereas the signaler of the advantaged population forms signals strategically. We study the expected utility of the populations as measured by the fraction of accepted members, as well as the false positive rates (FPR) and false negative rates (FNR).We first show the intuitive results that for a fixed environment, the advantaged population has higher expected utility, higher FPR, and lower FNR, than the disadvantaged one (despite having identical population quality), and that more accurate observations improve the expected utility of the advantaged population while harming that of the disadvantaged one. We next explore the introduction of a publicly-observable signal, such as a test score, as a potential intervention. Our main finding is that this natural intervention, intended to reduce the inequality between the populations' utilities, may actually exacerbate it in settings where observations and test scores are noisy.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287579
SP  - 249
EP  - 258
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287579
KW  - Fairness
KW  - information design
KW  - strategic signaling
KW  - university admissions
ER  - 

TY  - CONF
TI  - Downstream effects of affirmative action
AU  - Kannan, Sampath
AU  - Roth, Aaron
AU  - Ziani, Juba
T3  - FAT* '19
AB  - We study a two-stage model, in which students are 1) admitted to college on the basis of an entrance exam which is a noisy signal about their qualifications (type), and then 2) those students who were admitted to college can be hired by an employer as a function of their college grades, which are an independently drawn noisy signal of their type. Students are drawn from one of two populations, which might have different type distributions. We assume that the employer at the end of the pipeline is rational, in the sense that it computes a posterior distribution on student type conditional on all information that it has available (college admissions, grades, and group membership), and makes a decision based on posterior expectation. We then study what kinds of fairness goals can be achieved by the college by setting its admissions rule and grading policy. For example, the college might have the goal of guaranteeing equal opportunity across populations: that the probability of passing through the pipeline and being hired by the employer should be independent of group membership, conditioned on type. Alternately, the college might have the goal of incentivizing the employer to have a group blind hiring rule. We show that both goals can be achieved when the college does not report grades. On the other hand, we show that under reasonable conditions, these goals are impossible to achieve even in isolation when the college uses an (even minimally) informative grading policy.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287578
SP  - 240
EP  - 248
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287578
KW  - affirmative action
KW  - college admissions
KW  - job market
KW  - Long-term fairness
ER  - 

TY  - CONF
TI  - Beyond open vs. Closed: Balancing individual privacy and public accountability in data sharing
AU  - Young, Meg
AU  - Rodriguez, Luke
AU  - Keller, Emily
AU  - Sun, Feiyang
AU  - Sa, Boyang
AU  - Whittington, Jan
AU  - Howe, Bill
T3  - FAT* '19
AB  - Data too sensitive to be "open" for analysis and re-purposing typically remains "closed" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287577
SP  - 191
EP  - 200
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287577
KW  - privacy
KW  - algorithmic bias
KW  - data ethics
KW  - data governance
KW  - data sharing
ER  - 

TY  - CONF
TI  - The social cost of strategic classification
AU  - Milli, Smitha
AU  - Miller, John
AU  - Dragan, Anca D.
AU  - Hardt, Moritz
T3  - FAT* '19
AB  - Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift.We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population.Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287576
SP  - 230
EP  - 239
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287576
KW  - machine learning
KW  - fairness
KW  - Strategic classification
ER  - 

TY  - CONF
TI  - Racial categories in machine learning
AU  - Benthall, Sebastian
AU  - Haynes, Bruce D.
T3  - FAT* '19
AB  - Controversies around race and machine learning have sparked debate among computer scientists over how to design machine learning systems that guarantee fairness. These debates rarely engage with how racial identity is embedded in our social experience, making for sociological and psychological complexity. This complexity challenges the paradigm of considering fairness to be a formal property of supervised learning with respect to protected personal attributes. Racial identity is not simply a personal subjective quality. For people labeled "Black" it is an ascribed political category that has consequences for social differentiation embedded in systemic patterns of social inequality achieved through both social and spatial segregation. In the United States, racial classification can best be understood as a system of inherently unequal status categories that places whites as the most privileged category while signifying the Negro/black category as stigmatized. Social stigma is reinforced through the unequal distribution of societal rewards and goods along racial lines that is reinforced by state, corporate, and civic institutions and practices. This creates a dilemma for society and designers: be blind to racial group disparities and thereby reify racialized social inequality by no longer measuring systemic inequality, or be conscious of racial categories in a way that itself reifies race. We propose a third option. By preceding group fairness interventions with unsupervised learning to dynamically detect patterns of segregation, machine learning systems can mitigate the root cause of social disparities, social segregation and stratification, without further anchoring status categories of disadvantage.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287575
SP  - 289
EP  - 298
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287575
KW  - machine learning
KW  - fairness
KW  - racial classification
KW  - segregation
ER  - 

TY  - CONF
TI  - Explaining explanations in AI
AU  - Mittelstadt, Brent
AU  - Russell, Chris
AU  - Wachter, Sandra
T3  - FAT* '19
AB  - Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287574
SP  - 279
EP  - 288
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287574
KW  - Accountability
KW  - Explanations
KW  - Interpretability
KW  - Philosophy of Science
ER  - 

TY  - CONF
TI  - Measuring the biases that matter: The ethical and casual foundations for measures of fairness in algorithms
AU  - Glymour, Bruce
AU  - Herington, Jonathan
T3  - FAT* '19
AB  - Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of "procedural bias" diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of "outcome bias" capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of "behavior-relative error bias" capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of "score-relative error bias" capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized.In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287573
SP  - 269
EP  - 278
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287573
KW  - discrimination
KW  - fairness
KW  - Algorithmic decision-making
KW  - casual inference
ER  - 

TY  - CONF
TI  - Bias in bios: A case study of semantic representation bias in a high-stakes setting
AU  - De-Arteaga, Maria
AU  - Romanov, Alexey
AU  - Wallach, Hanna
AU  - Chayes, Jennifer
AU  - Borgs, Christian
AU  - Chouldechova, Alexandra
AU  - Geyik, Sahin
AU  - Kenthapadi, Krishnaram
AU  - Kalai, Adam Tauman
T3  - FAT* '19
AB  - We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators—such as first names and pronouns—in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are "scrubbed," and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287572
SP  - 120
EP  - 128
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287572
KW  - Supervised learning
KW  - algorithmic fairness
KW  - automated hiring
KW  - compounding injustices
KW  - gender bias
KW  - online recruiting
ER  - 

TY  - CONF
TI  - Fair algorithms for learning in allocation problems
AU  - Elzayn, Hadi
AU  - Jabbari, Shahin
AU  - Jung, Christopher
AU  - Kearns, Michael
AU  - Neel, Seth
AU  - Roth, Aaron
AU  - Schutzman, Zachary
T3  - FAT* '19
AB  - Settings such as lending and policing can be modeled by a centralized agent allocating a scarce resource (e.g. loans or police officers) amongst several groups, in order to maximize some objective (e.g. loans given that are repaid, or criminals that are apprehended). Often in such problems fairness is also a concern. One natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource in question, the probability of actually receiving it is approximately independent of the individual's group. For example, in lending this would mean that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it would mean that two individuals committing the same crime in different districts would have roughly equal chances of being arrested.In this paper, we formalize this general notion of fairness for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the allocator does not know the frequency of candidates (i.e. creditworthy individuals or criminals) in each group. This algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates in each group. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to and do not learn about crimes committed if the police presence in a district is low.As an application of our framework and algorithm, we consider the predictive policing problem, in which the resource being allocated to each group is the number of police officers assigned to each district. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. In this case, the fairness constraint asks that the probability that an individual who has committed a crime is arrested should be independent of the district in which they live. We investigate the performance of our learning algorithm on the Philadelphia Crime Incidents dataset.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287571
SP  - 170
EP  - 179
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287571
KW  - resource allocation
KW  - online learning
KW  - algorithmic fairness
KW  - censored feedback
ER  - 

TY  - CONF
TI  - Equality of voice: Towards fair representation in crowdsourced top-K recommendations
AU  - Chakraborty, Abhijnan
AU  - Patro, Gourab K.
AU  - Ganguly, Niloy
AU  - Gummadi, Krishna P.
AU  - Loiseau, Patrick
T3  - FAT* '19
AB  - To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups.To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287570
SP  - 129
EP  - 138
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287570
KW  - Fair Representation
KW  - Fairness in Recommendation
KW  - Most Popular News
KW  - Top-K Recommendation
KW  - Twitter Trends
ER  - 

TY  - CONF
TI  - Efficient search for diverse coherent explanations
AU  - Russell, Chris
T3  - FAT* '19
AB  - This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a "mixed polytope" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287569
SP  - 20
EP  - 28
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287569
KW  - Machine Learning
KW  - Counterfactual Explanation
KW  - Linear Program
ER  - 

TY  - CONF
TI  - The profiling potential of computer vision and the challenge of computational empiricism
AU  - Goldenfein, Jake
T3  - FAT* '19
AB  - Computer vision and other biometrics data science applications have commenced a new project of profiling people. Rather than using 'transaction generated information', these systems measure the 'real world' and produce an assessment of the 'world state' - in this case an assessment of some individual trait. Instead of using proxies or scores to evaluate people, they increasingly deploy a logic of revealing the truth about reality and the people within it. While these profiling knowledge claims are sometimes tentative, they increasingly suggest that only through computation can these excesses of reality be captured and understood. This article explores the bases of those claims in the systems of measurement, representation, and classification deployed in computer vision. It asks if there is something new in this type of knowledge claim, sketches an account of a new form of computational empiricism being operationalised, and questions what kind of human subject is being constructed by these technological systems and practices. Finally, the article explores legal mechanisms for contesting the emergence of computational empiricism as the dominant knowledge platform for understanding the world and the people within it.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287568
SP  - 110
EP  - 119
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287568
KW  - Biometrics
KW  - Computer Vision
KW  - Data Science
KW  - Computational Empiricism
KW  - Law and Policy
ER  - 

TY  - CONF
TI  - Problem formulation and fairness
AU  - Passi, Samir
AU  - Barocas, Solon
T3  - FAT* '19
AB  - Formulating data science problems is an uncertain and difficult process. It requires various forms of discretionary work to translate high-level objectives or strategic goals into tractable problems, necessitating, among other things, the identification of appropriate target variables and proxies. While these choices are rarely self-evident, normative assessments of data science projects often take them for granted, even though different translations can raise profoundly different ethical concerns. Whether we consider a data science project fair often has as much to do with the formulation of the problem as any property of the resulting model. Building on six months of ethnographic fieldwork with a corporate data science team—and channeling ideas from sociology and history of science, critical data studies, and early writing on knowledge discovery in databases—we describe the complex set of actors and activities involved in problem formulation. Our research demonstrates that the specification and operationalization of the problem are always negotiated and elastic, and rarely worked out with explicit normative considerations in mind. In so doing, we show that careful accounts of everyday data science work can help us better understand how and why data science problems are posed in certain ways—and why specific formulations prevail in practice, even in the face of what might seem like normatively preferable alternatives. We conclude by discussing the implications of our findings, arguing that effective normative interventions will require attending to the practical work of problem formulation.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287567
SP  - 39
EP  - 48
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287567
KW  - Fairness
KW  - Machine Learning
KW  - Data Science
KW  - Problem Formulation
KW  - Target Variable
ER  - 

TY  - CONF
TI  - Actionable recourse in linear classification
AU  - Ustun, Berk
AU  - Spangher, Alexander
AU  - Liu, Yang
T3  - FAT* '19
AB  - Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and – more importantly –will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287566
SP  - 10
EP  - 19
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287566
KW  - accountability
KW  - classification
KW  - audit
KW  - credit scoring
KW  - integer programming
KW  - recourse
ER  - 

TY  - CONF
TI  - Who's the guinea pig? Investigating online A/B/n tests in-the-Wild
AU  - Jiang, Shan
AU  - Martin, John
AU  - Wilson, Christo
T3  - FAT* '19
AB  - A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287565
SP  - 201
EP  - 210
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287565
KW  - personalization
KW  - A/B/n testing
KW  - online controlled experiments
ER  - 

TY  - CONF
TI  - Fairness through causal awareness: Learning causal latent-variable models for biased data
AU  - Madras, David
AU  - Creager, Elliot
AU  - Pitassi, Toniann
AU  - Zemel, Richard
T3  - FAT* '19
AB  - How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287564
SP  - 349
EP  - 358
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287564
KW  - fairness in machine learning
KW  - causal inference
KW  - variational inference
ER  - 

TY  - CONF
TI  - Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments
AU  - Green, Ben
AU  - Chen, Yiling
T3  - FAT* '19
AB  - Despite vigorous debates about the technical characteristics of risk assessments being deployed in the U.S. criminal justice system, remarkably little research has studied how these tools affect actual decision-making processes. After all, risk assessments do not make definitive decisions—they inform judges, who are the final arbiters. It is therefore essential that considerations of risk assessments be informed by rigorous studies of how judges actually interpret and use them. This paper takes a first step toward such research on human interactions with risk assessments through a controlled experimental study on Amazon Mechanical Turk. We found several behaviors that call into question the supposed efficacy and fairness of risk assessments: our study participants 1) underperformed the risk assessment even when presented with its predictions, 2) could not effectively evaluate the accuracy of their own or the risk assessment's predictions, and 3) exhibited behaviors fraught with "disparate interactions," whereby the use of risk assessments led to higher risk predictions about black defendants and lower risk predictions about white defendants. These results suggest the need for a new "algorithm-in-the-loop" framework that places machine learning decision-making aids into the sociotechnical context of improving human decisions rather than the technical context of generating the best prediction in the abstract. If risk assessments are to be used at all, they must be grounded in rigorous evaluations of their real-world impacts instead of in their theoretical potential.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287563
SP  - 90
EP  - 99
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287563
KW  - risk assessment
KW  - fairness
KW  - behavioral experiment
KW  - Mechanical Turk
ER  - 

TY  - CONF
TI  - Model reconstruction from model explanations
AU  - Milli, Smitha
AU  - Schmidt, Ludwig
AU  - Dragan, Anca D.
AU  - Hardt, Moritz
T3  - FAT* '19
AB  - We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations.On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive.Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287562
SP  - 1
EP  - 9
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287562
KW  - machine learning
KW  - Explanations
KW  - privacy
KW  - security
ER  - 

TY  - CONF
TI  - From soft classifiers to hard decisions: How fair can we be?
AU  - Canetti, Ran
AU  - Cohen, Aloni
AU  - Dikkala, Nishanth
AU  - Ramnarayan, Govind
AU  - Scheffler, Sarah
AU  - Smith, Adam
T3  - FAT* '19
AB  - A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a calibrated non-binary "scoring" classifier, and then to post-process this score to obtain a binary decision. We study various fairness (or, error-balance) properties of this methodology, when the non-binary scores are calibrated over all protected groups, and with a variety of post-processing algorithms. Specifically, we show:First, there does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain "nice" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups. Still, when the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for "nice" classifiers.Second, when the post-processing stage is allowed to defer on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.We evaluate our post-processing techniques using the COMPAS data set from 2016.
C1  - New York, NY, USA
C3  - Proceedings of the conference on fairness, accountability, and transparency
DA  - 2019///
PY  - 2019
DO  - 10.1145/3287560.3287561
SP  - 309
EP  - 318
PB  - Association for Computing Machinery
SN  - 978-1-4503-6125-5
UR  - https://doi.org/10.1145/3287560.3287561
KW  - classification
KW  - algorithmic fairness
KW  - post-processing
ER  - 

TY  - CONF
TI  - Algorithmic targeting of social policies: fairness, accuracy, and distributed governance
AU  - Noriega-Campero, Alejandro
AU  - Garcia-Bulle, Bernardo
AU  - Cantu, Luis Fernando
AU  - Bakker, Michiel A.
AU  - Tejerina, Luis
AU  - Pentland, Alex
T3  - FAT* '20
AB  - Targeted social policies are the main strategy for poverty alleviation across the developing world. These include targeted cash transfers (CTs), as well as targeted subsidies in health, education, housing, energy, childcare, and others. Due to the scale, diversity, and widespread relevance of targeted social policies like CTs, the algorithmic rules that decide who is eligible to benefit from them—and who is not—are among the most important algorithms operating in the world today. Here we report on a year-long engagement towards improving social targeting systems in a couple of developing countries. We demonstrate that a shift towards the use of AI methods in poverty-based targeting can substantially increase accuracy, extending the coverage of the poor by nearly a million people in two countries, without increasing expenditure. However, we also show that, absent explicit parity constraints, both status quo and AI-based systems induce disparities across population subgroups. Moreover, based on qualitative interviews with local social institutions, we find a lack of consensus on normative standards for prioritization and fairness criteria. Hence, we close by proposing a decision-support platform for distributed governance, which enables a diversity of institutions to customize the use of AI-based insights into their targeting decisions.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375784
SP  - 241
EP  - 251
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375784
KW  - algorithmic fairness
KW  - AI for social good
KW  - cash transfers
KW  - proxy means tests
KW  - targeted social programs
ER  - 

TY  - CONF
TI  - Bidding strategies with gender nondiscrimination constraints for online ad auctions
AU  - Nasr, Milad
AU  - Tschantz, Michael Carl
T3  - FAT* '20
AB  - Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375783
SP  - 337
EP  - 347
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375783
KW  - fairness constraints
KW  - MDPs
KW  - online auctions
KW  - targeted advertising
ER  - 

TY  - CONF
TI  - Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy
AU  - Yang, Kaiyu
AU  - Qinami, Klint
AU  - Fei-Fei, Li
AU  - Deng, Jia
AU  - Russakovsky, Olga
T3  - FAT* '20
AB  - Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375709
SP  - 547
EP  - 558
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375709
KW  - fairness
KW  - computer vision
KW  - dataset construction
KW  - representative datasets
ER  - 

TY  - CONF
TI  - Burn, dream and reboot! speculating backwards for the missing archive on non-coercive computing
AU  - Pritchard, Helen
AU  - Snodgrass, Eric
AU  - Morrison, Romi Ron
AU  - Britton, Loren
AU  - Moll, Joana
T3  - FAT* '20
AB  - Whether one is speaking of barbed wire, the assembly line or computer operating systems, the history of coercive technologies for the automation of tasks has focused on optimization, determinate outcomes and an ongoing disciplining of components and bodies. Automated technologies of the present emerge and are marked by this lineage of coercive modes of implementation, whose scarred history of techniques of discrimination, exploitation and extraction point to an archive of automated injustices in computing, a history that continues to charge present paradigms and practices of computing.This workshop addresses the history of coercive technologies through attuning to how we perform speculation within practices of computing through a renewed attention to this history. We go backwards into the archive, rather than racing forward and proposing ever new speculative futures of automation. This is because speculative creative approaches are often conceived and positioned as methodological toolkits for addressing computing practices by imagining for/with others for a "future otherwise". We argue that "speculation" as the easy-go-to of designers and artists trying to address automated injustices needs some undoing, as without work it will always be confined within ongoing legacies of coercive modes of computing practice. Instead of creating more just-worlds, the generation of ever-new futures by creative speculation often merely reinforces the project of coercive computing.For this workshop, drawing on queer approaches to resisting futures and informed by activist feminist engagements with archives, we invite participants to temporarily resist imagining futures and instead to speculate backwards. We speculate backwards to various moments, artefacts and practices within computing history. What does it mean to understand techniques of computing and automation as coercive infrastructures? How did so many of the dreams and seeming promises of computing turn into the coercive practices that we see today? Has computing as a practice become so imbued with coercive techniques that we find it hard to imagine otherwise? Together, we will build a speculative understanding and possible archive of non-coercive computing. In the words of Alexis Pauline Gumbs, the emerging archive proposes "how did their dreams make rooms to dream in"... or not, in the case of coercive practices of computing. And "what if she changes her dream?" What if we reboot this dream?1
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375697
SP  - 683
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375697
KW  - automation
KW  - optimization
KW  - social justice
KW  - critical computing
KW  - speculative design
KW  - trans*feminist technoscience
ER  - 

TY  - CONF
TI  - Hardwiring discriminatory police practices: the implications of data-driven technological policing on minority (ethnic and religious) people and communities
AU  - Williams, Patrick
AU  - Kind, Eric
T3  - FAT* '20
AB  - On data-based policing.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375695
SP  - 691
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375695
ER  - 

TY  - CONF
TI  - CtrlZ.AI zine fair: critical perspectives
AU  - Hanna, Alex
AU  - Denton, Emily
T3  - FAT* '20
AB  - The FAT* conference has begun the necessary conversation on the normative implications and ethical ramifications of sociotechnical systems. However, many scholars have pointed to the limitations in methodologies and scope of analysis (e.g. [8, 11]). In addition to these critiques, we add in the fact that those who are most affected by this technology do not have the skills, training, or technical aptitude to participate in these conversations. With the exception of the 2018 FAT* tutorial which featured Terrance Wilkerson (who had been labeled as likely to highly recidivate by COMPAS) and his partner, there has been silence from those most impacted by algorithmic unfairness at FAT*. This silence has been deafening, as FAT* conversations - with a few notable exceptions (e.g. [1, 4]) - have failed to discuss anti-racist politics, prison abolition, and social justice.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375692
SP  - 686
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375692
ER  - 

TY  - CONF
TI  - When not to design, build, or deploy
AU  - Barocas, Solon
AU  - Biega, Asia J.
AU  - Fish, Benjamin
AU  - Niklas, Jundefineddrzej
AU  - Stark, Luke
T3  - FAT* '20
AB  - Recent debate within the FAT* community has focused on how the field conceptualizes the problems it seeks to address, what approach the field should take in attempting to address these problems, and whether the field should even pursue some of the proposed remedies. Questions regarding when not to design, build, or deploy a technology are perhaps the most common expression of this trend. Identifying the problems to address is inextricably linked to the broader question of how to collectively make decisions about what technologies our societies need and want.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375691
SP  - 695
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375691
KW  - politics of tech refusal
KW  - technology policy
KW  - technology refusal
ER  - 

TY  - CONF
TI  - Fairness, accountability, transparency in AI at scale: lessons from national programs
AU  - Ahmad, Muhammad Aurangzeb
AU  - Teredesai, Ankur
AU  - Eckert, Carly
T3  - FAT* '20
AB  - The panel aims to elucidate how different national govenmental programs are implementing accountability of machine learning systems in healthcare and how accountability is operationlized in different cultural settings in legislation, policy and deployment. We have representatives from three different govenments, UAE, Singapore and Maldives who will discuss what accountability of AI and machine learning means in their contexts and use cases. We hope to have a fruitful conversation around FAT ML as it is operationalized ccross cultures, national boundries and legislative constraints.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375690
SP  - 690
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375690
ER  - 

TY  - CONF
TI  - Creating community-based tech policy: case studies, lessons learned, and what technologists and communities can do together
AU  - Sassaman, Hannah
AU  - Lee, Jennifer
AU  - Irvine, Jenessa
AU  - Narayan, Shankar
T3  - FAT* '20
AB  - What are the core ways the field of data science can center community voice and power throughout all the processes involved in conceptualizing, creating, and disseminating technology?? What are the most possible and most urgent ways communities can shape the field of algorithmic decision-making to center community power in the next few years? This interactive workshop will highlight some of the following lessons learned through our combined experience engaging with communities challenging technology in Seattle and Philadelphia, cities in the United States. We will discuss the historical context of disproportionate impacts of technology on marginalized and vulnerable communities; case studies including criminal justice risk assessments, face surveillance technologies, and surveillance regulations; and work in small-group and break-out sessions to engage questions about when and where technologists hold power, serve as gatekeepers, and can work in accountable partnership with impacted communities.By the end of the session, we hope that participants will learn how to actively center diverse communities in creating technology by examining successes, challenges, and ongoing work in Seattle and Philadelphia, through the following lessons we have learned:• that communities, policy-makers, and technologists need to work intimately together to lift up each other's' goals• that communities need to gain data justice and data literacy to understand and independently audit how a system is impacting them• that scientific analyses of algorithmic bias are powerful but heard most clearly when lifted up by local community members and stakeholders in decisions where algorithms might be deployed• that anecdotal stories of harm are most impactful on decisionmakers when tied to rigorous scientific analysis and examples from other communities that amplify and ground those stories• that communities and community goals and standards are often not heard in conversations between data scientists and people who deploy algorithms, as well as in decision-makers' conversations about what policy should look like• and that we need to begin to craft what it means for those with the least power in conversations about algorithmic fairness - those judged by those tools - to have far more, or even the most power in the future of their design or implementation.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375689
SP  - 685
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375689
KW  - algorithms
KW  - bias
KW  - criminal justice
KW  - surveillance
KW  - case studies
KW  - community-centered
KW  - disproportionate impact
ER  - 

TY  - CONF
TI  - Deconstructing FAT: using memories to collectively explore implicit assumptions, values and context in practices of debiasing and discrimination-awareness
AU  - Allhutter, Doris
AU  - Berendt, Bettina
T3  - FAT* '20
AB  - Research in fairness, accountability, and transparency (FAT) in socio-technical systems needs to take into account how practices of computing are entrenched with power relations in complex and multi-layered ways. Trying to disentangle the way in which structural discrimination and normative computational concepts and methods are intertwined, this frequently raises the question of WHO are the actors that shape technologies and research agendas—who gets to speak and to define bias, (un)fairness, and discrimination? "Deconstructing FAT" is a CRAFT workshop that aims at complicating this question by asking how "we" as researchers in FAT (often unknowingly) mobilize implicit assumptions, values and beliefs that reflect our own embeddedness in power relations, our disciplinary ways of thinking, and our historically, locally, and culturally-informed ways of solving computational problems or approaching our research. This is a vantage point to make visible and analyze the normativity of technical approaches, concepts and methods that are part of the repertoire of FAT research. Inspired by a previous international workshop [1], this CRAFT workshop engages an interdisciplinary panel of FAT researchers in a deconstruction exercise that traces the following issues:(1) FAT research frequently speaks of social bias that is amplified by algorithmic systems, of the problem of discriminatory consequences that is to be solved, and of underprivileged or vulnerable groups that need to be protected. What does this perspectivity imply in terms of the approaches, methods and metrics that are being applied? How do methods of debiasing and discrimination-awareness enact the epistemic power of a perspective of privilege as their norm?(2) FAT research has emphasized the need for multi- or interdisciplinary approaches to get a grip on the complex intertwining of social power relations and the normativity of computational methods, norms and practices. Clearly, multi- and interdisciplinary research includes different normative frameworks and ways of thinking that need to be negotiated. This is complicated by the fact that these frameworks are not fully transparent and ready for reflection. What are the normative implications of interdisciplinary collaboration in FAT research? (3) While many problems of discrimination, marginalization and exploitation can be similar across places, they can also have specific local shapes. How can FAT research e.g. consider historically grown specifics such as the effects of different colonial histories? If these specifics make patterns of discrimination have different and more nuanced dimensions than clear-cut 'redlining', what does this imply?To explore these questions, we use the method of 'mind scripting' which is based in theories of discourse, ideology, memory and affect and aims at investigating hidden patterns of meaning making in written memories of the panelists [2]. The workshop strives to challenge some of the implicit norms and tensions in FAT research and to trigger future directions.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375688
SP  - 687
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375688
KW  - deconstruction
KW  - discrimination-awareness in machine learning
ER  - 

TY  - CONF
TI  - Algorithmically encoded identities: reframing human classification
AU  - Baker, Dylan
AU  - Hanna, Alex
AU  - Denton, Emily
T3  - FAT* '20
AB  - Our aim with this workshop is to provide a venue within which the FAT* community can thoughtfully engage with identity and the categories which are imposed on people as part of making sense of their identities. Most people have nuanced and deeply personal understandings of what identity categories mean to them; however, sociotechnical systems must, through a set of classification decisions, reduce the nuance and complexity of those identities into discrete categories. The impact of misclassifications can range from the uncomfortable (e.g. displaying ads for items that aren't desirable) to devastating (e.g. being denied medical care; being evaluated as having a high risk of criminal recidivism). However, even the act of being classified can force an individual into categories which feel foreign and othering. Through this workshop, we hope to connect participants' personal understandings of identity to how identity is 'seen' and categorized by sociotechnical systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375687
SP  - 681
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375687
KW  - classification
KW  - identity
KW  - infrastructure studies
ER  - 

TY  - CONF
TI  - Centering disability perspectives in algorithmic fairness, accountability, &amp; transparency
AU  - Givens, Alexandra Reeve
AU  - Morris, Meredith Ringel
T3  - FAT* '20
AB  - It is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges for approaches to fairness, accountability, and transparency. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.This panel highlights areas of concern, present emerging research efforts, and enlist more researchers and advocates to study the potential impacts of algorithmic decision-making on people with disabilities. A key objective is to surface new research projects and collaborations, including by integrating a critical disability perspective into existing research and advocacy efforts focused on identifying sources of bias and advancing equity.In the technology space, discussion topics will include methods to assess the fairness of current AI systems, and strategies to develop new systems and bias mitigation approaches that ensure fairness for people with disabilities. For example, how do today's currently-deployed AI systems impact people with disabilities? If developing inclusive datasets is part of the solution, how can researchers ethically gather such data, and what risks might centralizing data about disability pose? What new privacy solutions must developers create to reduce the risk of deductive disclosure of identities of people with disabilities in "anonymized" datasets? How can AI models and bias mitigation techniques be developed that handle the unique challenges of disability, i.e., the "long tail" and low incidence of many types of disability - for instance, how do we ensure that data about disability are not treated as outliers? What are the pros and cons of developing custom/personalized AI models for people with disabilities versus ensuring that general models are inclusive?In the law and policy space, the framework for people with disabilities requires specific study. For example, the Americans with Disabilities Act (ADA) requires employers to adopt "reasonable accommodations" for qualified individuals with a disability. But what is a "reasonable accommodation" in the context of machine learning and AI? How will the ADA's unique standards interact with case law and scholarship about algorithmic bias against other protected groups? When the ADA governs what questions employers can ask about a candidate's disability, and HIPAA and the Genetic Information Privacy Act regulate the sharing of health information, how should we think about inferences from data that approximate such questions?Panelists will bring varied perspectives to this conversation, including backgrounds in computer science, disability studies, legal studies, and activism. In addition to their scholarly expertise, several panelists have direct lived experience with disability. The session format will consist of brief position statements from each panelist, followed by questions from the moderator, and then open questions from and discussion with the audience.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375686
SP  - 684
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375686
KW  - algorithmic bias
KW  - accessibility
KW  - AI FATE
KW  - disability studies
ER  - 

TY  - CONF
TI  - Lost in translation: an interactive workshop mapping interdisciplinary translations for epistemic justice
AU  - Wan, Evelyn
AU  - de Groot, Aviva
AU  - Jameson, Shazade
AU  - Păun, Mara
AU  - Lücking, Phillip
AU  - Klumbyte, Goda
AU  - Lämmerhirt, Danny
T3  - FAT* '20
AB  - There are gaps in understanding in and between those who design systems of AI/ ML, those who critique them, and those positioned between these discourses. This gap can be defined in multiple ways - e.g. methodological, epistemological, linguistic, or cultural. To bridge this gap requires a set of translations: the generation of a collaborative space and a new set of shared sensibilities that traverse disciplinary boundaries. This workshop aims to explore translations across multiple fields, and translations between theory and practice, as well as how interdisciplinary work could generate new operationalizable approaches.We define 'knowledge' as a social product (L. Code) which requires fair and broad epistemic cooperation in its generation, development, and dissemination. As a "marker for truth" (B. Williams) and therefore a basis for action, knowledge circulation sustains the systems of power which produce it in the first place (M. Foucault). Enabled by epistemic credence, authority or knowledge, epistemic power can be an important driver of, but also result from, other (e.g. economic, political) powers.To produce reliable output, our standards and methods should serve us all and exclude no-one. Critical theorists have long revealed failings of epistemic practices, resulting in the marginalization and exclusion of some types of knowledge. How can we cultivate more reflexive epistemic practices in the interdisciplinary research setting of FAT*?We frame this ideal as 'epistemic justice' (M. Geuskens), the positive of 'epistemic injustice', defined by M. Fricker as injustice that exists when people are wronged as a knower or as an epistemic subject. Epistemic justice is the proper use and allocation of epistemic power; the inclusion and balancing of all epistemic sources.As S. Jasanoff reminds us, any authoritative way of seeing must be legitimized in discourse and practice, showing that practices can be developed to value and engage with other viewpoints and possibly reshape our ways of knowing.Our workshop aims to address the following questions: how could critical theory or higher level critiques be translated into and anchored in ML/AI design practices - and vice versa? What kind of cartographies and methodologies are needed in order to identify issues that can act as the basis of collaborative research and design? How can we (un)learn our established ways of thinking for such collaborative work to take place? During the workshop, participants will create, share and explode prototypical workflows of designing, researching and critiquing algorithmic systems. We will identify moments in which translations and interdisciplinary interventions could or should happen in order to build actionable steps and methodological frameworks that advance epistemic justice and are conducive to future interdisciplinary collaboration.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375685
SP  - 692
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375685
KW  - workflow
KW  - algorithm development
KW  - critical theory
KW  - epistemic justice
KW  - interdisciplinary collaboration
KW  - methodologies
ER  - 

TY  - CONF
TI  - Ethics on the ground: from principles to practice
AU  - Barry, Marguerite
AU  - Kerr, Aphra
AU  - Smith, Oliver
T3  - FAT* '20
AB  - Surveys of public attitudes show that people believe it is possible to design ethical AI. However the everyday professional development context can offer minimal space for ethical reflection or oversight, creating a significant gap between public expectations and the performance of ethics in practice. This 2-part workshop includes an offsite visit to Telefónica Innovation Alpha and uses storytelling and theatre methods to examine how and where ethical reflection happens on the ground. It will explore the gaps in expectations and identify alternative approaches to more effective ethical performance. Bringing social scientists, data scientists, designers, civic rights activists and ethics consultants together to focus on AI/ML in the health context, it will foster critical and creative activities that will bring to the surface the structural, disciplinary, social and epistemological challenges to effective ethical performance in practice. Participants will explore and enact where, when and how meaningful interventions can happen.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375684
SP  - 688
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375684
KW  - explainable AI
KW  - fairness
KW  - eHealth
KW  - mHealth
KW  - human-computer interaction
KW  - ethics guidelines
KW  - ethics of AI
KW  - sociology of expectations
ER  - 

TY  - CONF
TI  - Where do algorithmic accountability and explainability frameworks take us in the real world? from theory to practice
AU  - Szymielewicz, Katarzyna
AU  - Bacciarelli, Anna
AU  - Hidvegi, Fanny
AU  - Foryciarz, Agata
AU  - Pénicaud, Soizic
AU  - Spielkamp, Matthias
T3  - FAT* '20
AB  - This hands-on session takes academic concepts and their formulation in policy initiatives around algorithmic accountability and explainability and tests them against real cases. In small groups we will (1) test selected frameworks on algorithmic accountability and explainability against a concrete case study (that likely constitutes a human rights violation) and (2) test different formats to explain important aspects of an automated decision-making process (such as input data, type of an algorithm used, design decisions and technical parameters, expected outcomes) to various audiences (end users, affected communities, watchdog organisations, public sector agencies and regulators). We invite participants with various backgrounds: researchers, technologists, human rights advocates, public servants and designers.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375683
SP  - 689
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375683
KW  - explanations
KW  - interpretability
KW  - discrimination
KW  - accountability
KW  - fairness
KW  - philosophy of science
KW  - algorithmic decision-making
KW  - algorithmic impact assessment
ER  - 

TY  - CONF
TI  - Manifesting the sociotechnical: experimenting with methods for social context and social justice
AU  - Goss, Ezra
AU  - Hu, Lily
AU  - Sabin, Manuel
AU  - Teeple, Stephanie
T3  - FAT* '20
AB  - Critiques of 'algorithmic fairness' have counseled against a purely technical approach. Recent work from the FAT* conference has warned specifically about abstracting away the social context that these automated systems are operating within and has suggested that "[fairness work] require[s] technical researchers to learn new skills or partner with social scientists" [Fairness and abstraction in sociotechnical systems, Selbst et al. 2019, FAT* '19]. That "social context" includes groups outside the academy organizing for data and/or tech justice (e.g., Allied Media Projects, Stop LAPD Spying Coalition, data4blacklives, etc). These struggles have deep historical roots but have become prominent in the past several years alongside broader citizen-science efforts. In this CRAFT session we as STEM researchers hope to initiate conversation about methods used by community organizers to analyze power relations present in that social context. We will take this time to learn together and discuss if/how these and other methods, collaborations and efforts can be used to actualize oft-mentioned critiques of algorithmic fairness and move toward a data justice-oriented approach.Many scholars and activists have spoken on how to approach social context when discussing algorithmic fairness interventions. Community organizing and attendant methods for power analysis present one such approach: documenting all stakeholders and entities relevant to an issue and the nature of the power differentials between them. The facilitators for this session are not experts in community organizing theory or practice. Instead, we will share what we have learned from our readings of decades of rich work and writings from community organizers. This session is a collective, interdisciplinary learning experience, open to all who see their interests as relevant to the conversation.We will open with a discussion of community organizing practice: What is community organizing, what are its goals, methods, past and ongoing examples? What disciplines and intellectual lineages does it draw from? We will incorporate key sources we have found helpful for synthesizing this knowledge so that participants can continue exposing themselves to the field after the conference. We will also consider the concept of social power, including power that the algorithmic fairness community holds. Noting that there are many ways to theorize and understand power, we will share the framings that have been most useful to us. We plan to present different tools, models and procedures for doing power analysis in various organizing settings.We will propose to our group that we conduct a power analysis of our own. We have prepared a hypothetical but realistic scenario involving risk assessment in a hospital setting as an example. However, we encourage participants to bring their own experiences to the table, especially if they pertain in any way to data injustice. We also invite participants to bring examples of ongoing organizing efforts with which algorithmic fairness researchers could act in solidarity. Participants will walk away from this session with 1) an understanding of the key terms and sources necessary to gain further exposure to these topics and 2) preliminary experience analyzing power in realistic, grounded scenarios.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375682
SP  - 693
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375682
KW  - fairness
KW  - community organizing
KW  - data justice
KW  - power analysis
ER  - 

TY  - CONF
TI  - Bridging the gap from AI ethics research to practice
AU  - Baxter, Kathy
AU  - Schlesinger, Yoav
AU  - Aerni, Sarah
AU  - Baker, Lewis
AU  - Dawson, Julie
AU  - Kenthapadi, Krishnaram
AU  - Kloumann, Isabel
AU  - Wallach, Hanna
T3  - FAT* '20
AB  - The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result.• Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search.• Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform.• Hanna Wallach contributes how Microsoft is applying fairness principles in practice.• Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm.• Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system.• Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform.Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375680
SP  - 682
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375680
KW  - artificial intelligence
KW  - fairness
KW  - ethics
KW  - data
KW  - ML fairness
KW  - algorithmic decision-making
KW  - practitioners
ER  - 

TY  - CONF
TI  - What's sex got to do with machine learning?
AU  - Hu, Lily
AU  - Kohler-Hausmann, Issa
T3  - FAT* '20
AB  - The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group "female" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world.We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the "effects" that sex purportedly "causes" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature.Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-à-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375674
SP  - 513
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375674
KW  - machine learning
KW  - discrimination
KW  - law
KW  - algorithmic fairness
KW  - causal inference
KW  - social philosophy
ER  - 

TY  - CONF
TI  - Can an algorithmic system be a 'friend' to a police officer's discretion? ACM FAT 2020 translation tutorial
AU  - Oswald, Marion
AU  - Powell, David
T3  - FAT* '20
AB  - This tutorial aims to increase understanding of the importance of discretion in police decision-making. It will guide computer scientists, policy-makers, lawyers and others in considering practical and technical issues crucial to avoiding the prejudicial and instead develop algorithms that are supportive - a 'friend'- to legitimate discretionary decision-making. It combines explanation of the relevant law and related literature with discussion based upon deep operational experience in the area of preventative and protective policing work.Autonomy and discretion are fundamental to police work, not only in relation to strategy and policy but for day-to-day operational decisions taken by front line officers. Such discretion 'recognizes the fallibility of interfacing rules with their field of application.' (Hildebrandt 2016). This discretion is not unbounded however and English common law expects discretion to be exercised reasonably and fairly. Conversely, discretion must not be fettered unlawfully, by failing to take a relevant factor into account when making a decision, or by abdicating responsibility to another person, body or 'thing'. Algorithmic systems have the potential to contribute to factors relevant to the decision in question at the point of interaction between their outputs and the real-world outcome for the victim, offender and/or community.Algorithmic decision tools present a number of challenges to legitimate discretionary police decision-making. Unnuanced outputs could be highly influential on the human decision-maker (Cooke and Michie 2012) and may undermine discretionary power to deal with atypical cases and 'un-thought of' factors that rely upon uncodified knowledge (Oswald 2018).Practical and technical considerations will be crucial to developing MLA that are supportive to discretionary decision-making. These include the methodological approach, design of the humancomputer interface having regard the decision-maker's responsibility to give reasons for their decision, the avoidance of unnuanced or over-confident framing of results, understanding of the policing context in which the MLA will operate, and consideration of the implications of organisational culture and processes to the MLA's influence.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375673
SP  - 698
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375673
KW  - machine learning
KW  - police
KW  - algorithms
KW  - discretion
ER  - 

TY  - CONF
TI  - Assessing the intersection of organizational structure and FAT* efforts within industry: implications tutorial
AU  - Rakova, Bogdana
AU  - Chowdhury, Rumman
AU  - Yang, Jingying
T3  - FAT* '20
AB  - The work within the Fairness, Accountability, and Transparency of ML (fair-ML) community will positively benefit from appreciating the role of organizational culture and structure in the effective practice of fair-ML efforts of individuals, teams, and initiatives within industry. In this tutorial session we will explore various organizational structures and possible leverage points to effectively intervene in the process of design, development, and deployment of AI systems, towards contributing to positive fair-ML outcomes. We will begin by presenting the results of interviews conducted during an ethnographic study among practitioners working in industry, including themes related to: origination and evolution, common challenges, ethical tensions, and effective enablers. The study was designed through the lens of Industrial Organizational Psychology and aims to create a mapping of the current state of the fair-ML organizational structures inside major AI companies. We also look at the most-desired future state to enable effective work to increase algorithmic accountability, as well as the key elements in the transition from the current to that future state. We investigate drivers for change as well as the tensions between creating an 'ethical' system vs one that is 'ethical' enough. After presenting our preliminary findings, the rest of the tutorial will be highly interactive. Starting with a facilitated activity in break out groups, we will discuss the already identified challenges, best practices, and mitigation strategies. Finally, we hope to create space for productive discussion among AI practitioners in industry, academic researchers within various fields working directly on algorithmic accountability and transparency, advocates for various communities most impacted by technology, and others. Based on the interactive component of the tutorial, facilitators and interested participants will collaborate on further developing the discussed challenges into scenarios and guidelines that will be published as a follow up report.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375672
SP  - 697
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375672
KW  - fair machine learning
KW  - empirical study
KW  - I/O psychology
KW  - need-finding
KW  - organizational structure
ER  - 

TY  - CONF
TI  - The meaning and measurement of bias: lessons from natural language processing
AU  - Jacobs, Abigail Z.
AU  - Blodgett, Su Lin
AU  - Barocas, Solon
AU  - Daumé, Hal
AU  - Wallach, Hanna
T3  - FAT* '20
AB  - The recent interest in identifying and mitigating bias in computational systems has introduced a wide range of different—and occasionally incomparable—proposals for what constitutes bias in such systems. This tutorial introduces the language of measurement modeling from the quantitative social sciences as a framework for examining how social, organizational, and political values enter computational systems and unpacking the varied normative concerns operationalized in different techniques for measuring "bias." We show that this framework helps to clarify the way unobservable theoretical constructs—such as "creditworthiness," "risk to society," or "tweet toxicity"—are turned into measurable quantities and how this process may introduce fairness-related harms. In particular, we demonstrate how to systematically assess the construct validity and reliability of these measurements to detect and characterize specific types of harms, which arise from mismatches between constructs and their operationalizations. We then take a critical look at existing approaches to examining "bias" in NLP models, ranging from work on embedding spaces to machine translation and hate speech detection. We show that measurement modeling can help uncover the implicit constructs that such work aims to capture when measuring "bias." In so doing, we illustrate the limits of current "debiasing" techniques, which have obscured the specific harms whose measurements they implicitly aim to reduce. By introducing the language of measurement modeling, we provide the FAT* community with a framework for making explicit and testing assumptions about unobservable theoretical constructs embedded in computational systems, thereby clarifying and uniting our understandings of fairness-related harms.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375671
SP  - 706
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375671
KW  - fairness
KW  - bias
KW  - construct validity
KW  - measurement
KW  - word embeddings
ER  - 

TY  - CONF
TI  - Experimentation with fairness-aware recommendation using librec-auto: hands-on tutorial
AU  - Burke, Robin Douglas
AU  - Mansoury, Masoud
AU  - Sonboli, Nasim
T3  - FAT* '20
AB  - The field of machine learning fairness has developed metrics, methodologies, and data sets for experimenting with classification algorithms. However, equivalent research is lacking in the area of personalized recommender systems. This 180-minute hands-on tutorial will introduce participants to concepts in fairness-aware recommendation, and metrics and methodologies in evaluating recommendation fairness. Participants will also gain hands-on experience with conducting fairness-aware recommendation experiments with the LibRec recommendation system using the libauto scripting platform, and learn the steps required to configure their own experiments, incorporate their own data sets, and design their own algorithms and metrics.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375670
SP  - 700
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375670
KW  - recommender systems
KW  - fairness
KW  - evaluation
KW  - software
ER  - 

TY  - CONF
TI  - From the total survey error framework to an error framework for digital traces of humans: translation tutorial
AU  - Sen, Indira
AU  - Flöck, Fabian
AU  - Weller, Katrin
AU  - Weiß, Bernd
AU  - Wagner, Claudia
T3  - FAT* '20
AB  - The digital traces of hundreds of millions of people offer increasingly comprehensive pictures of both individuals and groups on different platforms, but also allow inferences about broader target populations beyond those platforms. Studying the errors that can occur when digital traces are used to learn about humans and social phenomena is essential. Many similar errors also affect survey estimates, which survey designers have been addressing for decades, most notably using the Total Survey Error Framework (TSE). In this tutorial, we first introduce the audience to the concepts and guidelines of the TSE and how they are applied by survey practitioners in the social sciences. Second, we introduce our own conceptual framework to diagnose, understand, and avoid errors that may occur in studies that are based on digital traces of humans.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375669
SP  - 701
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375669
KW  - representativeness
KW  - computational social science
KW  - digital traces
KW  - measurement errors
KW  - survey methodology
ER  - 

TY  - CONF
TI  - Policy 101: an introduction to public policymaking in the EU and US
AU  - Duarte, Natasha
AU  - Adams, Stan
T3  - FAT* '20
AB  - Navigating the rules, processes, and venues through which public policy is made can seem daunting. But public participation in these processes is a crucial part of democratic governance. With a general understanding of when, where, and how to engage in policymaking, anyone can become a policy advocate. This tutorial will introduce some of the most common US (federal and state) and EU policymaking processes and provide guidance to experts in other domains (such as data and computer science) who want to get involved in policymaking. We will discuss the practical considerations involved in identifying and choosing among policymaking opportunities and discuss how to maximize the impact of policymaking interventions. This tutorial is intended to be interactive and will be improved by audience participation and questions.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375668
SP  - 703
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375668
KW  - policy
KW  - government
KW  - advocacy
KW  - public policy
ER  - 

TY  - CONF
TI  - AI explainability 360: hands-on tutorial
AU  - Arya, Vijay
AU  - Bellamy, Rachel K. E.
AU  - Chen, Pin-Yu
AU  - Dhurandhar, Amit
AU  - Hind, Michael
AU  - Hoffman, Samuel C.
AU  - Houde, Stephanie
AU  - Liao, Q. Vera
AU  - Luss, Ronny
AU  - Mojsilović, Aleksandra
AU  - Mourad, Sami
AU  - Pedemonte, Pablo
AU  - Raghavendra, Ramya
AU  - Richards, John
AU  - Sattigeri, Prasanna
AU  - Shanmugam, Karthikeyan
AU  - Singh, Moninder
AU  - Varshney, Kush R.
AU  - Wei, Dennis
AU  - Zhang, Yunfeng
T3  - FAT* '20
AB  - This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models.Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space.Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below).In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general.Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks.Here is a rough agenda of the tutorial:1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms.2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers).3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications.4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions.5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks.6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit.7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375667
SP  - 696
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375667
KW  - interpretability
KW  - explainability
KW  - transparency
KW  - taxonomy
KW  - open source
ER  - 

TY  - CONF
TI  - Positionality-aware machine learning: translation tutorial
AU  - Kaeser-Chen, Christine
AU  - Dubois, Elizabeth
AU  - Schüür, Friederike
AU  - Moss, Emanuel
T3  - FAT* '20
AB  - Positionality is a person's unique and always partial view of the world which is shaped by social and political contexts. Machine Learning (ML) systems have positionality, too, as a consequence of the choices we make when we develop ML systems. Being positionality-aware is key for ML practitioners to acknowledge and embrace the necessary choices embedded in ML by its creators.When groups form a shared view of the world, or group positionality, they have the power to embed and institutionalize their unique perspectives in artifacts such as standards and ontologies. For example, the international standard for reporting diseases and health conditions (International Classification of Diseases, ICD) is shaped by a distinctly medical, European and North American perspective. It dictates how we collect data, and limits what questions we can ask of data and what ML systems we can develop. Researchers struggle to study the effects of social factors on health outcomes because of what the ICD renders legible (usually in medicalized terms) and what it renders invisible (usually social contexts) in data. The ICD, as with all information infrastructures, promotes and propagates the perspective(s) of its creators. Over time, it establishes what counts as "truth".Positionality, and how it embeds itself in standards, ontologies, and data collection, is the root for bias in our data and algorithms. Every perspective has its limits - there is no view from nowhere. Without an awareness of positionality, the current debate on bias in machine learning is quite limited: adding more data to the set cannot remove bias. Instead, we propose positionality-aware ML, a new workflow focused on continuous evaluation and improvement of the fit between the positionality embedded in ML systems and the scenarios within which it is deployed.To demonstrate how to uncover positionality in standards, ontologies, data, and ML systems, we discuss recent work on online harassment of Canadian journalists and politicians on Twitter. Using legal definitions of hate speech and harassment, Twitter's community standards, and insight from interviews with journalists and politicians, we created standards and annotation guidelines for labeling the intensity of harassment in tweets. We then hand labeled a sample of data and through this process identified instances where positionality impacts choices about how many categories of harassment should exist, how to label boundary cases, and how to interpret messy data. We take three perspectives—technical, systems, socio-technical—that when combined illuminate areas of tension which serve as a signal of misalignment between the positionality embedded in the ML system and the deployment context. We demonstrate how the concept of positionality allows us to delineate sets of use cases that may not be suited for automated, ML solutions. Finally, we discuss strategies for developing positionality-aware ML systems, which embed a positionality appropriate for the application context, and continuously evolve to maintain this contextual fit, with an emphasis on the need for of democratic, egalitarian dialogues between knowledge-producing groups.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375666
SP  - 704
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375666
KW  - artificial intelligence
KW  - categories
KW  - positionality
ER  - 

TY  - CONF
TI  - Leap of FATE: human rights as a complementary framework for AI policy and practice
AU  - Cath, Corinne
AU  - Latonero, Mark
AU  - Marda, Vidushi
AU  - Pakzad, Roya
T3  - FAT* '20
AB  - The premise of this translation tutorial is that human rights serves as a complementary framework - in addition to Fairness, Accountability, Transparency, and Ethics - for guiding and governing artificial intelligence (AI) and machine learning research and development. Attendees will participate in a case study, which will demonstrate show how a human rights framework, grounded in international law, fundamental values, and global systems of accountability, can offer the technical community a practical approach to addressing global AI risks and harms. This tutorial discusses how human rights frameworks can inform, guide and govern AI policy and practice in a manner that is complementary to Fairness, Accountability, Transparency, and Ethics (FATE) frameworks. Using the case study of researchers developing a facial recognition API at a tech company and its use by a law enforcement client, we will engage the audience to think through the benefits and challenges of applying human rights frameworks to AI system design and deployment. We will do so by providing a brief overview of the international human rights law, and various non-binding human rights frameworks in relation to our current discussions around FATE and then apply them to contemporary debates and case studies
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375665
SP  - 702
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375665
KW  - policy
KW  - AI
KW  - ethics
KW  - law
KW  - governance
KW  - FAT
KW  - human rights
KW  - practice
ER  - 

TY  - CONF
TI  - Explainable AI in industry: practical challenges and lessons learned: implications tutorial
AU  - Gade, Krishna
AU  - Geyik, Sahin Cem
AU  - Kenthapadi, Krishnaram
AU  - Mithal, Varun
AU  - Taly, Ankur
T3  - FAT* '20
AB  - Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling.As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques.In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as:• Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations.• Sales: Understanding of sales predictions in terms of customer up-sell/churn.• Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent.• Lending: How to understand/interpret lending decisions made by an AI system.We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375664
SP  - 699
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375664
ER  - 

TY  - CONF
TI  - Two computer scientists and a cultural scientist get hit by a driver-less car: a method for situating knowledge in the cross-disciplinary study of F-A-T in machine learning: translation tutorial
AU  - Ganesh, Maya Indira
AU  - Dechesne, Francien
AU  - Waseem, Zeerak
T3  - FAT* '20
AB  - In a workshop organized in December 2017 in Leiden, the Netherlands, a group of lawyers, computer scientists, artists, activists and social and cultural scientists collectively read a computer science paper about 'improving fairness'. This session was perceived by many participants as eye-opening on how different epistemologies shape approaches to the problem, method and solutions, thus enabling further cross-disciplinary discussions during the rest of the workshop. For many participants it was both refreshing and challenging, in equal measure, to understand how another discipline approached the problem of fairness. Now, as a follow-up we propose a translation tutorial that will engage participants at the FAT* conference in a similar exercise. We will invite participants to work in small groups reading excerpts of academic papers from different disciplinary perspectives on the same theme. We argue that most of us do not read outside our disciplines and thus are not familiar with how the same issues might be framed and addressed by our peers. Thus the purpose will be to have participants reflect on the different genealogies of knowledge in research, and how they erect walls, or generate opportunities for more productive inter-disciplinary work. We argue that addressing, through technical measures or otherwise, matters of ethics, bias and discrimination in AI/ML technologies in society is complicated by the different constructions of knowledge about what ethics (or bias or discrimination) means to different groups of practitioners. In the current academic structure, there are scarce resources to test, build on-or even discard-methods to talk across disciplinary lines. This tutorial is thus proposed to see if this particular method might work.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375663
SP  - 707
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375663
KW  - discrimination
KW  - ethics
KW  - methodology
KW  - natural language processing
KW  - bias
KW  - social sciences
KW  - epistemology
KW  - cross-disciplinary
KW  - humanities
KW  - science
KW  - situated knowledge
ER  - 

TY  - CONF
TI  - Probing ML models for fairness with the what-if tool and SHAP: hands-on tutorial
AU  - Wexler, James
AU  - Pushkarna, Mahima
AU  - Robinson, Sara
AU  - Bolukbasi, Tolga
AU  - Zaldivar, Andrew
T3  - FAT* '20
AB  - As more and more industries use machine learning, it's important to understand how these models make predictions, and where bias can be introduced in the process. In this tutorial we'll walk through two open source frameworks for analyzing your models from a fairness perspective. We'll start with the What-If Tool, a visualization tool that you can run inside a Python notebook to analyze an ML model. With the What-If Tool, you can identify dataset imbalances, see how individual features impact your model's prediction through partial dependence plots, and analyze human-centered ML models from a fairness perspective using various optimization strategies.Then we'll look at SHAP, a tool for interpreting the output of any machine learning model, and seeing how a model arrived at predictions for individual datapoints. We will then show how to use SHAP and the What-If Tool together. After the tutorial you'll have the skills to get started with both of these tools on your own datasets, and be better equipped to analyze your models from a fairness perspective.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375662
SP  - 705
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375662
KW  - machine learning
KW  - fairness
KW  - data visualization
ER  - 

TY  - CONF
TI  - Explainable machine learning in deployment
AU  - Bhatt, Umang
AU  - Xiang, Alice
AU  - Sharma, Shubham
AU  - Weller, Adrian
AU  - Taly, Ankur
AU  - Jia, Yunhan
AU  - Ghosh, Joydeep
AU  - Puri, Ruchir
AU  - Moura, José M. F.
AU  - Eckersley, Peter
T3  - FAT* '20
AB  - Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375624
SP  - 648
EP  - 657
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375624
KW  - machine learning
KW  - explainability
KW  - transparency
KW  - deployed systems
KW  - qualitative study
ER  - 

TY  - CONF
TI  - Artificial mental phenomena: psychophysics as a framework to detect perception biases in AI models
AU  - Liang, Lizhen
AU  - Acuna, Daniel E.
T3  - FAT* '20
AB  - Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology—meant to relate quantities from the real world (i.e., "Physics") into subjective measures in the mind (i.e., "Psyche")—to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375623
SP  - 403
EP  - 412
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375623
KW  - artificial psychophysics
KW  - biases in sentiment analysis
KW  - biases in word embeddings
KW  - two-alternative forced choice task
ER  - 

TY  - CONF
TI  - Model agnostic interpretability of rankers via intent modelling
AU  - Singh, Jaspreet
AU  - Anand, Avishek
T3  - FAT* '20
AB  - A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models.We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3375234
SP  - 618
EP  - 628
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3375234
ER  - 

TY  - CONF
TI  - Value-laden disciplinary shifts in machine learning
AU  - Dotan, Ravit
AU  - Milli, Smitha
T3  - FAT* '20
AB  - As machine learning models are increasingly used for high-stakes decision making, scholars have sought to intervene to ensure that such models do not encode undesirable social and political values. However, little attention thus far has been given to how values influence the machine learning discipline as a whole. How do values influence what the discipline focuses on and the way it develops? If undesirable values are at play at the level of the discipline, then intervening on particular models will not suffice to address the problem. Instead, interventions at the disciplinary-level are required.This paper analyzes the discipline of machine learning through the lens of philosophy of science. We develop a conceptual framework to evaluate the process through which types of machine learning models (e.g. neural networks, support vector machines, graphical models) become predominant. The rise and fall of model-types is often framed as objective progress. However, such disciplinary shifts are more nuanced. First, we argue that the rise of a model-type is self-reinforcing-it influences the way model-types are evaluated. For example, the rise of deep learning was entangled with a greater focus on evaluations in compute-rich and data-rich environments. Second, the way model-types are evaluated encodes loaded social and political values. For example, a greater focus on evaluations in compute-rich and data-rich environments encodes values about centralization of power, privacy, and environmental concerns.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373157
SP  - 294
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373157
KW  - machine learning
KW  - deep learning
KW  - philosophy of science
KW  - values in science
ER  - 

TY  - CONF
TI  - The social lives of generative adversarial networks
AU  - Castelle, Michael
T3  - FAT* '20
AB  - Generative adversarial networks (GANs) are a genre of deep learning model of significant practical and theoretical interest for their facility in producing photorealistic 'fake' images which are plausibly similar, but not identical, to a corpus of training data. But from the perspective of a sociologist, the distinctive architecture of GANs is highly suggestive. First, a convolutional neural network for classification, on its own, is (at present) popularly considered to be an 'AI'; and a generative neural network is a kind of inversion of such a classification network (i.e. a layered transformation from a vector of numbers to an image, as opposed to a transformation from an image to a vector of numbers). If, then, in the training of GANs, these two 'AIs' interact with each other in a dyadic fashion, shouldn't we consider that form of learning... social? This observation can lead to some surprising associations as we compare and contrast GANs with the theories of the sociologist Pierre Bourdieu, whose concept of the so-called habitus is one which is simultaneously cognitive and social: a productive perception in which classification practices and practical action cannot be fully disentangled.Bourdieu had long been concerned with the reproduction of social stratification: his early works studied formal public schooling in France not as an egalitarian system but instead as one which unintentionally maintained the persistence of class distinctions. It was, he argued, through the cultural inculcation of an embodied and partially unconscious habitus—a "durably installed generative principle of regulated improvisations"—that, he argued, students from the upper classes are given an advantage which is only further reinforced throughout their educational trajectories. For Bourdieu, institutions of schooling instill "deeply interiorized master patterns" of behavior and thought (and classification) which in turn direct the acquisition of subsequent patterns, whose character is determined not simply by this cognitive layering but by their actual use in lived practice, especially early in childhood development.In this work I develop a productive analogy between the GAN architecture and Bourdieu's habitus, in three ways. First, I call attention to the fact that connectionist approaches and Bourdieu's theories were both conceived as revolts against rule-bound paradigms. In the 1980s, Rumelhart and McClelland used a multilayer neural network to learn the phonology of English past-tense verbs because "sometimes we don't follow the rules... language is full of exceptions to the rules"; and in the case of Bourdieu, the habitus was an answer to a long-standing question: "how can behaviour be regulated without being the product of obedience to rules?" Bourdieu strove to transgress what was then seen in the social sciences as a conceptual opposition between structure-based theories of social life and those which emphasized an embodied agency.Second, I suggest that concerns about bias and discrimination in machine learning in recent years can in part be attributed due to the increased use of ML models not just for static classification but for practical action. Similarly, the habitus for Bourdieu is simultaneously durable and transposable: its judgments may be relatively stable, but are capable of being deployed dynamically in novel and varying social situations—or what ML practitioners might call generalizability. We can thus theorize generative models (including GANs) as biased not just in their stereotyped classifications, but through their potential for actively generating new biased data. These generated actions then recursively become part of the social arena Bourdieu called the field, into which new agents are 'born' and for which they may know few alternatives.Finally, it is intriguing that GAN researchers and Bourdieu both extensively use metaphors from game theory. Goodfellow described the GAN architecture as a "two-player minimax game with value function V(G,D)", meaning that there is a single abstract function whose output value the discriminator is trying to maximize and which the generator is trying to minimize; but the dynamic nature of the GAN training process means that convergence to Nash equilibrium is nontrivial. But for Bourdieu, such a utility-based approach to artistic creation could not be more crude when compared to the social reality of art worlds: utilitarianism is, for him, "the degree zero of sociology", by which he means an isolated, inert, and amodal—and therefore not particularly sociological—starting point. Moreover, 19th-century bohemian culture was characterized primarily by its inversion of financial incentives, in which failure is a kind of success, and "selling out" (i.e. maximizing profit) worst of all; and thus the relentless optimization of neural networks may be fundamentally at odds with the "value functions" of many human artists. I conclude that deep learning, while primarily understood as a scientific and technical achievement, may also intentionally or unintentionally constitute a nascent, independent reinvention of social theory.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373156
SP  - 413
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373156
KW  - game theory
KW  - habitus
KW  - bias
KW  - generative adversarial networks
KW  - sociological theory
ER  - 

TY  - CONF
TI  - Preference-informed fairness
AU  - Kim, Michael P.
AU  - Korolova, Aleksandra
AU  - Rothblum, Guy N.
AU  - Yona, Gal
T3  - FAT* '20
AB  - In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373155
SP  - 546
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373155
KW  - algorithmic fairness
ER  - 

TY  - CONF
TI  - Assessing algorithmic fairness with unobserved protected class using data combination
AU  - Kallus, Nathan
AU  - Mao, Xiaojie
AU  - Zhou, Angela
T3  - FAT* '20
AB  - The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373154
SP  - 110
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373154
ER  - 

TY  - CONF
TI  - Algorithmic accountability in public administration: the GDPR paradox
AU  - Kang, Sunny Seon
T3  - FAT* '20
AB  - The EU General Data Protection Regulation ("GDPR") is often represented as a larger than life behemoth that will fundamentally transform the world of big data. Abstracted from its constituent parts of corresponding rights, responsibilities, and exemptions, the operative scope of the GDPR can be unduly aggrandized, when in reality, it caters to the specific policy objectives of legislators and institutional stakeholders.With much uncertainty ahead on the precise implementation of the GDPR, academic and policy discussions are debating the adequacy of protections for automated decision-making in GDPR Articles 13 (right to be informed of automated treatment), 15 (right of access by the data subject), and 22 (safeguards to profiling). Unfortunately, the literature to date disproportionately focuses on the impact of AI in the private sector, and deflects any extensive review of automated enforcement tools in public administration.Even though the GDPR enacts significant safeguards against automated decisions, it does so with deliberate design: to balance the interests of data protection with the growing demand for algorithms in the administrative state. In order to facilitate inter-agency data flows and sensitive data processing that fuel the predictive power of algorithmic enforcement tools, the GDPR decisively surrenders to the procedural autonomy of Member States to authorize these practices. Yet, due to a dearth of research on the GDPR's stance on government deployed algorithms, it is not widely known that public authorities can benefit from broadly worded exemptions to restrictions on automated decision-making, and even circumvent remedies for data subjects through national legislation.The potential for public authorities to invoke derogations from the GDPR must be contained by the fundamental guarantees of due process, judicial review, and equal treatment. This paper examines the interplay of these principles within the prospect of algorithmic decision-making by public authorities.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373153
SP  - 32
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373153
KW  - algorithmic accountability
KW  - automated decisions
KW  - due process
KW  - general data protection regulation (GDPR)
KW  - predictive enforcement tools
ER  - 

TY  - CONF
TI  - Onward for the freedom of others: marching beyond the AI ethics
AU  - Terzis, Petros
T3  - FAT* '20
AB  - The debate on the ethics of Artificial Intelligence brought together different stakeholders including but not limited to academics, policymakers, CEOs, activists, workers' representatives, lobbyists, journalists, and 'moral machines'. Prominent political institutions crafted principles for the 'ethical being' of the AI companies while tech giants were documenting ethics in a series of self-written guidelines. In parallel, a large community started to flourish, focusing on how to technically embed ethical parameters into algorithmic systems. Founded upon the philosophical work of Simone de Beauvoir and Jean-Paul Sartre, this paper explores the philosophical antinomies of the 'AI Ethics' debate as well as the conceptual disorientation of the 'fairness discussion'. By bringing the philosophy of existentialism to the dialogue, this paper attempts to challenge the dialectical monopoly of utilitarianism and sheds fresh light on the -already glaring- AI arena. Why is 'the AI Ethics guidelines' a futile battle doomed to dangerous abstraction? How this battle can harm our sense of collective freedom? Which is the uncomfortable reality that remains obscured by the smoke-gas of the 'AI Ethics' discussion? And eventually, what's the alternative? There seems to be a different pathway for discussing and implementing ethics; A pathway that sets the freedom of others at the epicenter of the battle for a sustainable and open to all future.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3373152
SP  - 220
EP  - 229
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3373152
KW  - artificial intelligence
KW  - ethics
KW  - algorithms
KW  - existentialism
KW  - philosophy
ER  - 

TY  - CONF
TI  - Auditing radicalization pathways on YouTube
AU  - Ribeiro, Manoel Horta
AU  - Ottoni, Raphael
AU  - West, Robert
AU  - Almeida, Virgílio A. F.
AU  - Meira, Wagner
T3  - FAT* '20
AB  - Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372879
SP  - 131
EP  - 141
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372879
KW  - algorithmic auditing
KW  - extremism
KW  - hate speech
KW  - radicalization
ER  - 

TY  - CONF
TI  - Fairness is not static: deeper understanding of long term fairness via simulation studies
AU  - D'Amour, Alexander
AU  - Srinivasan, Hansa
AU  - Atwood, James
AU  - Baljekar, Pallavi
AU  - Sculley, D.
AU  - Halpern, Yoni
T3  - FAT* '20
AB  - As machine learning becomes increasingly incorporated within high impact decision ecosystems, there is a growing need to understand the long-term behaviors of deployed ML-based decision systems and their potential consequences. Most approaches to understanding or improving the fairness of these systems have focused on static settings without considering long-term dynamics. This is understandable; long term dynamics are hard to assess, particularly because they do not align with the traditional supervised ML research framework that uses fixed data sets. To address this structural difficulty in the field, we advocate for the use of simulation as a key tool in studying the fairness of algorithms. We explore three toy examples of dynamical systems that have been previously studied in the context of fair decision making for bank loans, college admissions, and allocation of attention. By analyzing how learning agents interact with these systems in simulation, we are able to extend previous work, showing that static or single-step analyses do not give a complete picture of the long-term consequences of an ML-based decision system. We provide an extensible open-source software framework for implementing fairness-focused simulation studies and further reproducible research, available at https://github.com/google/ml-fairness-gym.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372878
SP  - 525
EP  - 534
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372878
ER  - 

TY  - CONF
TI  - Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination
AU  - Bogen, Miranda
AU  - Rieke, Aaron
AU  - Ahmed, Shazeda
T3  - FAT* '20
AB  - Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted.This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities.This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372877
SP  - 492
EP  - 500
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372877
ER  - 

TY  - CONF
TI  - The philosophical basis of algorithmic recourse
AU  - Venkatasubramanian, Suresh
AU  - Alfano, Mark
T3  - FAT* '20
AB  - Philosophers have established that certain ethically important values are modally robust in the sense that they systematically deliver correlative benefits across a range of counterfactual scenarios. In this paper, we contend that recourse - the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios - is such a modally robust good. In particular, we argue that two essential components of a good life - temporally extended agency and trust - are underwritten by recourse.We critique existing approaches to the conceptualization, operationalization and implementation of recourse. Based on these criticisms, we suggest a revised approach to recourse and give examples of how it might be implemented - especially for those who are least well off1.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372876
SP  - 284
EP  - 293
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372876
KW  - recourse
KW  - algorithmic decision making
KW  - precarity
KW  - robust goods
ER  - 

TY  - CONF
TI  - Multi-layered explanations from algorithmic impact assessments in the GDPR
AU  - Kaminski, Margot E.
AU  - Malgieri, Gianclaudio
T3  - FAT* '20
AB  - Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability—individual rights and systemic governance— and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372875
SP  - 68
EP  - 79
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372875
KW  - law
KW  - general data protection regulation
KW  - impact assessments
ER  - 

TY  - CONF
TI  - Toward situated interventions for algorithmic equity: lessons from the field
AU  - Katell, Michael
AU  - Young, Meg
AU  - Dailey, Dharma
AU  - Herman, Bernease
AU  - Guetler, Vivian
AU  - Tam, Aaron
AU  - Bintz, Corinne
AU  - Raz, Daniella
AU  - Krafft, P. M.
T3  - FAT* '20
AB  - Research to date aimed at the fairness, accountability, and transparency of algorithmic systems has largely focused on topics such as identifying failures of current systems and on technical interventions intended to reduce bias in computational processes. Researchers have given less attention to methods that account for the social and political contexts of specific, situated technical systems at their points of use. Co-developing algorithmic accountability interventions in communities supports outcomes that are more likely to address problems in their situated context and re-center power with those most disparately affected by the harms of algorithmic systems. In this paper we report on our experiences using participatory and co-design methods for algorithmic accountability in a project called the Algorithmic Equity Toolkit. The main insights we gleaned from our experiences were: (i) many meaningful interventions toward equitable algorithmic systems are non-technical; (ii) community organizations derive the most value from localized materials as opposed to what is "scalable" beyond a particular policy context; (iii) framing harms around algorithmic bias suggests that more accurate data is the solution, at the risk of missing deeper questions about whether some technologies should be used at all. More broadly, we found that community-based methods are important inroads to addressing algorithmic harms in their situated contexts.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372874
SP  - 45
EP  - 55
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372874
ER  - 

TY  - CONF
TI  - Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing
AU  - Raji, Inioluwa Deborah
AU  - Smart, Andrew
AU  - White, Rebecca N.
AU  - Mitchell, Margaret
AU  - Gebru, Timnit
AU  - Hutchinson, Ben
AU  - Smith-Loud, Jamila
AU  - Theron, Daniel
AU  - Barnes, Parker
T3  - FAT* '20
AB  - Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372873
SP  - 33
EP  - 44
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372873
KW  - machine learning
KW  - accountability
KW  - algorithmic audits
KW  - responsible innovation
ER  - 

TY  - CONF
TI  - Fair decision making using privacy-protected data
AU  - Pujol, David
AU  - McKenna, Ryan
AU  - Kuppam, Satya
AU  - Hay, Michael
AU  - Machanavajjhala, Ashwin
AU  - Miklau, Gerome
T3  - FAT* '20
AB  - Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a trade-off between protecting privacy and the accuracy of decisions, we initiate a first-of-its-kind study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making. We empirically investigate novel tradeoffs on two real-world decisions made using U.S. Census data (allocation of federal funds and assignment of voting rights benefits) as well as a classic apportionment problem.Our results show that if decisions are made using an ∈-differentially private version of the data, under strict privacy constraints (smaller ∈), the noise added to achieve privacy may disproportionately impact some groups over others. We propose novel measures of fairness in the context of randomized differentially private algorithms and identify a range of causes of outcome disparities. We also explore improved algorithms to remedy the unfairness observed.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372872
SP  - 189
EP  - 199
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372872
ER  - 

TY  - CONF
TI  - Roles for computing in social change
AU  - Abebe, Rediet
AU  - Barocas, Solon
AU  - Kleinberg, Jon
AU  - Levy, Karen
AU  - Raghavan, Manish
AU  - Robinson, David G.
T3  - FAT* '20
AB  - A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems — roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined — changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372871
SP  - 252
EP  - 260
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372871
KW  - discrimination
KW  - inequality
KW  - social change
KW  - societal implications of AI
ER  - 

TY  - CONF
TI  - Explainability fact sheets: a framework for systematic assessment of explainable approaches
AU  - Sokol, Kacper
AU  - Flach, Peter
T3  - FAT* '20
AB  - Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372870
SP  - 56
EP  - 67
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372870
KW  - interpretability
KW  - explainability
KW  - transparency
KW  - AI
KW  - taxonomy
KW  - ML
KW  - desiderata
KW  - fact sheet
KW  - work sheet
ER  - 

TY  - CONF
TI  - The false promise of risk assessments: epistemic reform and the limits of fairness
AU  - Green, Ben
T3  - FAT* '20
AB  - Risk assessments have proliferated in the United States criminal justice system. The theory of change motivating their adoption involves two key assumptions: first, that risk assessments will reduce human biases by making objective decisions, and second, that risk assessments will promote criminal justice reform. In this paper I interrogate both of these assumptions, concluding that risk assessments are an ill-advised tool for challenging the centrality and legitimacy of incarceration within the criminal justice system. First, risk assessments fail to provide objectivity, as their use creates numerous sites of discretion. Second, risk assessments provide no guarantee of reducing incarceration; instead, they risk legitimizing the criminal justice system's structural racism. I then consider, via an "epistemic reform," the path forward for criminal justice reform. I reinterpret recent results regarding the "impossibility of fairness" as not simply a tension between mathematical metrics but as evidence of a deeper tension between notions of equality. This expanded frame challenges the formalist, colorblind proceduralism at the heart of the criminal justice system and suggests a more structural approach to reform. Together, this analysis highlights how algorithmic fairness narrows the scope of judgments about justice and how "fair" algorithms can reinforce discrimination.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372869
SP  - 594
EP  - 606
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372869
KW  - risk assessment
KW  - fairness
KW  - social justice
KW  - criminal justice system
ER  - 

TY  - CONF
TI  - The concept of fairness in the GDPR: a linguistic and contextual interpretation
AU  - Malgieri, Gianclaudio
T3  - FAT* '20
AB  - There is a growing attention on the notion of fairness in the GDPR in the European legal literature. However, the principle of fairness in the Data Protection framework is still ambiguous and uncertain, as computer science literature and interpretative guidelines reveal.This paper looks for a better understanding of the concept of fairness in the data protection field through two parallel methodological tools: linguistic comparison and contextual interpretation.In terms of linguistic comparison, the paper analyses all translations of the world "fair" in the GDPR in the EU official languages, as the CJEU suggests in CILFIT Case for the interpretation of the EU law.The analysis takes into account also the translation of the notion of fairness in other contiguous fields (e.g. at Article 8 of the EU Charter of fundamental rights or in the Consumer field, e.g. Unfair terms directive or Unfair commercial practice directive).In general, the notion of fairness is translated with several different nuances (in accordance or in discordance with the previous Data protection Directive and with Article 8 of the Charter)In some versions different words are used interchangeably (it is the case of French, Spanish and Portuguese texts), in other versions there seems to be a specific rationale for using different terms in different parts of the GDPR (it is the case of German and Greek version).The analysis reveals three mean semantic notions: correctness (Italian, Swedish, Romanian), loyalty (French, Spanish, Portuguese and the German version of "Treu und Glaube") and equitability (French, Spanish and Portuguese).Interestingly, these three notions have common roots in the Western legal history: the Roman law notion of "bona fide".Taking into account both the value of "bona fide" in the current European legal contexts and also a contextual interpretation of the role of fairness in the GDPR, the preliminary conclusions is that fairness refers to a substantial balancing of interests among data controllers and data subjects.The approach of fairness is effect-based: what is relevant is not the formal respect of procedures (in terms of transparency, lawfulness or accountability), but the substantial mitigation of unfair imbalances that create situations of "vulnerability". Building on these reflections, the paper analyses how the notion of fairness and imbalance are related to the idea of vulnerability, within and beyond the GDPR.In sum, the article suggests that the best interpretation of the fairness principles in the GDPR (taking into account both the notion of procedural fairness and of fair balancing) is the mitigation of data subjects' vulnerabilities through specific safeguards and measures.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372868
SP  - 154
EP  - 166
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372868
KW  - GDPR
KW  - data protection
KW  - fairness
KW  - linguistic comparison
ER  - 

TY  - CONF
TI  - Implications of AI (un-)fairness in higher education admissions: the effects of perceived AI (un-)fairness on exit, voice and organizational reputation
AU  - Marcinkowski, Frank
AU  - Kieslich, Kimon
AU  - Starke, Christopher
AU  - Lünich, Marco
T3  - FAT* '20
AB  - Algorithmic decision-making (ADM) is becoming increasingly important in all areas of social life. In higher education, machine-learning systems have manifold uses because they can efficiently process large amounts of student data and use these data to arrive at effective decisions. Despite the potential upsides of ADM systems, fairness concerns are gaining momentum in academic and public discourses. The criticism largely focuses on the disparate effects of ADM. That is, algorithms may not serve as objective and fair decision-makers but, rather, reproduce biases existing within the respective training data. This study adopted a different approach by focusing on individual perceptions of fairness. Specifically, we looked at two different dimensions of perceived fairness: (i) procedural fairness and (ii) distributive fairness. Using cross-sectional survey data (n = 304) from a large German university, we tested whether students' assessments of fairness differ with respect to algorithmic vs. human decision-making (HDM) within the higher education context. Furthermore, we investigated whether fairness perceptions have subsequent effects on three different outcome variables, which are hugely important for universities: (1) exit, (2) voice, and (3) organizational reputation. The results of our survey suggest that participants evaluated ADM higher than HDM in terms of both procedural and distributive fairness. Concerning the subsequent effects of fairness perceptions, we find that (1) distributive fairness as well as procedural fairness perceptions have a negative impact on the intention to protest against an ADM system, whereas (2) only procedural fairness perceptions negatively affect the likelihood of exiting. Finally, (3) distributive fairness, but not procedural fairness perceptions have a positive effect on organizational reputation. For universities aiming to implement ADM systems, it is crucial, therefore, to take possible fairness issues and their further implications into account.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372867
SP  - 122
EP  - 130
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372867
KW  - artificial intelligence
KW  - algorithmic decision making
KW  - distributive fairness
KW  - exit
KW  - higher education systems
KW  - procedural fairness
KW  - reputation
KW  - voice
ER  - 

TY  - CONF
TI  - Recommendations and user agency: the reachability of collaboratively-filtered information
AU  - Dean, Sarah
AU  - Rich, Sarah
AU  - Recht, Benjamin
T3  - FAT* '20
AB  - Recommender systems often rely on models which are trained to maximize accuracy in predicting user preferences. When the systems are deployed, these models determine the availability of content and information to different users. The gap between these objectives gives rise to a potential for unintended consequences, contributing to phenomena such as filter bubbles and polarization. In this work, we consider directly the information availability problem through the lens of user recourse. Using ideas of reachability, we propose a computationally efficient audit for top-N linear recommender models. Furthermore, we describe the relationship between model complexity and the effort necessary for users to exert control over their recommendations. We use this insight to provide a novel perspective on the user cold-start problem. Finally, we demonstrate these concepts with an empirical investigation of a state-of-the-art model trained on a widely used movie ratings dataset.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372866
SP  - 436
EP  - 445
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372866
ER  - 

TY  - CONF
TI  - Data in new delhi's predictive policing system
AU  - Marda, Vidushi
AU  - Narayan, Shivangi
T3  - FAT* '20
AB  - In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372865
SP  - 317
EP  - 324
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372865
KW  - sociotechnical systems
KW  - predictive policing
KW  - fairness-aware machine learning
KW  - interdisciplinary
ER  - 

TY  - CONF
TI  - On the apparent conflict between individual and group fairness
AU  - Binns, Reuben
T3  - FAT* '20
AB  - A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372864
SP  - 514
EP  - 524
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372864
KW  - machine learning
KW  - discrimination
KW  - fairness
KW  - justice
KW  - statistical parity
KW  - individual fairness
ER  - 

TY  - CONF
TI  - Case study: predictive fairness to reduce misdemeanor recidivism through social service interventions
AU  - Rodolfa, Kit T.
AU  - Salomon, Erika
AU  - Haynes, Lauren
AU  - Mendieta, Iván Higuera
AU  - Larson, Jamie
AU  - Ghani, Rayid
T3  - FAT* '20
AB  - The criminal justice system is currently ill-equipped to improve outcomes of individuals who cycle in and out of the system with a series of misdemeanor offenses. Often due to constraints of caseload and poor record linkage, prior interactions with an individual may not be considered when an individual comes back into the system, let alone in a proactive manner through the application of diversion programs. The Los Angeles City Attorney's Office recently created a new Recidivism Reduction and Drug Diversion unit (R2D2) tasked with reducing recidivism in this population. Here we describe a collaboration with this new unit as a case study for the incorporation of predictive equity into machine learning based decision making in a resource-constrained setting. The program seeks to improve outcomes by developing individually-tailored social service interventions (i.e., diversions, conditional plea agreements, stayed sentencing, or other favorable case disposition based on appropriate social service linkage rather than traditional sentencing methods) for individuals likely to experience subsequent interactions with the criminal justice system, a time and resource-intensive undertaking that necessitates an ability to focus resources on individuals most likely to be involved in a future case. Seeking to achieve both efficiency (through predictive accuracy) and equity (improving outcomes in traditionally under-served communities and working to mitigate existing disparities in criminal justice outcomes), we discuss the equity outcomes we seek to achieve, describe the corresponding choice of a metric for measuring predictive fairness in this context, and explore a set of options for balancing equity and efficiency when building and selecting machine learning models in an operational public policy setting.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372863
SP  - 142
EP  - 153
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372863
KW  - algorithmic fairness
KW  - criminal justice
KW  - machine learning disparities
KW  - racial bias
ER  - 

TY  - CONF
TI  - Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from?
AU  - Geiger, R. Stuart
AU  - Yu, Kevin
AU  - Yang, Yanlai
AU  - Dai, Mindy
AU  - Qiu, Jie
AU  - Tang, Rebekah
AU  - Huang, Jenny
T3  - FAT* '20
AB  - Many machine learning projects for new application areas involve teams of humans who label data for a particular purpose, from hiring crowdworkers to the paper's authors labeling the data themselves. Such a task is quite similar to (or a form of) structured content analysis, which is a longstanding methodology in the social sciences and humanities, with many established best practices. In this paper, we investigate to what extent a sample of machine learning application papers in social computing — specifically papers from ArXiv and traditional publications performing an ML classification task on Twitter data — give specific details about whether such best practices were followed. Our team conducted multiple rounds of structured content analysis of each paper, making determinations such as: Does the paper report who the labelers were, what their qualifications were, whether they independently labeled the same items, whether inter-rater reliability metrics were disclosed, what level of training and/or instructions were given to labelers, whether compensation for crowdworkers is disclosed, and if the training data is publicly available. We find a wide divergence in whether such practices were followed and documented. Much of machine learning research and education focuses on what is done once a "gold standard" of training data is available, but we discuss issues around the equally-important aspect of whether such data is reliable in the first place.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372862
SP  - 325
EP  - 336
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372862
KW  - machine learning
KW  - data labeling
KW  - training data
KW  - human annotation
KW  - content analysis
KW  - meta-research
KW  - research integrity
ER  - 

TY  - CONF
TI  - The disparate equilibria of algorithmic decision making when individuals invest rationally
AU  - Liu, Lydia T.
AU  - Wilson, Ashia
AU  - Haghtalab, Nika
AU  - Kalai, Adam Tauman
AU  - Borgs, Christian
AU  - Chayes, Jennifer
T3  - FAT* '20
AB  - The long-term impact of algorithmic decision making is shaped by the dynamics between the deployed decision rule and individuals' response. Focusing on settings where each individual desires a positive classification—including many important applications such as hiring and school admissions, we study a dynamic learning setting where individuals invest in a positive outcome based on their group's expected gain and the decision rule is updated to maximize institutional benefit. By characterizing the equilibria of these dynamics, we show that natural challenges to desirable long-term outcomes arise due to heterogeneity across groups and the lack of realizability. We consider two interventions, decoupling the decision rule by group and subsidizing the cost of investment. We show that decoupling achieves optimal outcomes in the realizable case but has discrepant effects that may depend on the initial conditions otherwise. In contrast, subsidizing the cost of investment is shown to create better equilibria for the disadvantaged group even in the absence of realizability.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372861
SP  - 381
EP  - 391
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372861
KW  - machine learning
KW  - fairness
KW  - dynamics
KW  - statistical discrimination
ER  - 

TY  - CONF
TI  - From ethics washing to ethics bashing: a view on tech ethics from within moral philosophy
AU  - Bietti, Elettra
T3  - FAT* '20
AB  - The word 'ethics' is under siege in technology policy circles. Weaponized in support of deregulation, self-regulation or handsoff governance, "ethics" is increasingly identified with technology companies' self-regulatory efforts and with shallow appearances of ethical behavior. So-called "ethics washing" by tech companies is on the rise, prompting criticism and scrutiny from scholars and the tech community at large. In parallel to the growth of ethics washing, its condemnation has led to a tendency to engage in "ethics bashing." This consists in the trivialization of ethics and moral philosophy now understood as discrete tools or pre-formed social structures such as ethics boards, self-governance schemes or stakeholder groups.The misunderstandings underlying ethics bashing are at least threefold: (a) philosophy and "ethics" are seen as a communications strategy and as a form of instrumentalized cover-up or façade for unethical behavior, (b) philosophy is understood in opposition and as alternative to political representation and social organizing and (c) the role and importance of moral philosophy is downplayed and portrayed as mere "ivory tower" intellectualization of complex problems that need to be dealt with in practice.This paper argues that the rhetoric of ethics and morality should not be reductively instrumentalized, either by the industry in the form of "ethics washing," or by scholars and policy-makers in the form of "ethics bashing." Grappling with the role of philosophy and ethics requires moving beyond both tendencies and seeing ethics as a mode of inquiry that facilitates the evaluation of competing tech policy strategies. In other words, we must resist narrow reductivism of moral philosophy as instrumentalized performance and renew our faith in its intrinsic moral value as a mode of knowledgeseeking and inquiry. Far from mandating a self-regulatory scheme or a given governance structure, moral philosophy in fact facilitates the questioning and reconsideration of any given practice, situating it within a complex web of legal, political and economic institutions. Moral philosophy indeed can shed new light on human practices by adding needed perspective, explaining the relationship between technology and other worthy goals, situating technology within the human, the social, the political. It has become urgent to start considering technology ethics also from within and not only from outside of ethics.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372860
SP  - 210
EP  - 219
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372860
KW  - AI
KW  - ethics
KW  - moral philosophy
KW  - regulation
KW  - self-regulation
KW  - technology ethics
KW  - technology law
ER  - 

TY  - CONF
TI  - Studying up: reorienting the study of algorithmic fairness around issues of power
AU  - Barabas, Chelsea
AU  - Doyle, Colin
AU  - Rubinovitz, JB
AU  - Dinakar, Karthik
T3  - FAT* '20
AB  - Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of "studying up". We reflect on the contributions that the call to "study up" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation "upward". A case study from our own work illustrates what it looks like to reorient one's research questions "up" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that "study up". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372859
SP  - 167
EP  - 176
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372859
ER  - 

TY  - CONF
TI  - Interventions for ranking in the presence of implicit bias
AU  - Celis, L. Elisa
AU  - Mehrotra, Anay
AU  - Vishnoi, Nisheeth K.
T3  - FAT* '20
AB  - Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372858
SP  - 369
EP  - 380
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372858
KW  - interventions
KW  - algorithmic fairness
KW  - ranking
KW  - implicit bias
ER  - 

TY  - CONF
TI  - Fair classification and social welfare
AU  - Hu, Lily
AU  - Chen, Yiling
T3  - FAT* '20
AB  - Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of "fairness-to-welfare" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring "more fair" classifiers does not abide by the Pareto Principle—a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372857
SP  - 535
EP  - 545
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372857
ER  - 

TY  - CONF
TI  - Regulating transparency? Facebook, twitter and the german network enforcement act
AU  - Wagner, Ben
AU  - Rozgonyi, Krisztina
AU  - Sekwenz, Marie-Therese
AU  - Cobbe, Jennifer
AU  - Singh, Jatinder
T3  - FAT* '20
AB  - Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns — design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG.This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372856
SP  - 261
EP  - 271
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372856
ER  - 

TY  - CONF
TI  - Doctor XAI: an ontology-based approach to black-box sequential data classification explanations
AU  - Panigutti, Cecilia
AU  - Perotti, Alan
AU  - Pedreschi, Dino
T3  - FAT* '20
AB  - Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372855
SP  - 629
EP  - 639
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372855
KW  - machine learning
KW  - explainable artificial intelligence
KW  - healthcare data
ER  - 

TY  - CONF
TI  - Towards a more representative politics in the ethics of computer science
AU  - Moore, Jared
T3  - FAT* '20
AB  - Ethics curricula in computer science departments should include a focus on the political action of students. While 'ethics' holds significant sway over current discourse in computer science, recent work, particularly in data science, has shown that this discourse elides the underlying political nature of the problems that it aims to solve. In order to avoid these pitfalls—such as co-option, whitewashing, and assumed universal values—we should recognize and teach the political nature of computing technologies, largely through science and technology studies. Education is an essential focus not just intrinsically, but also because computing students end up joining the companies which have outsize impacts on our lives. At those companies, students both have a responsibility to society and agency beyond just engineering decisions, albeit not uniformly. I propose that we move away from strict ethics curricula and include examples of and calls for political action of students and future engineers. Through such examples—calls to action, practitioner reflections, legislative engagement, direct action—we might allow engineers to better recognize both their diverse agencies and responsibilities.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372854
SP  - 414
EP  - 424
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372854
KW  - politics
KW  - science and technology studies
KW  - activism
KW  - civics
ER  - 

TY  - CONF
TI  - POTs: protective optimization technologies
AU  - Kulynych, Bogdan
AU  - Overdorf, Rebekah
AU  - Troncoso, Carmela
AU  - Gürses, Seda
T3  - FAT* '20
AB  - Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks' ability to capture a variety of harms caused by systems.We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms' inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial.We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic beating applications, and recalibrating credit scoring for loan applicants.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372853
SP  - 177
EP  - 188
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372853
KW  - fairness and accountability
KW  - protective optimization technologies
ER  - 

TY  - CONF
TI  - Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making
AU  - Zhang, Yunfeng
AU  - Liao, Q. Vera
AU  - Bellamy, Rachel K. E.
T3  - FAT* '20
AB  - Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372852
SP  - 295
EP  - 305
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372852
KW  - trust
KW  - explainable AI
KW  - confidence
KW  - decision support
ER  - 

TY  - CONF
TI  - Counterfactual risk assessments, evaluation, and fairness
AU  - Coston, Amanda
AU  - Mishler, Alan
AU  - Kennedy, Edward H.
AU  - Chouldechova, Alexandra
T3  - FAT* '20
AB  - Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome.Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may—and as we show empirically, do—induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372851
SP  - 582
EP  - 593
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372851
ER  - 

TY  - CONF
TI  - Explaining machine learning classifiers through diverse counterfactual explanations
AU  - Mothilal, Ramaravind K.
AU  - Sharma, Amit
AU  - Tan, Chenhao
T3  - FAT* '20
AB  - Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372850
SP  - 607
EP  - 617
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372850
ER  - 

TY  - CONF
TI  - What does it mean to 'solve' the problem of discrimination in hiring? social, technical and legal perspectives from the UK on automated hiring systems
AU  - Sánchez-Monedero, Javier
AU  - Dencik, Lina
AU  - Edwards, Lilian
T3  - FAT* '20
AB  - Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective.In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372849
SP  - 458
EP  - 468
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372849
KW  - discrimination
KW  - GDPR
KW  - fairness
KW  - socio-technical systems
KW  - automated hiring
KW  - algorithmic decision-making
KW  - social justice
ER  - 

TY  - CONF
TI  - Multi-category fairness in sponsored search auctions
AU  - Ilvento, Christina
AU  - Jagadeesan, Meena
AU  - Chawla, Shuchi
T3  - FAT* '20
AB  - Fairness in advertising is a topic of particular concern motivated by theoretical and empirical observations in both the computer science and economics literature. We examine the problem of fairness in advertising for general purpose platforms that service advertisers from many different categories. First, we propose inter-category and intra-category fairness desiderata that take inspiration from individual fairness and envy-freeness. Second, we investigate the "platform utility" (a proxy for the quality of allocation) achievable by mechanisms satisfying these desiderata. More specifically, we compare the utility of fair mechanisms against the unfair optimum, and show by construction that our fairness desiderata are compatible with utility. Our mechanisms also enjoy nice implementation properties including metric-obliviousness, which allows the platform to produce fair allocations without needing to know the specifics of the fairness requirements.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372848
SP  - 348
EP  - 358
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372848
KW  - algorithmic fairness
KW  - utility
KW  - advertisement auctions
KW  - envy-freeness
KW  - individual fairness
ER  - 

TY  - CONF
TI  - Fairness and utilization in allocating resources with uncertain demand
AU  - Donahue, Kate
AU  - Kleinberg, Jon
T3  - FAT* '20
AB  - Resource allocation problems are a fundamental domain in which to evaluate the fairness properties of algorithms. The trade-offs between fairness and utilization have a long history in this domain. A recent line of work has considered fairness questions for resource allocation when the demands for the resource are distributed across multiple groups and drawn from probability distributions. In such cases, a natural fairness requirement is that individuals from different groups should have (approximately) equal probabilities of receiving the resource. A largely open question in this area has been to bound the gap between the maximum possible utilization of the resource and the maximum possible utilization subject to this fairness condition.Here, we obtain some of the first provable upper bounds on this gap. We obtain an upper bound for arbitrary distributions, as well as much stronger upper bounds for specific families of distributions that are typically used to model levels of demand. In particular, we find — somewhat surprisingly — that there are natural families of distributions (including Exponential and Weibull) for which the gap is non-existent: it is possible to simultaneously achieve maximum utilization and the given notion of fairness. Finally, we show that for power-law distributions, there is a non-trivial gap between the solutions, but this gap can be bounded by a constant factor independent of the parameters of the distribution.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372847
SP  - 658
EP  - 668
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372847
KW  - uncertainty
KW  - resource allocation
KW  - algorithmic fairness
KW  - power law distribution
KW  - weibull distribution
ER  - 

TY  - CONF
TI  - The impact of overbooking on a pre-trial risk assessment tool
AU  - Lum, Kristian
AU  - Boudin, Chesa
AU  - Price, Megan
T3  - FAT* '20
AB  - Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372846
SP  - 482
EP  - 491
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372846
KW  - risk assessment
KW  - fairness
KW  - overbooking
KW  - police accountability
ER  - 

TY  - CONF
TI  - FlipTest: fairness testing via optimal transport
AU  - Black, Emily
AU  - Yeom, Samuel
AU  - Fredrikson, Matt
T3  - FAT* '20
AB  - We present FlipTest, a black-box technique for uncovering discrimination in classifiers. FlipTest is motivated by the intuitive question: had an individual been of a different protected status, would the model have treated them differently? Rather than relying on causal information to answer this question, FlipTest leverages optimal transport to match individuals in different protected groups, creating similar pairs of in-distribution samples. We show how to use these instances to detect discrimination by constructing a flipset: the set of individuals whose classifier output changes post-translation, which corresponds to the set of people who may be harmed because of their group membership. To shed light on why the model treats a given subgroup differently, FlipTest produces a transparency report: a ranking of features that are most associated with the model's behavior on the flipset. Evaluating the approach on three case studies, we show that this provides a computationally inexpensive way to identify subgroups that may be harmed by model discrimination, including in cases where the model satisfies group fairness criteria.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372845
SP  - 111
EP  - 121
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372845
KW  - machine learning
KW  - fairness
KW  - disparate impact
KW  - optimal transport
ER  - 

TY  - CONF
TI  - Whose side are ethics codes on? power, responsibility and the social good
AU  - Washington, Anne L.
AU  - Kuo, Rachel
T3  - FAT* '20
AB  - The moral authority of ethics codes stems from an assumption that they serve a unified society, yet this ignores the political aspects of any shared resource. The sociologist Howard S. Becker challenged researchers to clarify their power and responsibility in the classic essay: Whose Side Are We On. Building on Becker's hierarchy of credibility, we report on a critical discourse analysis of data ethics codes and emerging conceptualizations of beneficence, or the "social good", of data technology. The analysis revealed that ethics codes from corporations and professional associations conflated consumers with society and were largely silent on agency. Interviews with community organizers about social change in the digital era supplement the analysis, surfacing the limits of technical solutions to concerns of marginalized communities. Given evidence that highlights the gulf between the documents and lived experiences, we argue that ethics codes that elevate consumers may simultaneously subordinate the needs of vulnerable populations. Understanding contested digital resources is central to the emerging field of public interest technology. We introduce the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes..
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372844
SP  - 230
EP  - 240
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372844
KW  - data science
KW  - digital differential vulnerability
KW  - digital vulnerability
KW  - ethics codes
KW  - public interest technology
KW  - social movements
ER  - 

TY  - CONF
TI  - Bias in word embeddings
AU  - Papakyriakopoulos, Orestis
AU  - Hegelich, Simon
AU  - Serrano, Juan Carlos Medina
AU  - Marco, Fabienne
T3  - FAT* '20
AB  - Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372843
SP  - 446
EP  - 457
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372843
KW  - fairness
KW  - mitigation
KW  - bias
KW  - detection
KW  - racism
KW  - word embeddings
KW  - diffusion
KW  - homophobia
KW  - sexism
ER  - 

TY  - CONF
TI  - The effects of competition and regulation on error inequality in data-driven markets
AU  - Elzayn, Hadi
AU  - Fish, Benjamin
T3  - FAT* '20
AB  - Recent work has documented instances of unfairness in deployed machine learning models, and significant researcher effort has been dedicated to creating algorithms that intrinsically consider fairness. In this work, we highlight another source of unfairness: market forces that drive differential investment in the data pipeline for differing groups. We develop a high-level model to study this question. First, we show that our model predicts unfairness in a monopoly setting. Then, we show that under all but the most extreme models, competition does not eliminate this tendency, and may even exacerbate it. Finally, we consider two avenues for regulating a machine-learning driven monopolist - relative error inequality and absolute error-bounds - and quantify the price of fairness (and who pays it). These models imply that mitigating fairness concerns may require policy-driven solutions, not only technological ones.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372842
SP  - 669
EP  - 679
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372842
KW  - game theory
KW  - algorithmic fairness
KW  - economics
KW  - data markets
KW  - industrial organization
KW  - learning theory
ER  - 

TY  - CONF
TI  - Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files
AU  - Borradaile, Glencora
AU  - Burkhardt, Brett
AU  - LeClerc, Alexandria
T3  - FAT* '20
AB  - Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372841
SP  - 570
EP  - 580
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372841
KW  - police
KW  - audit
KW  - surveillance
KW  - demographics
KW  - keywords
KW  - social media monitoring
ER  - 

TY  - CONF
TI  - Algorithmic realism: expanding the boundaries of algorithmic thought
AU  - Green, Ben
AU  - Viljoen, Salomé
T3  - FAT* '20
AB  - Although computer scientists are eager to help address social problems, the field faces a growing awareness that many well-intentioned applications of algorithms in social contexts have led to significant harm. We argue that addressing this gap between the field's desire to do good and the harmful impacts of many of its interventions requires looking to the epistemic and methodological underpinnings of algorithms. We diagnose the dominant mode of algorithmic reasoning as "algorithmic formalism" and describe how formalist orientations lead to harmful algorithmic interventions. Addressing these harms requires pursuing a new mode of algorithmic thinking that is attentive to the internal limits of algorithms and to the social concerns that fall beyond the bounds of algorithmic formalism. To understand what a methodological evolution beyond formalism looks like and what it may achieve, we turn to the twentieth century evolution in American legal thought from legal formalism to legal realism. Drawing on the lessons of legal realism, we propose a new mode of algorithmic thinking—"algorithmic realism"—that provides tools for computer scientists to account for the realities of social life and of algorithmic impacts. These realist approaches, although not foolproof, will better equip computer scientists to reduce algorithmic harms and to reason well about doing good.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372840
SP  - 19
EP  - 31
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372840
KW  - law
KW  - algorithms
KW  - critical algorithm studies
KW  - epistemology
KW  - STS
ER  - 

TY  - CONF
TI  - Fairness warnings and fair-MAML: learning fairly with minimal data
AU  - Slack, Dylan
AU  - Friedler, Sorelle A.
AU  - Givental, Emile
T3  - FAT* '20
AB  - Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372839
SP  - 200
EP  - 209
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372839
KW  - machine learning
KW  - covariate shift
KW  - fairness
KW  - meta-learning
ER  - 

TY  - CONF
TI  - Measuring justice in machine learning
AU  - Lundgard, Alan
T3  - FAT* '20
AB  - How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372838
SP  - 680
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372838
KW  - machine learning
KW  - discrimination
KW  - fairness
KW  - disability
KW  - justice
KW  - bias
KW  - distributive justice
KW  - theory
KW  - philosophy
KW  - accessibility
KW  - blind
KW  - capability
KW  - measure
KW  - non-visual access
KW  - operationalization
KW  - web accessibility
ER  - 

TY  - CONF
TI  - Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning
AU  - Sweeney, Chris
AU  - Najafian, Maryam
T3  - FAT* '20
AB  - The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372837
SP  - 359
EP  - 368
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372837
KW  - fairness
KW  - NLP
KW  - embeddings
ER  - 

TY  - CONF
TI  - Robustness in machine learning explanations: does it matter?
AU  - Hancox-Li, Leif
T3  - FAT* '20
AB  - The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372836
SP  - 640
EP  - 647
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372836
KW  - machine learning
KW  - explanation
KW  - artificial intelligence
KW  - ethics
KW  - robustness
KW  - methodology
KW  - epistemology
KW  - philosophy
KW  - objectivity
ER  - 

TY  - CONF
TI  - The case for voter-centered audits of search engines during political elections
AU  - Mustafaraj, Eni
AU  - Lurie, Emma
AU  - Devine, Claire
T3  - FAT* '20
AB  - Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's "related searches" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372835
SP  - 559
EP  - 569
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372835
KW  - bias
KW  - algorithm audits
KW  - elections
KW  - Google
KW  - search engines
KW  - voters
ER  - 

TY  - CONF
TI  - The relationship between trust in AI and trustworthy machine learning technologies
AU  - Toreini, Ehsan
AU  - Aitken, Mhairi
AU  - Coopamootoo, Kovila
AU  - Elliott, Karen
AU  - Zelaya, Carlos Gonzalez
AU  - van Moorsel, Aad
T3  - FAT* '20
AB  - To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372834
SP  - 272
EP  - 283
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372834
KW  - machine learning
KW  - trust
KW  - artificial intelligence
KW  - trustworthiness
ER  - 

TY  - CONF
TI  - What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability
AU  - Wieringa, Maranke
T3  - FAT* '20
AB  - As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372833
SP  - 1
EP  - 18
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372833
KW  - algorithmic accountability
KW  - algorithmic systems
KW  - accountability theory
KW  - data-driven governance
ER  - 

TY  - CONF
TI  - Integrating FATE/critical data studies into data science curricula: where are we going and how do we get there?
AU  - Bates, Jo
AU  - Cameron, David
AU  - Checco, Alessandro
AU  - Clough, Paul
AU  - Hopfgartner, Frank
AU  - Mazumdar, Suvodeep
AU  - Sbaffi, Laura
AU  - Stordy, Peter
AU  - de la Vega de León, Antonio
T3  - FAT* '20
AB  - There have been multiple calls for integrating topics related to fairness, accountability, transparency, ethics (FATE) and social justice into Data Science curricula, but little exploration of how this might work in practice. This paper presents the findings of a collaborative auto-ethnography (CAE) engaged in by a MSc Data Science teaching team based at University of Sheffield (UK) Information School where FATE/Critical Data Studies (CDS) topics have been a core part of the curriculum since 2015/16. In this paper, we adopt the CAE approach to reflect on our experiences of working at the intersection of disciplines, and our progress and future plans for integrating FATE/CDS into the curriculum. We identify a series of challenges for deeper FATE/CDS integration related to our own competencies and the wider socio-material context of Higher Education in the UK. We conclude with recommendations for ourselves and the wider FATE/CDS orientated Data Science community.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372832
SP  - 425
EP  - 435
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372832
KW  - data science
KW  - critical data studies
KW  - FATE
KW  - higher education
ER  - 

TY  - CONF
TI  - An empirical study on the perceived fairness of realistic, imperfect machine learning models
AU  - Harrison, Galen
AU  - Hanson, Julia
AU  - Jacinto, Christine
AU  - Ramirez, Julio
AU  - Ur, Blase
T3  - FAT* '20
AB  - There are many competing definitions of what statistical properties make a machine learning model fair. Unfortunately, research has shown that some key properties are mutually exclusive. Realistic models are thus necessarily imperfect, choosing one side of a trade-off or the other. To gauge perceptions of the fairness of such realistic, imperfect models, we conducted a between-subjects experiment with 502 Mechanical Turk workers. Each participant compared two models for deciding whether to grant bail to criminal defendants. The first model equalized one potentially desirable model property, with the other property varying across racial groups. The second model did the opposite. We tested pairwise trade-offs between the following four properties: accuracy; false positive rate; outcomes; and the consideration of race. We also varied which racial group the model disadvantaged. We observed a preference among participants for equalizing the false positive rate between groups over equalizing accuracy. Nonetheless, no preferences were overwhelming, and both sides of each trade-off we tested were strongly preferred by a non-trivial fraction of participants. We observed nuanced distinctions between participants considering a model "unbiased" and considering it "fair." Furthermore, even when a model within a trade-off pair was seen as fair and unbiased by a majority of participants, we did not observe consensus that a machine learning model was preferable to a human judge. Our results highlight challenges for building machine learning models that are perceived as fair and broadly acceptable in realistic situations.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372831
SP  - 392
EP  - 402
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372831
KW  - machine learning
KW  - accountability
KW  - fairness
KW  - data science
KW  - survey
ER  - 

TY  - CONF
TI  - The hidden assumptions behind counterfactual explanations and principal reasons
AU  - Barocas, Solon
AU  - Selbst, Andrew D.
AU  - Raghavan, Manish
T3  - FAT* '20
AB  - Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant—and withholding others.These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world—and the subjective choices necessary to compensate for this—must be understood before these techniques can be usefully implemented.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372830
SP  - 80
EP  - 89
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372830
ER  - 

TY  - CONF
TI  - Lessons from archives: strategies for collecting sociocultural data in machine learning
AU  - Jo, Eun Seo
AU  - Gebru, Timnit
T3  - FAT* '20
AB  - A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics &amp; privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372829
SP  - 306
EP  - 316
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372829
KW  - machine learning
KW  - datasets
KW  - data collection
KW  - ML fairness
KW  - archives
KW  - sociocultural data
ER  - 

TY  - CONF
TI  - Mitigating bias in algorithmic hiring: evaluating claims and practices
AU  - Raghavan, Manish
AU  - Barocas, Solon
AU  - Kleinberg, Jon
AU  - Levy, Karen
T3  - FAT* '20
AB  - There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372828
SP  - 469
EP  - 481
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372828
KW  - algorithmic bias
KW  - algorithmic hiring
KW  - discrimination law
ER  - 

TY  - CONF
TI  - "The human body is a black box": supporting clinical decision-making with deep learning
AU  - Sendak, Mark
AU  - Elish, Madeleine Clare
AU  - Gao, Michael
AU  - Futoma, Joseph
AU  - Ratliff, William
AU  - Nichols, Marshall
AU  - Bedoya, Armando
AU  - Balu, Suresh
AU  - O'Brien, Cara
T3  - FAT* '20
AB  - Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372827
SP  - 99
EP  - 109
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372827
KW  - trust
KW  - interpretability
KW  - deep learning
KW  - medicine
KW  - expertise
ER  - 

TY  - CONF
TI  - Towards a critical race methodology in algorithmic fairness
AU  - Hanna, Alex
AU  - Denton, Emily
AU  - Smart, Andrew
AU  - Smith-Loud, Jamila
T3  - FAT* '20
AB  - We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372826
SP  - 501
EP  - 512
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372826
KW  - algorithmic fairness
KW  - critical race theory
KW  - race and ethnicity
ER  - 

TY  - CONF
TI  - Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability
AU  - Roldán, José Mena
AU  - Vila, Oriol Pujol
AU  - Marca, Jordi Vitrià
T3  - FAT* '20
AB  - Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community.In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps:Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*)   Dir(α).Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: α = βy. The role of this scalar β is to control the spread of the distribution around the expected value, i.e. the original prediction y.Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter β. For each input, we combine β and the black-box prediction to obtain the corresponding distribution for the output ym,i   Dir(αi). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels.Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(α) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction.Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system.We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality.Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372825
SP  - 581
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372825
KW  - machine learning
KW  - black-box models
KW  - uncertainty
KW  - accuracy
KW  - auditability
ER  - 

TY  - CONF
TI  - Why does my model fail? contrastive local explanations for retail forecasting
AU  - Lucic, Ana
AU  - Haned, Hinda
AU  - de Rijke, Maarten
T3  - FAT* '20
AB  - In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be "black boxes," even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a "black-box" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.
C1  - New York, NY, USA
C3  - Proceedings of the 2020 conference on fairness, accountability, and transparency
DA  - 2020///
PY  - 2020
DO  - 10.1145/3351095.3372824
SP  - 90
EP  - 98
PB  - Association for Computing Machinery
SN  - 978-1-4503-6936-7
UR  - https://doi.org/10.1145/3351095.3372824
KW  - interpretability
KW  - explainability
KW  - erroneous predictions
ER  - 

TY  - CONF
TI  - Value cards: An educational toolkit for teaching social impacts of machine learning through deliberation
AU  - Shen, Hong
AU  - Deng, Wesley H.
AU  - Chattopadhyay, Aditi
AU  - Wu, Zhiwei Steven
AU  - Wang, Xu
AU  - Zhu, Haiyi
T3  - FAccT '21
AB  - Recently, there have been increasing calls for computer science curricula to complement existing technical training with topics related to Fairness, Accountability, Transparency and Ethics (FATE). In this paper, we present Value Cards, an educational toolkit to inform students and practitioners the social impacts of different machine learning models via deliberation. This paper presents an early use of our approach in a college-level computer science course. Through an in-class activity, we report empirical data for the initial effectiveness of our approach. Our results suggest that the use of the Value Cards toolkit can improve students' understanding of both the technical definitions and trade-offs of performance metrics and apply them in real-world contexts, help them recognize the significance of considering diverse social values in the development and deployment of algorithmic systems, and enable them to communicate, negotiate and synthesize the perspectives of diverse stakeholders. Our study also demonstrates a number of caveats we need to consider when using the different variants of the Value Cards toolkit. Finally, we discuss the challenges as well as future applications of our approach.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445971
SP  - 850
EP  - 861
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445971
KW  - Fairness
KW  - Machine Learning
KW  - Deliberation
KW  - CS Education
KW  - Value Cards
ER  - 

TY  - CONF
TI  - When the umpire is also a player: Bias in private label product recommendations on E-commerce marketplaces
AU  - Dash, Abhisek
AU  - Chakraborty, Abhijnan
AU  - Ghosh, Saptarshi
AU  - Mukherjee, Animesh
AU  - Gummadi, Krishna P.
T3  - FAccT '21
AB  - Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445944
SP  - 873
EP  - 884
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445944
KW  - algorithmic auditing
KW  - e-commerce marketplace
KW  - Recommendation
ER  - 

TY  - CONF
TI  - Epistemic values in feature importance methods: Lessons from feminist epistemology
AU  - Hancox-Li, Leif
AU  - Kumar, I. Elizabeth
T3  - FAccT '21
AB  - As the public seeks greater accountability and transparency from machine learning algorithms, the research literature on methods to explain algorithms and their outputs has rapidly expanded. Feature importance methods form a popular class of explanation methods. In this paper, we apply the lens of feminist epistemology to recent feature importance research. We investigate what epistemic values are implicitly embedded in feature importance methods and how or whether they are in conflict with feminist epistemology. We offer some suggestions on how to conduct research on explanations that respects feminist epistemic values, taking into account the importance of social context, the epistemic privileges of subjugated knowers, and adopting more interactional ways of knowing
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445943
SP  - 817
EP  - 826
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445943
KW  - machine learning
KW  - explanation
KW  - methodology
KW  - epistemology
KW  - philosophy
KW  - feminism
KW  - feature importance
ER  - 

TY  - CONF
TI  - The algorithmic leviathan: Arbitrariness, fairness, and opportunity in algorithmic decision making systems
AU  - Creel, Kathleen
AU  - Hellman, Deborah
T3  - FAccT '21
AB  - Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern?We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are "fair" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445942
SP  - 816
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445942
KW  - machine learning
KW  - fairness
KW  - automated hiring
KW  - algorithmic decision making
KW  - arbitrariness
KW  - opportunity
ER  - 

TY  - CONF
TI  - How can I choose an explainer? An application-grounded evaluation of post-hoc explanations
AU  - Jesus, Sérgio
AU  - Belém, Catarina
AU  - Balayan, Vladimir
AU  - Bento, João
AU  - Saleiro, Pedro
AU  - Bizarro, Pedro
AU  - Gama, João
T3  - FAccT '21
AB  - There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular XAI methods - LIME, SHAP, and TreeInterpreter - on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to case.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445941
SP  - 805
EP  - 815
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445941
KW  - Explainability
KW  - XAI
KW  - LIME
KW  - SHAP
KW  - Evaluation
KW  - User Study
ER  - 

TY  - CONF
TI  - Detecting discriminatory risk through data annotation based on Bayesian inferences
AU  - Beretta, Elena
AU  - Vetrò, Antonio
AU  - Lepri, Bruno
AU  - Martin, Juan Carlos De
T3  - FAccT '21
AB  - Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445940
SP  - 794
EP  - 804
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445940
KW  - machine learning
KW  - data labeling
KW  - data ethics
KW  - human annotation
KW  - race discrimination
KW  - sampling bias
ER  - 

TY  - CONF
TI  - The ethics of emotion in artificial intelligence systems
AU  - Stark, Luke
AU  - Hoey, Jesse
T3  - FAccT '21
AB  - In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445939
SP  - 782
EP  - 793
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445939
KW  - machine learning
KW  - privacy
KW  - artificial intelligence
KW  - AI
KW  - fairness
KW  - AI ethics
KW  - ethics
KW  - ML
KW  - emotion
KW  - Action Control Theory
KW  - affect
KW  - affective computing
KW  - Basic Emotion Theory
KW  - emotion AI
KW  - norms
ER  - 

TY  - CONF
TI  - An action-oriented AI policy toolkit for technology audits by community advocates and activists
AU  - Krafft, P. M.
AU  - Young, Meg
AU  - Katell, Michael
AU  - Lee, Jennifer E.
AU  - Narayan, Shankar
AU  - Epstein, Micah
AU  - Dailey, Dharma
AU  - Herman, Bernease
AU  - Tam, Aaron
AU  - Guetler, Vivian
AU  - Bintz, Corinne
AU  - Raz, Daniella
AU  - Jobe, Pa Ousman
AU  - Putz, Franziska
AU  - Robick, Brian
AU  - Barghouti, Bissan
T3  - FAccT '21
AB  - Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445938
SP  - 772
EP  - 781
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445938
KW  - accountability
KW  - regulation
KW  - algorithmic justice
KW  - algorithmic equity
KW  - participatory action research
KW  - Participatory design
KW  - surveillance
ER  - 

TY  - CONF
TI  - Outlining traceability: A principle for operationalizing accountability in computing systems
AU  - Kroll, Joshua A.
T3  - FAccT '21
AB  - Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445937
SP  - 758
EP  - 771
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445937
KW  - transparency
KW  - accountability
KW  - AI ethics
KW  - AI principles
KW  - traceability
ER  - 

TY  - CONF
TI  - On the moral justification of statistical parity
AU  - Hertweck, Corinna
AU  - Heitz, Christoph
AU  - Loi, Michele
T3  - FAccT '21
AB  - A crucial but often neglected aspect of algorithmic fairness is the question of how we justify enforcing a certain fairness metric from a moral perspective. When fairness metrics are proposed, they are typically argued for by highlighting their mathematical properties. Rarely are the moral assumptions beneath the metric explained. Our aim in this paper is to consider the moral aspects associated with the statistical fairness criterion of independence (statistical parity). To this end, we consider previous work, which discusses the two worldviews "What You See Is What You Get" (WYSIWYG) and "We're All Equal" (WAE) and by doing so provides some guidance for clarifying the possible assumptions in the design of algorithms. We present an extension of this work, which centers on morality. The most natural moral extension is that independence needs to be fulfilled if and only if differences in predictive features (e.g. high school grades and standardized test scores are predictive of performance at university) between socio-demographic groups are caused by unjust social disparities or measurement errors. Through two counterexamples, we demonstrate that this extension is not universally true. This means that the question of whether independence should be used or not cannot be satisfactorily answered by only considering the justness of differences in the predictive features.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445936
SP  - 747
EP  - 757
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445936
KW  - fairness
KW  - bias
KW  - distributive justice
KW  - independence
KW  - statistical parity
ER  - 

TY  - CONF
TI  - Algorithmic impact assessments and accountability: The co-construction of impacts
AU  - Metcalf, Jacob
AU  - Moss, Emanuel
AU  - Watkins, Elizabeth Anne
AU  - Singh, Ranjit
AU  - Elish, Madeleine Clare
T3  - FAccT '21
AB  - Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that "impacts" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445935
SP  - 735
EP  - 746
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445935
KW  - accountability
KW  - governance
KW  - algorithmic impact assessment
KW  - harm
KW  - impact
ER  - 

TY  - CONF
TI  - Chasing your long tails: Differentially private prediction in health care settings
AU  - Suriyakumar, Vinith M.
AU  - Papernot, Nicolas
AU  - Goldenberg, Anna
AU  - Ghassemi, Marzyeh
T3  - FAccT '21
AB  - Machine learning models in health care are often deployed in settings where it is important to protect patient privacy. In such settings, methods for differentially private (DP) learning provide a general-purpose approach to learn models with privacy guarantees. Modern methods for DP learning ensure privacy through the addition of calibrated noise. The resulting privacy-preserving models are unable to learn too much information about the tails of a data distribution, resulting in a loss of accuracy that can disproportionately affect small groups. In this paper, we study the effects of DP learning in health care. We use state-of-the-art methods for DP learning to train privacy-preserving models in clinical prediction tasks, including x-ray classification of images and mortality prediction in time series data. We use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy, utility, robustness to dataset shift and fairness. Our results highlight lesser-known limitations of methods for DP learning in health care, models that exhibit steep tradeoffs between privacy and utility, and models whose predictions are disproportionately influenced by large demographic groups in the training data. We discuss the costs and benefits of differentially private learning in health care with open directions for differential privacy, machine learning and health care.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445934
SP  - 723
EP  - 734
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445934
KW  - machine learning
KW  - privacy
KW  - fairness
KW  - robustness
KW  - health care
ER  - 

TY  - CONF
TI  - From optimizing engagement to measuring value
AU  - Milli, Smitha
AU  - Belli, Luca
AU  - Hardt, Moritz
T3  - FAccT '21
AB  - Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of value that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of "value".
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445933
SP  - 714
EP  - 722
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445933
ER  - 

TY  - CONF
TI  - Image representations learned with unsupervised pre-training contain human-like biases
AU  - Steed, Ryan
AU  - Caliskan, Aylin
T3  - FAccT '21
AB  - Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445932
SP  - 701
EP  - 713
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445932
KW  - computer vision
KW  - unsupervised learning
KW  - implicit bias
ER  - 

TY  - CONF
TI  - I agree with the decision, but they didn't deserve this: Future Developers' Perception of Fairness in Algorithmic Decisions
AU  - Kasinidou, Maria
AU  - Kleanthous, Styliani
AU  - Barlas, Pınar
AU  - Otterbacher, Jahna
T3  - FAccT '21
AB  - While professionals are increasingly relying on algorithmic systems for making a decision, on some occasions, algorithmic decisions may be perceived as biased or not just. Prior work has looked into the perception of algorithmic decision-making from the user's point of view. In this work, we investigate how students in fields adjacent to algorithm development perceive algorithmic decisionmaking. Participants (N=99) were asked to rate their agreement with statements regarding six constructs that are related to facets of fairness and justice in algorithmic decision-making in three separate scenarios. Two of the three scenarios were independent of each other, while the third scenario presented three different outcomes of the same algorithmic system, demonstrating perception changes triggered by different outputs. Quantitative analysis indicates that a) 'agreeing' with a decision does not mean the person 'deserves the outcome', b) perceiving the factors used in the decision-making as 'appropriate' does not make the decision of the system 'fair' and c) perceiving a system's decision as 'not fair' is affecting the participants' 'trust' in the system. In addition, participants found proportional distribution of benefits more fair than other approaches. Qualitative analysis provides further insights into that information the participants find essential to judge and understand an algorithmic decision-making system's fairness. Finally, the level of academic education has a role to play in the perception of fairness and justice in algorithmic decision-making.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445931
SP  - 690
EP  - 700
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445931
KW  - algorithmic accountability
KW  - algorithmic transparency
KW  - algorithmic fairness
KW  - algorithmic decision-making
ER  - 

TY  - CONF
TI  - The effect of the rooney rule on implicit bias in the long term
AU  - Celis, L. Elisa
AU  - Hays, Chris
AU  - Mehrotra, Anay
AU  - Vishnoi, Nisheeth K.
T3  - FAccT '21
AB  - The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change.In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist—independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445930
SP  - 678
EP  - 689
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445930
ER  - 

TY  - CONF
TI  - Black feminist musings on algorithmic oppression
AU  - Hampton, Lelia Marie
T3  - FAccT '21
AB  - This paper uses a theory of oppression to ground and extend algorithmic oppression. Algorithmic oppression is then situated through a Black feminist lens part of which entails highlighting the double bind of technology. To reconcile algorithmic oppression with respect to the fairness, accountability, and transparency community, I critique the language of the community. Lastly, I place algorithmic oppression in a broader conversation of feminist science, technology, and society studies to ground the discussion of ways forward through abolition and empowering marginalized communities.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445929
SP  - 1
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445929
KW  - abolition
KW  - Algorithmic oppression
KW  - Black feminism
ER  - 

TY  - CONF
TI  - Building and auditing fair algorithms: A case study in candidate screening
AU  - Wilson, Christo
AU  - Ghosh, Avijit
AU  - Jiang, Shan
AU  - Mislove, Alan
AU  - Baker, Lewis
AU  - Szary, Janelle
AU  - Trindel, Kelly
AU  - Polli, Frida
T3  - FAccT '21
AB  - Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of "fairness" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445928
SP  - 666
EP  - 677
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445928
KW  - fairness
KW  - adverse impact testing
KW  - algorithm auditing
KW  - four-fifths rule
ER  - 

TY  - CONF
TI  - A statistical test for probabilistic fairness
AU  - Taskesen, Bahar
AU  - Blanchet, Jose
AU  - Kuhn, Daniel
AU  - Nguyen, Viet Anh
T3  - FAccT '21
AB  - Algorithms are now routinely used to make consequential decisions that affect human lives. Examples include college admissions, medical interventions or law enforcement. While algorithms empower us to harness all information hidden in vast amounts of data, they may inadvertently amplify existing biases in the available datasets. This concern has sparked increasing interest in fair machine learning, which aims to quantify and mitigate algorithmic discrimination. Indeed, machine learning models should undergo intensive tests to detect algorithmic biases before being deployed at scale. In this paper, we use ideas from the theory of optimal transport to propose a statistical hypothesis test for detecting unfair classifiers. Leveraging the geometry of the feature space, the test statistic quantifies the distance of the empirical distribution supported on the test samples to the manifold of distributions that render a pre-trained classifier fair. We develop a rigorous hypothesis testing mechanism for assessing the probabilistic fairness of any pre-trained logistic classifier, and we show both theoretically as well as empirically that the proposed test is asymptotically correct. In addition, the proposed framework offers interpretability by identifying the most favorable perturbation of the data so that the given classifier becomes fair.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445927
SP  - 648
EP  - 665
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445927
KW  - fairness
KW  - algorithmic bias
KW  - Wasserstein distance
KW  - equalized odds
KW  - equal opportunity
ER  - 

TY  - CONF
TI  - From papers to programs: Courts, corporations, clinics and the battle over computerized psychological testing
AU  - Lussier, Kira
T3  - FAccT '21
AB  - This paper examines the role of technology firms in computerizing personality tests from the early 1960s to late 1980s. It focuses on the National Computer Systems (NCS) and their development of an automated interpretation for the Minnesota Multiphasic Personality inventory (MMPI). NCS trumpeted their computerized interpretation as a way to free up clerical labor and mitigate human bias. Yet psychologists cautioned that proprietary algorithms risked obscuring decision rules. I show how clinics, courtrooms, and businesses all had competing interests in the use of computerized personality tests. As I argue, the development of computerized psychological tests was shaped both by business concerns about intellectual property and profits and psychologists' concerns with validity and access to algorithms. Across these domains, the common claim was that computerized psychological testing could provide a technical fix for bias. This paper contributes to histories of computing emphasizing the importance of IP, the relationship between labor, technology, and expertise, and to histories of algorithms.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445926
SP  - 647
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445926
ER  - 

TY  - CONF
TI  - TILT: A GDPR-Aligned transparency information language and toolkit for practical privacy engineering
AU  - Grünewald, Elias
AU  - Pallas, Frank
T3  - FAccT '21
AB  - In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do.We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty.Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445925
SP  - 636
EP  - 646
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445925
KW  - privacy
KW  - GDPR
KW  - data protection
KW  - privacy by design
KW  - Data transparency
KW  - legal tech
KW  - privacy engineering
KW  - privacy law
KW  - web privacy
ER  - 

TY  - CONF
TI  - BOLD: Dataset and metrics for measuring biases in open-ended language generation
AU  - Dhamala, Jwala
AU  - Sun, Tony
AU  - Kumar, Varun
AU  - Krishna, Satyapriya
AU  - Pruksachatkun, Yada
AU  - Chang, Kai-Wei
AU  - Gupta, Rahul
T3  - FAccT '21
AB  - Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445924
SP  - 862
EP  - 872
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445924
KW  - Fairness
KW  - natural language generation
ER  - 

TY  - CONF
TI  - Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in AI
AU  - Jacovi, Alon
AU  - Marasović, Ana
AU  - Miller, Tim
AU  - Goldberg, Yoav
T3  - FAccT '21
AB  - Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445923
SP  - 624
EP  - 635
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445923
KW  - trust
KW  - artificial intelligence
KW  - trustworthy
KW  - sociology
KW  - contractual trust
KW  - distrust
KW  - formalization
KW  - warranted trust
ER  - 

TY  - CONF
TI  - On the dangers of stochastic parrots: Can language models be too big? 🦜
AU  - Bender, Emily M.
AU  - Gebru, Timnit
AU  - McMillan-Major, Angelina
AU  - Shmitchell, Shmargaret
T3  - FAccT '21
AB  - The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445922
SP  - 610
EP  - 623
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445922
ER  - 

TY  - CONF
TI  - Reviewable automated decision-making: A framework for accountable algorithmic systems
AU  - Cobbe, Jennifer
AU  - Lee, Michelle Seng Ah
AU  - Singh, Jatinder
T3  - FAccT '21
AB  - This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445921
SP  - 598
EP  - 609
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445921
KW  - machine learning
KW  - artificial intelligence
KW  - accountability
KW  - audit
KW  - Algorithmic systems
KW  - automated decision-making
ER  - 

TY  - CONF
TI  - One label, one billion faces: Usage and consistency of racial categories in computer vision
AU  - Khan, Zaid
AU  - Fu, Yun
T3  - FAccT '21
AB  - Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445920
SP  - 587
EP  - 597
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445920
KW  - fairness
KW  - computer vision
KW  - bias
KW  - datasets
KW  - faces
KW  - race
ER  - 

TY  - CONF
TI  - Fairness, equality, and power in algorithmic decision-making
AU  - Kasy, Maximilian
AU  - Abebe, Rediet
T3  - FAccT '21
AB  - Much of the debate on the impact of algorithms is concerned with fairness, defined as the absence of discrimination for individuals with the same "merit." Drawing on the theory of justice, we argue that leading notions of fairness suffer from three key limitations: they legitimize inequalities justified by "merit;" they are narrowly bracketed, considering only differences of treatment within the algorithm; and they consider between-group and not within-group differences. We contrast this fairness-based perspective with two alternate perspectives: the first focuses on inequality and the causal impact of algorithms and the second on the distribution of power. We formalize these perspectives drawing on techniques from causal inference and empirical economics, and characterize when they give divergent evaluations. We present theoretical results and empirical examples which demonstrate this tension. We further use these insights to present a guide for algorithmic auditing and discuss the importance of inequality- and power-centered frameworks in algorithmic decision-making.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445919
SP  - 576
EP  - 586
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445919
KW  - Algorithmic fairness
KW  - inequality
KW  - auditing
KW  - empirical economics
KW  - power
ER  - 

TY  - CONF
TI  - Towards accountability for machine learning datasets: Practices from software engineering and infrastructure
AU  - Hutchinson, Ben
AU  - Smart, Andrew
AU  - Hanna, Alex
AU  - Denton, Emily
AU  - Greer, Christina
AU  - Kjartansson, Oddur
AU  - Barnes, Parker
AU  - Mitchell, Margaret
T3  - FAccT '21
AB  - Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445918
SP  - 560
EP  - 575
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445918
KW  - machine learning
KW  - datasets
KW  - requirements engineering
ER  - 

TY  - CONF
TI  - Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario
AU  - Hamon, Ronan
AU  - Junklewitz, Henrik
AU  - Malgieri, Gianclaudio
AU  - Hert, Paul De
AU  - Beslay, Laurent
AU  - Sanchez, Ignacio
T3  - FAccT '21
AB  - Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI.We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR).Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment.Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445917
SP  - 549
EP  - 559
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445917
KW  - Explainability
KW  - GDPR
KW  - AI
KW  - Data Protection
KW  - Machine Learning
KW  - Black-Box
KW  - Automated Decision-Making
ER  - 

TY  - CONF
TI  - Censorship of online encyclopedias: Implications for NLP models
AU  - Yang, Eddie
AU  - Roberts, Margaret E.
T3  - FAccT '21
AB  - While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445916
SP  - 537
EP  - 548
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445916
KW  - machine learning
KW  - training data
KW  - censorship
KW  - word embeddings
ER  - 

TY  - CONF
TI  - Fair classification with group-dependent label noise
AU  - Wang, Jialu
AU  - Liu, Yang
AU  - Levy, Caleb
T3  - FAccT '21
AB  - This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445915
SP  - 526
EP  - 536
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445915
KW  - machine learning
KW  - algorithmic fairness
KW  - learning with noisy and biased labels
ER  - 

TY  - CONF
TI  - You can't sit with us: Exclusionary pedagogy in AI ethics education
AU  - Raji, Inioluwa Deborah
AU  - Scheuerman, Morgan Klaus
AU  - Amironesei, Razvan
T3  - FAccT '21
AB  - Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)—and its implications for AI—has led to the current "ethics crisis". However, we claim that the current AI ethics education space relies on a form of "exclusionary pedagogy," where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as "ethical unicorns" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445914
SP  - 515
EP  - 525
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445914
ER  - 

TY  - CONF
TI  - Fair clustering via equitable group representations
AU  - Abbasi, Mohsen
AU  - Bhaskara, Aditya
AU  - Venkatasubramanian, Suresh
T3  - FAccT '21
AB  - What does it mean for a clustering to be fair? One popular approach seeks to ensure that each cluster contains groups in (roughly) the same proportion in which they exist in the population. The normative principle at play is balance: any cluster might act as a representative of the data, and thus should reflect its diversity.But clustering also captures a different form of representativeness. A core principle in most clustering problems is that a cluster center should be representative of the cluster it represents, by being "close" to the points associated with it. This is so that we can effectively replace the points by their cluster centers without significant loss in fidelity, and indeed is a common "use case" for clustering. For such a clustering to be fair, the centers should "represent" different groups equally well. We call such a clustering a group-representative clustering.In this paper, we study the structure and computation of group-representative clusterings. We show that this notion naturally parallels the development of fairness notions in classification, with direct analogs of ideas like demographic parity and equal opportunity. We demonstrate how these notions are distinct from and cannot be captured by balance-based notions of fairness. We present approximation algorithms for group representative k-median clustering and couple this with an empirical evaluation on various real-world data sets. We also extend this idea to facility location, motivated by the current problem of assigning polling locations for voting
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445913
SP  - 504
EP  - 514
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445913
KW  - algorithmic fairness
KW  - clustering
KW  - representation
ER  - 

TY  - CONF
TI  - Bridging machine learning and mechanism design towards algorithmic fairness
AU  - Finocchiaro, Jessie
AU  - Maio, Roland
AU  - Monachou, Faidra
AU  - Patro, Gourab K
AU  - Raghavan, Manish
AU  - Stoica, Ana-Andreea
AU  - Tsirtsis, Stratis
T3  - FAccT '21
AB  - Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445912
SP  - 489
EP  - 503
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445912
ER  - 

TY  - CONF
TI  - Operationalizing framing to support multiperspective recommendations of opinion pieces
AU  - Mulder, Mats
AU  - Inel, Oana
AU  - Oosterman, Jasper
AU  - Tintarev, Nava
T3  - FAccT '21
AB  - Diversity in personalized news recommender systems is often defined as dissimilarity, and operationalized based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445911
SP  - 478
EP  - 488
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445911
KW  - recommender systems
KW  - framing aspects
KW  - viewpoint diversity
ER  - 

TY  - CONF
TI  - Fairness through robustness: Investigating robustness disparity in deep learning
AU  - Nanda, Vedant
AU  - Dooley, Samuel
AU  - Singla, Sahil
AU  - Feizi, Soheil
AU  - Dickerson, John P.
T3  - FAccT '21
AB  - Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445910
SP  - 466
EP  - 477
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445910
ER  - 

TY  - CONF
TI  - A pilot study in surveying clinical judgments to evaluate radiology report generation
AU  - Boag, William
AU  - Kané, Hassan
AU  - Rawat, Saumya
AU  - Wei, Jesse
AU  - Goehler, Alexander
T3  - FAccT '21
AB  - The recent release of many Chest X-Ray datasets has prompted a lot of interest in radiology report generation. To date, this has been framed as an image captioning task, where the machine takes an RGB image as input and generates a 2-3 sentence summary of findings as output. The quality of these reports has been canonically measured using metrics from the NLP community for language generation such as Machine Translation and Summarization. However, the evaluation metrics (e.g. BLEU, CIDEr) are inappropriate for the medical domain, where clinical correctness is critical. To address this, our team brought together machine learning experts with radiologists for a pilot study in co-designing a better metric for evaluating the quality of an algorithmically-generated radiology report. The interdisciplinary collaborative process involved multiple interviews, outreach, and preliminary annotation to design a larger scale study - which is now underway - to build a more meaningful evaluation tool.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445909
SP  - 458
EP  - 465
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445909
KW  - Participatory ML
KW  - Radiology Report Generation
ER  - 

TY  - CONF
TI  - A bayesian model of cash bail decisions
AU  - Williams, Joshua
AU  - Kolter, J. Zico
T3  - FAccT '21
AB  - The use of cash bail as a mechanism for detaining defendants pretrial is an often-criticized system that many have argued violates the presumption of "innocent until proven guilty." Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality - that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision. In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445908
SP  - 827
EP  - 837
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445908
ER  - 

TY  - CONF
TI  - Towards cross-lingual generalization of translation gender bias
AU  - Cho, Won Ik
AU  - Kim, Jiwon
AU  - Yang, Jaeyeong
AU  - Kim, Nam Soo
T3  - FAccT '21
AB  - Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445907
SP  - 449
EP  - 457
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445907
KW  - evaluation
KW  - gender bias
KW  - cross-linguality
KW  - machine translation
ER  - 

TY  - CONF
TI  - Socially fair k-Means clustering
AU  - Ghadiri, Mehrdad
AU  - Samadi, Samira
AU  - Vempala, Santosh
T3  - FAccT '21
AB  - We show that the popular k-means clustering algorithm (Lloyd's heuristic), used for a variety of scientific data, can result in outcomes that are unfavorable to subgroups of data (e.g., demographic groups). Such biased clusterings can have deleterious implications for human-centric applications such as resource allocation. We present a fair k-means objective and algorithm to choose cluster centers that provide equitable costs for different groups. The algorithm, Fair-Lloyd, is a modification of Lloyd's heuristic for k-means, inheriting its simplicity, efficiency, and stability. In comparison with standard Lloyd's, we find that on benchmark datasets, Fair-Lloyd exhibits unbiased performance by ensuring that all groups have equal costs in the output k-clustering, while incurring a negligible increase in running time, thus making it a viable fair option wherever k-means is currently used.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445906
SP  - 438
EP  - 448
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445906
ER  - 

TY  - CONF
TI  - Designing accountable systems
AU  - Kacianka, Severin
AU  - Pretschner, Alexander
T3  - FAccT '21
AB  - Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445905
SP  - 424
EP  - 437
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445905
KW  - Accountability
KW  - Socio-Technical Systems
KW  - Structural Causal Models
ER  - 

TY  - CONF
TI  - An agent-based model to evaluate interventions on online dating platforms to decrease racial homogamy
AU  - Ionescu, Stefania
AU  - Hannák, Anikó
AU  - Joseph, Kenneth
T3  - FAccT '21
AB  - Perhaps the most controversial questions in the study of online platforms today surround the extent to which platforms can intervene to reduce the societal ills perpetrated on them. Up for debate is whether there exist any effective and lasting interventions a platform can adopt to address, e.g., online bullying, or if other, more far-reaching change is necessary to address such problems. Empirical work is critical to addressing such questions. But it is also challenging, because it is time-consuming, expensive, and sometimes limited to the questions companies are willing to ask. To help focus and inform this empirical work, we here propose an agent-based modeling (ABM) approach. As an application, we analyze the impact of a set of interventions on a simulated online dating platform on the lack of long-term interracial relationships in an artificial society. In the real world, a lack of interracial relationships are a critical vehicle through which inequality is maintained. Our work shows that many previously hypothesized interventions online dating platforms could take to increase the number of interracial relationships from their website have limited effects, and that the effectiveness of any intervention is subject to assumptions about sociocultural structure. Further, interventions that are effective in increasing diversity in long-term relationships are at odds with platforms' profit-oriented goals. At a general level, the present work shows the value of using an ABM approach to help understand the potential effects and side effects of different interventions that a platform could take.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445904
SP  - 412
EP  - 423
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445904
KW  - social media
KW  - agent-based modeling
KW  - dating platforms
KW  - racism
ER  - 

TY  - CONF
TI  - High dimensional model explanations: An axiomatic approach
AU  - Patel, Neel
AU  - Strobel, Martin
AU  - Zick, Yair
T3  - FAccT '21
AB  - Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets.We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model.Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445903
SP  - 401
EP  - 411
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445903
ER  - 

TY  - CONF
TI  - Fairness in risk assessment instruments: Post-processing to achieve counterfactual equalized odds
AU  - Mishler, Alan
AU  - Kennedy, Edward H.
AU  - Chouldechova, Alexandra
T3  - FAccT '21
AB  - In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445902
SP  - 386
EP  - 400
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445902
KW  - risk assessment
KW  - fairness
KW  - counterfactual
KW  - post-processing
ER  - 

TY  - CONF
TI  - Measurement and fairness
AU  - Jacobs, Abigail Z.
AU  - Wallach, Hanna
T3  - FAccT '21
AB  - We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them—i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445901
SP  - 375
EP  - 385
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445901
KW  - fairness
KW  - construct reliability
KW  - construct validity
KW  - measurement
ER  - 

TY  - CONF
TI  - A Semiotics-based epistemic tool to reason about ethical issues in digital technology design and development
AU  - Barbosa, Simone Diniz Junqueira
AU  - Barbosa, Gabriel Diniz Junqueira
AU  - Souza, Clarisse Sieckenius de
AU  - Leitão, Carla Faria
T3  - FAccT '21
AB  - One of the important challenges regarding the development of morally responsible and ethically qualified digital technologies is how to support designers and developers in producing those technologies, especially when conceptualizing their vision of what the technology will be, how it will benefit users, and avoid doing harm. However, traditional software design and development life cycles do not explicitly support the reflection upon either ethical or moral issues. In this paper we look at how a number of ethical issues may be dealt with during digital technology design and development, to prevent damage and improve technological fairness, accountability, and transparency. Starting from mature work on semiotic theory and methods in human-computer interaction, we propose to extend the core artifact used in semiotic engineering of human-centered technology design, so as to directly address moral responsibility and ethical issues. The resulting extension is an epistemic tool, that is, an instrument to create and elaborate on this specific kind of knowledge. The paper describes the tool, illustrates how it is to be used, and discusses its promises and limitations against the background of related work. It also includes proposed empirical studies, accompanied by briefly described methodological challenges and considerations that deserve our attention.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445900
SP  - 363
EP  - 374
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445900
KW  - ethics
KW  - epistemic tool
KW  - semiotic engineering
ER  - 

TY  - CONF
TI  - Algorithmic recourse: from counterfactual explanations to interventions
AU  - Karimi, Amir-Hossein
AU  - Schölkopf, Bernhard
AU  - Valera, Isabel
T3  - FAccT '21
AB  - As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445899
SP  - 353
EP  - 362
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445899
KW  - explainable artificial intelligence
KW  - counterfactual explanations
KW  - contrastive explanations
KW  - algorithmic recourse
KW  - causal inference
KW  - consequential recommendations
KW  - minimal interventions
ER  - 

TY  - CONF
TI  - This whole thing smacks of gender: Algorithmic exclusion in bioimpedance-based body composition analysis
AU  - Albert, Kendra
AU  - Delano, Maggie
T3  - FAccT '21
AB  - Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third "gender" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445898
SP  - 342
EP  - 352
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445898
KW  - critical data/algorithm studies
KW  - bioelectrical impedance analysis
KW  - body composition
KW  - critical HCI and the design of algorithmic systems
KW  - data collection and curation
KW  - science and technology studies
KW  - sex/gender
ER  - 

TY  - CONF
TI  - Narratives and counternarratives on data sharing in africa
AU  - Abebe, Rediet
AU  - Aruleba, Kehinde
AU  - Birhane, Abeba
AU  - Kingsley, Sara
AU  - Obaido, George
AU  - Remy, Sekou L.
AU  - Sadagopan, Swathi
T3  - FAccT '21
AB  - As machine learning and data science applications grow ever more prevalent, there is an increased focus on data sharing and open data initiatives, particularly in the context of the African continent. Many argue that data sharing can support research and policy design to alleviate poverty, inequality, and derivative effects in Africa. Despite the fact that the datasets in question are often extracted from African communities, conversations around the challenges of accessing and sharing African data are too often driven by non-African stakeholders. These perspectives frequently employ a deficit narratives, often focusing on lack of education, training, and technological resources in the continent as the leading causes of friction in the data ecosystem.We argue that these narratives obfuscate and distort the full complexity of the African data sharing landscape. In particular, we use storytelling via fictional personas built from a series of interviews with African data experts to complicate dominant narratives and to provide counternarratives. Coupling these personas with research on data practices within the continent, we identify recurring barriers to data sharing as well as inequities in the distribution of data sharing benefits. In particular, we discuss issues arising from power imbalances resulting from the legacies of colonialism, ethno-centrism, and slavery, disinvestment in building trust, lack of acknowledgement of historical and present-day extractive practices, and Western-centric policies that are ill-suited to the African context. After outlining these problems, we discuss avenues for addressing them when sharing data generated in the continent.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445897
SP  - 329
EP  - 341
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445897
KW  - data access
KW  - Data sharing
KW  - decolonial theory
KW  - open data
KW  - storytelling
ER  - 

TY  - CONF
TI  - Re-imagining algorithmic fairness in india and beyond
AU  - Sambasivan, Nithya
AU  - Arnesen, Erin
AU  - Hutchinson, Ben
AU  - Doshi, Tulsee
AU  - Prabhakaran, Vinodkumar
T3  - FAccT '21
AB  - Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445896
SP  - 315
EP  - 328
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445896
KW  - India
KW  - algorithmic fairness
KW  - religion
KW  - gender
KW  - ability
KW  - anti-caste politics
KW  - caste
KW  - class
KW  - critical algorithmic studies
KW  - decoloniality
KW  - feminism
ER  - 

TY  - CONF
TI  - Fairness, welfare, and equity in personalized pricing
AU  - Kallus, Nathan
AU  - Zhou, Angela
T3  - FAccT '21
AB  - We study the interplay of fairness, welfare, and equity considerations in personalized pricing based on customer features. Sellers are increasingly able to conduct price personalization based on predictive modeling of demand conditional on covariates: setting customized interest rates, targeted discounts of consumer goods, and personalized subsidies of scarce resources with positive externalities like vaccines and bed nets. These different application areas may lead to different concerns around fairness, welfare, and equity on different objectives: price burdens on consumers, price envy, firm revenue, access to a good, equal access, and distributional consequences when the good in question further impacts downstream outcomes of interest. We conduct a comprehensive literature review in order to disentangle these different normative considerations and propose a taxonomy of different objectives with mathematical definitions. We focus on observational metrics that do not assume access to an underlying valuation distribution which is either unobserved due to binary feedback or ill-defined due to overriding behavioral concerns regarding interpreting revealed preferences. In the setting of personalized pricing for the provision of goods with positive benefits, we discuss how price optimization may provide unambiguous benefit by achieving a "triple bottom line": personalized pricing enables expanding access, which in turn may lead to gains in welfare due to heterogeneous utility, and improve revenue or budget utilization. We empirically demonstrate the potential benefits of personalized pricing in two settings: pricing subsidies for an elective vaccine, and the effects of personalized interest rates on downstream outcomes in microcredit.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445895
SP  - 296
EP  - 314
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445895
ER  - 

TY  - CONF
TI  - Leave-one-out unfairness
AU  - Black, Emily
AU  - Fredrikson, Matt
T3  - FAccT '21
AB  - We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445894
SP  - 285
EP  - 295
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445894
ER  - 

TY  - CONF
TI  - Spoken corpora data, automatic speech recognition, and bias against african american language: The case of habitual 'be'
AU  - Martin, Joshua L
T3  - FAccT '21
AB  - Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in "I be in my office by 7:30am", paraphrasable as "I am usually in my office by 7:30" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora – Switchboard, Fisher, TIMIT, and LibriSpeech – to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL – the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445893
SP  - 284
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445893
KW  - datasets
KW  - racial bias
KW  - African American Language
KW  - automatic speech recognition
KW  - linguistic bias
KW  - spoken corpora
ER  - 

TY  - CONF
TI  - Avoiding disparity amplification under different worldviews
AU  - Yeom, Samuel
AU  - Tschantz, Michael Carl
T3  - FAccT '21
AB  - We mathematically compare four competing definitions of group-level nondiscrimination: demographic parity, equalized odds, predictive parity, and calibration. Using the theoretical framework of Friedler et al., we study the properties of each definition under various worldviews, which are assumptions about how, if at all, the observed data is biased. We argue that different worldviews call for different definitions of fairness, and we specify the worldviews that, when combined with the desire to avoid a criterion for discrimination that we call disparity amplification, motivate demographic parity and equalized odds. We also argue that predictive parity and calibration are insufficient for avoiding disparity amplification because predictive parity allows an arbitrarily large inter-group disparity and calibration is not robust to post-processing. Finally, we define a worldview that is more realistic than the previously considered ones, and we introduce a new notion of fairness that corresponds to this worldview.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445892
SP  - 273
EP  - 283
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445892
KW  - fairness
KW  - calibration
KW  - demographic parity
KW  - disparity amplification
KW  - equalized odds
KW  - predictive parity
KW  - worldview
ER  - 

TY  - CONF
TI  - Algorithmic fairness in predicting opioid use disorder using machine learning
AU  - Kilby, Angela E.
T3  - FAccT '21
AB  - There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445891
SP  - 272
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445891
KW  - causality
KW  - fairness
KW  - ethics
KW  - algorithm development
KW  - algorithmic impacts on social phenomena
KW  - auditing
KW  - critical data/algorithm studies
KW  - disability studies
KW  - evaluations
KW  - social and organizational processes
ER  - 

TY  - CONF
TI  - The sanction of authority: Promoting public trust in AI
AU  - Knowles, Bran
AU  - Richards, John T.
T3  - FAccT '21
AB  - Trusted AI literature to date has focused on the trust needs of users who knowingly interact with discrete AIs. Conspicuously absent from the literature is a rigorous treatment of public trust in AI. We argue that public distrust of AI originates from the underdevelopment of a regulatory ecosystem that would guarantee the trustworthiness of the AIs that pervade society. Drawing from structuration theory and literature on institutional trust, we offer a model of public trust in AI that differs starkly from models driving Trusted AI efforts. This model provides a theoretical scaffolding for Trusted AI research which underscores the need to develop nothing less than a comprehensive and visibly functioning regulatory ecosystem. We elaborate the pivotal role of externally auditable AI documentation within this model and the work to be done to ensure it is effective, and outline a number of actions that would promote public trust in AI. We discuss how existing efforts to develop AI documentation within organizations—both to inform potential adopters of AI components and support the deliberations of risk and ethics review boards—is necessary but insufficient assurance of the trustworthiness of AI. We argue that being accountable to the public in ways that earn their trust, through elaborating rules for AI and developing resources for enforcing these rules, is what will ultimately make AI trustworthy enough to be woven into the fabric of our society.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445890
SP  - 262
EP  - 271
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445890
KW  - Trust
KW  - artificial intelligence
KW  - trustworthiness
KW  - face-work
KW  - institutional trust
KW  - structuration theory
ER  - 

TY  - CONF
TI  - Standardized tests and affirmative action: The role of bias and variance
AU  - Garg, Nikhil
AU  - Li, Hannah
AU  - Monachou, Faidra
T3  - FAccT '21
AB  - The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies—such as affirmative action—on admitted class composition.This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action.We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool.Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement.The full paper can be found at https://arxiv.org/abs/2010.04396.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445889
SP  - 261
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445889
ER  - 

TY  - CONF
TI  - What we can't measure, we can't understand: Challenges to demographic data procurement in the pursuit of fairness
AU  - Andrus, McKane
AU  - Spitzer, Elena
AU  - Brown, Jeffrey
AU  - Xiang, Alice
T3  - FAccT '21
AB  - As calls for fair and unbiased algorithmic systems increase, so too does the number of individuals working on algorithmic fairness in industry. However, these practitioners often do not have access to the demographic data they feel they need to detect bias in practice. Even with the growing variety of toolkits and strategies for working towards algorithmic fairness, they almost invariably require access to demographic attributes or proxies. We investigated this dilemma through semi-structured interviews with 38 practitioners and professionals either working in or adjacent to algorithmic fairness. Participants painted a complex picture of what demographic data availability and use look like on the ground, ranging from not having access to personal data of any kind to being legally required to collect and use demographic data for discrimination assessments. In many domains, demographic data collection raises a host of difficult questions, including how to balance privacy and fairness, how to define relevant social categories, how to ensure meaningful consent, and whether it is appropriate for private companies to infer someone's demographics. Our research suggests challenges that must be considered by businesses, regulators, researchers, and community groups in order to enable practitioners to address algorithmic bias in practice. Critically, we do not propose that the overall goal of future work should be to simply lower the barriers to collecting demographic data. Rather, our study surfaces a swath of normative questions about how, when, and whether this data should be procured, and, in cases where it is not, what should still be done to mitigate bias.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445888
SP  - 249
EP  - 260
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445888
KW  - fairness
KW  - data privacy
KW  - anti-discrimination
KW  - demographic data
KW  - sensitive data
KW  - special category data
ER  - 

TY  - CONF
TI  - Mitigating bias in set selection with noisy protected attributes
AU  - Mehrotra, Anay
AU  - Celis, L. Elisa
T3  - FAccT '21
AB  - Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result!Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a "denoised" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445887
SP  - 237
EP  - 248
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445887
ER  - 

TY  - CONF
TI  - The use and misuse of counterfactuals in ethical machine learning
AU  - Kasirzadeh, Atoosa
AU  - Smart, Andrew
T3  - FAccT '21
AB  - The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445886
SP  - 228
EP  - 236
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445886
KW  - Explainable AI
KW  - Explanation
KW  - Machine learning
KW  - Fairness
KW  - Ethical AI
KW  - Ethics of AI
KW  - Counterfactuals
KW  - Algorithmic Fairness
KW  - Philosophy of AI
KW  - Philosophy
KW  - Social category
KW  - Social kind
KW  - Social ontology
ER  - 

TY  - CONF
TI  - Data leverage: A framework for empowering the public in its relationship with technology companies
AU  - Vincent, Nicholas
AU  - Li, Hanlin
AU  - Tilly, Nicole
AU  - Chancellor, Stevie
AU  - Hecht, Brent
T3  - FAccT '21
AB  - Many powerful computing technologies rely on implicit and explicit data contributions from the public. This dependency suggests a potential source of leverage for the public in its relationship with technology companies: by reducing, stopping, redirecting, or otherwise manipulating data contributions, the public can reduce the effectiveness of many lucrative technologies. In this paper, we synthesize emerging research that seeks to better understand and help people action this data leverage. Drawing on prior work in areas including machine learning, human-computer interaction, and fairness and accountability in computing, we present a framework for understanding data leverage that highlights new opportunities to change technology company behavior related to privacy, economic inequality, content moderation and other areas of societal concern. Our framework also points towards ways that policymakers can bolster data leverage as a means of changing the balance of power between the public and tech companies.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445885
SP  - 215
EP  - 227
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445885
KW  - data poisoning
KW  - conscious data contribution
KW  - data leverage
KW  - data strikes
ER  - 

TY  - CONF
TI  - Evaluating fairness of machine learning models under uncertain and incomplete information
AU  - Awasthi, Pranjal
AU  - Beutel, Alex
AU  - Kleindessner, Matthäus
AU  - Morgenstern, Jamie
AU  - Wang, Xuezhi
T3  - FAccT '21
AB  - Training and evaluation of fair classifiers is a challenging problem. This is partly due to the fact that most fairness metrics of interest depend on both the sensitive attribute information and label information of the data points. In many scenarios it is not possible to collect large datasets with such information. An alternate approach that is commonly used is to separately train an attribute classifier on data with sensitive attribute information, and then use it later in the ML pipeline to evaluate the bias of a given classifier. While such decoupling helps alleviate the problem of demographic scarcity, it raises several natural questions such as: how should the attribute classifier be trained?, and how should one use a given attribute classifier for accurate bias estimation? In this work we study this question from both theoretical and empirical perspectives.We first experimentally demonstrate that the test accuracy of the attribute classifier is not always correlated with its effectiveness in bias estimation for a downstream model. In order to further investigate this phenomenon, we analyze an idealized theoretical model and characterize the structure of the optimal classifier. Our analysis has surprising and counter-intuitive implications where in certain regimes one might want to distribute the error of the attribute classifier as unevenly as possible among the different subgroups. Based on our analysis we develop heuristics for both training and using attribute classifiers for bias estimation in the data scarce regime. We empirically demonstrate the effectiveness of our approach on real and simulated data.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445884
SP  - 206
EP  - 214
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445884
KW  - Active Learning
KW  - Bias Estimation
KW  - Model Auditing
ER  - 

TY  - CONF
TI  - Removing spurious features can hurt accuracy and affect groups disproportionately
AU  - Khani, Fereshte
AU  - Liang, Percy
T3  - FAccT '21
AB  - Spurious features interfere with the goal of obtaining robust models that perform well across many groups within the population. A natural remedy is to remove such features from the model. However, in this work, we show that removing spurious features can surprisingly decrease accuracy due to the inductive biases of overparameterized models. In noiseless overparameterized linear regression, we completely characterize how the removal of spurious features affects accuracy across different groups (more generally, test distributions). In addition, we show that removal of spurious features can decrease the accuracy even on balanced datasets (where each target co-occurs equally with each spurious feature); and it can inadvertently make the model more susceptible to other spurious features. Finally, we show that robust self-training produces models that no longer depend on spurious features without affecting their overall accuracy. The empirical results on the Toxic-Comment-Detection and CelebA datasets show that our results hold in non-linear models.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445883
SP  - 196
EP  - 205
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445883
ER  - 

TY  - CONF
TI  - Better together? How externalities of size complicate notions of solidarity and actuarial fairness
AU  - Donahue, Kate
AU  - Barocas, Solon
T3  - FAccT '21
AB  - Consider a cost-sharing game with players of different costs: an example might be an insurance company calculating premiums for a population of mixed-risk individuals. Two natural and competing notions of fairness might be to a) charge each individual the same or b) charge each individual according to the cost that they bring to the pool. In the insurance literature, these approaches are referred to as "solidarity" and "actuarial fairness" and are commonly viewed as opposites. However, in insurance (and many other natural settings), the cost-sharing game also exhibits externalities of size: all else being equal, larger groups have lower average cost. In the insurance case, we analyze model where costs strictly decreases with pooling due to a reduction in the variability of losses. In this paper, we explore how this complicates traditional understandings of fairness, drawing on literature in cooperative game theory.First, we explore solidarity: we show that it is possible for both groups (high risk and low risk) to strictly benefit by joining an insurance pool where costs are evenly split, as opposed to being in separate risk pools. We build on this by producing a pricing scheme that maximally subsidizes the high risk group, while maintaining an incentive for lower risk people to stay in the insurance pool. Next, we demonstrate that with this new model, the price charged to each individual has to depend on the risk of other participants, making naive actuarial fairness inefficient. Furthermore, we prove that stable pricing schemes must be ones where players have the antisocial incentive desiring riskier partners, contradicting motivations for using actuarial fairness. Finally, we describe how these results relate to debates about fairness in machine learning and potential avenues for future research.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445882
SP  - 185
EP  - 195
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445882
KW  - insurance
KW  - actuarial fairness
KW  - cooperative game theory
KW  - fair cost sharing
KW  - solidarity
KW  - submodular cost function
ER  - 

TY  - CONF
TI  - Leveraging administrative data for bias audits: Assessing disparate coverage with mobility data for COVID-19 policy
AU  - Coston, Amanda
AU  - Guha, Neel
AU  - Ouyang, Derek
AU  - Lu, Lisa
AU  - Chouldechova, Alexandra
AU  - Ho, Daniel E.
T3  - FAccT '21
AB  - Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data—containing individual-level voter turnout for specific voting locations along with race and age—can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445881
SP  - 173
EP  - 184
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445881
ER  - 

TY  - CONF
TI  - Documenting computer vision datasets: An invitation to reflexive data practices
AU  - Miceli, Milagros
AU  - Yang, Tianling
AU  - Naudts, Laurens
AU  - Schuessler, Martin
AU  - Serbanescu, Diana
AU  - Hanna, Alex
T3  - FAccT '21
AB  - In industrial computer vision, discretionary decisions surrounding the production of image training data remain widely undocumented. Recent research taking issue with such opacity has proposed standardized processes for dataset documentation. In this paper, we expand this space of inquiry through fieldwork at two data processing companies and thirty interviews with data workers and computer vision practitioners. We identify four key issues that hinder the documentation of image datasets and the effective retrieval of production contexts. Finally, we propose reflexivity, understood as a collective consideration of social and intellectual factors that lead to praxis, as a necessary precondition for documentation. Reflexive documentation can help to expose the contexts, relations, routines, and power structures that shape data.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445880
SP  - 161
EP  - 172
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445880
KW  - machine learning
KW  - transparency
KW  - accountability
KW  - audits
KW  - data annotation
KW  - dataset documentation
KW  - datasheets for datasets
KW  - reflexivity
KW  - training data
ER  - 

TY  - CONF
TI  - Can you fake it until you make it? Impacts of differentially private synthetic data on downstream classification fairness
AU  - Cheng, Victoria
AU  - Suriyakumar, Vinith M.
AU  - Dullerud, Natalie
AU  - Joshi, Shalmali
AU  - Ghassemi, Marzyeh
T3  - FAccT '21
AB  - The recent adoption of machine learning models in high-risk settings such as medicine has increased demand for developments in privacy and fairness. Rebalancing skewed datasets using synthetic data created by generative adversarial networks (GANs) has shown potential to mitigate disparate impact on minoritized subgroups. However, such generative models are subject to privacy attacks that can expose sensitive data from the training dataset. Differential privacy (DP) is the current leading solution for privacy-preserving machine learning. Differentially private GANs (DP GANs) are often considered a potential solution for improving model fairness while maintaining privacy of sensitive training data. We investigate the impact of using synthetic images from DP GANs on downstream classification model utility and fairness. We demonstrate that existing DP GANs cannot simultaneously maintain model utility, privacy, and fairness. The images generated from GAN models trained with DP exhibit extreme decreases in image quality and utility which leads to poor downstream classification model performance. Our evaluation highlights the friction between privacy, fairness, and utility and how this directly translates into real loss of performance and representation in common machine learning settings. Our results show that additional work improving the utility and fairness of DP generative models is required before they can be utilized as a potential solution to privacy and fairness issues stemming from lack of diversity in the training dataset.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445879
SP  - 149
EP  - 160
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445879
ER  - 

TY  - CONF
TI  - Towards fair deep anomaly detection
AU  - Zhang, Hongjing
AU  - Davidson, Ian
T3  - FAccT '21
AB  - Anomaly detection aims to find instances that are considered unusual and is a fundamental problem of data science. Recently, deep anomaly detection methods were shown to achieve superior results particularly in complex data such as images. Our work focuses on deep one-class classification for anomaly detection which learns a mapping only from the normal samples. However, the non-linear transformation performed by deep learning can potentially find patterns associated with social bias. The challenge with adding fairness to deep anomaly detection is to ensure both making fair and correct anomaly predictions simultaneously. In this paper, we propose a new architecture for the fair anomaly detection approach (Deep Fair SVDD) and train it using an adversarial network to de-correlate the relationships between the sensitive attributes and the learned representations. This differs from how fairness is typically added namely as a regularizer or a constraint. Further, we propose two effective fairness measures and empirically demonstrate that existing deep anomaly detection methods are unfair. We show that our proposed approach can remove the unfairness largely with minimal loss on the anomaly detection performance. Lastly, we conduct an in-depth analysis to show the strength and limitations of our proposed model, including parameter analysis, feature visualization, and run-time analysis.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445878
SP  - 138
EP  - 148
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445878
KW  - machine learning
KW  - deep learning
KW  - anomaly detection
KW  - algorithmic fairness
KW  - adversarial learning
ER  - 

TY  - CONF
TI  - The effect of differential victim crime reporting on predictive policing systems
AU  - Akpinar, Nil-Jana
AU  - De-Arteaga, Maria
AU  - Chouldechova, Alexandra
T3  - FAccT '21
AB  - Police departments around the world have been experimenting with forms of place-based data-driven proactive policing for over two decades. Modern incarnations of such systems are commonly known as hot spot predictive policing. These systems predict where future crime is likely to concentrate such that police can allocate patrols to these areas and deter crime before it occurs. Previous research on fairness in predictive policing has concentrated on the feedback loops which occur when models are trained on discovered crime data, but has limited implications for models trained on victim crime reporting data. We demonstrate how differential victim crime reporting rates across geographical areas can lead to outcome disparities in common crime hot spot prediction models. Our analysis is based on a simulation1 patterned after district-level victimization and crime reporting survey data for Bogotá, Colombia. Our results suggest that differential crime reporting rates can lead to a displacement of predicted hotspots from high crime but low reporting areas to high or medium crime and high reporting areas. This may lead to misallocations both in the form of over-policing and under-policing.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445877
SP  - 838
EP  - 849
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445877
ER  - 

TY  - CONF
TI  - Group fairness: Independence revisited
AU  - Räz, Tim
T3  - FAccT '21
AB  - This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445876
SP  - 129
EP  - 137
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445876
KW  - fairness
KW  - accuracy
KW  - affirmative action
KW  - demographic parity
KW  - independence
KW  - separation
KW  - statistical parity
KW  - sufficiency
ER  - 

TY  - CONF
TI  - Differential tweetment: Mitigating racial dialect bias in harmful tweet detection
AU  - Ball-Burack, Ari
AU  - Lee, Michelle Seng Ah
AU  - Cobbe, Jennifer
AU  - Singh, Jatinder
T3  - FAccT '21
AB  - Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445875
SP  - 116
EP  - 128
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445875
KW  - fairness
KW  - bias
KW  - racial disparities
KW  - content moderation
KW  - dialect
ER  - 

TY  - CONF
TI  - Computer science communities: Who is speaking, and who is listening to the women? Using an ethics of care to promote diverse voices
AU  - Cheong, Marc
AU  - Leins, Kobi
AU  - Coghlan, Simon
T3  - FAccT '21
AB  - Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445874
SP  - 106
EP  - 115
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445874
KW  - computer science
KW  - research
KW  - gender
KW  - diversity
KW  - gender representation
KW  - publishing
KW  - sex equality
ER  - 

TY  - CONF
TI  - The distributive effects of risk prediction in environmental compliance: Algorithmic design, environmental justice, and public policy
AU  - Benami, Elinor
AU  - Whitaker, Reid
AU  - La, Vincent
AU  - Lin, Hongjin
AU  - Anderson, Brandon R.
AU  - Ho, Daniel E.
T3  - FAccT '21
AB  - Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445873
SP  - 90
EP  - 105
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445873
KW  - fairness
KW  - environmental protection
KW  - environmental justice
KW  - government
KW  - risk models
ER  - 

TY  - CONF
TI  - Representativeness in statistics, politics, and machine learning
AU  - Chasalow, Kyla
AU  - Levy, Karen
T3  - FAccT '21
AB  - Representativeness is a foundational yet slippery concept. Though familiar at first blush, it lacks a single precise meaning. Instead, meanings range from typical or characteristic, to a proportionate match between sample and population, to a more general sense of accuracy, generalizability, coverage, or inclusiveness. Moreover, the concept has long been contested. In statistics, debates about the merits and methods of selecting a representative sample date back to the late 19th century; in politics, debates about the value of likeness as a logic of political representation are older still. Today, as the concept crops up in the study of fairness and accountability in machine learning, we need to carefully consider the term's meanings in order to communicate clearly and account for their normative implications. In this paper, we ask what representativeness means, how it is mobilized socially, and what values and ideals it communicates or confronts. We trace the concept's history in statistics and discuss normative tensions concerning its relationship to likeness, exclusion, authority, and aspiration. We draw on these analyses to think through how representativeness is used in FAccT debates, with emphasis on data, shift, participation, and power.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445872
SP  - 77
EP  - 89
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445872
KW  - fairness
KW  - participation
KW  - bias
KW  - sampling
KW  - inclusion
KW  - representativeness
ER  - 

TY  - CONF
TI  - Fifty shades of grey: In praise of a nuanced approach towards trustworthy design
AU  - Thornton, Lauren
AU  - Knowles, Bran
AU  - Blair, Gordon
T3  - FAccT '21
AB  - Environmental data science is uniquely placed to respond to essentially complex and fantastically worthy challenges related to arresting planetary destruction. Trust is needed for facilitating collaboration between scientists who may share datasets and algorithms, and for crafting appropriate science-based policies. Achieving this trust is particularly challenging because of the numerous complexities, multi-scale variables, interdependencies and multi-level uncertainties inherent in environmental data science. Virtual Labs—easily accessible online environments provisioning access to datasets, analysis and visualisations—are socio-technical systems which, if carefully designed, might address these challenges and promote trust in a variety of ways. In addition to various system properties that can be utilised in support of effective collaboration, certain features which are commonly seen to benefit trust—transparency and provenance in particular—appear applicable to promoting trust in and through Virtual Labs. Attempting to realise these features in their design reveals, however, that their implementation is more nuanced and complex than it would appear. Using the lens of affordances, we argue for the need to carefully articulate these features, with consideration of multiple stakeholder needs on balance, so that these Virtual Labs do in fact promote trust. We argue that these features not be conceived as widgets that can be imported into a given context to promote trust; rather, whether they promote trust is a function of how systematically designers consider various (potentially conflicting) stakeholder trust needs.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445871
SP  - 64
EP  - 76
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445871
KW  - Trust
KW  - Affordances
KW  - Environmental Data Science
KW  - Virtual Research Environments
ER  - 

TY  - CONF
TI  - Designing an online infrastructure for collecting AI data from people with disabilities
AU  - Park, Joon Sung
AU  - Bragg, Danielle
AU  - Kamar, Ece
AU  - Morris, Meredith Ringel
T3  - FAccT '21
AB  - AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445870
SP  - 52
EP  - 63
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445870
KW  - disability
KW  - datasets
KW  - accessibility
KW  - AI FATE
KW  - inclusion
KW  - representation
ER  - 

TY  - CONF
TI  - Biases in generative art: A causal look from the lens of art history
AU  - Srinivasan, Ramya
AU  - Uchino, Kanji
T3  - FAccT '21
AB  - With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445869
SP  - 41
EP  - 51
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445869
KW  - AI
KW  - biases
KW  - generative art
KW  - socio-cultural impacts
KW  - style transfer
ER  - 

TY  - CONF
TI  - Corporate social responsibility via multi-armed bandits
AU  - Ron, Tom
AU  - Ben-Porat, Omer
AU  - Shalit, Uri
T3  - FAccT '21
AB  - We propose a multi-armed bandit setting where each arm corresponds to a subpopulation, and pulling an arm is equivalent to granting an opportunity to this subpopulation. In this setting the decision-maker's fairness policy governs the number of opportunities each subpopulation should receive, which typically depends on the (unknown) reward from granting an opportunity to this subpopulation. The decision-maker can decide whether to provide these opportunities, or pay a pre-defined monetary value for every withheld opportunity. The decision-maker's objective is to maximize her utility, which is the sum of rewards minus the cost paid for withheld opportunities. We provide a no-regret algorithm that maximizes the decision-maker's utility and complement our analysis with an almost-tight lower bound. Finally, we discuss the fairness policy and demonstrate its downstream implications on the utility and opportunities via simulations.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445868
SP  - 26
EP  - 40
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445868
ER  - 

TY  - CONF
TI  - Allocating opportunities in a dynamic model of intergenerational mobility
AU  - Heidari, Hoda
AU  - Kleinberg, Jon
T3  - FAccT '21
AB  - Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status — a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445867
SP  - 15
EP  - 25
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445867
KW  - continuous Markov Decision Processes (MDP)
KW  - Intergenerational mobility
KW  - optimal allocation policy
KW  - socioeconomic affirmative action
ER  - 

TY  - CONF
TI  - Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence
AU  - Kasirzadeh, Atoosa
T3  - FAccT '21
AB  - The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445866
SP  - 14
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445866
KW  - Explainable AI
KW  - Machine learning
KW  - Explainable Artificial Intelligence
KW  - Ethical AI
KW  - Ethics of AI
KW  - Interpretable Machine Learning
KW  - Explainable Machine Learning
KW  - Philosophy of AI
KW  - Philosophy of Explanation
ER  - 

TY  - CONF
TI  - Fairness violations and mitigation under covariate shift
AU  - Singh, Harvineet
AU  - Singh, Rina
AU  - Mhasawade, Vishwali
AU  - Chunara, Rumi
T3  - FAccT '21
AB  - We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445865
SP  - 3
EP  - 13
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445865
KW  - covariate shift
KW  - domain adaptation
KW  - algorithmic fairness
KW  - causal inference
ER  - 

TY  - CONF
TI  - Price discrimination with fairness constraints
AU  - Cohen, Maxime C.
AU  - Elmachtoub, Adam N.
AU  - Lei, Xiao
T3  - FAccT '21
AB  - Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints.In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations.We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature.Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.
C1  - New York, NY, USA
C3  - Proceedings of the 2021 ACM conference on fairness, accountability, and transparency
DA  - 2021///
PY  - 2021
DO  - 10.1145/3442188.3445864
SP  - 2
PB  - Association for Computing Machinery
SN  - 978-1-4503-8309-7
UR  - https://doi.org/10.1145/3442188.3445864
KW  - personalization
KW  - fairness
KW  - price discrimination
KW  - social welfare
ER  - 

TY  - CONF
TI  - CrowdWorkSheets: Accounting for individual and collective identities underlying crowdsourced dataset annotation
AU  - Díaz, Mark
AU  - Kivlichan, Ian
AU  - Rosen, Rachel
AU  - Baker, Dylan
AU  - Amironesei, Razvan
AU  - Prabhakaran, Vinodkumar
AU  - Denton, Emily
T3  - FAccT '22
AB  - Human annotated data plays a crucial role in machine learning (ML) research and development. However, the ethical considerations around the processes and decisions that go into dataset annotation have not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this space along two layers: (1) who the annotator is, and how the annotators’ lived experiences can impact their annotations, and (2) the relationship between the annotators and the crowdsourcing platforms, and what that relationship affords them. Finally, we introduce a novel framework, CrowdWorkSheets, for dataset developers to facilitate transparent documentation of key decisions points at various stages of the data annotation pipeline: task formulation, selection of annotators, platform and infrastructure choices, dataset analysis and evaluation, and dataset release and maintenance.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534647
SP  - 2342
EP  - 2351
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534647
ER  - 

TY  - CONF
TI  - Locality of technical objects and the role of structural interventions for systemic change
AU  - Cruz Cortés, Efrén
AU  - Rajtmajer, Sarah
AU  - Ghosh, Debashis
T3  - FAccT '22
AB  - Technical objects, like algorithms, exhibit causal capacities both in terms of their internal makeup and the position they occupy in relation to other objects and processes within a system. At the same time, systems encompassing technical objects interact with other systems themselves, producing a multi-scale structural composition. In the framework of fair artificial intelligence, typical causal inference interventions focus on the internal workings of technical objects (fairness constraints), and often forsake structural properties of the system. However, these interventions are often not sufficient to capture forms of discrimination and harm at a systemic level. To complement this approach we introduce the notion of locality and define structural interventions. We compare the effect of structural interventions on a system compared to local, structure-preserving interventions on technical objects. We focus on comparing interventions on generating mechanisms (representing social dynamics giving rise to discrimination) with constraining algorithms to satisfy some measure of fairness. This framework allows us to identify bias outside the algorithmic stage and propose joint interventions on social dynamics and algorithm design. We show how, for a model of financial lending, structural interventions can drive the system towards equality even when algorithmic interventions are unable to do so. This suggests that the responsibility of decision makers extends beyond ensuring that local fairness metrics are satisfied to an ecosystem that fosters equity for all.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534646
SP  - 2327
EP  - 2341
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534646
KW  - interventions
KW  - fairness
KW  - ethics
KW  - sociotechnical systems
KW  - lending
KW  - system dynamics
ER  - 

TY  - CONF
TI  - Enforcing group fairness in algorithmic decision making: Utility maximization under sufficiency
AU  - Baumann, Joachim
AU  - Hannák, Anikó
AU  - Heitz, Christoph
T3  - FAccT '22
AB  - Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two). We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity. We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534645
SP  - 2315
EP  - 2326
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534645
KW  - machine learning
KW  - algorithmic fairness
KW  - sufficiency
KW  - constrained utility optimization
KW  - fairness trade-offs
KW  - group fairness metrics
KW  - prediction-based decision making
ER  - 

TY  - CONF
TI  - Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems
AU  - Pastaltzidis, Ioannis
AU  - Dimitriou, Nikolaos
AU  - Quezada-Tavarez, Katherine
AU  - Aidinlis, Stergios
AU  - Marquenie, Thomas
AU  - Gurzawska, Agata
AU  - Tzovaras, Dimitrios
T3  - FAccT '22
AB  - Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534644
SP  - 2302
EP  - 2314
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534644
ER  - 

TY  - CONF
TI  - People are not coins: Morally distinct types of predictions necessitate different fairness constraints
AU  - Viganò, Eleonora
AU  - Hertweck, Corinna
AU  - Heitz, Christoph
AU  - Loi, Michele
T3  - FAccT '22
AB  - In a recent paper [1], Brian Hedden has argued that most of the group fairness constraints discussed in the machine learning literature are not necessary conditions for the fairness of predictions, and hence that there are no genuine fairness metrics. This is proven by discussing a special case of a fair prediction. In our paper, we show that Hedden's argument does not hold for the most common kind of predictions used in data science, which are about people and based on data from similar people; we call these “human-group-based practices.” We argue that there is a morally salient distinction between human-group-based practices and those that are based on data of only one person, which we call “human-individual-based practices.” Thus, what may be a necessary condition for the fairness of human-group-based practices may not be a necessary condition for the fairness of human-individual-based practices, on which Hedden's argument is based. Accordingly, the group fairness metrics discussed in the machine learning literature may still be relevant for most applications of prediction-based decision making.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534643
SP  - 2293
EP  - 2301
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534643
KW  - algorithmic decision making
KW  - fairness metrics
KW  - fair prediction
KW  - moral principles
ER  - 

TY  - CONF
TI  - What does it mean for a language model to preserve privacy?
AU  - Brown, Hannah
AU  - Lee, Katherine
AU  - Mireshghallah, Fatemehsadat
AU  - Shokri, Reza
AU  - Tramèr, Florian
T3  - FAccT '22
AB  - Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534642
SP  - 2280
EP  - 2292
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534642
KW  - Privacy
KW  - Natural Language Processing
KW  - Differential Privacy
KW  - Data Sanitization
ER  - 

TY  - CONF
TI  - Measuring fairness of rankings under noisy sensitive information
AU  - Ghazimatin, Azin
AU  - Kleindessner, Matthaus
AU  - Russell, Chris
AU  - Abedjan, Ziawasch
AU  - Golebiowski, Jacek
T3  - FAccT '22
AB  - Metrics commonly used to assess group fairness in ranking require the knowledge of group membership labels (e.g., whether a job applicant is male or female). Obtaining accurate group membership labels, however, may be costly, operationally difficult, or even infeasible. Where it is not possible to obtain these labels, one common solution is to use proxy labels in their place, which are typically predicted by machine learning models. Proxy labels are susceptible to systematic biases, and using them for fairness estimation can thus lead to unreliable assessments. We investigate the problem of measuring group fairness in ranking for a suite of divergence-based metrics in the presence of proxy labels. We show that under certain assumptions, fairness of a ranking can reliably be measured from the proxy labels. We formalize two assumptions and provide a theoretical analysis for each showing how the true metric values can be derived from the estimates based on proxy labels. We prove that without such assumptions fairness assessment based on proxy labels is impossible. Through extensive experiments on both synthetic and real datasets, we demonstrate the effectiveness of our proposed methods for recovering reliable fairness assessments in rankings.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534641
SP  - 2263
EP  - 2279
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534641
ER  - 

TY  - CONF
TI  - Limits of individual consent and models of distributed consent in online social networks
AU  - Lovato, Juniper L.
AU  - Allard, Antoine
AU  - Harp, Randall
AU  - Onaolapo, Jeremiah
AU  - Hébert-Dufresne, Laurent
T3  - FAccT '22
AB  - Personal data are not discrete in socially-networked digital environments. A user who consents to allow access to their profile can expose the personal data of their network connections to non-consented access. Therefore, the traditional consent model (informed and individual) is not appropriate in social networks where informed consent may not be possible for all users affected by data processing and where information is distributed across users. Here, we outline the adequacy of consent for data transactions. Informed by the shortcomings of individual consent, we introduce both a platform-specific model of “distributed consent” and a cross-platform model of a “consent passport.” In both models, individuals and groups can coordinate by giving consent conditional on that of their network connections. We simulate the impact of these distributed consent models on the observability of social networks and find that low adoption would allow macroscopic subsets of networks to preserve their connectivity and privacy.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534640
SP  - 2251
EP  - 2262
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534640
KW  - Data privacy
KW  - Privacy models
KW  - Data ethics
KW  - Limits of consent
KW  - Social networks
ER  - 

TY  - CONF
TI  - A review of taxonomies of explainable artificial intelligence (XAI) methods
AU  - Speith, Timo
T3  - FAccT '22
AB  - The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534639
SP  - 2239
EP  - 2250
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534639
KW  - interpretability
KW  - XAI
KW  - explainability
KW  - transparency
KW  - explainable artificial intelligence
KW  - review
KW  - taxonomy
ER  - 

TY  - CONF
TI  - Understanding and being understood: User strategies for identifying and recovering from mistranslations in machine translation-mediated chat
AU  - Robertson, Samantha
AU  - Díaz, Mark
T3  - FAccT '22
AB  - Machine translation (MT) is now widely and freely available, and has the potential to greatly improve cross-lingual communication. In order to use MT reliably and safely, end users must be able to assess the quality of system outputs and determine how much they can rely on them to guide their decisions and actions. However, it can be difficult for users to detect and recover from mistranslations due to limited language skills. In this work we collected 19 MT-mediated role-play conversations in housing and employment scenarios, and conducted in-depth interviews to understand how users identify and recover from translation errors. Participants communicated using four language pairs: English, and one of Spanish, Farsi, Igbo, or Tagalog. We conducted qualitative analysis to understand user challenges in light of limited system transparency, strategies for recovery, and the kinds of translation errors that proved more or less difficult for users to overcome. We found that users broadly lacked relevant and helpful information to guide their assessments of translation quality. Instances where a user erroneously thought they had understood a translation correctly were rare but held the potential for serious consequences in the real world. Finally, inaccurate and disfluent translations had social consequences for participants, because it was difficult to discern when a disfluent message was reflective of the other person’s intentions, or an artifact of imperfect MT. We draw on theories of grounding and repair in communication to contextualize these findings, and propose design implications for explainable AI (XAI) researchers, MT researchers, as well as collaboration among them to support transparency and explainability in MT. These directions include handling typos and non-standard grammar common in interpersonal communication, making MT in interfaces more visible to help users evaluate errors, supporting collaborative repair of conversation breakdowns, and communicating model strengths and weaknesses to users.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534638
SP  - 2223
EP  - 2238
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534638
KW  - human-AI interaction
KW  - explainable machine learning
KW  - machine translation
KW  - computer-mediated communication
ER  - 

TY  - CONF
TI  - Data governance in the age of large-scale data-driven language technology
AU  - Jernite, Yacine
AU  - Nguyen, Huu
AU  - Biderman, Stella
AU  - Rogers, Anna
AU  - Masoud, Maraim
AU  - Danchev, Valentin
AU  - Tan, Samson
AU  - Luccioni, Alexandra Sasha
AU  - Subramani, Nishant
AU  - Johnson, Isaac
AU  - Dupont, Gerard
AU  - Dodge, Jesse
AU  - Lo, Kyle
AU  - Talat, Zeerak
AU  - Radev, Dragomir
AU  - Gokaslan, Aaron
AU  - Nikpoor, Somaieh
AU  - Henderson, Peter
AU  - Bommasani, Rishi
AU  - Mitchell, Margaret
T3  - FAccT '22
AB  - The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534637
SP  - 2206
EP  - 2222
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534637
KW  - datasets
KW  - data rights
KW  - language data
KW  - technology governance
ER  - 

TY  - CONF
TI  - Seeing without looking: Analysis pipeline for child sexual abuse datasets
AU  - Laranjeira da Silva, Camila
AU  - Macedo, Joao
AU  - Avila, Sandra
AU  - dos Santos, Jefersson
T3  - FAccT '22
AB  - The online sharing and viewing of Child Sexual Abuse Material (CSAM) are growing fast, such that human experts can no longer handle the manual inspection. However, the automatic classification of CSAM is a challenging field of research, largely due to the inaccessibility of target data that is — and should forever be — private and in sole possession of law enforcement agencies. To aid researchers in drawing insights from unseen data and safely providing further understanding of CSAM images, we propose an analysis template that goes beyond the statistics of the dataset and respective labels. It focuses on the extraction of automatic signals, provided both by pre-trained machine learning models, e.g., object categories and pornography detection, as well as image metrics such as luminance and sharpness. Only aggregated statistics of sparse signals are provided to guarantee the anonymity of children and adolescents victimized. The pipeline allows filtering the data by applying thresholds to each specified signal and provides the distribution of such signals within the subset, correlations between signals, as well as a bias evaluation. We demonstrated our proposal on the Region-based annotated Child Pornography Dataset (RCPD), one of the few CSAM benchmarks in the literature, composed of over 2000 samples among regular and CSAM images, produced in partnership with Brazil’s Federal Police. Although noisy and limited in several senses, we argue that automatic signals can highlight important aspects of the overall distribution of data, which is valuable for databases that can not be disclosed. Our goal is to safely publicize the characteristics of CSAM datasets, encouraging researchers to join the field and perhaps other institutions to provide similar reports on their benchmarks.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534636
SP  - 2189
EP  - 2205
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534636
KW  - transparency
KW  - bias
KW  - dataset
KW  - child sexual abuse
KW  - sensitive media
ER  - 

TY  - CONF
TI  - The long arc of fairness: Formalisations and ethical discourse
AU  - Schwöbel, Pola
AU  - Remmers, Peter
T3  - FAccT '22
AB  - In recent years, the idea of formalising and modelling fairness for algorithmic decision making (ADM) has advanced to a point of sophisticated specialisation. However, the relations between technical (formalised) and ethical discourse on fairness are not always clear and productive. Arguing for an alternative perspective, we review existing fairness metrics and discuss some common issues. For instance, the fairness of procedures and distributions is often formalised and discussed statically, disregarding both structural preconditions of the status quo and downstream effects of a given intervention. We then introduce dynamic fairness modelling, a more comprehensive approach that realigns formal fairness metrics with arguments from the ethical discourse. A dynamic fairness model incorporates (1) ethical goals, (2) formal metrics to quantify decision procedures and outcomes and (3) mid-term or long-term downstream effects. By contextualising these elements of fairness-related processes, dynamic fairness modelling explicates formerly latent ethical aspects and thereby provides a helpful tool to navigate trade-offs between different fairness interventions. To illustrate the framework, we discuss an example application – the current European efforts to increase the number of women on company boards, e&nbsp;.g.&nbsp; via quota solutions – and present early technical work that fits within our framework.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534635
SP  - 2179
EP  - 2188
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534635
KW  - algorithmic fairness
KW  - algorithmic decision making
KW  - fairness metrics
KW  - dynamic fairness modelling
KW  - ethics of machine learning
ER  - 

TY  - CONF
TI  - From demo to design in teaching machine learning
AU  - Bilstrup, Karl-Emi Kjær
AU  - Kaspersen, Magnus Høholt
AU  - Assent, Ira
AU  - Enni, Simon
AU  - Petersen, Marianne Graves
T3  - FAccT '22
AB  - The prevalence of artificial intelligence (AI) and machine learning (ML) technologies in digital ecosystems has led to a push for AI literacy, giving everybody, including K-12 students, the necessary knowledge and abilities to engage critically with these new technologies. While there is an increasing focus on designing tools and activities for teaching machine learning, most tools sidestep engaging with the complexity and trade-offs inherent in the design of ML models in favor of demonstrating the power and functionality of the technology. In this paper, we investigate how a design perspective can inform the design of educational tools and activities for teaching machine learning. Through a literature review, we identify 34 tools and activities for teaching ML, and using a design perspective on ML system development, we examine strengths and limitations in how they engage students in the complex design considerations linked to the different components of machine learners. Based on this work, we suggest directions for furthering AI literacy through adopting a design approach in teaching ML.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534634
SP  - 2168
EP  - 2178
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534634
KW  - Design
KW  - AI literacy
ER  - 

TY  - CONF
TI  - Fast online ranking with fairness of exposure
AU  - Usunier, Nicolas
AU  - Do, Virginie
AU  - Dohmatob, Elvis
T3  - FAccT '22
AB  - As recommender systems become increasingly central for sorting and prioritizing the content available online, they have a growing impact on the opportunities or revenue of their items producers. For instance, they influence which recruiter a resume is recommended to, or to whom and how much a music track, video or news article is being exposed. This calls for recommendation approaches that not only maximize (a proxy of) user satisfaction, but also consider some notion of fairness in the exposure of items or groups of items. Formally, such recommendations are usually obtained by maximizing a concave objective function in the space of randomized rankings. When the total exposure of an item is defined as the sum of its exposure over users, the optimal rankings of every users become coupled, which makes the optimization process challenging. Existing approaches to find these rankings either solve the global optimization problem in a batch setting, i.e., for all users at once, which makes them inapplicable at scale, or are based on heuristics that have weak theoretical guarantees. In this paper, we propose the first efficient online algorithm to optimize concave objective functions in the space of rankings which applies to every concave and smooth objective function, such as the ones found for fairness of exposure. Based on online variants of the Frank-Wolfe algorithm, we show that our algorithm is computationally fast, generating rankings on-the-fly with computation cost dominated by the sort operation, memory efficient, and has strong theoretical guarantees. Compared to baseline policies that only maximize user-side performance, our algorithm allows to incorporate complex fairness of exposure criteria in the recommendations with negligible computational overhead. We present experiments on artificial music and movie recommendation tasks using Last.fm and MovieLens datasets which suggest that in practice, the algorithm rapidly reaches good performances on three different objectives representing different fairness of exposure criteria.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534633
SP  - 2157
EP  - 2167
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534633
KW  - recommender systems
KW  - fairness
KW  - online ranking
ER  - 

TY  - CONF
TI  - Promoting fairness in learned models by learning to active learn under parity constraints
AU  - Sharaf, Amr
AU  - Daume III, Hal
AU  - Ni, Renkun
T3  - FAccT '22
AB  - Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534632
SP  - 2149
EP  - 2156
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534632
KW  - meta-learning
KW  - active learning
ER  - 

TY  - CONF
TI  - Algorithmic tools in public employment services: Towards a jobseeker-centric perspective
AU  - Scott, Kristen M.
AU  - Wang, Sonja Mei
AU  - Miceli, Milagros
AU  - Delobelle, Pieter
AU  - Sztandar-Sztanderska, Karolina
AU  - Berendt, Bettina
T3  - FAccT '22
AB  - Data-driven and algorithmic systems have been introduced to support Public Employment Services (PES) throughout the world. Their deployment has sparked public controversy and, as a consequence, some of these systems have been removed from use or their role was reduced. Yet the implementation of similar systems continues. In this paper, we use a participatory approach to determine a course forward for research and development in this area. We draw attention to the needs and expectations of people directly affected by these systems, i.e., jobseekers. Our investigation comprises two workshops: the first a fact-finding workshop with academics, system developers, the public sector, and civil-society organizations, the second a co-design workshop with 13 unemployed migrants to Germany. Based on the discussion in the fact-finding workshop we identified challenges of existing PES (algorithmic) systems. From the co-design workshop we identified our participants’ needs and desires when contacting PES: the need for human contact, the expectation to receive genuine orientation, and the desire to be seen as a whole human being. We map these expectations to three design considerations for data-driven and algorithmic systems for PES: the importance of interpersonal interaction, jobseeker assessment as direction, and the challenge of mitigating misrepresentation. Finally, we argue that the limitations and risks of current systems cannot be addressed through minor adjustments but require a more fundamental change to the role of PES.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534631
SP  - 2138
EP  - 2148
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534631
KW  - Algorithmic Decision-Making
KW  - Participatory Design
KW  - Public Employment Services
ER  - 

TY  - CONF
TI  - Keep your friends close and your counterfactuals closer: Improved learning from closest rather than plausible counterfactual explanations in an abstract setting
AU  - Kuhl, Ulrike
AU  - Artelt, André
AU  - Hammer, Barbara
T3  - FAccT '22
AB  - Counterfactual explanations (CFEs) highlight changes to a model’s input that alter its prediction in a particular way. s have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of plausibility for automatically generated s, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of this constraint on user experience are yet unclear. In this study, we evaluate objective and subjective usability of plausible s in an iterative learning task. We rely on a game-like experimental design, revolving around an abstract scenario. Our results show that novice users benefit less from receiving plausible rather than closest s that induce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences for subjective usability between both groups. Following the view of psychological plausibility as comparative similarity, users in the closest condition may experience their s as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI. All source code and data of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534630
SP  - 2125
EP  - 2137
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534630
KW  - XAI
KW  - Human Factors
KW  - Counterfactual Explanations
KW  - Algorithm Evaluation
KW  - Quantitative User Study
ER  - 

TY  - CONF
TI  - The effects of crowd worker biases in fact-checking tasks
AU  - Draws, Tim
AU  - La Barbera, David
AU  - Soprano, Michael
AU  - Roitero, Kevin
AU  - Ceolin, Davide
AU  - Checco, Alessandro
AU  - Mizzaro, Stefano
T3  - FAccT '22
AB  - Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534629
SP  - 2114
EP  - 2124
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534629
KW  - Explainability
KW  - Truthfulness
KW  - Crowdsourcing
KW  - Bias
KW  - Misinformation
ER  - 

TY  - CONF
TI  - The conflict between explainable and accountable decision-making algorithms
AU  - Lima, Gabriel
AU  - Grgić-Hlača, Nina
AU  - Jeong, Jin Keun
AU  - Cha, Meeyoung
T3  - FAccT '22
AB  - Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534628
SP  - 2103
EP  - 2113
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534628
KW  - Accountability
KW  - Explainability
KW  - Artificial Intelligence
KW  - AI
KW  - Decision-Making
KW  - Algorithms
KW  - Blame
KW  - Responsibility
KW  - Designers
KW  - Patients
KW  - Users
ER  - 

TY  - CONF
TI  - Theories of “Gender” in NLP bias research
AU  - Devinney, Hannah
AU  - Björklund, Jenny
AU  - Björklund, Henrik
T3  - FAccT '22
AB  - The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534627
SP  - 2083
EP  - 2102
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534627
KW  - natural language processing
KW  - gender bias
KW  - gender studies
ER  - 

TY  - CONF
TI  - Designing up with value-sensitive design: Building a field guide for ethical ML development
AU  - Boyd, Karen
T3  - FAccT '22
AB  - If “studying up,” or researching powerful actors in a social system, can offer insight into the workings and effects of power in social systems, this paper argues that “designing up” will give researchers and designers a tool to intervene. This paper offers a conception of “designing up,” applies the structure of Value Sensitive Design (VSD) to accomplish it, and submits an example of a tool designed to support ethical sensitivity, especially particularization and judgment. The designed artifact is a field guide for ethical mitigation strategies that uses tool profiles and filters to aid machine learning (ML) engineers as they build understanding of an ethical issue they have recognized and as they match the particulars of their problem to a technical ethical mitigation. This guide may broaden its users’ awareness of potential ethical issues, important features of ethical issues and their mitigations, and the breadth of available mitigations. Additionally, it may encourage ethical sensitivity in future ML projects. Feedback from ML engineers and technology ethics researchers rendered several usability improvements and ideas for future development. The tool can be found at: https://ml-ethics-tool.web.app/.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3534626
SP  - 2069
EP  - 2082
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3534626
KW  - machine learning
KW  - AI ethics
KW  - ethics
KW  - datasets
KW  - development practices
KW  - ethical sensitivity
ER  - 

TY  - CONF
TI  - Disentangling the components of ethical research in machine learning
AU  - Ashurst, Carolyn
AU  - Barocas, Solon
AU  - Campbell, Rosie
AU  - Raji, Deborah
T3  - FAccT '22
AB  - While practical applications of machine learning have been the target of considerable normative scrutiny over the past decade, there is growing concern with machine learning research as well. Debates are currently unfolding about how the research community should develop its research agendas, conduct its research, evaluate its research contributions, and handle the publication and dissemination of its findings, among other matters. At times, these debates have been quite heated, with different actors adopting different positions on what it means to do machine learning research ethically. In this paper, we show that some of the disagreement owes to a lack of clarity about what ethical issues are at stake in machine learning research, how these issues—in particular, the concerns with research integrity, research process harms, and downstream consequences—relate to (or, more often, differ from) one another. We then explore which mechanisms are most appropriate for dealing with the different types of ethical issues, and highlight which ethical issues require more attention than they are currently receiving. Ultimately, we hope to foster more productive discussions about the responsibilities that the community bears in addressing the ethical challenges tied to machine learning research and how to best fulfil these responsibilities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533781
SP  - 2057
EP  - 2068
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533781
ER  - 

TY  - CONF
TI  - AI ethics statements: Analysis and lessons learnt from NeurIPS broader impact statements
AU  - Ashurst, Carolyn
AU  - Hine, Emmie
AU  - Sedille, Paul
AU  - Carlier, Alexis
T3  - FAccT '22
AB  - Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533780
SP  - 2047
EP  - 2056
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533780
KW  - AI governance
KW  - broader impacts
KW  - conference policies
KW  - ethics statements
KW  - NeurIPS policies
KW  - research ethics
ER  - 

TY  - CONF
TI  - Limits and possibilities for “Ethical AI” in open source: A study of deepfakes
AU  - Widder, David Gray
AU  - Nafus, Dawn
AU  - Dabbish, Laura
AU  - Herbsleb, James
T3  - FAccT '22
AB  - Open source software communities are a significant site of AI development, but “Ethical AI” discourses largely focus on the problems that arise in software produced by private companies. Design, policy and tooling interventions to encourage “Ethical AI” based on studies in private companies risk being ill-suited for an open source context, which operates under radically different organizational structures, cultural norms, and incentives. In this paper, we show that significant and understudied harms and possibilities originate from differing practices of transparency and accountability in the open source community. We conducted an interview study of an AI-enabled open source Deepfake project to understand how members of that community reason about the ethics of their work. We found that notions of the “Freedom 0” to use code without any restriction, alongside beliefs about technology neutrality and technological inevitability, were central to how community members framed their responsibilities, and the actions they believed were and were not available to them. We propose a continuum between harms resulting from how a system is implemented versus how it is used, and show how commitments to radical transparency in open source allow great ethical scrutiny for harms wrought by implementation bugs, but allow harms through (mis)use to proliferate, requiring a deeper toolbox for disincentivizing harmful use. We discuss how an assumption of control over downstream uses is often implicit in discourses of “Ethical AI”, but outline alternative possibilities for action in cases such as open source where this assumption may not hold.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533779
SP  - 2035
EP  - 2046
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533779
KW  - responsibility
KW  - ethics
KW  - agency
KW  - open source
KW  - deepfakes
KW  - free software
KW  - interview
ER  - 

TY  - CONF
TI  - Is calibration a fairness requirement? An argument from the point of view of moral philosophy and decision theory
AU  - Loi, Michele
AU  - Heitz, Christoph
T3  - FAccT '22
AB  - In this paper, we provide a moral analysis of two criteria of statistical fairness debated in the machine learning literature: 1) calibration between groups and 2) equality of false positive and false negative rates between groups. In our paper, we focus on moral arguments in support of either measure. The conflict between group calibration vs. false positive and false negative rate equality is one of the core issues in the debate about group fairness definitions among practitioners. For any thorough moral analysis, the meaning of the term “fairness” has to be made explicit and defined properly. For our paper, we equate fairness with (non-)discrimination, which is a legitimate understanding in the discussion about group fairness. More specifically, we equate it with “prima facie wrongful discrimination” in the sense this is used in Prof. Lippert-Rasmussen's treatment of this definition. In this paper, we argue that a violation of group calibration may be unfair in some cases, but not unfair in others. Our argument analyzes in great detail two specific hypothetical examples of usage of predictions in decision making. The most important practical implication is that between-group calibration is defensible as a bias standard in some cases but not others; we show this by referring to examples in which the violation of between-group calibration is discriminatory, and others in which it is not. This is in line with claims already advanced in the literature, that algorithmic fairness should be defined in a way that is sensitive to context. The most important practical implication is that arguments based on examples in which fairness requires between-group calibration, or equality in the false-positive/false-negative rates, do no generalize. For it may be that group calibration is a fairness requirement in one case, but not in another.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533245
SP  - 2026
EP  - 2034
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533245
KW  - fairness
KW  - prediction
KW  - calibration
KW  - equalized odds
KW  - opportunity
ER  - 

TY  - CONF
TI  - Reliable and safe use of machine translation in medical settings
AU  - Mehandru, Nikita
AU  - Robertson, Samantha
AU  - Salehi, Niloufar
T3  - FAccT '22
AB  - Language barriers between patients and clinicians contribute to disparities in quality of care. Machine Translation (MT) tools are widely used in healthcare settings, but even small mistranslations can have life-threatening consequences. We study how MT is currently used in medical settings through a qualitative interview study with 20 clinicians–physicians, surgeons, nurses, and midwives. We find that clinicians face challenges stemming from lack of time and resources, cultural barriers, and medical literacy rates, as well as accountability in cases of miscommunication. Clinicians have devised strategies to aid communication in the face of language barriers including back translation, non-verbal communication, and testing patient understanding. We propose design implications for machine translation systems including combining neural MT with pre-translated medical phrases, integrating translation support with multimodal communication, and providing interactive support for testing mutual understanding.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533244
SP  - 2016
EP  - 2025
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533244
KW  - Clinical Decision-Making
KW  - Machine Translation in Medicine
KW  - Reliability of AI Systems
ER  - 

TY  - CONF
TI  - Uncertainty and the social planner’s problem: Why sample complexity matters
AU  - Cousins, Cyrus
T3  - FAccT '22
AB  - Welfare measures overall utility across a population, whereas malfare measures overall disutility, and the social planner’s problem can be cast either as maximizing the former or minimizing the latter. We show novel bounds on the expectations and tail probabilities of estimators of welfare, malfare, and regret of per-group (dis)utility values, where estimates are made from a finite sample drawn from each group. In particular, we consider estimating these quantities for individual functions (e.g., allocations or classifiers) with standard probabilistic bounds, and optimizing and bounding generalization error over hypothesis classes (i.e., we quantify overfitting) using Rademacher averages. We then study algorithmic fairness through the lens of sample complexity, finding that because marginalized or minority groups are often understudied, and fewer data are therefore available, the social planner is more likely to overfit to these groups, thus even models that seem fair in training can be systematically biased against such groups. We argue that this effect can be mitigated by ensuring sufficient sample sizes for each group, and our sample complexity analysis characterizes these sample sizes. Motivated by these conclusions, we present progressive sampling algorithms to efficiently optimize various fairness objectives.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533243
SP  - 2004
EP  - 2015
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533243
ER  - 

TY  - CONF
TI  - What is proxy discrimination?
AU  - Tschantz, Michael Carl
T3  - FAccT '22
AB  - The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533242
SP  - 1993
EP  - 2003
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533242
KW  - discrimination
KW  - conceptual analysis
KW  - proxy
ER  - 

TY  - CONF
TI  - CounterFAccTual: How FAccT undermines its organizing principles
AU  - Gansky, Ben
AU  - McDonald, Sean
T3  - FAccT '22
AB  - This essay joins recent scholarship in arguing that FAccT's fundamental framing of the potential to achieve the normative conditions for justice through bettering the design of algorithmic systems is counterproductive to achieving said justice in practice. Insofar as the FAccT community's research tends to prioritize design-stage interventions, it ignores the fact that the majority of the contextual factors that practically determine FAccT outcomes happen in the implementation and impact stages of AI/ML lifecycles.We analyze an emergent and widely-cited movement within the FAccT community for attempting to honor the centrality of contextual factors in shaping social outcomes, a set of strategies we term ‘metadata maximalism’. Symptomatic of design-centered approaches, metadata maximalism abstracts away its reliance on institutions and structures of justice that are, by every observable metric, already struggling (where not failing) to provide accessible, enforceable rights. These justice infrastructures, moreover, are currently wildly under-equipped to manage the disputes arising from digital transformation and machine learning. The political economy of AI/ML implementation provides further obstructions to realizing rights. Data and software supply chains, in tandem with intellectual property protections, introduce structural sources of opacity. Where duties of care to vulnerable persons should reign, profit incentives are given legal and regulatory primacy. Errors are inevitable and inextricable from the development of machine learning systems.In the face of these realities, FAccT programs, including metadata maximalism, tend to project their efforts in a fundamentally counter-factual universe: one in which functioning institutions and processes for due diligence in implementation and for redress of harms are working and ready to interoperate with. Unfortunately, in our world, these institutions and processes have been captured by the interests they are meant to hold accountable, intentionally hollowed-out, and/or were never designed to function in today's sociotechnical landscape. Continuing to produce (fair! accountable! transparent!) data-enabled systems that operate in high-impact areas, irrespective of this landscape's radically insufficient paths to justice, given the unavoidability of errors and/or intentional misuse in implementation, and the exhaustively-demonstrated disproportionate distribution of resulting harms onto already-marginalized communities, is a choice - a choice to be CounterFAccTual.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533241
SP  - 1982
EP  - 1992
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533241
ER  - 

TY  - CONF
TI  - The spotlight: A general method for discovering systematic errors in deep learning models
AU  - d'Eon, Greg
AU  - d'Eon, Jason
AU  - Wright, James R.
AU  - Leyton-Brown, Kevin
T3  - FAccT '22
AB  - Supervised learning models often make systematic errors on rare subsets of the data. When these subsets correspond to explicit labels in the data (e.g., gender, race) such poor performance can be identified straightforwardly. This paper introduces a method for discovering systematic errors that do not correspond to such explicitly labelled subgroups. The key idea is that similar inputs tend to have similar representations in the final hidden layer of a neural network. We leverage this structure by “shining a spotlight” on this representation space to find contiguous regions in which the model performs poorly. We show that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and we verify its performance through quantitative experiments.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533240
SP  - 1962
EP  - 1981
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533240
KW  - deep learning
KW  - fairness
KW  - distributional robustness
KW  - auditing
ER  - 

TY  - CONF
TI  - Healthsheet: Development of a transparency artifact for health datasets
AU  - Rostamzadeh, Negar
AU  - Mincu, Diana
AU  - Roy, Subhrajit
AU  - Smart, Andrew
AU  - Wilcox, Lauren
AU  - Pushkarna, Mahima
AU  - Schrouff, Jessica
AU  - Amironesei, Razvan
AU  - Moorosi, Nyalleng
AU  - Heller, Katherine
T3  - FAccT '22
AB  - Machine learning (ML) approaches have demonstrated promising results in a wide range of healthcare applications. Data plays a crucial role in developing ML-based healthcare systems that directly affect people’s lives. Many of the ethical issues surrounding the use of ML in healthcare stem from structural inequalities underlying the way we collect, use, and handle data. Developing guidelines to improve documentation practices regarding the creation, use, and maintenance of ML healthcare datasets is therefore of critical importance. In this work, we introduce Healthsheet, a contextualized adaptation of the original datasheet questionnaire &nbsp;[22] for health-specific applications. Through a series of semi-structured interviews, we adapt the datasheets for healthcare data documentation. As part of the Healthsheet development process and to understand the obstacles researchers face in creating datasheets, we worked with three publicly-available healthcare datasets as our case studies, each with different types of structured data: Electronic health Records (EHR), clinical trial study data, and smartphone-based performance outcome measures. Our findings from the interviewee study and case studies show 1) that datasheets should be contextualized for healthcare, 2) that despite incentives to adopt accountability practices such as datasheets, there is a lack of consistency in the broader use of these practices 3) how the ML for health community views datasheets and particularly Healthsheets as diagnostic tool to surface the limitations and strength of datasets and 4) the relative importance of different fields in the datasheet to healthcare concerns.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533239
SP  - 1943
EP  - 1961
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533239
ER  - 

TY  - CONF
TI  - Fair ranking: a critical review, challenges, and future directions
AU  - Patro, Gourab K.
AU  - Porcaro, Lorenzo
AU  - Mitchell, Laura
AU  - Zhang, Qiuyue
AU  - Zehlike, Meike
AU  - Garg, Nikhil
T3  - FAccT '22
AB  - Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large “fair ranking” research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533238
SP  - 1929
EP  - 1942
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533238
KW  - Fairness
KW  - Ranking
KW  - Recommendation
KW  - Algorithmic Impact Assessment
KW  - Exposure
KW  - Strategic Behaviour
ER  - 

TY  - CONF
TI  - How platform-user power relations shape algorithmic accountability: A case study of instant loan platforms and financially stressed users in india
AU  - Ramesh, Divya
AU  - Kameswaran, Vaishnav
AU  - Wang, Ding
AU  - Sambasivan, Nithya
T3  - FAccT '22
AB  - Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533237
SP  - 1917
EP  - 1928
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533237
KW  - algorithmic accountability
KW  - algorithmic fairness
KW  - socio-technical systems
KW  - human-ai interaction
KW  - instant loans
ER  - 

TY  - CONF
TI  - Marrying fairness and explainability in supervised learning
AU  - Grabowicz, Przemyslaw A.
AU  - Perello, Nicholas
AU  - Mishra, Aarshee
T3  - FAccT '22
AB  - Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533236
SP  - 1905
EP  - 1916
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533236
KW  - machine learning
KW  - explainability
KW  - discrimination
KW  - supervised learning
KW  - algorithmic fairness
ER  - 

TY  - CONF
TI  - Model explanations with differential privacy
AU  - Patel, Neel
AU  - Shokri, Reza
AU  - Zick, Yair
T3  - FAccT '22
AB  - Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533235
SP  - 1895
EP  - 1904
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533235
KW  - Differential Privacy
KW  - Model Explainations
ER  - 

TY  - CONF
TI  - Measuring the carbon intensity of AI in cloud instances
AU  - Dodge, Jesse
AU  - Prewitt, Taylor
AU  - Tachet des Combes, Remi
AU  - Odmark, Erika
AU  - Schwartz, Roy
AU  - Strubell, Emma
AU  - Luccioni, Alexandra Sasha
AU  - Smith, Noah A.
AU  - DeCario, Nicole
AU  - Buchanan, Will
T3  - FAccT '22
AB  - The advent of cloud computing has provided people around the world with unprecedented access to computational power and enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, which precludes development of actionable tactics. We argue that cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions. In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models covering natural language processing and computer vision applications, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also present new results showing that the time of day has meaningful impact on operational software carbon intensity.Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533234
SP  - 1877
EP  - 1894
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533234
KW  - cloud
KW  - grid
KW  - carbon awareness
KW  - carbon intensity
KW  - CO2
KW  - emissions
ER  - 

TY  - CONF
TI  - Evaluation gaps in machine learning practice
AU  - Hutchinson, Ben
AU  - Rostamzadeh, Negar
AU  - Greer, Christina
AU  - Heller, Katherine
AU  - Prabhakaran, Vinodkumar
T3  - FAccT '22
AB  - Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533233
SP  - 1859
EP  - 1876
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533233
KW  - machine learning
KW  - evaluation
KW  - applications
ER  - 

TY  - CONF
TI  - On the existence of simpler machine learning models
AU  - Semenova, Lesia
AU  - Rudin, Cynthia
AU  - Parr, Ronald
T3  - FAccT '22
AB  - It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533232
SP  - 1827
EP  - 1858
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533232
KW  - Generalization
KW  - Interpretable Machine Learning
KW  - Model Multiplicity
KW  - Rashomon Set
KW  - Simplicity
ER  - 

TY  - CONF
TI  - Data cards: Purposeful and transparent dataset documentation for responsible AI
AU  - Pushkarna, Mahima
AU  - Zaldivar, Andrew
AU  - Kjartansson, Oddur
T3  - FAccT '22
AB  - As research and industry moves towards large-scale models capable of numerous downstream tasks, the complexity of understanding multi-modal datasets that give nuance to models rapidly increases. A clear and thorough understanding of a dataset’s origins, development, intent, ethical considerations and evolution becomes a necessary step for the responsible and informed deployment of models, especially those in people-facing contexts and high-risk domains. However, the burden of this understanding often falls on the intelligibility, conciseness, and comprehensiveness of the documentation. It requires consistency and comparability across the documentation of all datasets involved, and as such documentation must be treated as a user-centric product in and of itself. In this paper, we propose Data Cards for fostering transparent, purposeful and human-centered documentation of datasets within the practical contexts of industry and research. Data Cards are structured summaries of essential facts about various aspects of ML datasets needed by stakeholders across a dataset’s lifecycle for responsible AI development. These summaries provide explanations of processes and rationales that shape the data and consequently the models—such as upstream sources, data collection and annotation methods; training and evaluation methods, intended use; or decisions affecting model performance. We also present frameworks that ground Data Cards in real-world utility and human-centricity. Using two case studies, we report on desirable characteristics that support adoption across domains, organizational structures, and audience groups. Finally, we present lessons learned from deploying over 20 Data Cards.x
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533231
SP  - 1776
EP  - 1826
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533231
KW  - transparency
KW  - responsible AI
KW  - datasheets
KW  - model cards
KW  - dataset documentation
KW  - data cards
ER  - 

TY  - CONF
TI  - Models for understanding and quantifying feedback in societal systems
AU  - Reader, Lydia
AU  - Nokhiz, Pegah
AU  - Power, Cathleen
AU  - Patwari, Neal
AU  - Venkatasubramanian, Suresh
AU  - Friedler, Sorelle
T3  - FAccT '22
AB  - When it comes to long-term fairness in decision-making settings, many studies have focused on closed systems with a specific appointed decision-maker and certain engagement rules in place. However, if the objective is to achieve equity in a broader societal system, studying the system in isolation is insufficient. In a societal system, neither a singular decision maker nor defined agent behavior rules exist. Additionally, analysis of societal systems can be complicated by the presence of feedback, in which historical and current inequities influence future inequity. In this paper, we present a model to quantify feedback in social systems so that the long-term effects of a policy or decision process may be investigated, even when the feedback mechanisms are not individually characterized. We explore the dynamics of real social systems and find that many examples of feedback are qualitatively similar in their temporal characteristics. Using a key idea in linear systems theory, namely proportional-integral-derivative (PID) feedback, we propose a model to quantify three types of feedback. We illustrate how different components of the PID capture analogous aspects of societal dynamics such as the persistence of current inequity, the cumulative effects of long-term inequity, and the response to the speed at which society is changing. Our model does not attempt to describe underlying systems or capture individual actions. It is a system-based approach to study inequity in feedback loops, and as a result unlocks a direction to study social systems that would otherwise be almost impossible to model and can only be observed. Our framework helps elucidate the ability of fair policies to produce and sustain equity in the long-term.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533230
SP  - 1765
EP  - 1775
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533230
KW  - feedback
KW  - inequity
KW  - societal systems
ER  - 

TY  - CONF
TI  - Predictability and surprise in large generative models
AU  - Ganguli, Deep
AU  - Hernandez, Danny
AU  - Lovitt, Liane
AU  - Askell, Amanda
AU  - Bai, Yuntao
AU  - Chen, Anna
AU  - Conerly, Tom
AU  - Dassarma, Nova
AU  - Drain, Dawn
AU  - Elhage, Nelson
AU  - El Showk, Sheer
AU  - Fort, Stanislav
AU  - Hatfield-Dodds, Zac
AU  - Henighan, Tom
AU  - Johnston, Scott
AU  - Jones, Andy
AU  - Joseph, Nicholas
AU  - Kernian, Jackson
AU  - Kravec, Shauna
AU  - Mann, Ben
AU  - Nanda, Neel
AU  - Ndousse, Kamal
AU  - Olsson, Catherine
AU  - Amodei, Daniela
AU  - Brown, Tom
AU  - Kaplan, Jared
AU  - McCandlish, Sam
AU  - Olah, Christopher
AU  - Amodei, Dario
AU  - Clark, Jack
T3  - FAccT '22
AB  - Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533229
SP  - 1747
EP  - 1764
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533229
ER  - 

TY  - CONF
TI  - Adversarial scrutiny of evidentiary statistical software
AU  - Abebe, Rediet
AU  - Hardt, Moritz
AU  - Jin, Angela
AU  - Miller, John
AU  - Schmidt, Ludwig
AU  - Wexler, Rebecca
T3  - FAccT '22
AB  - The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533228
SP  - 1733
EP  - 1746
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533228
KW  - robust machine learning
KW  - adversarial scrutiny
KW  - black-box software
KW  - evidentiary software
KW  - statistical software
ER  - 

TY  - CONF
TI  - Testing concerns about technology's behavioral impacts with N-of-one trials
AU  - Matias, Nathan
AU  - Pennington, Eric
AU  - Chan, Zenobia
T3  - FAccT '22
AB  - As public trust in technology companies has declined, people are questioning the effects of digital technologies in their lives. In this context, many evidence-free claims from corporations and tech critics are widely circulated. How can members of the public make evidence-based decisions about digital technology in their lives? In clinical fields, N -of-one trials enable participant-investigators to make personalized causal discoveries about managing health, improving fitness, and improving their education. Similar methods could help community scientists understand and manage how they use digital technologies. In this paper, we introduce Conjecture, a system for coordinating N -of-one trials that can guide personal decisions about technology use and contribute to science. We describe N -of-one trials as a design challenge and present the design of the Conjecture system. We evaluate the system with a field experiment that tests folk theories about the influence of colorful screens on alleged phone addiction. We present findings on the design of N -of-one-trial systems based on submitted data, interviews, and surveys with 14 participants. Taken together, this paper introduces N -of-one trials as a fruitful direction for computer scientists designing industry-independent systems for evidence-based technology governance and accountability.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533227
SP  - 1722
EP  - 1732
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533227
KW  - community science
KW  - consumer protection
KW  - n-of-one trials
KW  - phone addiction
KW  - research software
ER  - 

TY  - CONF
TI  - Demographic-reliant algorithmic fairness: Characterizing the risks of demographic data collection in the pursuit of fairness
AU  - Andrus, McKane
AU  - Villeneuve, Sarah
T3  - FAccT '22
AB  - Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533226
SP  - 1709
EP  - 1721
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533226
KW  - discrimination
KW  - fairness
KW  - sexuality
KW  - categorization
KW  - gender
KW  - demographic data
KW  - sensitive data
KW  - measurement
KW  - race
KW  - identity
ER  - 

TY  - CONF
TI  - Fairness-aware model-agnostic positive and unlabeled learning
AU  - Wu, Ziwei
AU  - He, Jingrui
T3  - FAccT '22
AB  - With the increasing application of machine learning in high-stake decision-making problems, potential algorithmic bias towards people from certain social groups poses negative impacts on individuals and our society at large. In the real-world scenario, many such problems involve positive and unlabeled data such as medical diagnosis, criminal risk assessment and recommender systems. For instance, in medical diagnosis, only the diagnosed diseases will be recorded (positive) while others will not (unlabeled). Despite the large amount of existing work on fairness-aware machine learning in the (semi-)supervised and unsupervised settings, the fairness issue is largely under-explored in the aforementioned Positive and Unlabeled Learning (PUL) context, where it is usually more severe. In this paper, to alleviate this tension, we propose a fairness-aware PUL method named FairPUL. In particular, for binary classification over individuals from two populations, we aim to achieve similar true positive rates and false positive rates in both populations as our fairness metric. Based on the analysis of the optimal fair classifier for PUL, we design a model-agnostic post-processing framework, leveraging both the positive examples and unlabeled ones. Our framework is proven to be statistically consistent in terms of both the classification error and the fairness metric. Experiments on the synthetic and real-world data sets demonstrate that our framework outperforms state-of-the-art in both PUL and fair classification.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533225
SP  - 1698
EP  - 1708
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533225
KW  - Fairness
KW  - Machine Learning
KW  - Positive and Unlabeled Learning
ER  - 

TY  - CONF
TI  - NeuroView-RNN: It’s about time
AU  - Barberan, Cj
AU  - Alemmohammad, Sina
AU  - Liu, Naiming
AU  - Balestriero, Randall
AU  - Baraniuk, Richard
T3  - FAccT '22
AB  - Recurrent Neural Networks (RNNs) are important tools for processing sequential data such as time-series or video. Interpretability is defined as the ability to be understood by a person and is different from explainability, which is the ability to be explained in a mathematical formulation. A key interpretability issue with RNNs is that it is not clear how each hidden state per time step contributes to the decision-making process in a quantitative manner. We propose NeuroView-RNN as a family of new RNN architectures that explains how all the time steps are used for the decision-making process. Each member of the family is derived from a standard RNN architecture by concatenation of the hidden steps into a global linear classifier. The global linear classifier has all the hidden states as the input, so the weights of the classifier have a linear mapping to the hidden states. Hence, from the weights, NeuroView-RNN can quantify how important each time step is to a particular decision. As a bonus, NeuroView-RNN also offers higher accuracy in many cases compared to the RNNs and their variants. We showcase the benefits of NeuroView-RNN by evaluating on a multitude of diverse time-series datasets.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533224
SP  - 1683
EP  - 1697
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533224
KW  - interpretability
KW  - Recurrent neural networks
KW  - time series
ER  - 

TY  - CONF
TI  - What is the bureaucratic counterfactual? Categorical versus algorithmic prioritization in U.S. social policy
AU  - Johnson, Rebecca Ann
AU  - Zhang, Simone
T3  - FAccT '22
AB  - There is growing concern about governments’ use of algorithms to make high-stakes decisions. While an early wave of research focused on algorithms that predict risk to allocate punishment and suspicion, a newer wave of research studies algorithms that predict “need” or “benefit” to target beneficial resources, such as ranking those experiencing homelessness by their need for housing. The present paper argues that existing research on the role of algorithms in social policy could benefit from a counterfactual perspective that asks: given that a social service bureaucracy needs to make some decision about whom to help, what status quo prioritization method would algorithms replace? While a large body of research contrasts human versus algorithmic decision-making, social service bureaucracies target help not by giving street-level bureaucrats full discretion. Instead, they primarily target help through pre-algorithmic, rule-based methods. In this paper, we outline social policy’s current status quo method—categorical prioritization—where decision-makers manually (1) decide which attributes of help seekers should give those help seekers priority, (2) simplify any continuous measures of need into categories (e.g., household income falls below a threshold), and (3) manually choose the decision rules that map categories to priority levels. We draw on novel data and quantitative and qualitative social science methods to outline categorical prioritization in two case studies of United States social policy: waitlists for scarce housing vouchers and K-12 school finance formulas. We outline three main differences between categorical and algorithmic prioritization: is the basis for prioritization formalized; what role does power play in prioritization; and are decision rules for priority manually chosen or inductively derived from a predictive model. Concluding, we show how the counterfactual perspective underscores both the understudied costs of categorical prioritization in social policy and the understudied potential of predictive algorithms to narrow inequalities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533223
SP  - 1671
EP  - 1682
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533223
KW  - resource allocation
KW  - fairness and transparency
KW  - social policy
ER  - 

TY  - CONF
TI  - Stop the spread: A contextual integrity perspective on the appropriateness of COVID-19 vaccination certificates
AU  - Zhang, Shikun
AU  - Shvartzshnaider, Yan
AU  - Feng, Yuanyuan
AU  - Nissenbaum, Helen
AU  - Sadeh, Norman
T3  - FAccT '22
AB  - We present an empirical study exploring how privacy influences the acceptance of vaccination certificate (VC) deployments across different realistic usage scenarios. The study employed the privacy framework of Contextual Integrity, which has been shown to be particularly effective in capturing people’s privacy expectations across different contexts. We use a vignette methodology, where we selectively manipulate salient contextual parameters to learn whether and how they affect people’s attitudes towards VCs. We surveyed 890 participants from a demographically-stratified sample of the US population to gauge the acceptance and overall attitudes towards possible VC deployments to enforce vaccination mandates and the different information flows VCs might entail. Analysis of results collected as part of this study is used to derive general normative observations about different possible VC practices and to provide guidance for the possible deployments of VCs in different contexts.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533222
SP  - 1657
EP  - 1670
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533222
KW  - contextual integrity
KW  - information privacy
KW  - privacy norms
KW  - vaccination certificates
ER  - 

TY  - CONF
TI  - Human-algorithm collaboration: Achieving complementarity and avoiding unfairness
AU  - Donahue, Kate
AU  - Chouldechova, Alexandra
AU  - Kenthapadi, Krishnaram
T3  - FAccT '22
AB  - Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533221
SP  - 1639
EP  - 1656
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533221
ER  - 

TY  - CONF
TI  - Characterizing properties and trade-offs of centralized delegation mechanisms in liquid democracy
AU  - Brubach, Brian
AU  - Ballarin, Audrey
AU  - Nazeer, Heeba
T3  - FAccT '22
AB  - Liquid democracy is a form of transitive delegative democracy that has received a flurry of scholarly attention from the computer science community in recent years. In its simplest form, every agent starts with one vote and may have other votes assigned to them via delegation from other agents. They can choose to delegate all votes assigned to them to another agent or vote directly with all votes assigned to them. However, many proposed realizations of liquid democracy allow for agents to express their delegation/voting preferences in more complex ways (e.g., a ranked list of potential delegates) and employ a centralized delegation mechanism to compute the final vote tally. In doing so, centralized delegation mechanisms can make decisions that affect the outcome of a vote and where/whether agents are able to delegate their votes. Much of the analysis thus far has focused on the ability of these mechanisms to make a correct choice. We extend this analysis by introducing and formalizing other important properties of a centralized delegation mechanism in liquid democracy with respect to crucial features such as accountability, transparency, explainability, fairness, and user agency. In addition, we evaluate existing methods in terms of these properties, show how some prior work can be augmented to achieve desirable properties, prove impossibility results for achieving certain sets of properties simultaneously, and highlight directions for future work.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533219
SP  - 1629
EP  - 1638
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533219
KW  - transparency
KW  - accountability
KW  - fairness
KW  - computational social choice
KW  - liquid democracy
KW  - voting
ER  - 

TY  - CONF
TI  - “There is not enough information”: On the effects of explanations on perceptions of informational fairness and trustworthiness in automated decision-making
AU  - Schoeffer, Jakob
AU  - Kuehl, Niklas
AU  - Machowski, Yvette
T3  - FAccT '22
AB  - Automated decision systems (ADS) are increasingly used for consequential decision-making. These systems often rely on sophisticated yet opaque machine learning models, which do not allow for understanding how a given decision was arrived at. In this work, we conduct a human subject study to assess people’s perceptions of informational fairness (i.e., whether people think they are given adequate information on and explanation of the process and its outcomes) and trustworthiness of an underlying ADS when provided with varying types of information about the system. More specifically, we instantiate an ADS in the area of automated loan approval and generate different explanations that are commonly used in the literature. We randomize the amount of information that study participants get to see by providing certain groups of people with the same explanations as others plus additional explanations. From our quantitative analyses, we observe that different amounts of information as well as people’s (self-assessed) AI literacy significantly influence the perceived informational fairness, which, in turn, positively relates to perceived trustworthiness of the ADS. A comprehensive analysis of qualitative feedback sheds light on people’s desiderata for explanations, among which are (i) consistency (both with people’s expectations and across different explanations), (ii) disclosure of monotonic relationships between features and outcome, and (iii) actionability of recommendations.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533218
SP  - 1616
EP  - 1628
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533218
KW  - machine learning
KW  - explanations
KW  - Automated decision-making
KW  - trustworthiness
KW  - informational fairness
KW  - perceptions
ER  - 

TY  - CONF
TI  - Surfacing racial stereotypes through identity portrayal
AU  - Kambhatla, Gauri
AU  - Stewart, Ian
AU  - Mihalcea, Rada
T3  - FAccT '22
AB  - Content warning: this paper discusses and contains content that may be offensive or upsetting.People express racial stereotypes through conversations with others, increasingly in a digital format; as a result, the ability to computationally identify racial stereotypes could be beneficial to help mitigate some of the harmful effects of stereotyping. In this work, we seek to better understand how we can computationally surface racial stereotypes in text by identifying linguistic features associated with differences in racial identity portrayal, focused on two races (Black and White). We collect novel data of individuals’ self-presentation via crowdsourcing, where each crowdworker answers a set of prompts from their own perspective (real identity), and from the perspective of another racial identity (portrayed identity), keeping the gender constant. We use these responses as a dataset to identify stereotypes. Through a series of experiments based on classifications between real and portrayed identities, we show that generalizations and stereotypes appear to be more prevalent amongst white participants than black participants. Through analyses of predictive words and word usage patterns, we find that some of the most predictive features of an author portraying a different racial identity are known stereotypes, and reveal how people of different identities see themselves and others.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533217
SP  - 1604
EP  - 1615
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533217
KW  - datasets
KW  - racial bias
KW  - stereotypes
ER  - 

TY  - CONF
TI  - Assessing annotator identity sensitivity via item response theory: A case study in a hate speech corpus
AU  - Sachdeva, Pratik S.
AU  - Barreto, Renata
AU  - von Vacano, Claudia
AU  - Kennedy, Chris J.
T3  - FAccT '22
AB  - Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotator’s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrument’s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotator’s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533216
SP  - 1585
EP  - 1603
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533216
KW  - item response theory
KW  - hate speech
KW  - annotation
KW  - annotator sensitivity
KW  - differential rater functioning
ER  - 

TY  - CONF
TI  - Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem
AU  - Costanza-Chock, Sasha
AU  - Raji, Inioluwa Deborah
AU  - Buolamwini, Joy
T3  - FAccT '22
AB  - Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533213
SP  - 1571
EP  - 1583
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533213
KW  - algorithmic accountability
KW  - audit
KW  - AI policy
KW  - AI audit
KW  - AI bias
KW  - AI harm
KW  - algorithm audit
KW  - ethical AI
ER  - 

TY  - CONF
TI  - Algorithms Off-limits? If digital trade law restricts access to source code of software then accountability will suffer
AU  - Irion, Kristina
T3  - FAccT '22
AB  - Free trade agreements are increasingly used to construct an additional layer of protection for source code of software. This comes in the shape of a new prohibition for governments to require access to, or transfer of, source code of software, subject to certain exceptions. A clause on software source code is also part and parcel of an ambitious set of new rules on trade-related aspects of electronic commerce currently negotiated by 86 members of the World Trade Organization. Our understanding to date of how such a commitment inside trade law impacts on governments right to regulate digital technologies and the policy space that is allowed under trade law is limited. Access to software source code is for example necessary to meet regulatory and judicial needs in order to ensure that digital technologies are in conformity with individuals’ human rights and societal values. This article will unpack and analyze the implications of such a source code clause for current and future digital policies by governments that aim to ensure transparency, fairness and accountability of computer and machine learning algorithms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533212
SP  - 1561
EP  - 1570
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533212
KW  - Accountability
KW  - Transparency
KW  - Software
KW  - Fairness
KW  - Application Programming Interface
KW  - Computer algorithms
KW  - Digital policy
KW  - International trade law
KW  - Source code
ER  - 

TY  - CONF
TI  - ABCinML: Anticipatory bias correction in machine learning applications
AU  - Almuzaini, Abdulaziz A.
AU  - Bhatt, Chidansh A.
AU  - Pennock, David M.
AU  - Singh, Vivek K.
T3  - FAccT '22
AB  - The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533211
SP  - 1552
EP  - 1560
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533211
KW  - fairness
KW  - classification
KW  - algorithmic bias
ER  - 

TY  - CONF
TI  - On the power of randomization in fair classification and representation
AU  - Agarwal, Sushant
AU  - Deshpande, Amit
T3  - FAccT '22
AB  - Fair classification and fair representation learning are two important problems in supervised and unsupervised fair machine learning, respectively. Fair classification asks for a classifier that maximizes accuracy on a given data distribution subject to fairness constraints. Fair representation maps a given data distribution over the original feature space to a distribution over a new representation space such that all classifiers over the representation satisfy fairness. In this paper, we examine the power of randomization in both these problems to minimize the loss of accuracy that results when we impose fairness constraints. Previous work on fair classification has characterized the optimal fair classifiers on a given data distribution that maximize accuracy subject to fairness constraints, e.g., Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE). We refine these characterizations to demonstrate when the optimal randomized fair classifiers can surpass their deterministic counterparts in accuracy. We also show how the optimal randomized fair classifier that we characterize can be obtained as a solution to a convex optimization problem. Recent work has provided techniques to construct fair representations for a given data distribution such that any classifier over this representation satisfies DP. However, the classifiers on these fair representations either come with no or weak accuracy guarantees when compared to the optimal fair classifier on the original data distribution. Extending our ideas for randomized fair classification, we improve on these works, and construct DP-fair, EO-fair, and PE-fair representations that have provably optimal accuracy and suffer no accuracy loss compared to the optimal DP-fair, EO-fair, and PE-fair classifiers respectively on the original data distribution.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533209
SP  - 1542
EP  - 1551
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533209
KW  - machine learning
KW  - fairness
KW  - classification
KW  - randomization
KW  - representation
KW  - demographic parity
KW  - equal opportunity
ER  - 

TY  - CONF
TI  - Critical tools for machine learning: Working with intersectional critical concepts in machine learning systems design
AU  - Klumbytundefined, Goda
AU  - Draude, Claude
AU  - Taylor, Alex S.
T3  - FAccT '22
AB  - This paper investigates how intersectional critical theoretical concepts from social sciences and humanities research can be worked with in machine learning systems design. It does so by presenting a case study of a series of speculative design workshops, conducted in 2021. These workshops drew on intersectional feminist methodologies to construct interdisciplinary interventions in the design of machine learning systems, towards more inclusive, accountable, and contextualized systems design. The concepts of “situating/situated knowledges”, "figuration", "diffraction", and “critical fabulation/speculation” were taken up as theoretical and methodological tools for concept-led design workshops. This paper presents the design framework of the workshops and highlights tensions and possibilities with regards to interdisciplinary machine learning systems design towards more inclusive, contextualized, and accountable systems. It discusses the role that critical theoretical concepts can play in a design process and shows how such concepts can work as methodological tools that nonetheless require an open-ended experimental space to function. It presents insights and discussion points regarding what it means to work with critical intersectional knowledge that is inextricably connected to its historical and socio-political roots, and how this reframes what it might mean to design fair and accountable systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533207
SP  - 1528
EP  - 1541
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533207
KW  - Intersectionality
KW  - Experimental practice
KW  - Feminist epistemologies
KW  - Interdisciplinary methodologies
KW  - Machine learning systems design
ER  - 

TY  - CONF
TI  - Should attention be all we need? The epistemic and ethical implications of unification in machine learning
AU  - Fishman, Nic
AU  - Hancox-Li, Leif
T3  - FAccT '22
AB  - “Attention is all you need” has become a fundamental precept in machine learning research. Originally designed for machine translation, transformers and the attention mechanisms that underpin them now find success across many problem domains. With the apparent domain-agnostic success of transformers, many researchers are excited that similar model architectures can be successfully deployed across diverse applications in vision, language and beyond. We consider the benefits and risks of these waves of unification on both epistemic and ethical fronts. On the epistemic side, we argue that many of the arguments in favor of unification in the natural sciences fail to transfer over to the machine learning case, or transfer over only under assumptions that might not hold. Unification also introduces epistemic risks related to portability, path dependency, methodological diversity, and increased black-boxing. On the ethical side, we discuss risks emerging from epistemic concerns, further marginalizing underrepresented perspectives, the centralization of power, and having fewer models across more domains of application.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533206
SP  - 1516
EP  - 1527
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533206
KW  - machine learning
KW  - explanation
KW  - ethics
KW  - neural networks
KW  - philosophy
ER  - 

TY  - CONF
TI  - Justice in misinformation detection systems: An analysis of algorithms, stakeholders, and potential harms
AU  - Neumann, Terrence
AU  - De-Arteaga, Maria
AU  - Fazelpour, Sina
T3  - FAccT '22
AB  - Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533205
SP  - 1504
EP  - 1515
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533205
KW  - machine learning
KW  - justice
KW  - algorithmic fairness
KW  - informational justice
KW  - misinformation detection
ER  - 

TY  - CONF
TI  - Algorithmic fairness and vertical equity: Income fairness with IRS tax audit models
AU  - Black, Emily
AU  - Elzayn, Hadi
AU  - Chouldechova, Alexandra
AU  - Goldin, Jacob
AU  - Ho, Daniel
T3  - FAccT '22
AB  - This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533204
SP  - 1479
EP  - 1503
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533204
ER  - 

TY  - CONF
TI  - Adaptive sampling strategies to construct equitable training datasets
AU  - Cai, William
AU  - Encarnacion, Ro
AU  - Chern, Bobbie
AU  - Corbett-Davies, Sam
AU  - Bogen, Miranda
AU  - Bergman, Stevie
AU  - Goel, Sharad
T3  - FAccT '22
AB  - In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data—an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533203
SP  - 1467
EP  - 1478
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533203
KW  - machine learning
KW  - artificial intelligence
KW  - fairness
KW  - computer vision
KW  - Active learning
KW  - polygenic risk scores
KW  - representative data
ER  - 

TY  - CONF
TI  - How explainability contributes to trust in AI
AU  - Ferrario, Andrea
AU  - Loi, Michele
T3  - FAccT '22
AB  - We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as “explainability fosters trust in AI,” that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called “trust as anti-monitoring,” that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that “explainability fosters trust in AI” if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user’s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533202
SP  - 1457
EP  - 1466
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533202
KW  - trust
KW  - artificial intelligence
KW  - explainable artificial intelligence
KW  - healthcare
KW  - trustworthiness
KW  - ethics of artificial intelligence
ER  - 

TY  - CONF
TI  - Accountable data: The politics and pragmatics of disclosure datasets
AU  - Poirier, Lindsay
T3  - FAccT '22
AB  - This paper attends specifically to what I call “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit – what is ultimately being counted in the data – is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533201
SP  - 1446
EP  - 1456
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533201
KW  - accountability
KW  - data provenance
KW  - disclosure
KW  - infrastructure
ER  - 

TY  - CONF
TI  - At the tensions of south and north: Critical roles of global south stakeholders in AI governance
AU  - Png, Marie-Therese
T3  - FAccT '22
AB  - This paper aims to present a landscape of AI governance for and from the Global South, advanced by critical and decolonial-informed practitioners and scholars, and contrast this with the Inclusive AI Governance discourse led out of Global North institutions. By doing so, it identifies gaps in the dominant AI governance discourse, and bridges these gaps with relevant discourses of technology and power, localisation, and historical-geopolitical analyses of inequality led by Global South aligned actors. Specific areas of concern addressed by this paper include infrastructural and regulatory monopolies, harms associated with the labour and material supply chains of AI infrastructure, and commercial exploitation. By contrasting Global South and Global North discourses surrounding AI risks, this paper proposes a systemic restructuring of AI governance processes beyond current frameworks of Inclusive AI governance, offering three roles for Global South actors to substantively engage in AI governance processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533200
SP  - 1434
EP  - 1445
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533200
KW  - AI Policy
KW  - Coloniality of AI
KW  - Global South AI
KW  - Inclusive AI governance
ER  - 

TY  - CONF
TI  - Don’t throw it away! The utility of unlabeled data in fair decision making
AU  - Rateike, Miriam
AU  - Majumdar, Ayan
AU  - Mineeva, Olga
AU  - Gummadi, Krishna P.
AU  - Valera, Isabel
T3  - FAccT '22
AB  - Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533199
SP  - 1421
EP  - 1433
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533199
KW  - fairness
KW  - decision making
KW  - fair representation
KW  - label bias
KW  - selection bias
KW  - variational autoencoder
ER  - 

TY  - CONF
TI  - Decision time: Normative dimensions of algorithmic speed
AU  - Susser, Daniel
T3  - FAccT '22
AB  - Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533198
SP  - 1410
EP  - 1420
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533198
KW  - AI ethics
KW  - automation
KW  - time
KW  - data ethics
KW  - automated decision-making
KW  - speed
KW  - temporality
ER  - 

TY  - CONF
TI  - Towards fair unsupervised learning
AU  - Buet-Golfouse, Francois
AU  - Utyagulov, Islam
T3  - FAccT '22
AB  - Bias-mitigating techniques are now well established in the supervised learning literature and have shown their ability to tackle fairness-accuracy, as well as fairness-fairness trade-offs. These are usually predicated on different conceptions of fairness, such as demographic parity or equal odds that depend on the available labels in the dataset. However, it is often the case in practice that unsupervised learning is used as part of a machine learning pipeline (for instance, to perform dimensionality reduction or representation learning via SVD) or as a standalone model (for example, to derive a customer segmentation via k-means). It is thus crucial to develop approaches towards fair unsupervised learning. This work investigates fair unsupervised learning within the broad framework of generalised low-rank models (GLRM). Importantly, we introduce the concept of fairness functional that encompasses both traditional unsupervised learning techniques and min-max algorithms (whereby one minimises the maximum group loss). To do so, we design straightforward alternate convex search or biconvex gradient descent algorithms that also provide partial debiasing techniques. Finally, we show on benchmark datasets that our fair generalised low-rank models (“fGLRM”) perform well and help reduce disparity amongst groups while only incurring small runtime overheads.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533197
SP  - 1399
EP  - 1409
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533197
KW  - Fairness
KW  - Clustering
KW  - PCA
KW  - Unsupervised Learning
ER  - 

TY  - CONF
TI  - The alchemy of trust: The creative act of designing trustworthy socio-technical systems
AU  - Thornton, Lauren
AU  - Knowles, Bran
AU  - Blair, Gordon
T3  - FAccT '22
AB  - Trust is recognised as a significant and valuable component of socio-technical systems, facilitating numerous important benefits. Many trust models have been created throughout various streams of literature, describing trust for different stakeholders in different contexts. However, when designing a system with multiple stakeholders in their multiple contexts, how does one decide which trust model(s) to apply? And furthermore, how does one go from selecting a model or models to translating those into design? We review and analyse two prominent trust models, and apply them to the design of a trustworthy socio-technical system, namely virtual research environments. We show that a singular model cannot easily be imported and directly implemented into the design of such a system. We introduce the concept of alchemy as the most apt characterization of a successful design process, illustrating the need for designers to engage with the richness of the trust landscape and creatively experiment with components from multiple models to create the perfect blend for their context. We provide a demonstrative case study illustrating the process through which designers of socio-technical systems can become alchemists of trust.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533196
SP  - 1387
EP  - 1398
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533196
KW  - Trust
KW  - Socio-Technical Systems
KW  - Virtual Research Environments
KW  - Information Systems
KW  - System Design
KW  - Transdisciplinary Research
ER  - 

TY  - CONF
TI  - Confronting power and corporate capture at the FAccT conference
AU  - Young, Meg
AU  - Katell, Michael
AU  - Krafft, P.M.
T3  - FAccT '22
AB  - Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533194
SP  - 1375
EP  - 1386
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533194
KW  - agonism
KW  - conflict of interest
KW  - corporate capture
KW  - industry engagement
KW  - pymetrics
KW  - research funding
ER  - 

TY  - CONF
TI  - Who goes first? Influences of human-AI workflow on decision making in clinical imaging
AU  - Fogliato, Riccardo
AU  - Chappidi, Shreya
AU  - Lungren, Matthew
AU  - Fisher, Paul
AU  - Wilson, Diane
AU  - Fitzke, Michael
AU  - Parkinson, Mark
AU  - Horvitz, Eric
AU  - Inkpen, Kori
AU  - Nushi, Besmira
T3  - FAccT '22
AB  - Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients’ X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported that the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533193
SP  - 1362
EP  - 1374
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533193
KW  - decision making
KW  - anchoring bias
KW  - clinical imaging
KW  - human-AI collaboration
ER  - 

TY  - CONF
TI  - Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?
AU  - Schramowski, Patrick
AU  - Tauchmann, Christopher
AU  - Kersting, Kristian
T3  - FAccT '22
AB  - This paper contains images and descriptions that are offensive in nature.Large datasets underlying much of current machine learning raise serious issues concerning inappropriate content such as offensive, insulting, threatening, or might otherwise cause anxiety. This calls for increased dataset documentation, e.g., using datasheets. They, among other topics, encourage to reflect on the composition of the datasets. So far, this documentation, however, is done manually and therefore can be tedious and error-prone, especially for large image datasets. Here we ask the arguably “circular” question of whether a machine can help us reflect on inappropriate content, answering Question 16 in Datasheets. To this end, we propose to use the information stored in pre-trained transformer models to assist us in the documentation process. Specifically, prompt-tuning based on a dataset of socio-moral values steers CLIP to identify potentially inappropriate content, therefore reducing human labor. We then document the inappropriate images found using word clouds, based on captions generated using a vision-language model. The documentations of two popular, large-scale computer vision datasets—ImageNet and OpenImages—produced this way suggest that machines can indeed help dataset creators to answer Question 16 on inappropriate image content.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533192
SP  - 1350
EP  - 1361
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533192
KW  - Datasets
KW  - Dataset curation
KW  - Dataset documentation
KW  - Datasheets
ER  - 

TY  - CONF
TI  - The case for a legal compliance API for the enforcement of the EU’s digital services act on social media platforms
AU  - Goanta, Catalina
AU  - Bertaglia, Thales
AU  - Iamnitchi, Adriana
T3  - FAccT '22
AB  - In the course of under a year, the European Commission has launched some of the most important regulatory proposals to date on platform governance. The Commission’s goals behind cross-sectoral regulation of this sort include the protection of markets and democracies alike. While all these acts propose sophisticated rules for setting up new enforcement institutions and procedures, one aspect remains highly unclear: how digital enforcement will actually take place in practice. Focusing on the Digital Services Act (DSA), this discussion paper critically addresses issues around social media data access for the purpose of digital enforcement and proposes the use of a legal compliance application programming interface (API) as a means to facilitate compliance with the DSA and complementary European and national regulation. To contextualize this discussion, the paper pursues two scenarios that exemplify the harms arising out of content monetization affecting a particularly vulnerable category of social media users: children. The two scenarios are used to further reflect upon essential issues surrounding data access and legal compliance with the DSA and further applicable legal standards in the field of labour and consumer law.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533190
SP  - 1341
EP  - 1349
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533190
KW  - Digital Services Act
KW  - Legal Compliance API
KW  - monetization
KW  - social media platforms
ER  - 

TY  - CONF
TI  - Why am I not seeing it? Understanding users’ needs for counterfactual explanations in everyday recommendations
AU  - Shang, Ruoxi
AU  - Feng, K. J. Kevin
AU  - Shah, Chirag
T3  - FAccT '22
AB  - Intelligent everyday applications typically rely on automated Recommender Systems (RS) to generate recommendations that help users make decisions among a large number of options. Due to the increasing complexity of RS and the lack of transparency in its algorithmic decision-making, researchers have recognized the need to support users with explanations. While many traditional Explainable AI methods fall short in disclosing the internal intricacy of recommender systems, counterfactual explanations provide many desirable explainable features by offering human-like explanations that contrast an existing recommendation with alternatives. However, there is a lack of empirical research in understanding users’ needs of counterfactual explanations in their usage of everyday intelligent applications. In this paper, we investigate whether and when to provide counterfactual explanations to support people’s decision-making with everyday recommendations through a question-driven approach. We conducted a preliminary survey study and an interview study to understand how existing explanations might be insufficient to support users and elicit the triggers that prompt them to ask why not questions and seek additional explanations. The findings reveal that the utility of decision is a primary factor that may affect their counterfactual information needs. We then conducted an online scenario-based survey to quantify the correlation between utility and explanation needs and found significant correlations between the measured variables.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533189
SP  - 1330
EP  - 1340
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533189
KW  - Counterfactual explanations
KW  - Explainable recommender system
KW  - User studies
ER  - 

TY  - CONF
TI  - DualCF: Efficient model extraction attack from counterfactual explanations
AU  - Wang, Yongjie
AU  - Qian, Hangwei
AU  - Miao, Chunyan
T3  - FAccT '22
AB  - Cloud service providers have launched Machine-Learning-as-a-Service (MLaaS) platforms to allow users to access large-scale cloud-based models via APIs. In addition to prediction outputs, these APIs can also provide other information in a more human-understandable way, such as counterfactual explanations (CF). However, such extra information inevitably causes the cloud models to be more vulnerable to extraction attacks which aim to steal the internal functionality of models in the cloud. Due to the black-box nature of cloud models, however, a vast number of queries are inevitably required by existing attack strategies before the substitute model achieves high fidelity. In this paper, we propose a novel simple yet efficient querying strategy to greatly enhance the querying efficiency to steal a classification model. This is motivated by our observation that current querying strategies suffer from decision boundary shift issue induced by taking far-distant queries and close-to-boundary CFs into substitute model training. We then propose DualCF strategy to circumvent the above issues, which is achieved by taking not only CF but also counterfactual explanation of CF (CCF) as pairs of training samples for the substitute model. Extensive and comprehensive experimental evaluations are conducted on both synthetic and real-world datasets. The experimental results favorably illustrate that DualCF can produce a high-fidelity model with fewer queries efficiently and effectively.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533188
SP  - 1318
EP  - 1329
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533188
KW  - Counterfactual Explanations
KW  - Decision Boundary Shift
KW  - Model Extraction Attack
KW  - Model Security and Privacy
ER  - 

TY  - CONF
TI  - The algorithmic imprint
AU  - Ehsan, Upol
AU  - Singh, Ranjit
AU  - Metcalf, Jacob
AU  - Riedl, Mark
T3  - FAccT '22
AB  - When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533186
SP  - 1305
EP  - 1317
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533186
KW  - Algorithmic Impact Assessment
KW  - Algorithmic Imprint
KW  - Folk Theories of Algorithms
KW  - Global South
KW  - Infrastructure
KW  - Situated Fairness
KW  - User Perceptions
ER  - 

TY  - CONF
TI  - Evidence for hypodescent in visual semantic AI
AU  - Wolfe, Robert
AU  - Banaji, Mahzarin R.
AU  - Caliskan, Aylin
T3  - FAccT '22
AB  - We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p &lt; 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p &lt; 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p &lt; 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533185
SP  - 1293
EP  - 1304
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533185
KW  - racial bias
KW  - bias in AI
KW  - multimodal
KW  - visual semantics
KW  - hypodescent
KW  - language-image models
ER  - 

TY  - CONF
TI  - Gender and racial bias in visual question answering datasets
AU  - Hirota, Yusuke
AU  - Nakashima, Yuta
AU  - Garcia, Noa
T3  - FAccT '22
AB  - Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533184
SP  - 1280
EP  - 1292
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533184
KW  - datasets
KW  - gender stereotype
KW  - racial stereotype
KW  - visual question answering
ER  - 

TY  - CONF
TI  - Markedness in visual semantic AI
AU  - Wolfe, Robert
AU  - Caliskan, Aylin
T3  - FAccT '22
AB  - We evaluate the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ”a photo of a person” or to select a label denoting race or ethnicity, CLIP chooses the ”person” label 47.9% of the time for White individuals, compared with 5.0% or less for individuals who are Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ”person” label higher than labels denoting gender for Male individuals (26.7% of the time) vs. Female individuals (15.2% of the time). Age also affects whether an individual is marked by the model: Female individuals under the age of 20 are more likely than Male individuals to be marked with a gender label, but less likely to be marked with an age label, while Female individuals over the age of 40 are more likely to be marked based on age than Male individuals. We trace our results back to the CLIP embedding space by examining the self-similarity (mean pairwise cosine similarity) for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics (i.e., age, race, or gender) of the social group. The results indicate that, as age increases, the self-similarity of representations of Female individuals increases at a higher rate than for Male individuals, with the disparity most pronounced at the ”more than 70” age range. Six of the ten least self-similar social groups are individuals who are White and Male, while all ten of the most self-similar social groups are individuals under the age of 10 or over the age of 70, and six of the ten are Female individuals. Our results yield evidence that bias in CLIP is intersectional: existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are individuals who are White and Male and individuals who are Black and Female. CLIP is an English-language model trained on internet content gathered based on a query list generated from an American website (Wikipedia), and results indicate that CLIP reflects the biases of the language and society which produced this training data.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533183
SP  - 1269
EP  - 1279
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533183
KW  - age bias
KW  - bias in AI
KW  - language-and-vision AI
KW  - markedness
KW  - multimodal
KW  - visual semantics
ER  - 

TY  - CONF
TI  - Designing for responsible trust in AI systems: A communication perspective
AU  - Liao, Q.Vera
AU  - Sundar, S. Shyam
T3  - FAccT '22
AB  - Current literature and public discourse on “trust in AI” are often focused on the principles underlying trustworthy AI, with insufficient attention paid to how people develop trust. Given that AI systems differ in their level of trustworthiness, two open questions come to the fore: how should AI trustworthiness be responsibly communicated to ensure appropriate and equitable trust judgments by different users, and how can we protect users from deceptive attempts to earn their trust? We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments. Besides AI-generated content, we highlight transparency and interaction as AI systems’ affordances that present a wide range of trustworthiness cues to users. By bringing to light the variety of users’ cognitive processes to make trust judgments and their potential limitations, we urge technology creators to make conscious decisions in choosing reliable trustworthiness cues for target users and, as an industry, to regulate this space and prevent malicious use. Towards these goals, we define the concepts of warranted trustworthiness cues and expensive trustworthiness cues, and propose a checklist of requirements to help technology creators identify appropriate cues to use. We present a hypothetical use case to illustrate how practitioners can use MATCH to design AI systems responsibly, and discuss future directions for research and industry efforts aimed at promoting responsible trust in AI.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533182
SP  - 1257
EP  - 1268
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533182
KW  - human-AI interaction
KW  - human-centered AI
KW  - AI design
KW  - Trust in AI
ER  - 

TY  - CONF
TI  - Learning resource allocation policies from observational data with an application to homeless services delivery
AU  - Rahmattalabi, Aida
AU  - Vayanos, Phebe
AU  - Dullerud, Kathryn
AU  - Rice, Eric
T3  - FAccT '22
AB  - We study the problem of learning, from observational data, fair and interpretable policies that effectively match heterogeneous individuals to scarce resources of different types. We model this problem as a multi-class multi-server queuing system where both individuals and resources arrive stochastically over time. Each individual, upon arrival, is assigned to a queue where they wait to be matched to a resource. The resources are assigned in a first come first served (FCFS) fashion according to an eligibility structure that encodes the resource types that serve each queue. We propose a methodology based on techniques in modern causal inference to construct the individual queues as well as learn the matching outcomes and provide a mixed-integer optimization (MIO) formulation to optimize the eligibility structure. The MIO problem maximizes policy outcome subject to wait time and fairness constraints. It is very flexible, allowing for additional linear domain constraints. We conduct extensive analyses using synthetic and real-world data. In particular, we evaluate our framework using data from the U.S. Homeless Management Information System (HMIS). We obtain wait times as low as an FCFS policy while improving the rate of exit from homelessness for underserved or vulnerable groups (7% higher for the Black individuals and 15% higher for those below 17 years old) and overall.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533181
SP  - 1240
EP  - 1256
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533181
KW  - Causal Inference
KW  - Fairness in AI
KW  - Mixed-integer Optimization
KW  - Observational Data
ER  - 

TY  - CONF
TI  - Multiaccurate proxies for downstream fairness
AU  - Diana, Emily
AU  - Gill, Wesley
AU  - Kearns, Michael
AU  - Kenthapadi, Krishnaram
AU  - Roth, Aaron
AU  - Sharifi-Malvajerdi, Saeed
T3  - FAccT '22
AB  - We study the problem of training a model that must obey demographic fairness conditions when the sensitive features are not available at training time — in other words, how can we train a model to be fair by race when we don’t have data about race? We adopt a fairness pipeline perspective, in which an “upstream” learner that does have access to the sensitive features will learn a proxy model for these features from the other attributes. The goal of the proxy is to allow a general “downstream” learner — with minimal assumptions on their prediction task — to be able to use the proxy to train a model that is fair with respect to the true sensitive features. We show that obeying multiaccuracy constraints with respect to the downstream model class suffices for this purpose, provide sample- and oracle efficient-algorithms and generalization bounds for learning such proxies, and conduct an experimental evaluation. In general, multiaccuracy is much easier to satisfy than classification accuracy, and can be satisfied even when the sensitive features are hard to predict.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533180
SP  - 1207
EP  - 1239
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533180
KW  - game theory
KW  - algorithmic fairness
KW  - multiaccuracy
KW  - proxy variables
ER  - 

TY  - CONF
TI  - The road to explainability is paved with bias: Measuring the fairness of explanations
AU  - Balagopalan, Aparna
AU  - Zhang, Haoran
AU  - Hamidieh, Kimia
AU  - Hartvigsen, Thomas
AU  - Rudzicz, Frank
AU  - Ghassemi, Marzyeh
T3  - FAccT '22
AB  - Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533179
SP  - 1194
EP  - 1206
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533179
KW  - machine learning
KW  - explainability
KW  - fairness
ER  - 

TY  - CONF
TI  - Multi stage screening: Enforcing fairness and maximizing efficiency in a pre-existing pipeline
AU  - Blum, Avrim
AU  - Stangl, Kevin
AU  - Vakilian, Ali
T3  - FAccT '22
AB  - Consider an actor making selection decisions (e.g., hiring) using a series of classifiers, which we term a sequential screening process. The early stages (e.g. resume screen, coding screen, phone interview) filter out some of the applicants, and in the final stage an expensive but accurate test (e.g. a full interview) is applied to those individuals that make it to the final stage. Since the final stage is expensive, if there are multiple groups with different fractions of positives in them at the penultimate stage (even if a slight gap), then the firm may naturally only choose to apply the final (interview) stage solely to the highest precision group which would be clearly unfair to the other groups. Even if the firm is required to interview all those who pass to the final round, the tests themselves could have the property that qualified individuals from some groups pass more easily than qualified individuals from others. Accordingly, we consider requiring Equality of Opportunity (qualified members of each group have the same chance of reaching the final stage and being interviewed). We then examine the goal of maximizing quantities of interest to the decision maker subject to this constraint, via modification of the probabilities of promotion through the screening process at each stage based on performance at the previous stage. We exhibit algorithms for satisfying Equal Opportunity over the selection process and maximizing precision (the fraction of interviews that yield qualified candidates) as well as linear combinations of precision and recall (recall determines the number of applicants needed per hire) at the end of the final stage. We also present examples showing that the solution space is non-convex, which motivate our combinatorial exact and (FPTAS) approximation algorithms for maximizing the linear combination of precision and recall. Finally, we discuss the ‘price of’ adding additional restrictions, such as not allowing the decision-maker to use group membership in its decision process.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533178
SP  - 1178
EP  - 1193
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533178
ER  - 

TY  - CONF
TI  - Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders
AU  - Stapleton, Logan
AU  - Lee, Min Hun
AU  - Qing, Diana
AU  - Wright, Marya
AU  - Chouldechova, Alexandra
AU  - Holstein, Ken
AU  - Wu, Zhiwei Steven
AU  - Zhu, Haiyi
T3  - FAccT '22
AB  - Child welfare agencies across the United States are turning to data-driven predictive technologies (commonly called predictive analytics) which use government administrative data to assist workers’ decision-making. While some prior work has explored impacted stakeholders’ concerns with current uses of data-driven predictive risk models (PRMs), less work has asked stakeholders whether such tools ought to be used in the first place. In this work, we conducted a set of seven design workshops with 35 stakeholders who have been impacted by the child welfare system or who work in it to understand their beliefs and concerns around PRMs, and to engage them in imagining new uses of data and technologies in the child welfare system. We found that participants worried current PRMs perpetuate or exacerbate existing problems in child welfare. Participants suggested new ways to use data and data-driven tools to better support impacted communities and suggested paths to mitigate possible harms of these tools. Participants also suggested low-tech or no-tech alternatives to PRMs to address problems in child welfare. Our study sheds light on how researchers and designers can work in solidarity with impacted communities, possibly to circumvent or oppose child welfare agencies.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533177
SP  - 1162
EP  - 1177
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533177
KW  - machine learning
KW  - human-centered AI
KW  - child welfare
KW  - impacted stakeholder
KW  - participatory design
ER  - 

TY  - CONF
TI  - Brain computer interfaces and human rights: Brave new rights for a brave new world
AU  - Botes, Marietjie Wilhelmina Maria
T3  - FAccT '22
AB  - Digital health applications include a wide range of wearable, implantable, injectable and ingestible digital medical devices. Many of these devices use machine learning algorithms to assist medical prognosis and decision-making. One of the most compelling digital medical device developments is brain-computer interfaces (BCIs) which entails the connecting of a person's brain to a computer, or to another device outside the human body. BCIs allow bidirectional communication and control between the human brain and the outside world by exporting brain data or altering brain activity. Although being marveled at for its clinical promises, this technological advancement also raises novel ethical, legal, social and technical implications (ELSTI). Debates in this regard centers around patient autonomy, equity, trustworthiness in healthcare, data protection and security, risks of dehumanization, the limitations of machine learning-based decision-making, and the influence that BCIs have on what it means to be human and human rights. Since the adoption of the Universal Declaration of Human Rights (UDHR) after World War II, the landscape that give rise to these human rights has evolved enormously. Human life and humans’ role in society are being transformed and threatened by technologies that were never imagined at the time the UDHR was adopted. BCIs, in particular, harbor the greatest possibility of social and individual disruption through its capability to record, interpret, manipulate, or alter brain activity that may potentially alter what it means to be human and how we control humans in future. Cutting edge technological innovations that increasingly blur the lines between human and computer beg the rethinking and extension of existing human rights to remain relevant in a digitized world. In this paper sui generis human rights such as mental privacy, the right to identity or self, agency or free will and fair access to cognitive augmentation will be discussed and how a regulatory framework must be adapted to act as technology enablers, whilst ensuring fairness, accountability, and transparency in sociotechnical systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533176
SP  - 1154
EP  - 1161
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533176
KW  - autonomy
KW  - identity
KW  - human rights
KW  - brain computer interfaces
KW  - neurological privacy
ER  - 

TY  - CONF
TI  - Smallset timelines: A visual representation of data preprocessing decisions
AU  - Lucchesi, Lydia R.
AU  - Kuhnert, Petra M.
AU  - Davis, Jenny L.
AU  - Xie, Lexing
T3  - FAccT '22
AB  - Data preprocessing is a crucial stage in the data analysis pipeline, with both technical and social aspects to consider. Yet, the attention it receives is often lacking in research practice and dissemination. We present the Smallset Timeline, a visualisation to help reflect on and communicate data preprocessing decisions. A “Smallset” is a small selection of rows from the original dataset containing instances of dataset alterations. The Timeline is comprised of Smallset snapshots representing different points in the preprocessing stage and captions to describe the alterations visualised at each point. Edits, additions, and deletions to the dataset are highlighted with colour. We develop the R software package, smallsets, that can create Smallset Timelines from R and Python data preprocessing scripts. Constructing the figure asks practitioners to reflect on and revise decisions as necessary, while sharing it aims to make the process accessible to a diverse range of audiences. We present two case studies to illustrate use of the Smallset Timeline for visualising preprocessing decisions. Case studies include software defect data and income survey benchmark data, in which preprocessing affects levels of data loss and group fairness in prediction tasks, respectively. We envision Smallset Timelines as a go-to data provenance tool, enabling better documentation and communication of preprocessing tasks at large.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533175
SP  - 1136
EP  - 1153
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533175
KW  - communication
KW  - visualization
KW  - reflexivity
KW  - data preprocessing
KW  - open-source software
ER  - 

TY  - CONF
TI  - Auditing for gerrymandering by identifying disenfranchised individuals
AU  - Lin, Jerry
AU  - Chen, Carolyn
AU  - Chmielewski, Marc
AU  - Zaman, Samia
AU  - Fain, Brandon
T3  - FAccT '22
AB  - Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533174
SP  - 1125
EP  - 1135
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533174
KW  - fairness
KW  - audit
KW  - gerrymandering
KW  - simulated annealing
ER  - 

TY  - CONF
TI  - An algorithmic framework for bias bounties
AU  - Globus-Harris, Ira
AU  - Kearns, Michael
AU  - Roth, Aaron
T3  - FAccT '22
AB  - We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533172
SP  - 1106
EP  - 1124
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533172
KW  - bias bounty
KW  - multigroup fairness
KW  - subgroup fairness
ER  - 

TY  - CONF
TI  - Trade-offs between group fairness metrics in societal resource allocation
AU  - Mashiat, Tasfia
AU  - Gitiaux, Xavier
AU  - Rangwala, Huzefa
AU  - Fowler, Patrick
AU  - Das, Sanmay
T3  - FAccT '22
AB  - We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533171
SP  - 1095
EP  - 1105
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533171
KW  - algorithmic fairness
KW  - Resource allocation
KW  - fairness metrics
ER  - 

TY  - CONF
TI  - Rational shapley values
AU  - Watson, David
T3  - FAccT '22
AB  - Explaining the predictions of opaque machine learning algorithms is an important and challenging task, especially as complex models are increasingly used to assist in high-stakes decisions such as those arising in healthcare and finance. Most popular tools for post-hoc explainable artificial intelligence (XAI) are either insensitive to context (e.g., feature attributions) or difficult to summarize (e.g., counterfactuals). In this paper, I introduce rational Shapley values, a novel XAI method that synthesizes and extends these seemingly incompatible approaches in a rigorous, flexible manner. I leverage tools from decision theory and causal modeling to formalize and implement a pragmatic approach that resolves a number of known challenges in XAI. By pairing the distribution of random variables with the appropriate reference class for a given explanation task, I illustrate through theory and experiments how user goals and knowledge can inform and constrain the solution set in an iterative fashion. The method compares favorably to state of the art XAI tools in a range of quantitative and qualitative comparisons.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533170
SP  - 1083
EP  - 1094
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533170
KW  - Explainable artificial intelligence
KW  - Interpretable machine learning
KW  - Counterfactuals
KW  - Decision theory
KW  - Shapley values
ER  - 

TY  - CONF
TI  - Tackling algorithmic disability discrimination in the hiring process: An ethical, legal and technical analysis
AU  - Buyl, Maarten
AU  - Cociancig, Christina
AU  - Frattone, Cristina
AU  - Roekens, Nele
T3  - FAccT '22
AB  - Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533169
SP  - 1071
EP  - 1082
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533169
KW  - Artificial Intelligence Act
KW  - social justice
KW  - algorithmic discrimination
KW  - automated hiring systems
KW  - data protection law
KW  - equality law
KW  - ethics of discrimination
KW  - persons with disabilities
KW  - reasonable accommodation
ER  - 

TY  - CONF
TI  - Counterfactual shapley additive explanations
AU  - Albini, Emanuele
AU  - Long, Jason
AU  - Dervovic, Danial
AU  - Magazzeni, Daniele
T3  - FAccT '22
AB  - Feature attributions are a common paradigm for model explanations due to their simplicity in assigning a single numeric score for each input feature to a model. In the actionable recourse setting, wherein the goal of the explanations is to improve outcomes for model consumers, it is often unclear how feature attributions should be correctly used. With this work, we aim to strengthen and clarify the link between actionable recourse and feature attributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP (CF-SHAP), that incorporates counterfactual information to produce a background dataset for use within the marginal (a.k.a. interventional) Shapley value framework. We motivate the need within the actionable recourse setting for careful consideration of background datasets when using Shapley values for feature attributions with numerous synthetic examples. Moreover, we demonstrate the efficacy of CF-SHAP by proposing and justifying a quantitative score for feature attributions, counterfactual-ability, showing that as measured by this metric, CF-SHAP is superior to existing methods when evaluated on public datasets using tree ensembles.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533168
SP  - 1054
EP  - 1070
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533168
KW  - XAI
KW  - explainability
KW  - SHAP
KW  - counterfactual explanations
KW  - feature importance
KW  - actionable recourse
KW  - feature attributions
KW  - Shapley values
ER  - 

TY  - CONF
TI  - FADE: FAir double ensemble learning for observable and counterfactual outcomes
AU  - Mishler, Alan
AU  - Kennedy, Edward H.
T3  - FAccT '22
AB  - Methods for building fair predictors often involve tradeoffs between fairness and accuracy and between different fairness criteria. Recent work seeks to characterize these tradeoffs in specific problem settings, but these methods often do not accommodate users who wish to improve the fairness of an existing benchmark model without sacrificing accuracy, or vice versa. These results are also typically restricted to observable accuracy and fairness criteria. We develop a flexible framework for fair ensemble learning that allows users to efficiently explore the fairness-accuracy space or to improve the fairness or accuracy of a benchmark model. Our framework can simultaneously target multiple observable or counterfactual fairness criteria, and it enables users to combine a large number of previously trained and newly trained predictors. We provide theoretical guarantees that our estimators converge at fast rates. We apply our method on both simulated and real data, with respect to both observable and counterfactual accuracy and fairness criteria. We show that, surprisingly, multiple unfairness measures can sometimes be minimized simultaneously with little impact on accuracy, relative to unconstrained predictors or existing benchmark models.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533167
SP  - 1053
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533167
KW  - fairness
KW  - counterfactual
KW  - ensemble learning
KW  - semiparametric
ER  - 

TY  - CONF
TI  - Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare
AU  - Pfohl, Stephen
AU  - Xu, Yizhe
AU  - Foryciarz, Agata
AU  - Ignatiadis, Nikolaos
AU  - Genkins, Julian
AU  - Shah, Nigam
T3  - FAccT '22
AB  - A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533166
SP  - 1039
EP  - 1052
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533166
KW  - fairness
KW  - healthcare
KW  - cardiovascular disease
ER  - 

TY  - CONF
TI  - A data-driven simulation of the new york state foster care system
AU  - Du, Yuhao
AU  - Ionescu, Stefania
AU  - Sage, Melanie
AU  - Joseph, Kenneth
T3  - FAccT '22
AB  - We introduce an analytic pipeline to model and simulate youth trajectories through the New York state foster care system. Our goal in doing so is to forecast how proposed interventions may impact the foster care system’s ability to achieve it’s stated goals before these interventions are actually implemented and impact the lives of thousands of youth. Here, we focus on two specific stated goals of the system: racial equity, and, as codified most recently by the 2018 Family First Prevention Services Act (FFPSA), a focus on keeping all youth out of foster care. We also focus on one specific potential intervention— a predictive model, proposed in prior work and implemented elsewhere in the U.S., which aims to determine whether or not a youth is in need of care. We use our method to explore how the implementation of this predictive model in New York would impact racial equity and the number of youth in care. While our findings, as in any simulation model, ultimately rely on modeling assumptions, we find evidence that the model would not necessarily achieve either goal. Primarily, then, we aim to further promote the use of data-driven simulation to help understand the ramifications of algorithmic interventions in public systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533165
SP  - 1028
EP  - 1038
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533165
KW  - Simulation
KW  - Child Welfare
KW  - Racial Equity
KW  - Social Policy
ER  - 

TY  - CONF
TI  - Regulating facial processing technologies: Tensions between legal and technical considerations in the application of illinois BIPA
AU  - Yew, Rui-Jie
AU  - Xiang, Alice
T3  - FAccT '22
AB  - Harms resulting from the development and deployment of facial processing technologies (FPT) have been met with increasing controversy. Several states and cities in the U.S. have banned the use of facial recognition by law enforcement and governments, but FPT are still being developed and used in a wide variety of contexts where they primarily are regulated by state biometric information privacy laws. Among these laws, the 2008 Illinois Biometric Information Privacy Act (BIPA) has generated a significant amount of litigation. Yet, with most BIPA lawsuits reaching settlements before there have been meaningful clarifications of relevant technical intricacies and legal definitions, there remains a great degree of uncertainty as to how exactly this law applies to FPT. What we have found through applications of BIPA in FPT litigation so far, however, points to potential disconnects between technical and legal communities. This paper analyzes what we know based on BIPA court proceedings and highlights these points of tension: areas where the technical operationalization of BIPA may create unintended and undesirable incentives for FPT development, as well as areas where BIPA litigation can bring to light the limitations of solely technical methods in achieving legal privacy values. These factors are relevant for (i) reasoning about biometric information privacy laws as a governing mechanism for FPT, (ii) assessing the potential harms of FPT, and (iii) providing incentives for the mitigation of these harms. By illuminating these considerations, we hope to empower courts and lawmakers to take a more nuanced approach to regulating FPT and developers to better understand privacy values in the current U.S. legal landscape.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533163
SP  - 1017
EP  - 1027
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533163
KW  - biometric privacy
KW  - facial recognition
KW  - privacy policy
ER  - 

TY  - CONF
TI  - Models for classifying AI systems: the switch, the ladder, and the matrix
AU  - Mökander, Jakob
AU  - Sheth, Margi
AU  - Watson, David
AU  - Floridi, Luciano
T3  - FAccT '22
AB  - Organisations that design and deploy systems based on artificial intelligence (AI) increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. A major obstacle to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the practical purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533162
SP  - 1016
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533162
KW  - Artificial intelligence
KW  - Ethics
KW  - Governance
KW  - Material scope
ER  - 

TY  - CONF
TI  - Subverting machines, fluctuating identities: Re-learning human categorization
AU  - Lu, Christina
AU  - Kay, Jackie
AU  - McKee, Kevin
T3  - FAccT '22
AB  - Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533161
SP  - 1005
EP  - 1015
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533161
KW  - algorithmic fairness
KW  - identity systems
KW  - social construction
KW  - theories of identity
ER  - 

TY  - CONF
TI  - Beyond fairness: Reparative algorithms to address historical injustices of housing discrimination in the US
AU  - So, Wonyoung
AU  - Lohia, Pranay
AU  - Pimplikar, Rakesh
AU  - Hosoi, A.E.
AU  - D'Ignazio, Catherine
T3  - FAccT '22
AB  - Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533160
SP  - 988
EP  - 1004
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533160
KW  - fairness
KW  - housing
KW  - mortgage lending
KW  - racial wealth gap
KW  - reparations
ER  - 

TY  - CONF
TI  - Female, white, 27? Bias evaluation on data and algorithms for affect recognition in faces
AU  - Pahl, Jaspar
AU  - Rieger, Ines
AU  - Möller, Anna
AU  - Wittenberg, Thomas
AU  - Schmid, Ute
T3  - FAccT '22
AB  - Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533159
SP  - 973
EP  - 987
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533159
KW  - fairness
KW  - bias
KW  - affective computing
KW  - action units
KW  - algorithm evaluation
KW  - categorical emotions
KW  - data evaluation
KW  - metadata post-annotation
ER  - 

TY  - CONF
TI  - The fallacy of AI functionality
AU  - Raji, Inioluwa Deborah
AU  - Kumar, I. Elizabeth
AU  - Horowitz, Aaron
AU  - Selbst, Andrew
T3  - FAccT '22
AB  - Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, despite this reality, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on “ethical” or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we analyze a set of case studies to create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy challenge, operating as a necessary first step towards protecting affected communities from algorithmic harm.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533158
SP  - 959
EP  - 972
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533158
ER  - 

TY  - CONF
TI  - The forgotten margins of AI ethics
AU  - Birhane, Abeba
AU  - Ruane, Elayne
AU  - Laurent, Thomas
AU  - S. Brown, Matthew
AU  - Flowers, Johnathan
AU  - Ventresque, Anthony
AU  - L. Dancy, Christopher
T3  - FAccT '22
AB  - How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533157
SP  - 948
EP  - 958
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533157
L1  - https://dl.acm.org/doi/pdf/10.1145/3531146.3533157
KW  - AI Ethics
KW  - Justice
KW  - FAccT
KW  - AIES
KW  - Trends
ER  - 

TY  - CONF
TI  - German AI start-ups and “AI ethics”: Using a social practice lens for assessing and implementing socio-technical innovation
AU  - Sloane, Mona
AU  - Zakrzewski, Janina
T3  - FAccT '22
AB  - The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533156
SP  - 935
EP  - 947
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533156
KW  - transparency
KW  - accountability
KW  - fairness
KW  - AI ethics
KW  - regulation
KW  - innovation
KW  - organizations
KW  - social practice
KW  - socio-cultural history
KW  - start-ups
ER  - 

TY  - CONF
TI  - Prediction as extraction of discretion
AU  - Hong, Sun-ha
T3  - FAccT '22
AB  - I argue that data-driven predictions work primarily as instruments for systematic extraction of discretionary power – the practical capacity to make everyday decisions and define one's situation. This extractive relation reprises a long historical pattern, in which new methods of producing knowledge generate a redistribution of epistemic power: who declares what kind of truth about me, to count for what kinds of decisions? I argue that prediction as extraction of discretion is normal and fundamental to the technology, rather than isolated cases of bias or error. Synthesising critical observations across anthropology, history of technology and critical data studies, the paper demonstrates this dynamic in two contemporary domains: (1) crime and policing demonstrates how predictive systems are extractive by design. Rather than neutral models led astray by garbage data, pre-existing interests thoroughly shape how prediction conceives of its object, its measures, and most importantly, what it does not measure and in doing so devalues. (2) I then examine the prediction of productivity in the long tradition of extracting discretion as a means to extract labour power. Making human behaviour more predictable for the client of prediction (the manager, the corporation, the police officer) often means making life and work more unpredictable for the target of prediction (the employee, the applicant, the citizen).
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533155
SP  - 925
EP  - 934
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533155
KW  - prediction
KW  - power
KW  - discretion
KW  - labor
KW  - policing
ER  - 

TY  - CONF
TI  - Multi-disciplinary fairness considerations in machine learning for clinical trials
AU  - Chien, Isabel
AU  - Deliu, Nina
AU  - Turner, Richard
AU  - Weller, Adrian
AU  - Villar, Sofia
AU  - Kilbertus, Niki
T3  - FAccT '22
AB  - While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533154
SP  - 906
EP  - 924
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533154
KW  - adaptive clinical trials
KW  - clinical trials
KW  - health informatics
KW  - machine learning for healthcare
ER  - 

TY  - CONF
TI  - Post-hoc explanations fail to achieve their purpose in adversarial contexts
AU  - Bordt, Sebastian
AU  - Finck, Michèle
AU  - Raidl, Eric
AU  - von Luxburg, Ulrike
T3  - FAccT '22
AB  - Existing and planned legislation stipulates various obligations to provide information about machine learning algorithms and their functioning, often interpreted as obligations to “explain”. Many researchers suggest using post-hoc explanation algorithms for this purpose. In this paper, we combine legal, philosophical and technical arguments to show that post-hoc explanation algorithms are unsuitable to achieve the law’s objectives. Indeed, most situations where explanations are requested are adversarial, meaning that the explanation provider and receiver have opposing interests and incentives, so that the provider might manipulate the explanation for her own ends. We show that this fundamental conflict cannot be resolved because of the high degree of ambiguity of post-hoc explanations in realistic application scenarios. As a consequence, post-hoc explanation algorithms are unsuitable to achieve the transparency objectives inherent to the legal norms. Instead, there is a need to more explicitly discuss the objectives underlying “explainability” obligations as these can often be better achieved through other mechanisms. There is an urgent need for a more open and honest discussion regarding the potential and limitations of post-hoc explanations in adversarial contexts, in particular in light of the current negotiations of the European Union’s draft Artificial Intelligence Act.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533153
SP  - 891
EP  - 905
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533153
KW  - Explainability
KW  - Transparency
KW  - LIME
KW  - SHAP
KW  - GDPR
KW  - Artificial Intelligence Act
KW  - Regulation
KW  - Counterfactual Explanations
ER  - 

TY  - CONF
TI  - On the fairness of machine-assisted human decisions
AU  - McLaughlin, Bryce
AU  - Spiess, Jann
AU  - Gillis, Talia
T3  - FAccT '22
AB  - When machine-learning algorithms are deployed in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider how properties of machine predictions affect the resulting human decisions. We show in a formal model that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. While our concrete results rely on specific assumptions about the data, algorithm, and decision-maker, they show more broadly that any study of critical properties of complex decision systems, such as the fairness of machine-assisted human decisions, should go beyond focusing on the underlying algorithmic predictions in isolation.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533152
SP  - 890
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533152
KW  - Fairness
KW  - Machine Learning
KW  - Decision Support Systems
KW  - Human Computer Interaction
KW  - Protected Classes
ER  - 

TY  - CONF
TI  - Promoting ethical awareness in communication analysis: Investigating potentials and limits of visual analytics for intelligence applications
AU  - Fischer, Maximilian T.
AU  - Hirsbrunner, Simon David
AU  - Jentner, Wolfgang
AU  - Miller, Matthias
AU  - Keim, Daniel A.
AU  - Helm, Paula
T3  - FAccT '22
AB  - Digital systems for analyzing human communication data have become prevalent in recent years. This may be related to the increasing abundance of data that can be harnessed but can hardly be managed manually. Intelligence analysis of communications data in investigative journalism, criminal intelligence, and law present particularly interesting cases, as they must take into account the often highly sensitive properties of the underlying operations and data. At the same time, these are areas where increasingly automated, sophisticated approaches and tailored systems can be particularly useful and relevant, especially in terms of Big Data manageability. However, by the shifting of responsibilities, this also poses dangers. In addition to privacy concerns, these dangers relate to uncertain or poor data quality, leading to discrimination and potentially misleading insights. Other problems relate to a lack of transparency and traceability, making it difficult to accurately identify problems and determine appropriate remedial strategies. Visual analytics combines machine learning methods with interactive visual interfaces to enable human sense- and decision-making. This technique can be key for designing and operating meaningful interactive communication analysis systems that consider these ethical challenges. In this interdisciplinary work, a joint endeavor of computer scientists, ethicists, and scholars in Science &amp; Technology Studies, we investigate and evaluate opportunities and risks involved in using Visual analytics approaches for communication analysis in intelligence applications in particular. We introduce, at first, the common technological systems used in communication analysis, with a special focus on intelligence analysis in criminal investigations, further discussing the domain-specific ethical implications, tensions, and risks involved. We then make the case of how tailored Visual Analytics approaches may reduce and mitigate the described problems, both theoretically and through practical examples. Offering interactive analysis capabilities and what-if explorations while facilitating guidance, provenance generation, and bias awareness (through nudges, for example) can improve analysts’ understanding of their data, increasing trustworthiness, accountability, and generating knowledge. We show that finding Visual Analytics design solutions for ethical issues is not a mere optimization task with an ideal final solution. Design solutions for specific ethical problems (e.g., privacy) often trigger new ethical issues (e.g., accountability) in other areas. Balancing out and negotiating these trade-offs has, as we argue, to be an integral aspect of the system design process from the outset. Finally, our work identifies existing gaps and highlights research opportunities, further describing how our results can be transferred to other domains. With this contribution, we aim at informing more ethically-aware approaches to communication analysis in intelligence operations.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533151
SP  - 877
EP  - 889
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533151
KW  - Machine Learning
KW  - Interdisciplinary Research
KW  - Communication Analysis
KW  - Critical Algorithm Studies
KW  - Critical Data Studies
KW  - Ethic Awareness
KW  - Intelligence Analysis
KW  - Technology Studies
KW  - Visual Analytics
KW  - Science &amp
ER  - 

TY  - CONF
TI  - Accountability in an algorithmic society: Relationality, responsibility, and robustness in machine learning
AU  - Cooper, A. Feder
AU  - Moss, Emanuel
AU  - Laufer, Benjamin
AU  - Nissenbaum, Helen
T3  - FAccT '22
AB  - In 1996, Accountability in a Computerized Society&nbsp;[95] issued a clarion call concerning the erosion of accountability in society due to the ubiquitous delegation of consequential functions to computerized systems. Nissenbaum [95] described four barriers to accountability that computerization presented, which we revisit in relation to the ascendance of data-driven algorithmic systems—i.e., machine learning or artificial intelligence—to uncover new challenges for accountability that these systems present. Nissenbaum’s original paper grounded discussion of the barriers in moral philosophy; we bring this analysis together with recent scholarship on relational accountability frameworks and discuss how the barriers present difficulties for instantiating a unified moral, relational framework in practice for data-driven algorithmic systems. We conclude by discussing ways of weakening the barriers in order to do so.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533150
SP  - 864
EP  - 876
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533150
KW  - accountability
KW  - data-driven algorithmic systems
KW  - moral philosophy
KW  - relationality
KW  - robustness
ER  - 

TY  - CONF
TI  - Model multiplicity: Opportunities, concerns, and solutions
AU  - Black, Emily
AU  - Raghavan, Manish
AU  - Barocas, Solon
T3  - FAccT '22
AB  - Recent scholarship has brought attention to the fact that there often exist multiple models for a given prediction task with equal accuracy that differ in their individual-level predictions or aggregate properties. This phenomenon—which we call model multiplicity—can introduce a good deal of flexibility into the model selection process, creating a range of exciting opportunities. By demonstrating that there are many different ways of making equally accurate predictions, multiplicity gives model developers the freedom to prioritize other values in their model selection process without having to abandon their commitment to maximizing accuracy. However, multiplicity also brings to light a concerning truth: model selection on the basis of accuracy alone—the default procedure in many deployment scenarios—fails to consider what might be meaningful differences between equally accurate models with respect to other criteria such as fairness, robustness, and interpretability. Unless these criteria are taken into account explicitly, developers might end up making unnecessary trade-offs or could even mask intentional discrimination. Furthermore, the prospect that there might exist another model of equal accuracy that flips a prediction for a particular individual may lead to a crisis in justifiability: why should an individual be subject to an adverse model outcome if there exists an equally accurate model that treats them more favorably? In this work, we investigate how to take advantage of the flexibility afforded by model multiplicity while addressing the concerns with justifiability that it might raise?
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533149
SP  - 850
EP  - 863
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533149
KW  - discrimination
KW  - fairness
KW  - recourse
KW  - arbitrariness
KW  - Model multiplicity
KW  - predictive multiplicity
KW  - procedural multiplicity
ER  - 

TY  - CONF
TI  - Learning to limit data collection via scaling laws: A computational interpretation for the legal principle of data minimization
AU  - Shanmugam, Divya
AU  - Diaz, Fernando
AU  - Shabanian, Samira
AU  - Finck, Michele
AU  - Biega, Asia
T3  - FAccT '22
AB  - Modern machine learning systems are increasingly characterized by extensive personal data collection, despite the diminishing returns and increasing societal costs of such practices. Yet, data minimisation is one of the core data protection principles enshrined in the European Union’s General Data Protection Regulation (’GDPR’) and requires that only personal data that is adequate, relevant and limited to what is necessary is processed. However, the principle has seen limited adoption due to the lack of technical interpretation. In this work, we build on literature in machine learning and law to propose FIDO, a Framework for Inhibiting Data Overcollection. FIDO learns to limit data collection based on an interpretation of data minimization tied to system performance. Concretely, FIDO provides a data collection stopping criterion by iteratively updating an estimate of the performance curve, or the relationship between dataset size and performance, as data is acquired. FIDO estimates the performance curve via a piecewise power law technique that models distinct phases of an algorithm’s performance throughout data collection separately. Empirical experiments show that the framework produces accurate performance curves and data collection stopping criteria across datasets and feature acquisition algorithms. We further demonstrate that many other families of curves systematically overestimate the return on additional data. Results and analysis from our investigation offer deeper insights into the relevant considerations when designing a data minimization framework, including the impacts of active feature acquisition on individual users and the feasability of user-specific data minimization. We conclude with practical recommendations for the implementation of data minimization.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533148
SP  - 839
EP  - 849
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533148
ER  - 

TY  - CONF
TI  - How are ML-Based online content moderation systems actually used? Studying community size, local activity, and disparate treatment
AU  - Wang, Leijie
AU  - Zhu, Haiyi
T3  - FAccT '22
AB  - Machine learning-based predictive systems are increasingly used to assist online groups and communities in various content moderation tasks. However, there are limited quantitative understandings of whether and how different groups and communities use such predictive systems differently according to their community characteristics. In this research, we conducted a field evaluation of how content moderation systems are used in 17 Wikipedia language communities. We found that 1) larger communities tend to use predictive systems to identify the most damaging edits, while smaller communities tend to use them to identify any edit that could be damaging; 2) predictive systems are used less in content areas where there are more local editing activities; 3) predictive systems have mixed effects on reducing disparate treatment between anonymous and registered editors across communities of different characteristics. Finally, we discuss the theoretical and practical implications for future human-centered moderation algorithms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533147
SP  - 824
EP  - 838
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533147
KW  - Fairness
KW  - Causal inference
KW  - Content moderation
KW  - Online communities
KW  - Wikipedia
ER  - 

TY  - CONF
TI  - Fair representation clustering with several protected classes
AU  - Dai, Zhen
AU  - Makarychev, Yury
AU  - Vakilian, Ali
T3  - FAccT '22
AB  - We study the problem of fair k-median where each cluster is required to have a fair representation of individuals from different groups. In the fair representation k-median problem, we are given a set of points X in a metric space. Each point x ∈ X belongs to one of ℓ groups. Further, we are given fair representation parameters αj and βj for each group j ∈ [ℓ]. We say that a k-clustering C1, ⋅⋅⋅, Ck fairly represents all groups if the number of points from group j in cluster Ci is between αj|Ci| and βj|Ci| for every j ∈ [ℓ] and i ∈ [k]. The goal is to find a set of k centers and an assignment such that the clustering defined by fairly represents all groups and minimizes the ℓ1-objective ∑x ∈ Xd(x, ϕ(x)). We present an O(log k)-approximation algorithm that runs in time nO(ℓ). Note that the known algorithms for the problem either (i) violate the fairness constraints by an additive term or (ii) run in time that is exponential in both k and ℓ. We also consider an important special case of the problem where and for all j ∈ [ℓ]. For this special case, we present an O(log k)-approximation algorithm that runs in time.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533146
SP  - 814
EP  - 823
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533146
KW  - clustering
KW  - approximation algorithm
KW  - fair k-median
KW  - randomized algorithm
ER  - 

TY  - CONF
TI  - Trucks don’t mean trump: Diagnosing human error in image analysis
AU  - Zamfirescu-Pereira, J.D.
AU  - Chen, Jerry
AU  - Wen, Emily
AU  - Koenecke, Allison
AU  - Garg, Nikhil
AU  - Pierson, Emma
T3  - FAccT '22
AB  - Algorithms provide powerful tools for detecting and dissecting human bias and error. Here, we develop machine learning methods to to analyze how humans err in a particular high-stakes task: image interpretation. We leverage a unique dataset of 16,135,392 human predictions of whether a neighborhood voted for Donald Trump or Joe Biden in the 2020 US election, based on a Google Street View image. We show that by training a machine learning estimator of the Bayes optimal decision for each image, we can provide an actionable decomposition of human error into bias, variance, and noise terms, and further identify specific features (like pickup trucks) which lead humans astray. Our methods can be applied to ensure that human-in-the-loop decision-making is accurate and fair and are also applicable to black-box algorithmic systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533145
SP  - 799
EP  - 813
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533145
KW  - image analysis
KW  - diagnosing bias
KW  - human error
ER  - 

TY  - CONF
TI  - Exploring the role of grammar and word choice in bias toward african american english (AAE) in hate speech classification
AU  - Harris, Camille
AU  - Halevy, Matan
AU  - Howard, Ayanna
AU  - Bruckman, Amy
AU  - Yang, Diyi
T3  - FAccT '22
AB  - Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533144
SP  - 789
EP  - 798
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533144
KW  - Fairness
KW  - Linguistics
KW  - Natural Language Processing
KW  - Social Media
KW  - African American English
KW  - Hate Speech
ER  - 

TY  - CONF
TI  - Behavioral use licensing for responsible AI
AU  - Contractor, Danish
AU  - McDuff, Daniel
AU  - Haines, Julia Katherine
AU  - Lee, Jenny
AU  - Hines, Christopher
AU  - Hecht, Brent
AU  - Vincent, Nicholas
AU  - Li, Hanlin
T3  - FAccT '22
AB  - With the growing reliance on artificial intelligence (AI) for many different applications, the sharing of code, data, and models is important to ensure the replicability and democratization of scientific knowledge. Many high-profile academic publishing venues expect code and models to be submitted and released with papers. Furthermore, developers often want to release these assets to encourage development of technology that leverages their frameworks and services. A number of organizations have expressed concerns about the inappropriate or irresponsible use of AI and have proposed ethical guidelines around the application of such systems. While such guidelines can help set norms and shape policy, they are not easily enforceable. In this paper, we advocate the use of licensing to enable legally enforceable behavioral use conditions on software and code and provide several case studies that demonstrate the feasibility of behavioral use licensing. We envision how licensing may be implemented in accordance with existing responsible AI guidelines.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533143
SP  - 778
EP  - 788
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533143
KW  - AI licensing
KW  - enforceable mechanisms
KW  - ethical guidelines and principles
ER  - 

TY  - CONF
TI  - Imperfect inferences: A practical assessment
AU  - Rieke, Aaron
AU  - Southerland, Vincent
AU  - Svirsky, Dan
AU  - Hsu, Mingwei
T3  - FAccT '22
AB  - Measuring racial disparities is challenging, especially when demographic labels are unavailable. Recently, some researchers and advocates have argued that companies should infer race and other demographic factors to help them understand and address discrimination. Others have been more skeptical, emphasizing the inaccuracy of racial inferences, critiquing the conceptualization of demographic categories themselves, and arguing that the use of demographic data might encourage algorithmic tweaks where more radical interventions are needed. We conduct a novel empirical analysis that informs this debate, using a dataset of self-reported demographic information provided by users of the ride-hailing service Uber who consented to share this information for research purposes. As a threshold matter, we show how this data reflects the enduring power of racism in society. We find differences by race across a range of outcomes. For example, among self-reported African-American riders, we see racial differences on factors from iOS use to local pollution levels. We then turn to a practical assessment of racial inference methodologies and offer two key findings. First, every inference method we tested has significant errors, miscategorizing people relative to their self-reports (even as the self-reports themselves suffer from selection bias). Second, and most importantly, we found that the inference methods worked: they reliably confirmed directional racial disparities that we knew were reflected in our dataset. Our analysis also suggests that the choice of inference methods should be informed by the measurement task. For example, disparities that are geographic in nature might be best captured by inferences that rely on geography; discrimination based on a person’s name might be best detected by inferences that rely on names. In conclusion, our analysis shows that common racial inference methods have real and practical utility in shedding light on aggregate, directional disparities, despite their imperfections. While the recent literature has identified notable challenges regarding the collection and use of this data, these challenges should not be seen as dispositive.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533140
SP  - 767
EP  - 777
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533140
KW  - discrimination
KW  - fairness
KW  - inference
KW  - race
KW  - demographics
KW  - civil rights
ER  - 

TY  - CONF
TI  - Attribute privacy: Framework and mechanisms
AU  - Zhang, Wanrong
AU  - Ohrimenko, Olga
AU  - Cummings, Rachel
T3  - FAccT '22
AB  - Ensuring the privacy of training data is a growing concern since many machine learning models are trained on confidential and potentially sensitive data. Much attention has been devoted to methods for protecting individual privacy during analyses of large datasets. However in many settings, global properties of the dataset may also be sensitive (e.g., mortality rate in a hospital rather than presence of a particular patient in the dataset). In this work, we depart from individual privacy to initiate the study of attribute privacy, where a data owner is concerned about revealing sensitive properties of a whole dataset during analysis. We propose definitions to capture attribute privacy in two relevant cases where global attributes may need to be protected: (1) properties of a specific dataset and (2) parameters of the underlying distribution from which dataset is sampled. We also provide two efficient mechanisms for specific data distributions and one general but inefficient mechanism that satisfy attribute privacy for these settings. We base our results on a novel and non-trivial use of the Pufferfish framework to account for correlations across attributes in the data, thus addressing “the challenging problem of developing Pufferfish instantiations and algorithms for general aggregate secrets” that was left open by Kifer and Machanavajjhala in 2014 [15].
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533139
SP  - 757
EP  - 766
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533139
KW  - attribute privacy
KW  - formal privacy frameworks
KW  - privacy-preserving mechanisms
KW  - Pufferfish privacy
ER  - 

TY  - CONF
TI  - Robots enact malignant stereotypes
AU  - Hundt, Andrew
AU  - Agnew, William
AU  - Zeng, Vicky
AU  - Kacianka, Severin
AU  - Gombolay, Matthew
T3  - FAccT '22
AB  - Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)&nbsp;[18, 80], Natural Language Processing (NLP)&nbsp;[6], or both, in the case of large image and caption models such as OpenAI CLIP&nbsp;[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533138
SP  - 743
EP  - 756
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533138
ER  - 

TY  - CONF
TI  - Making the unaccountable internet: The changing meaning of accounting in the early ARPANET
AU  - Cooper, A. Feder
AU  - Vidan, Gili
T3  - FAccT '22
AB  - Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533137
SP  - 726
EP  - 742
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533137
KW  - Accountability
KW  - Accountable systems
KW  - Accounting
KW  - Internet governance
KW  - Resource sharing
ER  - 

TY  - CONF
TI  - Achieving fairness via post-processing in web-scale recommender Systems✱
AU  - Nandy, Preetam
AU  - DiCiccio, Cyrus
AU  - Venugopalan, Divya
AU  - Logan, Heloise
AU  - Basu, Kinjal
AU  - El Karoui, Noureddine
T3  - FAccT '22
AB  - Building fair recommender systems is a challenging and crucial area of study due to its immense impact on society. We extended the definitions of two commonly accepted notions of fairness to recommender systems, namely equality of opportunity and equalized odds. These fairness measures ensure that equally “qualified” (or “unqualified”) candidates are treated equally regardless of their protected attribute status (such as gender or race). We propose scalable methods for achieving equality of opportunity and equalized odds in rankings in the presence of position bias, which commonly plagues data generated from recommender systems. Our algorithms are model agnostic in the sense that they depend only on the final scores provided by a model, making them easily applicable to virtually all web-scale recommender systems. We conduct extensive simulations as well as real-world experiments to show the efficacy of our approach.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533136
SP  - 715
EP  - 725
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533136
ER  - 

TY  - CONF
TI  - Sensible AI: Re-imagining interpretability and explainability using sensemaking theory
AU  - Kaur, Harmanpreet
AU  - Adar, Eytan
AU  - Gilbert, Eric
AU  - Lampe, Cliff
T3  - FAccT '22
AB  - Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact—an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick’s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders’ needs—we build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533135
SP  - 702
EP  - 714
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533135
KW  - interpretability
KW  - explainability
KW  - organizations
KW  - sensemaking
ER  - 

TY  - CONF
TI  - The death of the legal subject: How predictive algorithms are (re)constructing legal subjectivity
AU  - Geddes, Katrina
T3  - FAccT '22
AB  - This paper explores the epistemological differences between the socio-political legal subject of Western liberalism, and the algorithmic subject of informational capitalism. It argues that the increasing use of predictive algorithms in judicial decision-making is reconstructing both the nature and experience of legal subjectivity in a manner that is incompatible with law's normative commitments to individualized justice. Whereas algorithmic subjectivity derives its epistemic authority from population-level insights, legal subjectivity has historically derived credibility from its close approximation of the underlying individual, through careful evaluation of their mental and physical autonomy, prior to any assignment of legal liability. With the introduction of predictive algorithms in judicial decision-making, knowledge about the legal subject is increasingly algorithmically produced, in a manner that discounts, and effectively displaces, qualitative knowledge about the legal subject's intentions, motivations, and moral capabilities. This results in the death of the legal subject, or the emergence of new, algorithmic practices of signification that no longer require the input of the underlying individual. As algorithms increasingly guide judicial decision-making, the shifting epistemology of legal subjectivity has long-term consequences for the legitimacy of legal institutions.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533134
SP  - 691
EP  - 701
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533134
KW  - law
KW  - algorithmic fairness
KW  - critical algorithm studies
KW  - epistemology
KW  - STS
KW  - inequality
KW  - expressive power
KW  - legal subjectivity
ER  - 

TY  - CONF
TI  - Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability
AU  - Norval, Chris
AU  - Cornelius, Kristin
AU  - Cobbe, Jennifer
AU  - Singh, Jatinder
T3  - FAccT '22
AB  - There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims. This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533133
SP  - 679
EP  - 690
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533133
KW  - transparency
KW  - GDPR
KW  - accountability
KW  - usability
KW  - data rights
KW  - document engineering
KW  - interfaces
ER  - 

TY  - CONF
TI  - Towards intersectional feminist and participatory ML: A case study in supporting feminicide counterdata collection
AU  - Suresh, Harini
AU  - Movva, Rajiv
AU  - Dogan, Amelia Lee
AU  - Bhargava, Rahul
AU  - Cruxen, Isadora
AU  - Cuba, Angeles Martinez
AU  - Taurino, Guilia
AU  - So, Wonyoung
AU  - D'Ignazio, Catherine
T3  - FAccT '22
AB  - Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide&nbsp;—&nbsp;gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process&nbsp;—&nbsp;with quantitative, qualitative and participatory steps&nbsp;—&nbsp;focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533132
SP  - 667
EP  - 678
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533132
ER  - 

TY  - CONF
TI  - Don’t let ricci v. DeStefano hold you back: A bias-aware legal solution to the hiring paradox
AU  - Salem, Jad
AU  - Desai, Deven
AU  - Gupta, Swati
T3  - FAccT '22
AB  - Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action. In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533129
SP  - 651
EP  - 666
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533129
KW  - uncertainty
KW  - bias
KW  - anti-discrimination laws
KW  - hiring
KW  - resume screening
ER  - 

TY  - CONF
TI  - Subverting fair image search with generative adversarial perturbations
AU  - Ghosh, Avijit
AU  - Jagielski, Matthew
AU  - Wilson, Christo
T3  - FAccT '22
AB  - In this work we explore the intersection fairness and robustness in the context of ranking: when a ranking model has been calibrated to achieve some definition of fairness, is it possible for an external adversary to make the ranking model behave unfairly without having access to the model or training data? To investigate this question, we present a case study in which we develop and then attack a state-of-the-art, fairness-aware image search engine using images that have been maliciously modified using a Generative Adversarial Perturbation (GAP) model&nbsp;[75]. These perturbations attempt to cause the fair re-ranking algorithm to unfairly boost the rank of images containing people from an adversary-selected subpopulation. We present results from extensive experiments demonstrating that our attacks can successfully confer significant unfair advantage to people from the majority class relative to fairly-ranked baseline search results. We demonstrate that our attacks are robust across a number of variables, that they have close to zero impact on the relevance of search results, and that they succeed under a strict threat model. Our findings highlight the danger of deploying fair machine learning algorithms in-the-wild when (1) the data necessary to achieve fairness may be adversarially manipulated, and (2) the models themselves are not robust against attacks.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533128
SP  - 637
EP  - 650
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533128
KW  - Adversarial Machine Learning
KW  - Information Retrieval
KW  - Demographic Inference
KW  - Fair Ranking
ER  - 

TY  - CONF
TI  - Human interpretation of saliency-based explanation over text
AU  - Schuff, Hendrik
AU  - Jacovi, Alon
AU  - Adel, Heike
AU  - Goldberg, Yoav
AU  - Vu, Ngoc Thang
T3  - FAccT '22
AB  - While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople’s interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees’ importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533127
SP  - 611
EP  - 636
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533127
KW  - interpretability
KW  - explainability
KW  - perception
KW  - human
KW  - saliency
KW  - cognitive bias
KW  - feature attribution
KW  - generalized additive mixed model
KW  - text
ER  - 

TY  - CONF
TI  - Fairness for AUC via feature augmentation
AU  - Fong, Hortense
AU  - Kumar, Vineet
AU  - Mehrotra, Anay
AU  - Vishnoi, Nisheeth K.
T3  - FAccT '22
AB  - We study fairness in the context of classification where the performance is measured by the area under the curve (AUC) of the receiver operating characteristic. AUC is commonly used when both Type&nbsp;I (false positive) and Type II (false negative) errors are important. However, the same classifier can have significantly varying AUCs for different protected groups and, in real-world applications, it is often desirable to reduce such cross-group differences. We address the problem of how to select additional features to most greatly improve AUC for the disadvantaged group. Our results establish that the unconditional variance of features does not inform us about AUC fairness but class-conditional variance does. Using this connection, we develop a novel approach, fairAUC, based on feature augmentation (adding features) to mitigate bias between identifiable groups. We evaluate fairAUC on synthetic and real-world (COMPAS) datasets and find that it significantly improves AUC for the disadvantaged group relative to benchmarks maximizing overall AUC and minimizing bias between groups.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533126
SP  - 610
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533126
KW  - Classification
KW  - data collection and curation
KW  - Area under the ROC curve (AUC)
KW  - feature augmentation
ER  - 

TY  - CONF
TI  - Selection in the presence of implicit bias: The advantage of intersectional constraints
AU  - Mehrotra, Anay
AU  - Pradelski, Bary S. R.
AU  - Vishnoi, Nisheeth K.
T3  - FAccT '22
AB  - In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533124
SP  - 599
EP  - 609
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533124
KW  - Intersectionality
KW  - Implicit bias
KW  - Intersectional biases
KW  - Rooney Rule
KW  - selection
ER  - 

TY  - CONF
TI  - Normative logics of algorithmic accountability
AU  - Donia, Joseph
T3  - FAccT '22
AB  - The relevance of algorithms in contemporary life is often appreciated when they ‘fail’—either because they did not perform as expected, or because they led to outcomes that were later determined to be unacceptable. As a result, academic, policy, and public discourse has increasingly emphasized accountability as a desirable, if not elusive, feature of system design, and component of effective governance. Accountability, however, is a versatile concept that has been operationalized in a number of ways across different use-contexts, policy settings, and research disciplines. While accountability is often framed as a normative good, it is unclear exactly what kind of normative work it is expected to do, and how it is expected to do it. Informed by perspectives from critical data studies and science and technology studies, this article introduces five normative logics underpinning discussions of algorithmic accountability that appear in the academic research literature: (1) accountability as verification, (2) accountability as representation, (3) accountability as social licence, (4) accountability as fiduciary duty, and (5) accountability as legal compliance. These normative logics, and the resulting rules, codes, and practices that constitute an emerging set of algorithmic accountability regimes, are especially discussed in terms of the presumed agency of actors involved. The article suggests that implicit assumptions characterizing each of ‘algorithms’ and ‘accountability’ are highly significant for each other, and that more explicit acknowledgement of this can lead to improved understanding of the diverse knowledge claims and practical goals associated with different logics of algorithmic accountability, and relatedly, the agency of different actors to pursue it in its different forms. Link to full text: josephdonia.com/facct-2022
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533123
SP  - 598
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533123
KW  - Accountability
KW  - Ethics
KW  - Algorithms
KW  - Governance
KW  - Agency
ER  - 

TY  - CONF
TI  - REAL ML: Recognizing, exploring, and articulating limitations of machine learning research
AU  - Smith, Jessie J.
AU  - Amershi, Saleema
AU  - Barocas, Solon
AU  - Wallach, Hanna
AU  - Wortman Vaughan, Jennifer
T3  - FAccT '22
AB  - Transparency around limitations can improve the scientific rigor of research, help ensure appropriate interpretation of research findings, and make research claims more credible. Despite these benefits, the machine learning (ML) research community lacks well-developed norms around disclosing and discussing limitations. To address this gap, we conduct an iterative design process with 30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided activities to help ML researchers recognize, explore, and articulate the limitations of their research. Using a three-stage interview and survey study, we identify ML researchers’ perceptions of limitations, as well as the challenges they face when recognizing, exploring, and articulating limitations. We develop REAL ML to address some of these practical challenges, and highlight additional cultural challenges that will require broader shifts in community norms to address. We hope our study and REAL ML help move the ML research community toward more active and appropriate engagement with limitations.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533122
SP  - 587
EP  - 597
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533122
KW  - Machine Learning
KW  - Community Standards
KW  - Limitations
KW  - Research Practices
KW  - Toolkit
ER  - 

TY  - CONF
TI  - Best vs. All: Equity and accuracy of standardized test score reporting
AU  - Niu, Mingzi
AU  - Kannan, Sampath
AU  - Roth, Aaron
AU  - Vohra, Rakesh
T3  - FAccT '22
AB  - We study a game theoretic model of standardized testing for college admissions. Students are of two types; High and Low. There is a college that would like to admit the High type students. Students take a potentially costly standardized exam which provides a noisy signal of their type. The students come from two populations, which are identical in talent (i.e. the type distribution is the same), but differ in their access to resources: the higher resourced population can at their option take the exam multiple times, whereas the lower resourced population can only take the exam once. We study two models of score reporting, which capture existing policies used by colleges. The first policy (sometimes known as “super-scoring”) allows students to report the max of the scores they achieve. The other policy requires that all scores be reported. We find in our model that requiring that all scores be reported results in superior outcomes in equilibrium, both from the perspective of the college (the admissions rule is more accurate), and from the perspective of equity across populations: a student’s probability of admission is independent of their population, conditional on their type. In particular, the false positive rates and false negative rates are identical in this setting, across the highly and poorly resourced student populations. This is the case despite the fact that the more highly resourced students can—at their option—either report a more accurate signal of their type, or pool with the lower resourced population under this policy. This represents an unusual situation in the algorithmic fairness literature where the goals of accuracy and equity are in alignment, and do not need to be traded off against one another.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533121
SP  - 574
EP  - 586
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533121
KW  - algorithm fairness
KW  - score reporting policies
KW  - screen accuracy
ER  - 

TY  - CONF
TI  - Ethical concerns and perceptions of consumer neurotechnology from lived experiences of mental workload tracking
AU  - Midha, Serena
AU  - Wilson, Max L.
AU  - Sharples, Sarah
T3  - FAccT '22
AB  - With rapid growth in the development of consumer neurotechnology, it is imperative to consider the ethical implications that this might have in order to minimise consumer harm. Whilst ethical and legal guidelines for commercialisation have previously been suggested, we aimed to further this discussion by investigating the ethical concerns held by potential end users of consumer neurotechnology. 19 participants who had previously experienced mental workload tracking in their daily lives were interviewed about their ethical concerns and perceptions of this type of future neurotechnology. An Interpretive Phenomenological Analysis (IPA) approach identified three superordinate themes. These related to concerns surrounding privacy, data validity and misinterpretation, and personal identity. The findings provide further validation for previous research and highlight further ethical considerations that should be factored into the commercialisation of neurotechnology.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533119
SP  - 564
EP  - 573
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533119
KW  - ethics
KW  - mental workload
KW  - neuroethics
KW  - neurotechnology
ER  - 

TY  - CONF
TI  - Towards a multi-stakeholder value-based assessment framework for algorithmic systems
AU  - Yurrita, Mireia
AU  - Murray-Rust, Dave
AU  - Balayn, Agathe
AU  - Bozzon, Alessandro
T3  - FAccT '22
AB  - In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML-driven systems are still underrepresented in auditing processes. Such unaddressed values mainly deal with contextual factors that cannot be easily quantified. In this paper, we develop a value-based assessment framework that is not limited to bias auditing and that covers prominent ethical principles for algorithmic systems. Our framework presents a circular arrangement of values with two bipolar dimensions that make common motivations and potential tensions explicit. In order to operationalize these high-level principles, values are then broken down into specific criteria and their manifestations. However, some of these value-specific criteria are mutually exclusive and require negotiation. As opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stakeholders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. We, therefore, contribute to current ML auditing practices with an assessment framework that visualizes closeness and tensions between values and we give guidelines on how to operationalize them, while opening up the evaluation and deliberation process to a wide range of stakeholders.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533118
SP  - 535
EP  - 563
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533118
KW  - values
KW  - algorithm assessment
KW  - ML development and deployment pipeline
KW  - multi-stakeholder
ER  - 

TY  - CONF
TI  - Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition
AU  - Markl, Nina
T3  - FAccT '22
AB  - All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533117
SP  - 521
EP  - 534
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533117
KW  - algorithmic bias
KW  - speech recognition
KW  - language variation
KW  - speech and language technologies
ER  - 

TY  - CONF
TI  - Goodbye tracking? Impact of iOS app tracking transparency and privacy labels
AU  - Kollnig, Konrad
AU  - Shuba, Anastasia
AU  - Van Kleek, Max
AU  - Binns, Reuben
AU  - Shadbolt, Nigel
T3  - FAccT '22
AB  - Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules. We find that Apple’s new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users&nbsp;–&nbsp;a positive development for privacy. However, the number of tracking libraries has&nbsp;–&nbsp;on average&nbsp;–&nbsp;roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple’s policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring from its new tracking rules. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading, especially in less popular apps. Overall, our observations suggest that, while Apple’s changes make tracking individual users more difficult, they motivate a countermovement, and reinforce existing market power of gatekeeper companies with access to large troves of first-party data. Making the privacy properties of apps transparent through large-scale analysis remains a difficult target for independent researchers, and a key obstacle to meaningful, accountable and verifiable privacy protections.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533116
SP  - 508
EP  - 520
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533116
KW  - privacy
KW  - data protection
KW  - App Tracking Transparency
KW  - Apple
KW  - gatekeeper power
KW  - iOS
KW  - mobile apps
KW  - platform policies
KW  - Privacy Nutrition Labels
ER  - 

TY  - CONF
TI  - Affirmative algorithms: Relational equality as algorithmic fairness
AU  - Zhang, Marilyn
T3  - FAccT '22
AB  - Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533115
SP  - 495
EP  - 507
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533115
KW  - fairness
KW  - algorithmic fairness
KW  - criminal justice
KW  - philosophy
KW  - affirmative algorithms
KW  - pretrial risk assessments
KW  - relational equality
ER  - 

TY  - CONF
TI  - Are “Intersectionally fair” AI algorithms really fair to women of color? A philosophical analysis
AU  - Kong, Youjin
T3  - FAccT '22
AB  - A growing number of studies on fairness in artificial intelligence (AI) use the notion of intersectionality to measure AI fairness. Most of these studies take intersectional fairness to be a matter of statistical parity among intersectional subgroups: an AI algorithm is “intersectionally fair” if the probability of the outcome is roughly the same across all subgroups defined by different combinations of the protected attributes. This paper identifies and examines three fundamental problems with this dominant interpretation of intersectional fairness in AI. First, the dominant approach is so preoccupied with the intersection of attributes/categories (e.g., race, gender) that it fails to address the intersection of oppression (e.g., racism, sexism), which is more central to intersectionality as a critical framework. Second, the dominant approach faces a dilemma between infinite regress and fairness gerrymandering: it either keeps splitting groups into smaller subgroups or arbitrarily selects protected groups. Lastly, the dominant view fails to capture what it really means for AI algorithms to be fair, in terms of both distributive and non-distributive fairness. I distinguish a strong sense of AI fairness from a weak sense that is prevalent in the literature, and conclude by envisioning paths towards strong intersectional fairness in AI.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533114
SP  - 485
EP  - 494
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533114
KW  - Fairness and Bias in AI
KW  - Feminist and Critical Race Social Philosophy
KW  - Intersectionality
KW  - Philosophical Analysis of Fairness
ER  - 

TY  - CONF
TI  - Exploring how machine learning practitioners (try to) use fairness toolkits
AU  - Deng, Wesley Hanwen
AU  - Nagireddy, Manish
AU  - Lee, Michelle Seng Ah
AU  - Singh, Jatinder
AU  - Wu, Zhiwei Steven
AU  - Holstein, Kenneth
AU  - Zhu, Haiyi
T3  - FAccT '22
AB  - Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533113
SP  - 473
EP  - 484
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533113
ER  - 

TY  - CONF
TI  - Equi-explanation maps: Concise and informative global summary explanations
AU  - Chowdhury, Tanya
AU  - Rahimi, Razieh
AU  - Allan, James
T3  - FAccT '22
AB  - We attempt to summarize the model logic of a black-box classification model in order to generate concise and informative global explanations. We propose equi-explanation maps, a new explanation data-structure that presents the region of interest as a union of equi-explanation subspaces along with their explanation vectors. We then propose E-Map, a method to generate equi-explanation maps. We demonstrate the broad utility of our approach by generating equi-explanation maps for various binary classification models (Logistic Regression, SVM, MLP, and XGBoost) on the UCI Heart disease dataset and the Pima Indians diabetes dataset. Each subspace in our generated map is the union of d-dimensional hyper-cuboids which can be compactly represented for the sake of interpretability. For each of these subspaces, we present linear explanations assigning a weight to each explanation feature. We justify the use of equi-explanation maps in comparison to other global explanation methods by evaluating in terms of interpretability, fidelity, and informativeness. A user study further corroborates the use of equi-explanation maps to generate compact and informative global explanations.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533112
SP  - 464
EP  - 472
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533112
KW  - explainability
KW  - explaining classifiers
KW  - global explanations
KW  - model-logic subspaces
KW  - subspace interpretability
ER  - 

TY  - CONF
TI  - Tech worker organizing for power and accountability
AU  - Boag, William
AU  - Suresh, Harini
AU  - Lepe, Bianca
AU  - D'Ignazio, Catherine
T3  - FAccT '22
AB  - In recent years, there has been a growing interest in the field of “AI Ethics” and related areas. This field is purposefully broad, allowing for the intersection of numerous subfields and disciplines. However, a lot of work in this area thus far has centered computational methods, leading to a narrow lens where technical tools are framed as solutions for broader sociotechnical problems. In this work, we discuss a less-explored mode of what it can mean to “do” AI Ethics: tech worker collective action. Through collective action, the employees of powerful tech companies can act as a countervailing force against strong corporate impulses to grow or make a profit to the detriment of other values. In this work, we ground these efforts in existing scholarship of social movements and labor organizing. We characterize 150 documented collective actions, and explore several case studies of successful campaigns. Looking forward, we also identify under-explored types of actions, and provide conceptual frameworks and inspiration for how to utilize worker organizing as an effective lever for change.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533111
SP  - 452
EP  - 463
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533111
ER  - 

TY  - CONF
TI  - The model card authoring toolkit: Toward community-centered, deliberation-driven AI design
AU  - Shen, Hong
AU  - Wang, Leijie
AU  - Deng, Wesley H.
AU  - Brusse, Ciell
AU  - Velgersdijk, Ronald
AU  - Zhu, Haiyi
T3  - FAccT '22
AB  - There have been increasing calls for centering impacted communities – both online and offline – in the design of the AI systems that will be deployed in their communities. However, the complicated nature of a community’s goals and needs, as well as the complexity of AI’s development procedures, outputs, and potential impacts, often prevents effective participation. In this paper, we present the Model Card Authoring Toolkit, a toolkit that supports community members to understand, navigate and negotiate a spectrum of machine learning models via deliberation and pick the ones that best align with their collective values. Through a series of workshops, we conduct an empirical investigation of the initial effectiveness of our approach in two online communities – English and Dutch Wikipedia, and document how our participants collectively set the threshold for a machine learning based quality prediction system used in their communities’ content moderation applications. Our results suggest that the use of the Model Card Authoring Toolkit helps improve the understanding of the trade-offs across multiple community goals on AI design, engage community members to discuss and negotiate the trade-offs, and facilitate collective and informed decision-making in their own community contexts. Finally, we discuss the challenges for a community-centered, deliberation-driven approach for AI design as well as potential design implications.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533110
SP  - 440
EP  - 451
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533110
ER  - 

TY  - CONF
TI  - Interactive model cards: A human-centered approach to model documentation
AU  - Crisan, Anamaria
AU  - Drouhard, Margaret
AU  - Vig, Jesse
AU  - Rajani, Nazneen
T3  - FAccT '22
AB  - Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability &amp; interpretability; sensemaking &amp; skepticism; and trust &amp; safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533108
SP  - 427
EP  - 439
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533108
KW  - model cards
KW  - human centered design
KW  - interactive data visualization
ER  - 

TY  - CONF
TI  - Four years of FAccT: A reflexive, mixed-methods analysis of research contributions, shortcomings, and future prospects
AU  - Laufer, Benjamin
AU  - Jain, Sameer
AU  - Cooper, A. Feder
AU  - Kleinberg, Jon
AU  - Heidari, Hoda
T3  - FAccT '22
AB  - Fairness, Accountability, and Transparency (FAccT) for socio-technical systems has been a thriving area of research in recent years. An ACM conference bearing the same name has been the central venue for scholars in this area to come together, provide peer feedback to one another, and publish their work. This reflexive study aims to shed light on FAccT’s activities to date and identify major gaps and opportunities for translating contributions into broader positive impact. To this end, we utilize a mixed-methods research design. On the qualitative front, we develop a protocol for reviewing and coding prior FAccT papers, tracing their distribution of topics, methods, datasets, and disciplinary roots. We also design and administer a questionnaire to reflect the voices of FAccT community members and affiliates on a wide range of topics. On the quantitative front, we use the full text and citation network associated with prior FAccT publications to provide further evidence about topics and values represented in FAccT. We organize the findings from our analysis into four main dimensions: the themes present in FAccT scholarship, the values that underpin the work, the impact of the contributions both within academic circles and beyond, and the practices and informal norms of the community that has formed around FAccT. Finally, our work identifies several suggestions on directions for change, as voiced by community members.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533107
SP  - 401
EP  - 426
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533107
L1  - https://dl.acm.org/doi/pdf/10.1145/3531146.3533107
KW  - reflexivity
KW  - impact
KW  - community perspectives
KW  - FAccT
KW  - mixed methods
KW  - topics
KW  - values
ER  - 

TY  - CONF
TI  - Mind the gap: Autonomous systems, the responsibility gap, and moral entanglement
AU  - Goetze, Trystan S.
T3  - FAccT '22
AB  - When a computer system causes harm, who is responsible? This question has renewed significance given the proliferation of autonomous systems enabled by modern artificial intelligence techniques. At the root of this problem is a philosophical difficulty known in the literature as the responsibility gap. That is to say, because of the causal distance between the designers of autonomous systems and the eventual outcomes of those systems, the dilution of agency within the large and complex teams that design autonomous systems, and the impossibility of fully predicting how autonomous systems will behave once deployed, determining who is morally responsible for harms caused by autonomous systems is unclear at a conceptual level. I review past work on this topic, criticizing prior works for suggesting workarounds rather than philosophical answers to the conceptual problem presented by the responsibility gap. The view I develop, drawing on my earlier work on vicarious moral responsibility, explains why computing professionals are ethically required to take responsibility for the systems they design, despite not being blameworthy for the harms these systems may cause.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533106
SP  - 390
EP  - 400
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533106
KW  - accountability
KW  - autonomous systems
KW  - ethics of artificial intelligence
KW  - moral responsibility
KW  - computer ethics
KW  - lethal autonomous weapons systems (LAWS)
KW  - professional responsibility
ER  - 

TY  - CONF
TI  - De-biasing “bias” measurement
AU  - Lum, Kristian
AU  - Zhang, Yunfeng
AU  - Bower, Amanda
T3  - FAccT '22
AB  - When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533105
SP  - 379
EP  - 389
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533105
ER  - 

TY  - CONF
TI  - Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants
AU  - Meyer, Mikaela
AU  - Horowitz, Aaron
AU  - Marshall, Erica
AU  - Lum, Kristian
T3  - FAccT '22
AB  - In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that “flips the script.” Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were “especially lengthy.” We then use a second-stage model to predict the defendant’s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533104
SP  - 366
EP  - 378
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533104
KW  - risk assessment
KW  - criminal justice
KW  - heteroscedastic Bayesian additive regression trees
KW  - LASSO
KW  - perspective reversal
KW  - two-stage model
ER  - 

TY  - CONF
TI  - Causal inference struggles with agency on online platforms
AU  - Milli, Smitha
AU  - Belli, Luca
AU  - Hardt, Moritz
T3  - FAccT '22
AB  - Online platforms regularly conduct randomized experiments to understand how changes to the platform causally affect various outcomes of interest. However, experimentation on online platforms has been criticized for having, among other issues, a lack of meaningful oversight and user consent. As platforms give users greater agency, it becomes possible to conduct observational studies in which users self-select into the treatment of interest as an alternative to experiments in which the platform controls whether the user receives treatment or not. In this paper, we conduct four large-scale within-study comparisons on Twitter aimed at assessing the effectiveness of observational studies derived from user self-selection on online platforms. In a within-study comparison, treatment effects from an observational study are assessed based on how effectively they replicate results from a randomized experiment with the same target population. We test the naive difference in group means estimator, exact matching, regression adjustment, and inverse probability of treatment weighting while controlling for plausible confounding variables. In all cases, all observational estimates perform poorly at recovering the ground-truth estimate from the analogous randomized experiments. In all cases except one, the observational estimates have the opposite sign of the randomized estimate. Our results suggest that observational studies derived from user self-selection are a poor alternative to randomized experimentation on online platforms. In discussing our results, we postulate a “Catch-22” that suggests that the success of causal inference in these settings may be at odds with the original motivations for providing users with greater agency.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533103
SP  - 357
EP  - 365
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533103
ER  - 

TY  - CONF
TI  - An outcome test of discrimination for ranked lists
AU  - Roth, Jonathan
AU  - Saint-Jacques, Guillaume
AU  - Yu, YinYin
T3  - FAccT '22
AB  - This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533102
SP  - 350
EP  - 356
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533102
ER  - 

TY  - CONF
TI  - How different groups prioritize ethical values for responsible AI
AU  - Jakesch, Maurice
AU  - Buçinca, Zana
AU  - Amershi, Saleema
AU  - Olteanu, Alexandra
T3  - FAccT '22
AB  - Private companies, public sector organizations, and academic groups have outlined ethical values they consider important for responsible artificial intelligence technologies. While their recommendations converge on a set of central values, little is known about the values a more representative public would find important for the AI technologies they interact with and might be affected by. We conducted a survey examining how individuals perceive and prioritize responsible AI values across three groups: a representative sample of the US population (N=743), a sample of crowdworkers (N=755), and a sample of AI practitioners (N=175). Our results empirically confirm a common concern: AI practitioners’ value priorities differ from those of the general public. Compared to the US-representative sample, AI practitioners appear to consider responsible AI values as less important and emphasize a different set of values. In contrast, self-identified women and black respondents found responsible AI values more important than other groups. Surprisingly, more liberal-leaning participants, rather than participants reporting experiences with discrimination, were more likely to prioritize fairness than other groups. Our findings highlight the importance of paying attention to who gets to define “responsible AI.”
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533097
SP  - 310
EP  - 323
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533097
KW  - Responsible AI
KW  - empirical ethics
KW  - value-sensitive design
ER  - 

TY  - CONF
TI  - GetFair: Generalized fairness tuning of classification models
AU  - Sikdar, Sandipan
AU  - Lemmerich, Florian
AU  - Strohmaier, Markus
T3  - FAccT '22
AB  - We present GetFair, a novel framework for tuning fairness of classification models. The fair classification problem deals with training models for a given classification task where data points have sensitive attributes. The goal of fair classification models is to not only generate accurate classification results but also to prevent discrimination against subpopulations (i.e., individuals with a specific value for the sensitive attribute). Existing methods for enhancing fairness of classification models, however, are often specifically designed for a particular fairness metric or a classifier model. They may also not be suitable for scenarios with incomplete training data or where optimizing for multiple fairness metrics is important. GetFair represents a general solution to this problem. The GetFair approach works in the following way: First, a given classifier is trained on training data without any fairness objective. This is followed by a reinforcement learning inspired tuning procedure which updates the parameters of the learned model on a given fairness objective. This disentangles classifier training from fairness tuning, making our framework more general and allowing for the adoption of any parameterized classifier model. Because fairness metrics are designed as reward functions during tuning, GetFair generalizes across any fairness metric. We demonstrate the generalizability of GetFair via evaluation over a benchmark suite of datasets, classification models, and fairness metrics. In addition, GetFair can also be deployed in settings where the training data is incomplete or the classifier needs to be tuned on multiple fairness metrics. GetFair not only contributes a flexible method to the repertoire of tools available to improve the fairness of classification models, it also seamlessly adapts to settings where existing fair classification methods may not be suitable or applicable.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533094
SP  - 289
EP  - 299
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533094
KW  - classifier models
KW  - Fair classification
KW  - fairness metrics
KW  - sensitive attribute
ER  - 

TY  - CONF
TI  - Equitable public bus network optimization for social good: A case study of singapore
AU  - Tedjopurnomo, David
AU  - Bao, Zhifeng
AU  - Choudhury, Farhana
AU  - Luo, Hui
AU  - Qin, A. K.
T3  - FAccT '22
AB  - Public bus transport is a major backbone of many cities’ socioeconomic activities. As such, the topic of public bus network optimization has received substantial attention in Geographic Information System (GIS) research. Unfortunately, most of the current literature are focused on improving only the efficiency of the bus network, neglecting the important equity factors. Optimizing only the efficiency of a bus network may cause these limited public transportation resources to be shifted away from areas with disadvantaged demographics, compounding the equity problem. In this work, we make the first attempt to explore the intricacies of the equitable public bus network optimization problem by performing a case study of Singapore’s public bus network. We describe the challenges in designing an equitable public bus network, tackle the fundamental problem of formulating efficiency and equity metrics, perform exploratory experiments to assess each metric’s real-life impact, and analyze the challenges of the equitable bus network optimization task. For our experiments, we have curated and combined Singapore’s bus network data, road network data, census area boundaries data, and demographics data into a unified dataset which we released publicly. Our objective is not only to explore this important yet relatively unexplored problem, but also to inspire more discussion and research.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533092
SP  - 278
EP  - 288
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533092
KW  - datasets
KW  - metric
KW  - bus network
KW  - equity
KW  - public transportation
ER  - 

TY  - CONF
TI  - It’s just not that simple: An empirical study of the accuracy-explainability trade-off in machine learning for public policy
AU  - Bell, Andrew
AU  - Solano-Kamaiko, Ian
AU  - Nov, Oded
AU  - Stoyanovich, Julia
T3  - FAccT '22
AB  - To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque. Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533090
SP  - 248
EP  - 266
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533090
KW  - machine learning
KW  - explainability
KW  - responsible AI
KW  - public policy
ER  - 

TY  - CONF
TI  - Bias in automated speaker recognition
AU  - Hutiri, Wiebke Toussaint
AU  - Ding, Aaron Yi
T3  - FAccT '22
AB  - Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533089
SP  - 230
EP  - 247
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533089
KW  - fairness
KW  - evaluation
KW  - bias
KW  - audit
KW  - speaker recognition
KW  - speaker verification
ER  - 

TY  - CONF
TI  - Taxonomy of risks posed by language models
AU  - Weidinger, Laura
AU  - Uesato, Jonathan
AU  - Rauh, Maribeth
AU  - Griffin, Conor
AU  - Huang, Po-Sen
AU  - Mellor, John
AU  - Glaese, Amelia
AU  - Cheng, Myra
AU  - Balle, Borja
AU  - Kasirzadeh, Atoosa
AU  - Biles, Courtney
AU  - Brown, Sasha
AU  - Kenton, Zac
AU  - Hawkins, Will
AU  - Stepleton, Tom
AU  - Birhane, Abeba
AU  - Hendricks, Lisa Anne
AU  - Rimell, Laura
AU  - Isaac, William
AU  - Haas, Julia
AU  - Legassick, Sean
AU  - Irving, Geoffrey
AU  - Gabriel, Iason
T3  - FAccT '22
AB  - Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533088
SP  - 214
EP  - 229
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533088
KW  - risk assessment
KW  - responsible AI
KW  - language models
KW  - responsible innovation
KW  - technology risks
ER  - 

TY  - CONF
TI  - Treatment effect risk: Bounds and inference
AU  - Kallus, Nathan
T3  - FAccT '22
AB  - Since the average treatment effect (ATE) measures the change in social welfare, even if positive, there is a risk of negative effect on, say, some 10% of the population. Assessing such risk is difficult, however, because any one individual treatment effect (ITE) is never observed so the 10% worst-affected cannot be identified, while distributional treatment effects only compare the first deciles within each treatment group, which does not correspond to any 10%-subpopulation. In this paper we consider how to nonetheless assess this important risk measure, formalized as the conditional value at risk (CVaR) of the ITE-distribution. We leverage the availability of pre-treatment covariates and characterize the tightest-possible upper and lower bounds on ITE-CVaR given by the covariate-conditional average treatment effect (CATE) function. We then proceed to study how to estimate these bounds efficiently from data and construct confidence intervals. This is challenging even in randomized experiments as it requires understanding the distribution of the unknown CATE function, which can be very complex if we use rich covariates so as to best control for heterogeneity. We develop a debiasing method that overcomes this and prove it enjoys favorable statistical properties even when CATE and other nuisances are estimated by black-box machine learning or even inconsistently. Studying a hypothetical change to French job-search counseling services, our bounds and inference demonstrate a small social benefit entails a negative impact on a substantial subpopulation.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533087
SP  - 213
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533087
KW  - Conditional average treatment effect
KW  - Conditional value at risk
KW  - Debiased machine learning
KW  - Individual treatment effect
KW  - Partial identification
KW  - Program evaluation
ER  - 

TY  - CONF
TI  - A framework for deprecating datasets: Standardizing documentation, identification, and communication
AU  - Luccioni, Alexandra Sasha
AU  - Corry, Frances
AU  - Sridharan, Hamsini
AU  - Ananny, Mike
AU  - Schultz, Jason
AU  - Crawford, Kate
T3  - FAccT '22
AB  - Datasets are central to training machine learning (ML) models. The ML community has recently made significant improvements to data stewardship and documentation practices across the model development life cycle. However, the act of deprecating, or deleting, datasets has been largely overlooked, and there are currently no standardized approaches for structuring this stage of the dataset life cycle. In this paper, we study the practice of dataset deprecation in ML, identify several cases of datasets that continued to circulate despite having been deprecated, and describe the different technical, legal, ethical, and organizational issues raised by such continuations. We then propose a Dataset Deprecation Framework that includes considerations of risk, mitigation of impact, appeal mechanisms, timeline, post-deprecation protocols, and publication checks that can be adapted and implemented by the ML community. Finally, we propose creating a centralized, sustainable repository system for archiving datasets, tracking dataset modifications or deprecations, and facilitating practices of care and stewardship that can be integrated into research and publication processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533086
SP  - 199
EP  - 212
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533086
KW  - datasets
KW  - data stewardship data management dataset deprecation
ER  - 

TY  - CONF
TI  - Pareto-improving Data-Sharing
AU  - Gradwohl, Ronen
AU  - Tennenholtz, Moshe
T3  - FAccT '22
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533085
SP  - 197
EP  - 198
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533085
ER  - 

TY  - CONF
TI  - AI opacity and explainability in tort litigation
AU  - Fraser, Henry
AU  - Simcock, Rhyle
AU  - Snoswell, Aaron J.
T3  - FAccT '22
AB  - A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533084
SP  - 185
EP  - 196
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533084
KW  - Explainable AI
KW  - Accidents
KW  - Law
KW  - Autonomous Vehicle
KW  - AI Opacity
KW  - Causation
KW  - Civil Procedure
KW  - Damages
KW  - Evidence
KW  - Expert Evidence
KW  - Negligence
KW  - Product Liability
ER  - 

TY  - CONF
TI  - The values encoded in machine learning research
AU  - Birhane, Abeba
AU  - Kalluri, Pratyusha
AU  - Card, Dallas
AU  - Agnew, William
AU  - Dotan, Ravit
AU  - Bao, Michelle
T3  - FAccT '22
AB  - Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15%) and far fewer discuss negative potential (1%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533083
SP  - 173
EP  - 184
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533083
KW  - Corporate ties
KW  - Encoded values of ML
KW  - ICML
KW  - NeurIPS
KW  - Power asymmetries
ER  - 

TY  - CONF
TI  - Automating care: Online food delivery work during the CoVID-19 crisis in india
AU  - Singh, Anubha
AU  - Park, Tina
T3  - FAccT '22
AB  - On March 23, 2020, the Government of India (GoI) announced one of the strictest nationwide lockdowns in the world to curb the spread of novel SARS-CoV-2, otherwise known as CoVID-19. The country came to a standstill overnight and the service industry, including small businesses and restaurants, took a massive financial hit. The unknown nature of the virus and its spread deepened anxiety among the general public, quickly turning to distrust towards any “outside” contact with goods and people. In the hopes of (re)building consumer trust, food delivery platforms Zomato and Swiggy began providing digital solutions to exhibit care towards their customers, including: (1) sharing delivery workers’ live temperatures alongside the workers’ profile inside the app; (2) mandating the use of the controversial contact tracing app Aarogya Setu for the workers; (3) monitoring workers’ usage of masks through random selfie requests; and (4) sharing specific worker vaccination details on the app for customers to view, including vaccination date and the vaccine’s serial number. Such invasive data gathering infrastructures to address public health threats have long focused on the surveillance of laborers, migrants, and the bodies of other marginalized communities. Framed as public health management, such biometric and health data gathering is treated as a necessary feature of caring for the well-being of the general public. However, such datafication practices - ones which primarily focus on the extraction of data from one specific community in order to mollify the concerns of another - normalizes the false perception that disease is transmitted unidirectionally: from worker to the consumer. By centering food delivery workers’ experiences during the pandemic and examining the normalization of such surveillance in the name of care and recovery, this paper aims to examine how new regimes of care are manufactured and legitimized using harmful and unethical datafication practices.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533082
SP  - 160
EP  - 172
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533082
KW  - CoVID-19
KW  - Gig Workers
KW  - Platform Ethics
KW  - Politics of Care
KW  - Quantification
ER  - 

TY  - CONF
TI  - Minimax demographic group fairness in federated learning
AU  - Papadaki, Afroditi
AU  - Martinez, Natalia
AU  - Bertran, Martin
AU  - Sapiro, Guillermo
AU  - Rodrigues, Miguel
T3  - FAccT '22
AB  - Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533081
SP  - 142
EP  - 159
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533081
KW  - Federated Learning
KW  - Algorithmic Fairness
KW  - Minimax Group Fairness
ER  - 

TY  - CONF
TI  - What people think AI should infer from faces
AU  - Engelmann, Severin
AU  - Ullstein, Chiara
AU  - Papakyriakopoulos, Orestis
AU  - Grossklags, Jens
T3  - FAccT '22
AB  - Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533080
SP  - 128
EP  - 141
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533080
KW  - artificial intelligence
KW  - computer vision
KW  - human faces
KW  - participatory AI ethics
ER  - 

TY  - CONF
TI  - Providing item-side individual fairness for deep recommender systems
AU  - Wang, Xiuling
AU  - Wang, Wendy Hui
T3  - FAccT '22
AB  - Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et&nbsp;al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533079
SP  - 117
EP  - 127
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533079
KW  - algorithmic fairness in machine learning
KW  - deep recommender systems
KW  - Individual fairness
ER  - 

TY  - CONF
TI  - When learning becomes impossible
AU  - Asher, Nicholas
AU  - Hunter, Julie
T3  - FAccT '22
AB  - We formally analyze an epistemic bias we call interpretive blindness (IB), in which under certain conditions a learner will be incapable of learning. IB is now common in our society, but it is a natural consequence of Bayesian inference and what we argue are mild assumptions about the relation between belief and evidence. IB a special problem for learning from testimony, in which one acquires information only from text or conversation. We show that IB follows from a codependence between background beliefs and interpretation in a Bayesian setting and the nature of contemporary testimony. We argue that a particular characteristic of contemporary testimony, argumentative completeness, can preclude learning in hierarchical Bayesian settings, even in the presence of constraints that are designed to promote good epistemic practices.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533078
SP  - 107
EP  - 116
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533078
KW  - learning
KW  - Bayesian learning
KW  - bias
KW  - agent modeling
KW  - echo chambers
KW  - philosophical foundations
ER  - 

TY  - CONF
TI  - News from generative artificial intelligence is believed less
AU  - Longoni, Chiara
AU  - Fradkin, Andrey
AU  - Cian, Luca
AU  - Pennycook, Gordon
T3  - FAccT '22
AB  - Artificial Intelligence (AI) can generate text virtually indistinguishable from text written by humans. A key question, then, is whether people believe news headlines generated by AI as much as news headlines generated by humans. AI is viewed as lacking human motives and emotions, suggesting that people might view news written by AI as more accurate. By contrast, two pre-registered experiments on representative U.S. samples (N = 4,034) showed that people rated news headlines written by AI as less accurate than those written by humans. People were more likely to incorrectly rate news headlines written by AI (vs. a human) as inaccurate when they were actually true, and more likely to correctly rate them as inaccurate when they were indeed false. Our findings are important given the increasing adoption of AI in news generation, and the associated ethical and governance pressures to disclose it use and address standards of transparency and accountability.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533077
SP  - 97
EP  - 106
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533077
KW  - algorithmic transparency
KW  - fairness
KW  - generative artificial intelligence
KW  - news
KW  - news generation
ER  - 

TY  - CONF
TI  - FAccT-Check on AI regulation: Systematic evaluation of AI regulation on the example of the legislation on the use of AI in the public sector in the german federal state of schleswig-holstein
AU  - Simbeck, Katharina
T3  - FAccT '22
AB  - In the framework of the current discussions about regulating Artificial Intelligence (AI) and machine learning (ML), the small Federal State of Schleswig-Holstein in Northern Germany hurries ahead and adopts legislation on the Use of AI in the public sector. The legislation aims on the one hand to enable the use of AI in the public sector by creating a legal framework and to limit its potential discriminatory effect on the other hand. Contrary to the European AI Act, which is valid for all companies and organizations in Europe, and contrary to the Chinese administrative rule on Internet information recommender systems, the Schleswig-Holstein “IT Deployment Law” (ITDL) would therefore only apply to public administrations and agencies in the federal state. The legislation addresses several AI risks, including fairness and transparency, and mitigates them with approaches quite different from the proposed European AI Act (AIA). In this paper, the legislation will be systematically reviewed and discussed with regards to its definition of AI, risk handling, fairness, accountability, and transparency.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533076
SP  - 89
EP  - 96
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533076
KW  - AI fairness
KW  - AI regulation
KW  - AI transparency
ER  - 

TY  - CONF
TI  - Fairness indicators for systematic assessments of visual feature extractors
AU  - Goyal, Priya
AU  - Soriano, Adriana Romero
AU  - Hazirbas, Caner
AU  - Sagun, Levent
AU  - Usunier, Nicolas
T3  - FAccT '22
AB  - Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world. We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to “off-the-shelf” models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size. The results of our fairness indicators on these systems suggest that blatant disparities still exist, which highlight the importance on the relationship between the context of the task and contents of a datasets. The code will be released to encourage the use of indicators.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533074
SP  - 70
EP  - 88
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533074
KW  - metrics
KW  - Fairness
KW  - Computer Vision
KW  - benchmarks
ER  - 

TY  - CONF
TI  - Learning to break deep perceptual hashing: The use case NeuralHash
AU  - Struppek, Lukas
AU  - Hintersdorf, Dominik
AU  - Neider, Daniel
AU  - Kersting, Kristian
T3  - FAccT '22
AB  - Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.1
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533073
SP  - 58
EP  - 69
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533073
KW  - deep learning
KW  - privacy
KW  - neural networks
KW  - client-side scanning
KW  - neuralhash
KW  - perceptual hashing
ER  - 

TY  - CONF
TI  - #FuckTheAlgorithm: algorithmic imaginaries and political resistance
AU  - Benjamin, Garfield
T3  - FAccT '22
AB  - This paper applies and extends the concept of algorithmic imaginaries in the context of political resistance to sociotechnical injustice. Focusing on the 2020 UK OfQual protests, the role of the ”fuck the algorithm” chant is examined as an imaginary of resistance to confront power in sociotechnical systems. The protest is analysed as a shift in algorithmic imaginaries amidst evolving uses of #FuckTheAlgorithm on social media as part of everyday practices of resistance.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533072
SP  - 46
EP  - 57
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533072
KW  - social media
KW  - critical algorithm studies
KW  - science and technology studies
KW  - algorithmic imaginaries
KW  - protest
ER  - 

TY  - CONF
TI  - A Data-driven analysis of the interplay between Criminological theory and predictive policing algorithms
AU  - Chapman, Adriane
AU  - Grylls, Philip
AU  - Ugwudike, Pamela
AU  - Gammack, David
AU  - Ayling, Jacqui
T3  - FAccT '22
AB  - Previous studies have focused on the biases and feedback loops that occur in predictive policing algorithms. These studies show how systemically and institutionally biased data leads to these feedback loops when predictive policing algorithms are applied in real life. We take a step back, and show that the choice in algorithm can be embedded in a specific criminological theory, and that the choice of a model on its own even without biased data can create biased feedback loops. By synthesizing “historical” data, in which we control the relationships between crimes, location and time, we show that the current predictive policing algorithms create biased feedback loops even with completely random data. We then review the process of creation and deployment of these predictive systems, and highlight when good practices, such as fitting a model to data, “go bad” within the context of larger system development and deployment. Using best practices from previous work on assessing and mitigating the impact of new technologies, we highlight where the design of these algorithms has broken down. The study also found that multidisciplinary analysis of such systems is vital for uncovering these issues and shows that any study of equitable AI should involve a systematic and holistic analysis of their design rationalities.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533071
SP  - 36
EP  - 45
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533071
KW  - impact of data on algorithms
KW  - model design
ER  - 

TY  - CONF
TI  - Dynamic privacy budget allocation improves data efficiency of differentially private gradient descent
AU  - Hong, Junyuan
AU  - Wang, Zhangyang
AU  - Zhou, Jiayu
T3  - FAccT '22
AB  - Protecting privacy in learning while maintaining the model performance has become increasingly critical in many applications that involve sensitive data. A popular private learning framework is differentially private learning composed of many privatized gradient iterations by noising and clipping. Under the privacy constraint, it has been shown that the dynamic policies could improve the final iterate loss, namely the quality of published models. In this talk, we will introduce these dynamic techniques for learning rate, batch size, noise magnitude and gradient clipping. Also, we discuss how the dynamic policy could change the convergence bounds which further provides insight of the impact of dynamic methods.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533070
SP  - 11
EP  - 35
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533070
KW  - machine learning
KW  - privacy
ER  - 

TY  - CONF
TI  - Interdisciplinarity, gender diversity, and network structure predict the centrality of AI organizations
AU  - Vlasceanu, Madalina
AU  - Dudik, Miroslav
AU  - Momennejad, Ida
T3  - FAccT '22
AB  - Artificial intelligence (AI) research plays an increasingly important role in society, impacting key aspects of human life. From face recognition algorithms aiding national security in airports, to software that advises judges in criminal cases, and medical staff in healthcare, AI research is shaping critical facets of our experience in the world. But who are the people and institutional bodies behind this influential research? What are the predictors of influence of AI researchers and research organizations? We study this question using social network analysis, in an exploration of the structural characteristics, i.e., network topology, of research organizations that shape modern AI. In a sample of 149 organizations with 9,987 affiliated authors of published papers in a major AI conference (NeurIPS) and two major conferences that specifically focus on societal impacts of AI (FAccT and AIES), we find that both industry and academic research organizations with influential authors are more interdisciplinary, have a greater fraction of women, are more hierarchical, and less clustered, even when controlling for the size of the organizations. The influence is operationalized as betweenness centrality in co-authorship networks, i.e., how often an author is on the shortest path connecting any pair of authors, acting as a bridge connecting otherwise distant (or even disconneted) members of the network, such as their own co-authors who are not each other’s co-author themselves. Using this operationalization, we also find that women have less influence in the AI community, determined as lower betweenness centrality in co-authorship networks. These results suggest that while diverse AI institutions are more influential, the individuals contributing to the increased diversity are marginalized in the AI field. We discuss these results in the context of current events with important societal implications.
C1  - New York, NY, USA
C3  - Proceedings of the 2022 ACM conference on fairness, accountability, and transparency
DA  - 2022///
PY  - 2022
DO  - 10.1145/3531146.3533069
SP  - 1
EP  - 10
PB  - Association for Computing Machinery
SN  - 978-1-4503-9352-2
UR  - https://doi.org/10.1145/3531146.3533069
KW  - artificial intelligence
KW  - organizational structure
KW  - gender diversity
KW  - interdisciplinarity
ER  - 

TY  - CONF
TI  - Queer in AI: A case study in community-led participatory AI
AU  - Queerinai, Organizers Of
AU  - Ovalle, Anaelia
AU  - Subramonian, Arjun
AU  - Singh, Ashwin
AU  - Voelcker, Claas
AU  - Sutherland, Danica J.
AU  - Locatelli, Davide
AU  - Breznik, Eva
AU  - Klubicka, Filip
AU  - Yuan, Hang
AU  - J, Hetvi
AU  - Zhang, Huan
AU  - Shriram, Jaidev
AU  - Lehman, Kruno
AU  - Soldaini, Luca
AU  - Sap, Maarten
AU  - Deisenroth, Marc Peter
AU  - Pacheco, Maria Leonor
AU  - Ryskina, Maria
AU  - Mundt, Martin
AU  - Agarwal, Milind
AU  - Mclean, Nyx
AU  - Xu, Pan
AU  - Pranav, A
AU  - Korpan, Raj
AU  - Ray, Ruchira
AU  - Mathew, Sarah
AU  - Arora, Sarthak
AU  - John, St
AU  - Anand, Tanvi
AU  - Agrawal, Vishakha
AU  - Agnew, William
AU  - Long, Yanan
AU  - Wang, Zijie J.
AU  - Talat, Zeerak
AU  - Ghosh, Avijit
AU  - Dennler, Nathaniel
AU  - Noseworthy, Michael
AU  - Jha, Sharvani
AU  - Baylor, Emi
AU  - Joshi, Aditya
AU  - Bilenko, Natalia Y.
AU  - Mcnamara, Andrew
AU  - Gontijo-Lopes, Raphael
AU  - Markham, Alex
AU  - Dong, Evyn
AU  - Kay, Jackie
AU  - Saraswat, Manu
AU  - Vytla, Nikhil
AU  - Stark, Luke
T3  - FAccT '23
AB  - Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594134
SP  - 1882
EP  - 1895
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594134
ER  - 

TY  - CONF
TI  - Reconciling governmental use of online targeting with democracy
AU  - Andrić, Katja
AU  - Kasirzadeh, Atoosa
T3  - FAccT '23
AB  - The societal and epistemological implications of online targeted advertising have been scrutinized by AI ethicists, legal scholars, and policymakers alike. However, the government’s use of online targeting and its consequential socio-political ramifications remain under-explored from a critical socio-technical standpoint. This paper investigates the socio-political implications of governmental online targeting, using a case study of the UK government’s application of such techniques for public policy objectives. We argue that this practice undermines democratic ideals, as it engenders three primary concerns — Transparency, Privacy, and Equality — that clash with fundamental democratic doctrines and values. To address these concerns, the paper introduces a preliminary blueprint for an AI governance framework that harmonizes governmental use of online targeting with certain democratic principles. Furthermore, we advocate for the creation of an independent, non-governmental regulatory body responsible for overseeing the process and monitoring the government’s use of online targeting, a critical measure for preserving democratic values.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594133
SP  - 1871
EP  - 1881
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594133
KW  - targeted advertising
KW  - public policy
KW  - AI governance
KW  - democracy
KW  - democracy and AI
KW  - governmental online targeting
ER  - 

TY  - CONF
TI  - Disparities in text-to-image model concept possession across languages
AU  - Saxon, Michael
AU  - Wang, William Yang
T3  - FAccT '23
AB  - We propose the notion of conceptual possession in generative text-to-image (T2I) systems, wherein a model is considered to possess a concept if it can generate a distinctive, correct, and self-consistent population of images for a simple prompt containing that concept. We use this idea to develop a model benchmark of multilingual parity in conceptual possession across a set of almost 200 tangible nouns across 7 languages: English, Spanish, German, Chinese, Japanese, Hebrew, and Indonesian. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and that despite its simplicity our method captures the necessary conditions for the impressive “creative” generative abilities users expect from T2I models. Our benchmark will guide future work in reducing disparities across languages, improving accessibility of these technologies.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594123
SP  - 1870
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594123
KW  - text-to-image models
KW  - benchmark
KW  - dall-e
KW  - multilingual accessibility
KW  - stable diffusion
ER  - 

TY  - CONF
TI  - Achieving diversity in counterfactual explanations: a review and discussion
AU  - Laugel, Thibault
AU  - Jeyasothy, Adulam
AU  - Lesot, Marie-Jeanne
AU  - Marsala, Christophe
AU  - Detyniecki, Marcin
T3  - FAccT '23
AB  - In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594122
SP  - 1859
EP  - 1869
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594122
KW  - interpretability
KW  - XAI
KW  - explainability
KW  - transparency
KW  - counterfactual explanations
KW  - review
KW  - survey
KW  - actionable recourse
KW  - diversity.
ER  - 

TY  - CONF
TI  - Legal taxonomies of machine bias: Revisiting direct discrimination
AU  - Binns, Reuben
AU  - Adams-Prassl, Jeremias
AU  - Kelly-Lyth, Aislinn
T3  - FAccT '23
AB  - Previous literature on ‘fair’ machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594121
SP  - 1850
EP  - 1858
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594121
ER  - 

TY  - CONF
TI  - Bias as boundary object: Unpacking the politics of an austerity algorithm using bias frameworks
AU  - Grill, Gabriel
AU  - Fischer, Fabian
AU  - Cech, Florian
T3  - FAccT '23
AB  - Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis "technical biases," like issues around measurement, rigidity, and coarseness of variables, "emergent biases," such as disruptive events that change the labor market, and, finally, "preexisting biases," like the use of variables that act as proxies for inequality. Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined “unbiased" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594120
SP  - 1838
EP  - 1849
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594120
KW  - algorithmic bias
KW  - infrastructure studies
KW  - job seeker profiling
KW  - public employment services
ER  - 

TY  - CONF
TI  - The slow violence of surveillance capitalism: How online behavioral advertising harms people
AU  - Wu, Yuxi
AU  - Bice, Sydney
AU  - Edwards, W. Keith
AU  - Das, Sauvik
T3  - FAccT '23
AB  - People’s negative reactions to online behavioral advertising (OBA) are well-documented. However, past work has primarily focused on cataloguing these reactions and exploring how to change them, rather than understanding the ways these negative reactions affect people’s lived experiences. Drawing upon scholarship on socio-technical harms in human-computer interaction and computer-supported cooperative work, we investigate and categorize the different ways people report having been harmed by OBA. Through an online survey with 420 participants, we identified four key harms arising from OBA: psychological distress, loss of autonomy, constriction of user behavior, and algorithmic marginalization and traumatization. We next discuss the “slow violence” inflicted by OBA and the normalization of people’s affective discomfort with OBA, and how the two can present an opportunity for researchers to re-conceptualize OBA—and the invasive data practices it entails—as not just abstractly concerning to people, but as actively harmful.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594119
SP  - 1826
EP  - 1837
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594119
ER  - 

TY  - CONF
TI  - Fairness auditing in urban decisions using LP-based data combination
AU  - Yang, Jingyi
AU  - Miller, Joel
AU  - Ohannessian, Mesrob
T3  - FAccT '23
AB  - Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594118
SP  - 1817
EP  - 1825
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594118
KW  - disparate impact
KW  - proxy variables
KW  - data combination
KW  - fairness auditing
KW  - linear programming
KW  - urban data
ER  - 

TY  - CONF
TI  - Detection and mitigation of algorithmic bias via predictive parity
AU  - DiCiccio, Cyrus
AU  - Hsu, Brian
AU  - Yu, Yinyin
AU  - Nandy, Preetam
AU  - Basu, Kinjal
T3  - FAccT '23
AB  - Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594117
SP  - 1801
EP  - 1816
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594117
KW  - algorithmic fairness
KW  - dependent data
KW  - mitigation of bias
KW  - testing for fairness
ER  - 

TY  - CONF
TI  - On the impact of machine learning randomness on group fairness
AU  - Ganesh, Prakhar
AU  - Chang, Hongyan
AU  - Strobel, Martin
AU  - Shokri, Reza
T3  - FAccT '23
AB  - Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594116
SP  - 1789
EP  - 1800
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594116
KW  - fairness
KW  - evaluation
KW  - neural networks
KW  - randomness in training
ER  - 

TY  - CONF
TI  - Discrimination through image selection by job advertisers on facebook
AU  - Nagaraj Rao, Varun
AU  - Korolova, Aleksandra
T3  - FAccT '23
AB  - Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools. In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery – through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594115
SP  - 1772
EP  - 1788
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594115
ER  - 

TY  - CONF
TI  - Skin deep: Investigating subjectivity in skin tone annotations for computer vision benchmark datasets
AU  - Barrett, Teanna
AU  - Chen, Quanze
AU  - Zhang, Amy
T3  - FAccT '23
AB  - To investigate the well-observed racial disparities in computer vision systems that analyze images of humans, researchers have turned to skin tone as a more objective annotation than race metadata for fairness performance evaluations. However, the current state of skin tone annotation procedures is highly varied. For instance, researchers use a range of untested scales and skin tone categories, have unclear annotation procedures, and provide inadequate analyses of uncertainty. In addition, little attention is paid to the positionality of the humans involved in the annotation process—both designers and annotators alike—and the historical and sociological context of skin tone in the United States. Our work is the first to investigate the skin tone annotation process as a sociotechnical project. We surveyed recent skin tone annotation procedures and conducted annotation experiments to examine how subjective understandings of skin tone are embedded in skin tone annotation procedures. Our systematic literature review revealed the uninterrogated association between skin tone and race and the limited effort to analyze annotator uncertainty in current procedures for skin tone annotation in computer vision evaluation. Our experiments demonstrated that design decisions in the annotation procedure such as the order in which the skin tone scale is presented or additional context in the image (i.e., presence of a face) significantly affected the resulting inter-annotator agreement and individual uncertainty of skin tone annotations. We call for greater reflexivity in the design, analysis, and documentation of procedures for evaluation using skin tone.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594114
SP  - 1757
EP  - 1771
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594114
KW  - computer vision
KW  - facial recognition
KW  - model evaluation
KW  - fairness benchmark datasets
KW  - skin tone annotation
ER  - 

TY  - CONF
TI  - Using supervised learning to estimate inequality in the size and persistence of income shocks
AU  - Bruns-Smith, David
AU  - Feller, Avi
AU  - Nakamura, Emi
T3  - FAccT '23
AB  - Household responses to income shocks are important drivers of financial fragility, the evolution of wealth inequality, and the effectiveness of fiscal and monetary policy. Traditional approaches to measuring the size and persistence of income shocks are based on restrictive econometric models that impose strong homogeneity across households and over time. In this paper, we propose a more flexible, machine learning framework for estimating income shocks that allows for variation across all observable features and time horizons. First, we propose non-parametric estimands for shocks and shock persistence. We then show how to estimate these quantities by using off-the-shelf supervised learning tools to approximate the conditional expectation of future income given present information. We solve this income prediction problem in a large Icelandic administrative dataset, and then use the estimated shocks to document several features of labor income risk in Iceland that are not captured by standard economic income models.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594113
SP  - 1747
EP  - 1756
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594113
KW  - uncertainty quantification
KW  - time series forecasting
KW  - income inequality
ER  - 

TY  - CONF
TI  - Representation online matters: Practical end-to-end diversification in search and recommender systems
AU  - Silva, Pedro
AU  - Juneja, Bhawna
AU  - Desai, Shloka
AU  - Singh, Ashudeep
AU  - Fawaz, Nadia
T3  - FAccT '23
AB  - As the use of online platforms continues to grow across all demographics, users often express a desire to feel represented in the content. To improve representation in search results and recommendations, we introduce end-to-end diversification, ensuring that diverse content flows throughout the various stages of these systems, from retrieval to ranking. We develop, experiment, and deploy scalable diversification mechanisms in multiple production surfaces on the Pinterest platform, including Search, Related Products, and New User Homefeed, to improve the representation of different skin tones in beauty and fashion content. Diversification in production systems includes three components: identifying requests that will trigger diversification, ensuring diverse content is retrieved from the large content corpus during the retrieval stage, and finally, balancing the diversity-utility trade-off in a self-adjusting manner in the ranking stage. Our approaches, which evolved from using Strong-OR logical operator to bucketized retrieval at the retrieval stage and from greedy re-rankers to multi-objective optimization using determinantal point processes for the ranking stage, balances diversity and utility while enabling fast iterations and scalable expansion to diversification over multiple dimensions. Our experiments indicate that these approaches significantly improve diversity metrics, with a neutral to a positive impact on utility metrics and improved user satisfaction, both qualitatively and quantitatively, in production.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594112
SP  - 1735
EP  - 1746
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594112
KW  - Recommender Systems
KW  - Representation
KW  - Diversity
KW  - DPP
KW  - Inclusive
KW  - Online Platforms.
KW  - Search
KW  - Skin Tone
ER  - 

TY  - CONF
TI  - Capturing humans’ mental models of AI: An item response theory approach
AU  - Kelly, Markelle
AU  - Kumar, Aakriti
AU  - Smyth, Padhraic
AU  - Steyvers, Mark
T3  - FAccT '23
AB  - Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate’s performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people’s perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants’ own self-perception. Our results indicate that people expect AI agents’ performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594111
SP  - 1723
EP  - 1734
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594111
KW  - mental models
KW  - human-AI interaction
KW  - theory of mind
ER  - 

TY  - CONF
TI  - Representation, self-determination, and refusal: Queer people’s experiences with targeted advertising
AU  - Sampson, Princess
AU  - Encarnacion, Ro
AU  - Metaxa, Danaë
T3  - FAccT '23
AB  - Targeted online advertising systems increasingly draw scrutiny for the surveillance underpinning their collection of people’s private data, and subsequent automated categorization and inference. The experiences of LGBTQ+ people, whose identities call into question dominant assumptions about who is seen as “normal,” and deserving of privacy, autonomy, and the right to self-determination, are a fruitful site for exploring the impacts of ad targeting. We conducted semi-structured interviews with LGBTQ+ individuals (N=18) to understand their experiences with online advertising, their perceptions of ad targeting, and the interplay of these systems with their queerness and other identities. Our results reflect participants’ overall negative experiences with online ad content—they described it as stereotypical and tokenizing in its lack of diversity and nuance. But their desires for better ad content also clashed with their more fundamental distrust and rejection of the non-consensual and extractive nature of ad targeting. They voiced privacy concerns about continuous data aggregation and behavior tracking, a desire for greater control over their data and attention, and even the right to opt-out entirely. Drawing on scholarship from queer and feminist theory, we explore targeted ads’ homonormativity in their failure to represent multiply-marginalized queer people, the harms of automated inference and categorization to identity formation and self-determination, and the theory of refusal underlying participants’ queer visions for a better online experience.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594110
SP  - 1711
EP  - 1722
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594110
KW  - targeted advertising
KW  - homonormativity
KW  - LGBTQ+ populations
KW  - refusal
ER  - 

TY  - CONF
TI  - Bias against 93 stigmatized groups in masked language models and downstream sentiment classification tasks
AU  - Mei, Katelyn
AU  - Fereidooni, Sonia
AU  - Caliskan, Aylin
T3  - FAccT '23
AB  - Warning: The content of this paper may be upsetting or triggering.The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence "They are people who have less than a high school education." is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594109
SP  - 1699
EP  - 1710
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594109
KW  - AI ethics
KW  - language models
KW  - sentiment classification
KW  - AI bias
KW  - representation learning
KW  - prompting
KW  - stigma in language models
ER  - 

TY  - CONF
TI  - Help or hinder? Evaluating the impact of fairness metrics and algorithms in visualizations for consensus ranking
AU  - Shrestha, Hilson
AU  - Cachel, Kathleen
AU  - Alkhathlan, Mallak
AU  - Rundensteiner, Elke
AU  - Harrison, Lane
T3  - FAccT '23
AB  - For applications where multiple stakeholders provide recommendations, a fair consensus ranking must not only ensure that the preferences of rankers are well represented, but must also mitigate disadvantages among socio-demographic groups in the final result. However, there is little empirical guidance on the value or challenges of visualizing and integrating fairness metrics and algorithms into human-in-the-loop systems to aid decision-makers. In this work, we design a study to analyze the effectiveness of integrating such fairness metrics-based visualization and algorithms. We explore this through a task-based crowdsourced experiment comparing an interactive visualization system for constructing consensus rankings, ConsensusFuse, with a similar system that includes visual encodings of fairness metrics and fair-rank generation algorithms, FairFuse. We analyze the measure of fairness, agreement of rankers’ decisions, and user interactions in constructing the fair consensus ranking across these two systems. In our study with 200 participants, results suggest that providing these fairness-oriented support features nudges users to align their decision with the fairness metrics while minimizing the tedious process of manually having to amend the consensus ranking. We discuss the implications of these results for the design of next-generation fairness oriented-systems and along with emerging directions for future research.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594108
SP  - 1685
EP  - 1698
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594108
KW  - fairness
KW  - visualization
KW  - empirical study
ER  - 

TY  - CONF
TI  - Cross-institutional transfer learning for educational models: Implications for model performance, fairness, and equity
AU  - Gardner, Joshua
AU  - Yu, Renzhe
AU  - Nguyen, Quan
AU  - Brooks, Christopher
AU  - Kizilcec, Rene
T3  - FAccT '23
AB  - Modern machine learning increasingly supports paradigms that are multi-institutional (using data from multiple institutions during training) or cross-institutional (using models from multiple institutions for inference), but the empirical effects of these paradigms are not well understood. This study investigates cross-institutional learning via an empirical case study in higher education. We propose a framework and metrics for assessing the utility and fairness of student dropout prediction models that are transferred across institutions. We examine the feasibility of cross-institutional transfer under real-world data- and model-sharing constraints, quantifying model biases for intersectional student identities, characterizing potential disparate impact due to these biases, and investigating the impact of various cross-institutional ensembling approaches on fairness and overall model performance. We perform this analysis on data representing over 200,000 enrolled students annually from four universities without sharing training data between institutions. We find that a simple zero-shot cross-institutional transfer procedure can achieve similar performance to locally-trained models for all institutions in our study, without sacrificing model fairness. We also find that stacked ensembling provides no additional benefits to overall performance or fairness compared to either a local model or the zero-shot transfer procedure we tested. We find no evidence of a fairness-accuracy tradeoff across dozens of models and transfer schemes evaluated. Our auditing procedure also highlights the importance of intersectional fairness analysis, revealing performance disparities at the intersection of sensitive identity groups that are concealed under one-dimensional analysis.1
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594107
SP  - 1664
EP  - 1684
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594107
KW  - Transfer Learning
KW  - Education
KW  - Algorithmic Fairness
KW  - Intersectionality
KW  - Dropout Prediction
ER  - 

TY  - CONF
TI  - The many faces of fairness: Exploring the institutional logics of multistakeholder microlending recommendation
AU  - Smith, Jessie J.
AU  - Buhayh, Anas
AU  - Kathait, Anushka
AU  - Ragothaman, Pradeep
AU  - Mattei, Nicholas
AU  - Burke, Robin
AU  - Voida, Amy
T3  - FAccT '23
AB  - Recommender systems have a variety of stakeholders. Applying concepts of fairness in such systems requires attention to stakeholders’ complex and often-conflicting needs. Since fairness is socially constructed, there are numerous definitions, both in the social science and machine learning literatures. Still, it is rare for machine learning researchers to develop their metrics in close consideration of their social context. More often, standard definitions are adopted and assumed to be applicable across contexts and stakeholders. Our research starts with a recommendation context and then seeks to understand the breadth of the fairness considerations of associated stakeholders. In this paper, we report on the results of a semi-structured interview study with 23 employees who work for the Kiva microlending platform. We characterize the many different ways in which they enact and strive toward fairness for microlending recommendations in their own work, uncover the ways in which these different enactments of fairness are in tension with each other, and identify how stakeholders are differentially prioritized. Finally, we reflect on the implications of this study for future research and for the design of multistakeholder recommender systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594106
SP  - 1652
EP  - 1663
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594106
KW  - recommender systems
KW  - fairness
KW  - institutional logics
KW  - international development
KW  - microlending
KW  - multistakeholder recommendation
ER  - 

TY  - CONF
TI  - Reducing access disparities in networks using edge Augmentation
AU  - Bashardoust, Ashkan
AU  - Friedler, Sorelle
AU  - Scheidegger, Carlos
AU  - Sullivan, Blair D.
AU  - Venkatasubramanian, Suresh
T3  - FAccT '23
AB  - In social networks, a node’s position is, in and of itself, a form of social capital. Better-positioned members not only benefit from (faster) access to diverse information, but innately have more potential influence on information spread. Structural biases often arise from network formation, and can lead to significant disparities in information access based on position. Further, processes such as link recommendation can exacerbate this inequality by relying on network structure to augment connectivity. In this paper, we argue that one can understand and quantify this social capital through the lens of information flow in the network. In contrast to prior work, we consider the setting where all nodes may be sources of distinct information, and a node’s (dis)advantage takes into account its ability to access all information available on the network, not just that from a single source. We introduce three new measures of advantage (broadcast, influence, and control), which are quantified in terms of position in the network using access signatures – vectors that represent a node’s ability to share information with each other node in the network. We then consider the problem of improving equity by making interventions to increase the access of the least-advantaged nodes. Since all nodes are already sources of information in our model, we argue that edge augmentation is most appropriate for mitigating bias in the network structure, and frame a budgeted intervention problem for maximizing broadcast (minimum pairwise access) over the network. Finally, we propose heuristic strategies for selecting edge augmentations and empirically evaluate their performance on a corpus of real-world social networks. We demonstrate that a small number of interventions can not only significantly increase the broadcast measure of access for the least-advantaged nodes (over 5 times more than random), but also simultaneously improve the minimum influence. Additional analysis shows that edge augmentations targeted at improving minimum pairwise access can also dramatically shrink the gap in advantage between nodes (over ) and reduce disparities between their access signatures.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594105
SP  - 1635
EP  - 1651
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594105
KW  - algorithmic fairness
KW  - social networks
KW  - edge interventions
KW  - information access
ER  - 

TY  - CONF
TI  - Interrogating the T in FAccT
AU  - Corbett, Eric
AU  - Denton, Emily
T3  - FAccT '23
AB  - Fairness, accountability, and transparency are the three conceptual foundations of the FAccT conference. Transparency, however, has yet to be scrutinized to the same degree as accountability and fairness. As a result, we don't know: What does this community mean when it talks about transparency? How are we doing transparency? And to what ends? What commitments does (or should) the T in FAccT signify? This paper interrogates the T in FAccT using perspectives from critical transparency literature. Subsequently, we argue that FAccT might be better off dropping the T from its title for two reasons: (1) transparency can often be counterproductive to FAccT's primary objectives and (2) it is misleading as FAccT is mainly preoccupied with explainability rather than actual transparency. If we want to keep the T, we need to reframe how we think about and do transparency by making transparency contingent, reclaiming it from explainability, and bringing people into transparency processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594104
SP  - 1624
EP  - 1634
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594104
KW  - Interpretability
KW  - Explainability
KW  - Transparency
KW  - Critical Transparency Studies
ER  - 

TY  - CONF
TI  - Arbitrary decisions are a hidden cost of differentially private training
AU  - Kulynych, Bogdan
AU  - Hsu, Hsiang
AU  - Troncoso, Carmela
AU  - Calmon, Flavio P.
T3  - FAccT '23
AB  - Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze—both theoretically and through extensive experiments—the predictive-multiplicity cost of three DP-ensuring algorithms: output perturbation, objective perturbation, and DP-SGD. We demonstrate that the degree of predictive multiplicity rises as the level of privacy increases, and is unevenly distributed across individuals and demographic groups in the data. Because randomness used to ensure DP during training explains predictions for some examples, our results highlight a fundamental challenge to the justifiability of decisions supported by differentially-private models in high-stakes settings. We conclude that practitioners should audit the predictive multiplicity of their DP-ensuring algorithms before deploying them in applications of individual-level consequence.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594103
SP  - 1609
EP  - 1623
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594103
ER  - 

TY  - CONF
TI  - Improving fairness in AI models on electronic health records: The case for federated learning methods
AU  - Poulain, Raphael
AU  - Bin Tarek, Mirza Farhan
AU  - Beheshti, Rahmatollah
T3  - FAccT '23
AB  - Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models’ overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method’s superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). Our code is available at https://github.com/healthylaife/FairFedAvg.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594102
SP  - 1599
EP  - 1608
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594102
KW  - Federated Learning
KW  - Algorithmic Fairness
KW  - Adversarial Fairness
ER  - 

TY  - CONF
TI  - Counterfactual prediction under outcome measurement error
AU  - Guerdan, Luke
AU  - Coston, Amanda
AU  - Holstein, Kenneth
AU  - Wu, Zhiwei Steven
T3  - FAccT '23
AB  - Across domains such as medicine, employment, and criminal justice, predictive models often target labels that imperfectly reflect the outcomes of interest to experts and policymakers. For example, clinical risk assessments deployed to inform physician decision-making often predict measures of healthcare utilization (e.g., costs, hospitalization) as a proxy for patient medical need. These proxies can be subject to outcome measurement error when they systematically differ from the target outcome they are intended to measure. However, prior modeling efforts to characterize and mitigate outcome measurement error overlook the fact that the decision being informed by a model often serves as a risk-mitigating intervention that impacts the target outcome of interest and its recorded proxy. Thus, in these settings, addressing measurement error requires counterfactual modeling of treatment effects on outcomes. In this work, we study intersectional threats to model reliability introduced by outcome measurement error, treatment effects, and selection bias from historical decision-making policies. We develop an unbiased risk minimization method which, given knowledge of proxy measurement error properties, corrects for the combined effects of these challenges. We also develop a method for estimating treatment-dependent measurement error parameters when these are unknown in advance. We demonstrate the utility of our approach theoretically and via experiments on real-world data from randomized controlled trials conducted in healthcare and employment domains. As importantly, we demonstrate that models correcting for outcome measurement error or treatment effects alone suffer from considerable reliability limitations. Our work underscores the importance of considering intersectional threats to model validity during the design and evaluation of predictive models for decision support.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594101
SP  - 1584
EP  - 1598
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594101
KW  - causal inference
KW  - validity
KW  - measurement
KW  - algorithmic decision support
KW  - model evaluation
ER  - 

TY  - CONF
TI  - The misuse of AUC: What high impact risk assessment gets wrong
AU  - Kwegyir-Aggrey, Kweku
AU  - Gerchick, Marissa
AU  - Mohan, Malika
AU  - Horowitz, Aaron
AU  - Venkatasubramanian, Suresh
T3  - FAccT '23
AB  - When determining which machine learning model best performs some high impact risk assessment task, practitioners commonly use the Area under the Curve (AUC) to defend and validate their model choices. In this paper, we argue that the current use and understanding of AUC as a model performance metric misunderstands the way the metric was intended to be used. To this end, we characterize the misuse of AUC and illustrate how this misuse negatively manifests in the real world across several risk assessment domains. We locate this disconnect in the way the original interpretation of AUC has shifted over time to the point where issues pertaining to decision thresholds, class balance, statistical uncertainty, and protected groups remain unaddressed by AUC-based model comparisons, and where model choices that should be the purview of policymakers are hidden behind the veil of mathematical rigor. We conclude that current model validation practices involving AUC are not robust, and often invalid.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594100
SP  - 1570
EP  - 1583
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594100
ER  - 

TY  - CONF
TI  - The progression of disparities within the criminal justice system: Differential enforcement and risk assessment instruments
AU  - Zilka, Miri
AU  - Fogliato, Riccardo
AU  - Hron, Jiri
AU  - Butcher, Bradley
AU  - Ashurst, Carolyn
AU  - Weller, Adrian
T3  - FAccT '23
AB  - Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person’s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5–2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5–11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594099
SP  - 1553
EP  - 1569
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594099
ER  - 

TY  - CONF
TI  - Auditing cross-cultural consistency of human-annotated labels for recommendation systems
AU  - Pang, Rock Yuren
AU  - Cenatempo, Jack
AU  - Graham, Franklyn
AU  - Kuehn, Bridgette
AU  - Whisenant, Maddy
AU  - Botchway, Portia
AU  - Stone Perez, Katie
AU  - Koenecke, Allison
T3  - FAccT '23
AB  - Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models—based on these labels—are applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are “consistently conceptualized” across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594098
SP  - 1531
EP  - 1552
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594098
KW  - causal inference
KW  - linguistic bias
KW  - label bias
KW  - cultural bias
KW  - human annotations
KW  - poststratification
KW  - recommendation systems
ER  - 

TY  - CONF
TI  - Personalized pricing with group fairness constraint
AU  - Chen, Xin
AU  - Xu, Zexing
AU  - Zhao, Zishuo
AU  - Zhou, Yuan
T3  - FAccT '23
AB  - In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers’ features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594097
SP  - 1520
EP  - 1530
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594097
KW  - group fairness
KW  - social welfare
KW  - statistical parity
KW  - personalized pricing
ER  - 

TY  - CONF
TI  - What's fair is… fair? Presenting JustEFAB, an ethical framework for operationalizing medical ethics and social justice in the integration of clinical machine learning: JustEFAB
AU  - Mccradden, Melissa
AU  - Odusi, Oluwadara
AU  - Joshi, Shalmali
AU  - Akrout, Ismail
AU  - Ndlovu, Kagiso
AU  - Glocker, Ben
AU  - Maicas, Gabriel
AU  - Liu, Xiaoxuan
AU  - Mazwi, Mjaye
AU  - Garnett, Tee
AU  - Oakden-Rayner, Lauren
AU  - Alfred, Myrtede
AU  - Sihlahla, Irvine
AU  - Shafei, Oswa
AU  - Goldenberg, Anna
T3  - FAccT '23
AB  - The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594096
SP  - 1505
EP  - 1519
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594096
KW  - accountability
KW  - fairness
KW  - ethics
KW  - algorithmic bias
KW  - clinical machine learning
KW  - health policy
KW  - healthcare
KW  - justice
KW  - organizational ethics
KW  - safe deployment
ER  - 

TY  - CONF
TI  - Easily accessible text-to-image generation amplifies demographic stereotypes at large scale
AU  - Bianchi, Federico
AU  - Kalluri, Pratyusha
AU  - Durmus, Esin
AU  - Ladhak, Faisal
AU  - Cheng, Myra
AU  - Nozza, Debora
AU  - Hashimoto, Tatsunori
AU  - Jurafsky, Dan
AU  - Zou, James
AU  - Caliskan, Aylin
T3  - FAccT '23
AB  - Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594095
SP  - 1493
EP  - 1504
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594095
ER  - 

TY  - CONF
TI  - Examining risks of racial biases in NLP tools for child protective services
AU  - Field, Anjalie
AU  - Coston, Amanda
AU  - Gandhi, Nupoor
AU  - Chouldechova, Alexandra
AU  - Putnam-Hornstein, Emily
AU  - Steier, David
AU  - Tsvetkov, Yulia
T3  - FAccT '23
AB  - Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594094
SP  - 1479
EP  - 1492
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594094
KW  - bias
KW  - NLP
KW  - CPS
KW  - race
KW  - child protection system
KW  - text processing
ER  - 

TY  - CONF
TI  - Co-designing for transparency: Lessons from building a document organization tool in the criminal justice domain
AU  - Nigatu, Hellina Hailu
AU  - Pickoff-White, Lisa
AU  - Canny, John
AU  - Chasins, Sarah
T3  - FAccT '23
AB  - Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594093
SP  - 1463
EP  - 1478
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594093
KW  - Co-Design
KW  - Collaborative Design
KW  - Document Organization
KW  - User-Centered Design
ER  - 

TY  - CONF
TI  - Taking algorithms to courts: A relational approach to algorithmic accountability
AU  - Metcalf, Jacob
AU  - Singh, Ranjit
AU  - Moss, Emanuel
AU  - Tafesse, Emnet
AU  - Watkins, Elizabeth Anne
T3  - FAccT '23
AB  - In widely used sociological descriptions of how accountability is structured through institutions, an “actor” (e.g., the developer) is accountable to a “forum” (e.g., regulatory agencies) empowered to pass judgements on and demand changes from the actor or enforce sanctions. However, questions about structuring accountability persist: why and how is a forum compelled to keep making demands of the actor when such demands are called for? To whom is a forum accountable in the performance of its responsibilities, and how can its practices and decisions be contested? In the context of algorithmic accountability, we contend that a robust accountability regime requires a triadic relationship, wherein the forum is also accountable to another entity: the public(s). Typically, as is the case with environmental impact assessments, public(s) make demands upon the forum's judgements and procedures through the courts, thereby establishing a minimum standard of due diligence. However, core challenges relating to: (1) lack of documentation, (2) difficulties in claiming standing, and (3) struggles around admissibility of expert evidence on and achieving consensus over the workings of algorithmic systems in adversarial proceedings prevent the public from approaching the courts when faced with algorithmic harms. In this paper, we demonstrate that the courts are the primary route—and the primary roadblock—in the pursuit of redress for algorithmic harms. Courts often find algorithmic harms non-cognizable and rarely require developers to address material claims of harm. To address the core challenges of taking algorithms to court, we develop a relational approach to algorithmic accountability that emphasizes not what the actors do nor the results of their actions, but rather how interlocking relationships of accountability are constituted in a triadic relationship between actors, forums, and public(s). As is the case in other regulatory domains, we believe that impact assessments (and similar accountability documentation) can provide the grounds for contestation between these parties, but only when that triad is structured such that the public(s) are able to cohere around shared experiences and interests, contest the outcomes of algorithmic systems that affect their lives, and make demands upon the other parties. Where courts now find algorithmic harms non-cognizable, an impact assessment regime can potentially create procedural rights to protect substantive rights of the public(s). This would require algorithmic accountability policies currently under consideration to provide the public(s) with adequate standing in courts, and opportunities to access and contest the actor's documentation and the forum's judgments.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594092
SP  - 1450
EP  - 1462
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594092
ER  - 

TY  - CONF
TI  - Group fairness without demographics using social networks
AU  - Liu, David
AU  - Do, Virginie
AU  - Usunier, Nicolas
AU  - Nickel, Maximilian
T3  - FAccT '23
AB  - Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a “group-free" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594091
SP  - 1432
EP  - 1449
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594091
KW  - group fairness
KW  - homophily
KW  - social networks
ER  - 

TY  - CONF
TI  - Navigating the audit landscape: A framework for developing transparent and auditable XR
AU  - Norval, Chris
AU  - Cloete, Richard
AU  - Singh, Jatinder
T3  - FAccT '23
AB  - “Extended reality” (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount – where information revealing and enabling some reconstruction of an XR system’s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why. Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework’s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594090
SP  - 1418
EP  - 1431
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594090
KW  - trust
KW  - transparency
KW  - responsibility
KW  - accountability
KW  - augmented reality
KW  - virtual reality
KW  - auditability
KW  - audit
KW  - reviewability
KW  - mixed reality
ER  - 

TY  - CONF
TI  - Organizational governance of emerging technologies: AI adoption in healthcare
AU  - Kim, Jee Young
AU  - Boag, William
AU  - Gulamali, Freya
AU  - Hasan, Alifia
AU  - Hogg, Henry David Jeffry
AU  - Lifson, Mark
AU  - Mulligan, Deirdre
AU  - Patel, Manesh
AU  - Raji, Inioluwa Deborah
AU  - Sehgal, Ajai
AU  - Shaw, Keo
AU  - Tobey, Danny
AU  - Valladares, Alexandra
AU  - Vidal, David
AU  - Balu, Suresh
AU  - Sendak, Mark
T3  - FAccT '23
AB  - Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594089
SP  - 1396
EP  - 1417
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594089
ER  - 

TY  - CONF
TI  - (anti)-intentional harms: The conceptual pitfalls of emotion AI in education
AU  - DiBerardino, Nathalie
AU  - Stark, Luke
T3  - FAccT '23
AB  - ‘Emotion AI’ is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options—interpretability, technical reform, and non-use—for their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594088
SP  - 1386
EP  - 1395
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594088
KW  - artificial intelligence
KW  - emotion
KW  - education
KW  - affect
KW  - affective computing
KW  - emotion AI
KW  - anti-intentionalism
KW  - Basic Emotion Theory (BET)
KW  - digital classroom
KW  - human capital
KW  - intentionalism
KW  - socio-emotional learning (SEL)
KW  - theories of emotion
ER  - 

TY  - CONF
TI  - Towards a science of human-AI decision making: An overview of design space in empirical human-subject studies
AU  - Lai, Vivian
AU  - Chen, Chacha
AU  - Smith-Renner, Alison
AU  - Liao, Q. Vera
AU  - Tan, Chenhao
T3  - FAccT '23
AB  - AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594087
SP  - 1369
EP  - 1385
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594087
ER  - 

TY  - CONF
TI  - Can querying for bias leak protected attributes? Achieving privacy with smooth sensitivity
AU  - Hamman, Faisal
AU  - Chen, Jiahao
AU  - Dutta, Sanghamitra
T3  - FAccT '23
AB  - Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594086
SP  - 1358
EP  - 1368
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594086
KW  - algorithmic fairness
KW  - differential privacy
KW  - machine learning.
KW  - compliance
KW  - compressed sensing
ER  - 

TY  - CONF
TI  - Fairer together: Mitigating disparate exposure in kemeny rank aggregation
AU  - Cachel, Kathleen
AU  - Rundensteiner, Elke
T3  - FAccT '23
AB  - In social choice, traditional Kemeny rank aggregation combines the preferences of voters, expressed as rankings, into a single consensus ranking without consideration for how this ranking may unfairly affect marginalized groups (i.e., racial or gender). Developing fair rank aggregation methods is critical due to their societal influence in applications prioritizing job applicants, funding proposals, and scheduling medical patients. In this work, we introduce the Fair Exposure Kemeny Aggregation Problem (FairExp-kap) for combining vast and diverse voter preferences into a single ranking that is not only a suitable consensus, but ensures opportunities are not withheld from marginalized groups. In formalizing FairExp-kap, we extend the fairness of exposure notion from information retrieval to the rank aggregation context and present a complimentary metric for voter preference representation. We design algorithms for solving FairExp-kap that explicitly account for position bias, a common ranking-based concern that end-users pay more attention to higher ranked candidates. epik solves FairExp-kap exactly by incorporating non-pairwise fairness of exposure into the pairwise Kemeny optimization; while the approximate epira is a candidate swapping algorithm, that guarantees ranked candidate fairness. Utilizing comprehensive synthetic simulations and six real-world datasets, we show the efficacy of our approach illustrating that we succeed in mitigating disparate group exposure unfairness in consensus rankings, while maximally representing voter preferences.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594085
SP  - 1347
EP  - 1357
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594085
KW  - group fairness
KW  - fair exposure rank aggregation
KW  - rank aggregation
KW  - voting rules
ER  - 

TY  - CONF
TI  - A sociotechnical audit: Assessing police use of facial recognition
AU  - Radiya-Dixit, Evani
AU  - Neff, Gina
T3  - FAccT '23
AB  - Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594084
SP  - 1334
EP  - 1346
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594084
KW  - accountability
KW  - algorithmic audits
KW  - ethical and legal considerations
KW  - facial recognition technology
ER  - 

TY  - CONF
TI  - An empirical analysis of racial categories in the algorithmic fairness literature
AU  - Abdu, Amina A.
AU  - Pasquetto, Irene V.
AU  - Jacobs, Abigail Z.
T3  - FAccT '23
AB  - Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594083
SP  - 1324
EP  - 1333
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594083
KW  - algorithmic fairness
KW  - racial categories
KW  - state race-making
ER  - 

TY  - CONF
TI  - A systematic review of ethics disclosures in predictive mental health research
AU  - Ajmani, Leah Hope
AU  - Chancellor, Stevie
AU  - Mehta, Bijal
AU  - Fiesler, Casey
AU  - Zimmer, Michael
AU  - De Choudhury, Munmun
T3  - FAccT '23
AB  - Applied machine learning (ML) has not yet coalesced on standard practices for research ethics. For ML that predicts mental illness using social media data, ambiguous ethical standards can impact peoples’ lives because of the area’s sensitivity and material consequences on health. Transparency of current ethics practices in research is important to document decision-making and improve research practice. We present a systematic literature review of 129 studies that predict mental illness using social media data and ML, and the ethics disclosures they make in research publications. Rates of disclosure are going up over time, but this trend is slow moving – it will take another eight years for the average paper to have coverage on 75% of studied ethics categories. Certain practices are more readily adopted, or "stickier", over time, though we found prioritization of data-driven disclosures rather than human-centered. These inconsistently reported ethical considerations indicate a gap between what ML ethicists believe ought to be and what actually is done. We advocate for closing this gap through increased transparency of practice and formal mechanisms to support disclosure.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594082
SP  - 1311
EP  - 1323
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594082
KW  - systematic literature review
KW  - ethics
KW  - mental health
KW  - social media
ER  - 

TY  - CONF
TI  - The devil is in the details: Interrogating values embedded in the allegheny family screening tool
AU  - Gerchick, Marissa
AU  - Jegede, Tobi
AU  - Shah, Tarak
AU  - Gutierrez, Ana
AU  - Beiers, Sophie
AU  - Shemtov, Noam
AU  - Xu, Kath
AU  - Samant, Anjana
AU  - Horowitz, Aaron
T3  - FAccT '23
AB  - The design decisions of developers and researchers in creating algorithmic tools — like constructing variables, performing feature selection, and binning model outputs — are sometimes cast as objective technical processes. In reality, these decisions are far from objective, and they are sometimes even made arbitrarily. In this work, we examine how algorithmic design choices can function as policy decisions through an audit of a deployed algorithmic tool, the Allegheny Family Screening Tool (AFST), used to screen calls to a child welfare agency about alleged child neglect in Allegheny County, Pennsylvania. We analyze design decisions in the AFST’s development process related to feature selection, data collection, and post-processing, highlighting three values implicitly embedded in the tool through these decisions. By aggregating risk scores at the household level, the AFST effectively treats families as “risky” by association. In choosing to use training data from the criminal legal system and behavioral health agencies, the AFST prioritizes “making decisions based on as much information as possible,” even when that information is potentially biased across race, disability, and other protected statuses. Finally, by including static features in the model that identify whether a person has ever been affected by the criminal legal system or relied on public benefits, the AFST chooses to mark families in perpetuity, compounding the impacts of systemic discrimination and foreclosing opportunities for recourse for families impacted by the tool. We explore the impacts of these decisions, individually and together, arguing that they function as policy choices that may have discriminatory effects and raise concerns about lack of democratic oversight.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594081
SP  - 1292
EP  - 1310
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594081
KW  - policy
KW  - accountability
KW  - Algorithm
KW  - design
KW  - audit
KW  - values
ER  - 

TY  - CONF
TI  - Diverse perspectives can mitigate political bias in crowdsourced content moderation
AU  - Thebault-Spieker, Jacob
AU  - Venkatagiri, Sukrit
AU  - Mine, Naomi
AU  - Luther, Kurt
T3  - FAccT '23
AB  - In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594080
SP  - 1280
EP  - 1291
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594080
ER  - 

TY  - CONF
TI  - AI regulation is (not) all you need
AU  - Lucaj, Laura
AU  - van der Smagt, Patrick
AU  - Benbouzid, Djalel
T3  - FAccT '23
AB  - The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594079
SP  - 1267
EP  - 1279
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594079
KW  - machine learning
KW  - algorithmic auditing
KW  - AI regulation
KW  - ethical AI
ER  - 

TY  - CONF
TI  - “I’m fully who I am”: Towards centering transgender and non-binary voices to measure biases in open language generation
AU  - Ovalle, Anaelia
AU  - Goyal, Palash
AU  - Dhamala, Jwala
AU  - Jaggers, Zachary
AU  - Chang, Kai-Wei
AU  - Galstyan, Aram
AU  - Zemel, Richard
AU  - Gupta, Rahul
T3  - FAccT '23
AB  - Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594078
SP  - 1246
EP  - 1266
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594078
KW  - Algorithmic Fairness
KW  - AI Fairness Auditing
KW  - Natural Language Generation
KW  - Queer Harms in AI
ER  - 

TY  - CONF
TI  - Enhancing AI fairness through impact assessment in the European Union: a legal and computer science perspective
AU  - Calvi, Alessandra
AU  - Kotzinos, Dimitris
T3  - FAccT '23
AB  - How to protect people from algorithmic harms? A promising solution, although in its infancy, is algorithmic impact assessment (AIA). AIAs are iterative processes used to investigate the possible short and long terms societal impacts of AI systems before their use, but with ongoing monitoring and periodic revisiting even after their implementation. When conducted in a participatory and transparent fashion, they could create bridges across the legal, social and computer science domains, promoting the accountability of the entity performing them as well as public scrutiny. They could enable to re-attach the societal and regulatory context to the mathematical definition of fairness, thus expanding the formalistic approach thereto. Whilst the regulatory framework in the European Union currently lacks the obligation to perform such AIA, some other provisions are expected to play a role in AI development, leading the way towards more widespread adoption of AIA. These include the Data Protection Impact Assessment (DPIA) under the General Data Protection Regulation (GDPR), the risk assessment process under the Digital Services Act (DSA) and the Conformity Assessment (CA) foreseen under the AI Regulation proposal.In this paper, after briefly introducing the plurality of definitions of fairness in the legal, social and computer science domains, and explaining to which extent the current and upcoming legal framework mandates the adoption of fairness metrics, we will illustrate how AIA could create bridges between all these disciplines, allowing us to build fairer AI solutions. We will then recognise the role of DPIA, DSA risk assessment and CA by discussing the contributions they can offer towards AIA but also identify the aspects lacking therein. We will then identify how these assessment provisions could aid the overall technical discussion of introducing and assessing fairness in AI-based models and processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594076
SP  - 1229
EP  - 1245
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594076
ER  - 

TY  - CONF
TI  - Disentangling and operationalizing AI fairness at LinkedIn
AU  - Quiñonero Candela, Joaquin
AU  - Wu, Yuwen
AU  - Hsu, Brian
AU  - Jain, Sakshi
AU  - Ramos, Jennifer
AU  - Adams, Jon
AU  - Hallman, Robert
AU  - Basu, Kinjal
T3  - FAccT '23
AB  - Operationalizing AI fairness at LinkedIn’s scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn’s AI fairness framework, shares the principles that support it, and illustrates their application through a case study. We hope this paper will encourage other big tech companies to join us in sharing their approach to operationalizing AI fairness at scale, so that together we can keep advancing this constantly evolving field.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594075
SP  - 1213
EP  - 1228
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594075
KW  - operationalization
KW  - equity
KW  - AI Fairness strategy
KW  - large-organizational process
ER  - 

TY  - CONF
TI  - Explainability in AI policies: A critical review of communications, reports, regulations, and standards in the EU, US, and UK
AU  - Nannini, Luca
AU  - Balayn, Agathe
AU  - Smith, Adam Leon
T3  - FAccT '23
AB  - Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594074
SP  - 1198
EP  - 1212
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594074
KW  - Explainable AI
KW  - AI policy
KW  - social epistemology
ER  - 

TY  - CONF
TI  - Understanding accountability in algorithmic supply chains
AU  - Cobbe, Jennifer
AU  - Veale, Michael
AU  - Singh, Jatinder
T3  - FAccT '23
AB  - Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by ‘many hands’. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594073
SP  - 1186
EP  - 1197
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594073
KW  - accountability horizon
KW  - AI as a Service
KW  - Algorithmic accountability
KW  - general purpose AI
KW  - political economy
KW  - supply chains
ER  - 

TY  - CONF
TI  - Contrastive language-vision AI models pretrained on web-scraped multimodal data exhibit sexual objectification bias
AU  - Wolfe, Robert
AU  - Yang, Yiwei
AU  - Howe, Bill
AU  - Caliskan, Aylin
T3  - FAccT '23
AB  - Warning: The content of this paper may be upsetting or triggering.Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt; 0.80) and sadness (d &gt; 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of "a [age] year old girl" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594072
SP  - 1174
EP  - 1185
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594072
KW  - gender bias
KW  - AI bias
KW  - generative AI
KW  - AI bias in applications
KW  - AI bias propagation
KW  - language-vision AI
KW  - representation learning
KW  - sexualization
KW  - text-to-image generators
ER  - 

TY  - CONF
TI  - Going public: the role of public participation approaches in commercial AI labs
AU  - Groves, Lara
AU  - Peppin, Aidan
AU  - Strait, Andrew
AU  - Brennan, Jenny
T3  - FAccT '23
AB  - In recent years, discussions of responsible AI practices have seen growing support for ‘participatory AI’ approaches, intended to involve members of the public in the design and development of AI systems. Prior research has identified a lack of standardised methods or approaches for how to use participatory approaches in the AI development process. At present, there is a dearth of evidence on attitudes to and approaches for participation in the sites driving major AI developments: commercial AI labs. Through 12 semi-structured interviews with industry practitioners and subject-matter experts, this paper explores how commercial AI labs understand participatory AI approaches and the obstacles they have faced implementing these practices in the development of AI systems and research. We find that while interviewees view participation as a normative project that helps achieve ‘societally beneficial’ AI systems, practitioners face numerous barriers to embedding participatory approaches in their companies: participation is expensive and resource intensive, it is ‘atomised’ within companies, there is concern about exploitation, there is no incentive to be transparent about its adoption, and it is complicated by a lack of clear context. These barriers result in a piecemeal approach to participation that confers no decision-making power to participants and has little ongoing impact for AI labs. This paper’s contribution is to provide novel empirical research on the implementation of public participation in commercial AI labs, and shed light on the current challenges of using participatory approaches in this context.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594071
SP  - 1162
EP  - 1173
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594071
KW  - artificial intelligence
KW  - commercial AI
KW  - participatory AI
ER  - 

TY  - CONF
TI  - The dimensions of data labor: A road map for researchers, activists, and policymakers to empower data producers
AU  - Li, Hanlin
AU  - Vincent, Nicholas
AU  - Chancellor, Stevie
AU  - Hecht, Brent
T3  - FAccT '23
AB  - Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as “data labor”, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data’s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594070
SP  - 1151
EP  - 1161
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594070
KW  - data leverage
KW  - empowerment
KW  - user-generated data
ER  - 

TY  - CONF
TI  - The role of explainable AI in the context of the AI Act
AU  - Panigutti, Cecilia
AU  - Hamon, Ronan
AU  - Hupont, Isabelle
AU  - Fernandez Llorca, David
AU  - Fano Yela, Delia
AU  - Junklewitz, Henrik
AU  - Scalzo, Salvatore
AU  - Mazzini, Gabriele
AU  - Sanchez, Ignacio
AU  - Soler Garrido, Josep
AU  - Gomez, Emilia
T3  - FAccT '23
AB  - The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594069
SP  - 1139
EP  - 1150
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594069
KW  - XAI
KW  - transparency
KW  - explainable artificial intelligence
KW  - trustworthy AI
KW  - AI Act
KW  - EU regulation
KW  - human oversight
ER  - 

TY  - CONF
TI  - On the richness of calibration
AU  - Höltgen, Benedikt
AU  - Williamson, Robert C
T3  - FAccT '23
AB  - Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group errors, generalising previously proposed calibration scores. Complementary to such population-level scores, we explore calibration scores at the individual level and analyse their relationship to choices of grouping. We draw on these insights to introduce and axiomatise fairness deviation measures for population-level scores. We demonstrate that with appropriate choices of grouping, these novel global fairness scores can provide notions of (sub-)group or individual fairness.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594068
SP  - 1124
EP  - 1138
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594068
KW  - fairness
KW  - evaluation
KW  - forecasting
KW  - calibration
KW  - multicalibration
ER  - 

TY  - CONF
TI  - Regulating ChatGPT and other large generative AI models
AU  - Hacker, Philipp
AU  - Engel, Andreas
AU  - Mauer, Marco
T3  - FAccT '23
AB  - Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594067
SP  - 1112
EP  - 1123
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594067
ER  - 

TY  - CONF
TI  - Emotions and dynamic assemblages: A study of automated social security using qualitative longitudinal research
AU  - Currie, Morgan
AU  - Podoletz, Lena
T3  - FAccT '23
AB  - In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to. We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area. Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users’ lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users’ interactions with ADMs shapes their actions responding to these systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594066
SP  - 1101
EP  - 1111
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594066
KW  - Qualitative Research
KW  - Automated Social Security
KW  - Digital Social Security
KW  - Interviews
KW  - Longitudinal Research
ER  - 

TY  - CONF
TI  - More data types more problems: A temporal analysis of complexity, stability, and sensitivity in privacy policies
AU  - Lovato, Juniper
AU  - Mueller, Philip
AU  - Suchdev, Parisa
AU  - Dodds, Peter
T3  - FAccT '23
AB  - Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594065
SP  - 1088
EP  - 1100
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594065
KW  - Privacy
KW  - Data Science
KW  - NLP
KW  - Data Ethics
KW  - Data Privacy
KW  - Networks
KW  - Privacy Policies
ER  - 

TY  - CONF
TI  - Co-design perspectives on algorithm transparency reporting: Guidelines and prototypes
AU  - Luria, Michal
T3  - FAccT '23
AB  - Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes’ strengths and weaknesses, and areas of exploration for future work.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594064
SP  - 1076
EP  - 1087
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594064
ER  - 

TY  - CONF
TI  - Ethical considerations in the early detection of Alzheimer's disease using speech and AI
AU  - Petti, Ulla
AU  - Nyrup, Rune
AU  - Skopek, Jeffrey M.
AU  - Korhonen, Anna
T3  - FAccT '23
AB  - While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594063
SP  - 1062
EP  - 1075
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594063
ER  - 

TY  - CONF
TI  - Which stereotypes are moderated and under-moderated in search engine autocompletion?
AU  - Leidinger, Alina
AU  - Rogers, Richard
T3  - FAccT '23
AB  - Warning: This paper contains content that may be offensive or upsetting.Language technologies that perpetuate stereotypes actively cement social hierarchies. This study enquires into the moderation of stereotypes in autocompletion results by Google, DuckDuckGo and Yahoo! We investigate the moderation of derogatory stereotypes for social groups, examining the content and sentiment of the autocompletions. We thereby demonstrate which categories are highly moderated (i.e., sexual orientation, religious affiliation, political groups and communities or peoples) and which less so (age and gender), both overall and per engine. We found that under-moderated categories contain results with negative sentiment and derogatory stereotypes. We also identify distinctive moderation strategies per engine, with Google and DuckDuckGo moderating greatly and Yahoo! being more permissive. The research has implications for both moderation of stereotypes in commercial autocompletion tools, as well as large language models in NLP, particularly the question of the content deserving of moderation.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594062
SP  - 1049
EP  - 1061
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594062
KW  - debiasing
KW  - content moderation
KW  - natural language generation
KW  - stereotypes
KW  - Google Autocompletion
KW  - search engine autocompletion
ER  - 

TY  - CONF
TI  - Measuring and mitigating voting access disparities: a study of race and polling locations in Florida and North Carolina
AU  - Abbasi, Mohsen
AU  - Barrett, Calvin
AU  - Lum, Kristian
AU  - Friedler, Sorelle A.
AU  - Venkatasubramanian, Suresh
T3  - FAccT '23
AB  - Voter suppression and associated racial disparities in access to voting are long-standing civil rights concerns in the United States. A history of violent explicit discouragement has shifted to more subtle access limitations that can include long lines and wait times, long travel times to reach a polling station, and other logistical barriers to voting. Our focus in this work is on quantifying disparities in voting access pertaining to the overall time-to-vote, and how they could be remedied via a better choice of polling location or provisioning more sites where voters can cast ballots. However, appropriately calibrating access disparities is difficult because of the need to account for factors such as population density and different community expectations for reasonable travel times. In this paper, we perform one of the first large-scale studies of voter access to polling locations, using real-world voter data from Florida and North Carolina in the 2020 general election. We develop a methodology for the calibrated measurement of disparities in polling location "load" and distance to polling locations based on a novel normalized distance metric to model the voter experience of distance. We find that voter turnout is reduced when this normalized distance to polling locations increases, and that non-white voters had to travel further to the polls in Florida (using this normalized distance) than White voters. We also introduce algorithms, with modifications to handle scale, that can reduce these disparities by suggesting new polling locations from a given list of identified public locations (including schools and libraries). The developed voting access measurement methodology and algorithmic remediation technique demonstrates that better polling location placement is possible.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594061
SP  - 1038
EP  - 1048
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594061
KW  - Measurement
KW  - Fairness
KW  - Clustering
KW  - Voting Access
ER  - 

TY  - CONF
TI  - Towards labor transparency in situated computational systems impact research
AU  - Jing, Felicia S.
AU  - Berger, Sara E.
AU  - Becerra Sandoval, Juana Catalina
T3  - FAccT '23
AB  - Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594060
SP  - 1026
EP  - 1037
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594060
KW  - transparency
KW  - documentation
KW  - inclusion
KW  - impact
KW  - labor
KW  - participatory design
KW  - agonism
ER  - 

TY  - CONF
TI  - ACROCPoLis: A descriptive framework for making sense of fairness
AU  - Aler Tubella, Andrea
AU  - Coelho Mollo, Dimitri
AU  - Dahlgren Lindström, Adam
AU  - Devinney, Hannah
AU  - Dignum, Virginia
AU  - Ericson, Petter
AU  - Jonsson, Anna
AU  - Kampik, Timotheus
AU  - Lenaerts, Tom
AU  - Mendez, Julian Alfredo
AU  - Nieves, Juan Carlos
T3  - FAccT '23
AB  - Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594059
SP  - 1014
EP  - 1025
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594059
KW  - Algorithmic fairness
KW  - responsible AI
KW  - social impact of AI
KW  - socio-technical processes
ER  - 

TY  - CONF
TI  - Bias on demand: A modelling framework that generates synthetic data with bias
AU  - Baumann, Joachim
AU  - Castelnovo, Alessandro
AU  - Crupi, Riccardo
AU  - Inverardi, Nicole
AU  - Regoli, Daniele
T3  - FAccT '23
AB  - Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people’s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594058
SP  - 1002
EP  - 1013
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594058
KW  - fairness
KW  - bias
KW  - synthetic data
KW  - moral worldviews
ER  - 

TY  - CONF
TI  - Robustness implies fairness in causal algorithmic recourse
AU  - Ehyaei, Ahmad-Reza
AU  - Karimi, Amir-Hossein
AU  - Schoelkopf, Bernhard
AU  - Maghsudi, Setareh
T3  - FAccT '23
AB  - Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594057
SP  - 984
EP  - 1001
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594057
KW  - explainable AI
KW  - fairness
KW  - robustness
KW  - algorithmic recourse
KW  - counterfactual explanation
ER  - 

TY  - CONF
TI  - “We try to empower them” - exploring future technologies to support migrant jobseekers
AU  - Wang, Sonja Mei
AU  - Scott, Kristen M
AU  - Artemenko, Margarita
AU  - Miceli, Milagros
AU  - Berendt, Bettina
T3  - FAccT '23
AB  - Previous work on technology in Public Employment Services and job market chances has focused on profiling systems that are intended for tasks such as assessing and classifying jobseekers. To integrate into the local job market, migrants and refugees seek support from the Public Employment Services (PES), but also non-profit, non-governmental organizations (herein referred to as third sector organizations, or TSOs). How do design visions for technologies to support jobseekers change when developed not under bureaucratic rules but by people interacting directly and informally with jobseekers? We focus on the perspectives of TSO workers assisting migrants and refugees seeking support for their job search. Through interviews and a design fiction exercise, we investigate (1) the role of TSO workers, (2) factors beyond those used in profiling systems that they consider relevant, and (3) their ideal technology. We describe how TSO workers contextualize formal criteria used in profiling systems while prioritising jobseekers’ personal interests and strengths. Based on our findings on existing tools and methods, and imagined future technologies, we propose a software-based project that expands existing job taxonomies into a coordinated resource combining job characteristics, required competencies, and soft skills to support multiple informational tools for jobseekers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594056
SP  - 972
EP  - 983
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594056
ER  - 

TY  - CONF
TI  - Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda
AU  - Domínguez Hernández, Andrés
AU  - Owen, Richard
AU  - Nielsen, Dan Saattrup
AU  - Mcconville, Ryan
T3  - FAccT '23
AB  - Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of “truth” used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies—key moments during model development that could lead to different future outcomes, uncertainty and harmful effects as these tools are deployed by social media platforms. We conclude by offering a tentative path toward reflexive and responsible development of ML tools for moderating misinformation and other harmful content online.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594055
SP  - 971
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594055
ER  - 

TY  - CONF
TI  - On the impact of explanations on understanding of algorithmic decision-making
AU  - Schmude, Timothée
AU  - Koesten, Laura
AU  - Möller, Torsten
AU  - Tschiatschek, Sebastian
T3  - FAccT '23
AB  - Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by "high-risk" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems’ adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a "high-risk" ADM system to participants and analyse their responses both inductively and deductively, using the "six facets of understanding" framework by Wiggins &amp; McTighe [63]. Our findings indicate that the "six facets" framework is a promising approach to analyse participants’ thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the "dialogue" modality as a valid explanation approach to increase participant engagement and interaction with the "explainer", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants’ perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the "six facets" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594054
SP  - 959
EP  - 970
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594054
KW  - XAI
KW  - algorithmic fairness
KW  - algorithmic decision-making
KW  - learning Sciences
KW  - qualitative methods
ER  - 

TY  - CONF
TI  - Questioning the ability of feature-based explanations to empower non-experts in robo-advised financial decision-making
AU  - Bertrand, Astrid
AU  - Eagan, James R.
AU  - Maxwell, Winston
T3  - FAccT '23
AB  - Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a “fully informed decision”. In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either “dialogic”—more textual—or “graphical”—more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users’ trust in the recommendations of the robo-advisor, sometimes to the users’ detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594053
SP  - 943
EP  - 958
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594053
KW  - intelligibility
KW  - explainability
KW  - AI regulation
KW  - financial inclusion
ER  - 

TY  - CONF
TI  - AI in the public eye: Investigating public AI literacy through AI art
AU  - Hemment, Drew
AU  - Currie, Morgan
AU  - Bennett, SJ
AU  - Elwes, Jake
AU  - Ridler, Anna
AU  - Sinders, Caroline
AU  - Vidmar, Matjaz
AU  - Hill, Robin
AU  - Warner, Holly
T3  - FAccT '23
AB  - Recent advances in diffusion models and large language models have underpinned a new generation of powerful and accessible tools, and some of the most publicly visible applications are for artistic endeavour. Such tools, however, provide little scope for deeper understanding of AI systems, while the growing public interest in them can eclipse notice of the vibrant community of artists who have long worked with other forms of AI. We explore the potential for AI Art – particularly work in which AI is both tool and topic – to facilitate public AI literacies and consider how tactics developed before the current generative AI boom have continued relevance today. We look at the strategies of critical AI artists to scaffold public understanding of AI and enhance legibility for non-experts. This paper also investigates how collaborations between artists and AI researchers and designers can illuminate key technical and social issues relevant to the development of AI. The study entailed workshops between three professional artists who work with AI and a cross-disciplinary set of academic participants. This paper reports on these workshops and presents the intentions and strategies expressed by the artists, as well as insights of relevance to the research community on public AI literacies. We find that critical AI art can link underlying technical systems to structural issues of power and facilitate experiential learning that is situated and embodied, valuing interpretation over explanation. The findings also demonstrate the importance of transdisciplinary conversations around art, ethics and the political economy of AI technologies and how these dialogues may feed into AI design processes.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594052
SP  - 931
EP  - 942
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594052
KW  - Transparency
KW  - Artificial Intelligence
KW  - Interdisciplinary Research
KW  - Education
KW  - Culture
KW  - AI literacy
KW  - AI Art
KW  - Art
KW  - Computational Art
KW  - Creative AI
KW  - Dialogue
KW  - Society
ER  - 

TY  - CONF
TI  - Implementing fairness constraints in markets using taxes and subsidies
AU  - Peysakhovich, Alexander
AU  - Kroer, Christian
AU  - Usunier, Nicolas
T3  - FAccT '23
AB  - Fisher markets are those where buyers with budgets compete for scarce items, a natural model for many real world markets including online advertising. A market equilibrium is a set of prices and allocations of items such that supply meets demand. We show how market designers can use taxes or subsidies in Fisher markets to ensure that market equilibrium outcomes fall within certain constraints. We show how these taxes and subsidies can be computed even in an online setting where the market designer does not have access to private valuations. We adapt various types of fairness constraints proposed in existing literature to the market case and show who benefits and who loses from these constraints, as well as the extent to which properties of markets including Pareto optimality, envy-freeness, and incentive compatibility are preserved. We find that some prior discussed constraints have few guarantees in terms of who is made better or worse off by their imposition.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594051
SP  - 916
EP  - 930
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594051
ER  - 

TY  - CONF
TI  - To be high-risk, or not to Be—Semantic specifications and implications of the AI act’s high-risk AI applications and harmonised standards
AU  - Golpayegani, Delaram
AU  - Pandit, Harshvardhan J.
AU  - Lewis, Dave
T3  - FAccT '23
AB  - The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594050
SP  - 905
EP  - 915
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594050
KW  - taxonomy
KW  - semantic web
KW  - AI Act
KW  - harmonised standards
KW  - high-risk AI
ER  - 

TY  - CONF
TI  - Augmented datasheets for speech datasets and ethical decision-making
AU  - Papakyriakopoulos, Orestis
AU  - Choi, Anna Seo Gyeong
AU  - Thong, William
AU  - Zhao, Dora
AU  - Andrews, Jerone
AU  - Bourke, Rebecca
AU  - Xiang, Alice
AU  - Koenecke, Allison
T3  - FAccT '23
AB  - Speech datasets are crucial for training Speech Language Technologies (SLT); however, the lack of diversity of the underlying training data can lead to serious limitations in building equitable and robust SLT products, especially along dimensions of language, accent, dialect, variety, and speech impairment—and the intersectionality of speech features with socioeconomic and demographic features. Furthermore, there is often a lack of oversight on the underlying training data—commonly built on massive web-crawling and/or publicly available speech—with regard to the ethics of such data collection. To encourage standardized documentation of such speech data components, we introduce an augmented datasheet for speech datasets1, which can be used in addition to “Datasheets for Datasets” [78]. We then exemplify the importance of each question in our augmented datasheet based on in-depth literature reviews of speech data used in domains such as machine learning, linguistics, and health. Finally, we encourage practitioners—ranging from dataset creators to researchers—to use our augmented datasheet to better define the scope, properties, and limits of speech datasets, while also encouraging consideration of data-subject protection and user community empowerment. Ethical dataset creation is not a one-size-fits-all process, but dataset creators can use our augmented datasheet to reflexively consider the social context of related SLT applications and data sources in order to foster more inclusive SLT products downstream.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594049
SP  - 881
EP  - 904
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594049
KW  - transparency
KW  - ethics
KW  - speech
KW  - datasets
KW  - datasheets
ER  - 

TY  - CONF
TI  - Maximal fairness
AU  - Defrance, Marybeth
AU  - De Bie, Tijl
T3  - FAccT '23
AB  - Fairness in AI has garnered quite some attention in research, and increasingly also in society. The so-called "Impossibility Theorem" has been one of the more striking research results with both theoretical and practical consequences, as it states that satisfying a certain combination of fairness measures is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness measures that can be simultaneously satisfied. The fairness measures used are demographic parity, equal opportunity, predictive equality, predictive parity, false omission rate parity, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness measures are possible, among which are seven combinations of two measures, and five combinations of three measures. Our work raises interesting questions regarding the practical relevance of each of these 12 maximal fairness notions in various scenarios.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594048
SP  - 851
EP  - 880
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594048
KW  - Fairness
KW  - Fairness in AI
KW  - Combining Fairness definitions
KW  - confusion tables
KW  - Fairness definitions
ER  - 

TY  - CONF
TI  - Datafication genealogies beyond algorithmic fairness: Making up racialised subjects
AU  - Valdivia, Ana
AU  - Tazzioli, Martina
T3  - FAccT '23
AB  - A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594047
SP  - 840
EP  - 850
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594047
KW  - classification
KW  - borders
KW  - datafication
KW  - genealogies
KW  - racialised subjects
ER  - 

TY  - CONF
TI  - UNFair: Search engine manipulation, undetectable by amortized inequity
AU  - De Jonge, Tim
AU  - Hiemstra, Djoerd
T3  - FAccT '23
AB  - Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594046
SP  - 830
EP  - 839
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594046
KW  - Fairness
KW  - Information Retrieval
KW  - Exposure
KW  - Search Engine Manipulation Effect
KW  - UNFair
ER  - 

TY  - CONF
TI  - On (assessing) the fairness of risk score models
AU  - Petersen, Eike
AU  - Ganz, Melanie
AU  - Holm, Sune
AU  - Feragen, Aasa
T3  - FAccT '23
AB  - Recent work on algorithmic fairness has largely focused on the fairness of discrete decisions, or classifications. While such decisions are often based on risk score models, the fairness of the risk models themselves has received considerably less attention. Risk models are of interest for a number of reasons, including the fact that they communicate uncertainty about the potential outcomes to users, thus representing a way to enable meaningful human oversight. Here, we address fairness desiderata for risk score models. We identify the provision of similar epistemic value to different groups as a key desideratum for risk score fairness, and we show how even fair risk scores can lead to unfair risk-based rankings. Further, we address how to assess the fairness of risk score models quantitatively, including a discussion of metric choices and meaningful statistical comparisons between groups. In this context, we also introduce a novel calibration error metric that is less sample size-biased than previously proposed metrics, enabling meaningful comparisons between groups of different sizes. We illustrate our methodology – which is widely applicable in many other settings – in two case studies, one in recidivism risk prediction, and one in risk of major depressive disorder (MDD) prediction.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594045
SP  - 817
EP  - 829
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594045
KW  - Ethics
KW  - Algorithmic fairness
KW  - Ranking
KW  - Calibration
KW  - Risk scores
KW  - Major depressive disorder
KW  - Recidivism
ER  - 

TY  - CONF
TI  - Algorithmic unfairness through the lens of EU non-discrimination law: Or why the law is not a decision tree
AU  - Weerts, Hilde
AU  - Xenidis, Raphaële
AU  - Tarissan, Fabien
AU  - Olsen, Henrik Palmer
AU  - Pechenizkiy, Mykola
T3  - FAccT '23
AB  - Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594044
SP  - 805
EP  - 816
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594044
KW  - machine learning
KW  - artificial intelligence
KW  - algorithmic fairness
KW  - EU non-discrimination law
ER  - 

TY  - CONF
TI  - “I think you might like this”: Exploring effects of confidence signal patterns on trust in and reliance on conversational recommender systems
AU  - Radensky, Marissa
AU  - Séguin, Julie Anne
AU  - Lim, Jang Soo
AU  - Olson, Kristen
AU  - Geiger, Robert
T3  - FAccT '23
AB  - With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as “you might like...” or “I think you will like...,” but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594043
SP  - 792
EP  - 804
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594043
KW  - trust
KW  - human-AI interaction
KW  - confidence
KW  - conversational recommender systems
KW  - reliance
ER  - 

TY  - CONF
TI  - Gender animus can still exist under favorable disparate impact: a cautionary tale from online P2P lending
AU  - Shen, Xudong
AU  - Tan, Tianhui
AU  - Phan, Tuan
AU  - Keppo, Jussi
T3  - FAccT '23
AB  - This paper investigates gender discrimination and its underlying drivers on a prominent Chinese online peer-to-peer (P2P) lending platform. While existing studies on P2P lending focus on disparate treatment (DT), DT narrowly recognizes direct discrimination and overlooks indirect and proxy discrimination, providing an incomplete picture. In this work, we measure a broadened discrimination notion called disparate impact (DI), which encompasses any disparity in the loan’s funding rate that does not commensurate with the actual return rate. We develop a two-stage predictor substitution approach to estimate DI from observational data. Our findings reveal (i) female borrowers, given identical actual return rates, are 3.97% more likely to receive funding, (ii) at least of this DI favoring female is indirect or proxy discrimination, and (iii) DT indeed underestimates the overall female favoritism by . However, we also identify the overall female favoritism can be explained by one specific discrimination driver, rational statistical discrimination, wherein investors accurately predict the expected return rate from imperfect observations. Furthermore, female borrowers still require 2% higher expected return rate to secure funding, indicating another driver taste-based discrimination co-exists and is against female. These results altogether tell a cautionary tale: on one hand, P2P lending provides a valuable alternative credit market where the affirmative action to support female naturally emerges from the rational crowd; on the other hand, while the overall discrimination effect (both in terms of DI or DT) favors female, concerning taste-based discrimination can persist and can be obscured by other co-existing discrimination drivers, such as statistical discrimination.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594042
SP  - 775
EP  - 791
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594042
KW  - Disparate Impact
KW  - Gender Discrimination
KW  - P2P Lending
KW  - Statistical Discrimination
KW  - Taste-base discrimination
ER  - 

TY  - CONF
TI  - Algorithmic decisions, desire for control, and the preference for human review over algorithmic review
AU  - Lyons, Henrietta
AU  - Miller, Tim
AU  - Velloso, Eduardo
T3  - FAccT '23
AB  - In this paper, we explore why decision subjects generally express a preference for human reviewers of algorithmic decisions over algorithmic reviewers. We theorise that decision subjects desire control over the decision-making process in order to increase their chance of receiving a favourable outcome. To this end, human reviewers will be seen as easier to influence than algorithmic reviewers, thus providing more control. Using an online study we find that: (1) people who have a greater Desire for Control over their lives exhibit a stronger preference for human review; (2) interaction with a reviewer is important because it enables influence and ensures understanding; and (3) the higher the impact of a decision, the greater the incentive to influence the outcome, and the greater the preference for human review. Our qualitative results confirm that outcome favourability is a driver for reviewer preference, but so is the desire to be treated with dignity.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594041
SP  - 764
EP  - 774
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594041
KW  - accountability
KW  - algorithmic fairness
KW  - algorithmic decision-making
KW  - and transparency
KW  - contestability
KW  - reviewability
ER  - 

TY  - CONF
TI  - FairAssign: Stochastically fair driver assignment in gig delivery platforms
AU  - Singh, Daman Deep
AU  - Das, Syamantak
AU  - Chakraborty, Abhijnan
T3  - FAccT '23
AB  - Increasing adoption of online commerce has created income opportunities for millions of delivery drivers who deliver items, from clothes and smartphones to foods and medicines, to customers. Despite their indispensability for the ecosystem, the drivers are often deprived of employment benefits and their earnings are tied to the number of successful deliveries, forcing them to go on repeated strikes to demand fair wage. In addition to low wages, there is considerable variability in driver incomes. One major component contributing to this variability is the static assignment of drivers to delivery zones as different zones likely have different workloads and hence earning opportunities. To reduce this variability, we directly engage with the gig delivery drivers to understand their perspectives on fair income distribution, and incorporate the same by proposing FairAssign for dynamic assignment of drivers to delivery zones to ensure fair distribution of earning opportunities. Specifically, we introduce a framework for stochastic pairwise fairness where, based on a similarity measure between the drivers, individual drivers are assigned to probability distributions over different zones such that similar individuals are mapped to statistically similar distributions. To realize these distributions, we develop a randomized dependent rounding based efficient sampling algorithm such that the workload constraints in each zone are satisfied, and the expected travel cost is minimized. Extensive experiments on real-world food delivery data and semi-synthetic ecommerce data show the efficacy of FairAssign over other baselines.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594040
SP  - 753
EP  - 763
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594040
KW  - Dependent Rounding
KW  - Ecommerce Logistics
KW  - Fair Driver Assignment
KW  - Food Delivery.
KW  - Last Mile Delivery
KW  - Stochastic Fairness
ER  - 

TY  - CONF
TI  - Add-remove-or-relabel: Practitioner-friendly bias mitigation via influential fairness
AU  - Richardson, Brianna
AU  - Sattigeri, Prasanna
AU  - Wei, Dennis
AU  - Ramamurthy, Karthikeyan Natesan
AU  - Varshney, Kush
AU  - Dhurandhar, Amit
AU  - Gilbert, Juan E.
T3  - FAccT '23
AB  - Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594039
SP  - 736
EP  - 752
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594039
KW  - machine learning
KW  - fairness
KW  - ethics
KW  - bias mitigation
ER  - 

TY  - CONF
TI  - Your browsing history may cost you: A framework for discovering differential pricing in non-transparent markets
AU  - Karan, Aditya
AU  - Balepur, Naina
AU  - Sundaram, Hari
T3  - FAccT '23
AB  - In many online markets we “shop alone” — there is no way for us to know the prices other consumers paid for the same goods. Could this lack of price transparency lead to differential pricing? To answer this question, we present a generalized framework to audit online markets for differential pricing using automated agents. Consensus is a key idea in our work: for a successful black-box audit, both the experimenter and seller must agree on the agents’ attributes. We audit two competitive online travel markets on kayak.com (flight and hotel markets) and construct queries representative of the demand for goods. Crucially, we assume ignorance of the sellers’ pricing mechanisms while conducting these audits. We conservatively implement consensus with nine distinct profiles based on behavior, not demographics. We use a structural causal model for price differences and estimate model parameters using Bayesian inference. We can unambiguously show that many sellers (but not all) demonstrate behavior-driven differential pricing. In the flight market, some profiles are nearly more likely to see a worse price than the best performing profile, and nearly more likely in the hotel market. While the control profile (with no browsing history) was on average offered the best prices in the flight market, surprisingly, other profiles outperformed the control in the hotel market. The price difference between any pair of profiles occurring by chance is  0.44intheflightmarketand 0.09 for hotels. However, the expected loss of welfare for any profile when compared to the best profile can be as much as  6.00forflightsand 3.00 for hotels (i.e., 15 × and 33 × the price difference by chance respectively). This illustrates the need for new market designs or policies that encourage more transparent market design to overcome differential pricing practices.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594038
SP  - 717
EP  - 735
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594038
ER  - 

TY  - CONF
TI  - Investigating practices and opportunities for cross-functional collaboration around AI fairness in industry practice
AU  - Deng, Wesley Hanwen
AU  - Yildirim, Nur
AU  - Chang, Monica
AU  - Eslami, Motahhare
AU  - Holstein, Kenneth
AU  - Madaio, Michael
T3  - FAccT '23
AB  - An emerging body of research indicates that ineffective cross-functional collaboration – the interdisciplinary work done by industry practitioners across roles – represents a major barrier to addressing issues of fairness in AI design and development. In this research, we sought to better understand practitioners’ current practices and tactics to enact cross-functional collaboration for AI fairness, in order to identify opportunities to support more effective collaboration. We conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies. We found that practitioners engaged in bridging work to overcome frictions in understanding, contextualization, and evaluation around AI fairness across roles. In addition, in organizational contexts with a lack of resources and incentives for fairness work, practitioners often piggybacked on existing requirements (e.g., for privacy assessments) and AI development norms (e.g., the use of quantitative evaluation metrics), although they worry that these tactics may be fundamentally compromised. Finally, we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness. We close by discussing opportunities for both FAccT researchers and AI practitioners to better support cross-functional collaboration for fairness in the design and development of AI systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594037
SP  - 705
EP  - 716
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594037
KW  - fairness
KW  - collaboration
KW  - interdisciplinarity
KW  - AI development
ER  - 

TY  - CONF
TI  - Ground(less) truth: A causal framework for proxy labels in human-algorithm decision-making
AU  - Guerdan, Luke
AU  - Coston, Amanda
AU  - Wu, Zhiwei Steven
AU  - Holstein, Kenneth
T3  - FAccT '23
AB  - A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on “ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the “toxicity” of online comments, or future “job performance” – predictive models target proxy labels that are readily available in existing datasets. Predictive models’ reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594036
SP  - 688
EP  - 704
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594036
KW  - validity
KW  - measurement
KW  - label bias
KW  - algorithmic decision support
KW  - causal diagrams
KW  - human-AI decision-making
ER  - 

TY  - CONF
TI  - On the site of predictive justice
AU  - Lazar, Seth
AU  - Stone, Jake
T3  - FAccT '23
AB  - Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these ‘cheap’ predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594035
SP  - 687
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594035
KW  - Algorithmic fairness
KW  - Justice
KW  - Philosophy
KW  - Epistemic ethics
KW  - Predictive justice
ER  - 

TY  - CONF
TI  - How redundant are redundant encodings? Blindness in the wild and racial disparity when race is unobserved
AU  - Cheng, Lingwei
AU  - Gallegos, Isabel O
AU  - Ouyang, Derek
AU  - Goldin, Jacob
AU  - Ho, Dan
T3  - FAccT '23
AB  - We address two emerging concerns in algorithmic fairness: (i) redundant encodings of race – the notion that machine learning models encode race with probability nearing one as the feature set grows – which is widely noted in theory, with little empirical evidence; and (ii) the lack of race and ethnicity data in many domains, where state-of-the-art remains (Naive) Bayesian Improved Surname Geocoding (BISG) that relies on name and geographic information. We leverage a novel and highly granular dataset of over 7.7 million patients’ electronic health records to provide one of the first empirical studies of redundant encodings in a realistic health care setting and examine the ability to assess health care disparities when race may be missing. First, we show that machine learning (random forest) applied to name and geographic information can improve on BISG, driven primarily by better performance in identifying minority groups. Second, contrary to theoretical concerns about redundant encodings as undercutting anti-discrimination law’s anti-classification principle, additional electronic health information provides little marginal information about race and ethnicity: race still remains measured with substantial noise. Third, we show how machine learning can enable the disaggregation of racial categories, responding to longstanding critiques of the government race reporting standard. Fourth, we show that an increasing feature set can differentially impact performance on majority and minority groups. Our findings address important questions for fairness in machine learning and algorithmic decision-making, enabling the assessment of disparities, tempering concerns about redundant encodings in one important setting, and demonstrating how bigger data can shape the accuracy of race imputations in nuanced ways.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594034
SP  - 667
EP  - 686
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594034
ER  - 

TY  - CONF
TI  - Harms from increasingly agentic algorithmic systems
AU  - Chan, Alan
AU  - Salganik, Rebecca
AU  - Markelius, Alva
AU  - Pang, Chris
AU  - Rajkumar, Nitarshan
AU  - Krasheninnikov, Dmitrii
AU  - Langosco, Lauro
AU  - He, Zhonghao
AU  - Duan, Yawen
AU  - Carroll, Micah
AU  - Lin, Michelle
AU  - Mayhew, Alex
AU  - Collins, Katherine
AU  - Molamohammadi, Maryam
AU  - Burden, John
AU  - Zhao, Wanru
AU  - Rismani, Shalaleh
AU  - Voudouris, Konstantinos
AU  - Bhatt, Umang
AU  - Weller, Adrian
AU  - Krueger, David
AU  - Maharaj, Tegan
T3  - FAccT '23
AB  - Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594033
SP  - 651
EP  - 666
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594033
KW  - ethics
KW  - safety
KW  - autonomy
KW  - agency
KW  - algorithmic systems
KW  - sociotechnical systems
KW  - harms
KW  - power
KW  - FATE
KW  - delayed impacts
KW  - negative externalities
ER  - 

TY  - CONF
TI  - You sound depressed: A case study on sonde health’s diagnostic use of voice analysis AI
AU  - Ma, Anna
AU  - Patitsas, Elizabeth
AU  - Sterne, Jonathan
T3  - FAccT '23
AB  - There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer “objective” depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user’s voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde’s application programming interface, examining each of these acoustic features (“vocal biomarkers”), and problematizing Sonde’s claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of “objectivity”, and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594032
SP  - 639
EP  - 650
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594032
KW  - disability studies
KW  - critical disability studies
KW  - disability surveillance
ER  - 

TY  - CONF
TI  - Care and coordination in algorithmic systems: An economies of worth approach
AU  - Rudnik, John
AU  - Brewer, Robin
T3  - FAccT '23
AB  - Algorithmic decision-making has permeated health and care domains (e.g., automated diagnoses, fall detection, caregiver staffing). Researchers have raised concerns about how these algorithms are built and how they shape fair and ethical care practices. To investigate algorithm development and understand its impact on people who provide and coordinate care, we conducted a case study of a U.S.-based senior care network and platform. We interviewed 14 technologists, 9 paid caregivers, and 7 care coordinators to explore their interactions with the platform’s algorithms. We find that technologists draw on a multitude of moral frameworks to navigate complex and contradictory demands and expectations. Despite technologists’ espoused commitments to fairness, accountability, and transparency, the platform reassembles problematic aspects of care labor. By analyzing how technologists justify their work, the problems that they claim to solve, the solutions they present, and caregivers’ and coordinators’ experiences, we advance fairness research that focuses on agency and power asymmetries in algorithmic platforms. We (1) make an empirical contribution, revealing tensions when developing and implementing algorithms and (2) provide insight into the social processes that reproduce power asymmetries in algorithmic decision-making.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594031
SP  - 627
EP  - 638
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594031
KW  - care
KW  - algorithms
KW  - morality
KW  - qualitative study
KW  - coordination
ER  - 

TY  - CONF
TI  - Data collaboratives with the use of decentralised learning
AU  - Zuziak, Maciej Krzysztof
AU  - Hinrichs, Onntje
AU  - Abdrassulova, Aizhan
AU  - Rinzivillo, Salvatore
T3  - FAccT '23
AB  - The endeavor to find appropriate data governance frameworks capable of reconciling conflicting interests in data has dramatically gained importance across disciplines and has been discussed among legal scholars, computer scientists as well as policy-makers alike. The predominant part of the current discussion is centered around the challenging task of creating a data governance framework where data is ‘as open as possible and as closed as necessary’. In this article, we elaborate on modern approaches to data governance and their limitations. It analyses how propositions evolved from property rights in data towards the creation of data access and data sharing obligations and how the corresponding debates reflect the difficulty of developing approaches that reconcile seemingly opposite objectives – such as giving individuals and businesses more control over ‘their’ data while at the same time ensuring its availability to different stakeholders. Furthermore, we propose a wider acknowledgement of data collaboratives powered by decentralised learning techniques as a possible remedy to the shortcomings of current data governance schemes. Hence, we propose a mild formalization of the set of existing technological solutions that could inform existing approaches to data governance issues. Our proposition is based on an abstractive notion of collaborative computation as well as on several principles that are essential for our definition of data collaboratives. By adopting an interdisciplinary perspective on data governance, this article highlights how innovative technological solutions can enhance control over data while at the same time ensuring its availability to other stakeholders and thereby contributing to the achievement of the policy goals of the European Strategy for Data.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594029
SP  - 615
EP  - 625
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594029
KW  - Data Sharing
KW  - Data Access
KW  - Data Governance
KW  - Decentralised Learning
KW  - European Strategy for Data
ER  - 

TY  - CONF
TI  - Runtime monitoring of dynamic fairness properties
AU  - Henzinger, Thomas
AU  - Karimi, Mahyar
AU  - Kueffner, Konstantin
AU  - Mallik, Kaushik
T3  - FAccT '23
AB  - A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank’s credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator’s allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594028
SP  - 604
EP  - 614
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594028
KW  - algorithmic fairness
KW  - dynamic fairness
KW  - online statistical estimator
KW  - runtime monitor
ER  - 

TY  - CONF
TI  - Power and resistance in the twitter bias discourse
AU  - Lopez, Paola
T3  - FAccT '23
AB  - In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first "algorithmic bias bounty challenge", inviting the general public to detect algorithmic harm in the cropping tool. Twitter’s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse? This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter’s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter’s response to the bias accusations and the way in which the company was able to effectively stabilize its position – rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594027
SP  - 603
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594027
KW  - Twitter
KW  - bias
KW  - power
KW  - discourse
KW  - Foucault
KW  - image cropping
KW  - resistance
ER  - 

TY  - CONF
TI  - Honor ethics: The challenge of globalizing value alignment in AI
AU  - Wu, Stephen Tze-Inn
AU  - Demetriou, Daniel
AU  - Husain, Rudwan Ali
T3  - FAccT '23
AB  - Some researchers have recognized that privileged communities dominate the discourse on AI Ethics, and other voices need to be heard. As such, we identify the current ethics milieu as arising from WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and aim to expand the discussion to non-WEIRD global communities, who are also stakeholders in global sociotechnical systems. We argue that accounting for honor, along with its values and related concepts, would better approximate a global ethical perspective. This complex concept already underlies some of the WEIRD discourse on AI ethics, but certain cultural forms of honor also bring overlooked issues and perspectives to light. We first describe honor according to recent empirical and philosophical scholarship. We then review “consensus” principles for AI ethics framed from an honor-based perspective, grounding comparisons and contrasts via example settings such as content moderation, job hiring, and genomics databases. A better appreciation of the marginalized concept of honor could, we hope, lead to more productive AI value alignment discussions, and to AI systems that better reflect the needs and values of users around the globe.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594026
SP  - 593
EP  - 602
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594026
KW  - artificial intelligence
KW  - AI Ethics
KW  - cultures of honor
KW  - ethics alignment
KW  - honor cultures
KW  - universal human rights
ER  - 

TY  - CONF
TI  - Invigorating ubuntu ethics in AI for healthcare: Enabling equitable care
AU  - Amugongo, Lameck Mbangula
AU  - Bidwell, Nicola J.
AU  - Corrigan, Caitlin C.
T3  - FAccT '23
AB  - The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of “I am because we are” in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594024
SP  - 583
EP  - 592
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594024
KW  - algorithmic fairness
KW  - algorithmic trust
KW  - equitable care
KW  - relational ethics
ER  - 

TY  - CONF
TI  - Can workers meaningfully consent to workplace wellbeing technologies?
AU  - Chowdhary, Shreya
AU  - Kawakami, Anna
AU  - Gray, Mary L
AU  - Suh, Jina
AU  - Olteanu, Alexandra
AU  - Saha, Koustuv
T3  - FAccT '23
AB  - Sensing technologies deployed in the workplace can unobtrusively collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the inherent and structural power dynamics in the workplace, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers’ meaningful consent raises privacy and ethical concerns. This paper unpacks challenges workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants’ expectations and capacity to consent to these technologies. We sketched possible interventions that could better support meaningful consent to workplace wellbeing technologies, by drawing on critical computing and feminist scholarship—which reframes consent from a purely individual choice to a structural condition experienced at the individual level that needs to be freely given, reversible, informed, enthusiastic, and specific (FRIES). The focus groups revealed how workers are vulnerable to “meaningless” consent—as they may be subject to power dynamics that minimize their ability to withhold consent and may thus experience an erosion of autonomy in their workplace, also undermining the value of data gathered in the name of “wellbeing.” To meaningfully consent, participants wanted changes to how the technology works and is being used, as well as to the policies and practices surrounding the technology. Our mapping of what prevents workers from meaningfully consenting to workplace wellbeing technologies (challenges) and what they require to do so (interventions) illustrates how the lack of meaningful consent is a structural problem requiring socio-technical solutions.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594023
SP  - 569
EP  - 582
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594023
KW  - privacy
KW  - ethics
KW  - workplace
KW  - data governance
KW  - power
KW  - sensing
ER  - 

TY  - CONF
TI  - Striving for affirmative algorithmic futures: How the social sciences can promote more equitable and just algorithmic system design
AU  - Theus, Anna-Lena
T3  - FAccT '23
AB  - The social sciences have a keen eye for the complex forces that shape our diverse experiences and excel in uncovering an issue's genesis by making sense of how the past has shaped the present. What is sometimes missing though is the practical application of this knowledge. Computer science disciplines and human-computer interaction on the other hand, are very skilled at identifying existing issues and at proposing practical solutions but sometimes miss to unpack and to scrutinize a problem's history and evolution. And while a lot of valuable domain specific knowledge exists, interdisciplinary socio-technical expertise is still scarce. This paper argues that a strong connection between these fields can counter the development of algorithmic systems that lead to inequitable consequences and instead support the design of algorithmic systems that result in more just outcomes and cultivate, what I call, affirmative algorithmic futures. To that end, this paper introduces a compendium of theoretical concepts and practical measures rooted in social science scholarship that foster socio-technical algorithmic system design practices and promote knowledge mobilization between disciplines.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594022
SP  - 558
EP  - 568
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594022
KW  - Affirmative algorithmic futures
KW  - Algorithmic system design
KW  - Compendium
KW  - Socio-technical knowledge
ER  - 

TY  - CONF
TI  - Delayed and indirect impacts of link recommendations
AU  - Zhang, Han
AU  - Lu, Shangen
AU  - Wang, Yixin
AU  - Curmei, Mihaela
T3  - FAccT '23
AB  - The impacts of link recommendations on social networks are challenging to evaluate, due to feedback loops between algorithmic recommendations and underlying network dynamics. Observational studies have limitations in answering causal questions; naive A/B experiments often result in biased evaluations due to unaccounted network interference and finally, existing simulations primarily employ static network models that do not take into account dynamics. Departing from existing approaches, we employ simulations to study dynamic impacts of link recommendations. Specifically, we propose an extension to the Jackson-Rogers network evolution model and investigate how link recommendations affect network evolution over time. Our experiments demonstrate that link recommendations can have surprising delayed and indirect effects on the structural properties of networks. Effects of recommendations vary in the short-term and long-term, such as the immediate reduction in degree inequality but eventual increase in degree inequality through friend-of-friend recommendations. Furthermore, even after recommendations are discontinued, their impacts can persist in the network, in part by altering natural network evolution dynamics. These results provide valuable insights into the interplay between algorithmic interventions and natural network dynamics and highlight the limitations of current evaluation paradigms.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594021
SP  - 545
EP  - 557
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594021
KW  - recommender systems
KW  - dynamic graphs
KW  - link recommendation
ER  - 

TY  - CONF
TI  - Detecting disparities in police deployments using dashcam data
AU  - Franchi, Matt
AU  - Zamfirescu-Pereira, J.D.
AU  - Ju, Wendy
AU  - Pierson, Emma
T3  - FAccT '23
AB  - Large-scale policing data is vital for detecting inequity in police behavior and policing algorithms. However, one important type of policing data remains largely unavailable within the United States: aggregated police deployment data capturing which neighborhoods have the heaviest police presences. Here we show that disparities in police deployment levels can be quantified by detecting police vehicles in dashcam images of public street scenes. Using a dataset of 24,803,854 dashcam images from rideshare drivers in New York City, we find that police vehicles can be detected with high accuracy (average precision 0.82, AUC 0.99) and identify 233,596 images which contain police vehicles. There is substantial inequality across neighborhoods in police vehicle deployment levels. The neighborhood with the highest deployment levels has almost 20 times higher levels than the neighborhood with the lowest. Two strikingly different types of areas experience high police vehicle deployments — 1) dense, higher-income, commercial areas and 2) lower-income neighborhoods with higher proportions of Black and Hispanic residents. We discuss the implications of these disparities for policing equity and for algorithms trained on policing data.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594020
SP  - 534
EP  - 544
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594020
KW  - object detection
KW  - policing
KW  - dashcam
ER  - 

TY  - CONF
TI  - Representation in AI evaluations
AU  - Bergman, A. Stevie
AU  - Hendricks, Lisa Anne
AU  - Rauh, Maribeth
AU  - Wu, Boxi
AU  - Agnew, William
AU  - Kunesch, Markus
AU  - Duan, Isabella
AU  - Gabriel, Iason
AU  - Isaac, William
T3  - FAccT '23
AB  - Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with "representation" or "representativeness" generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be "representative"? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594019
SP  - 519
EP  - 533
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594019
KW  - responsible AI
KW  - datasets
KW  - machine learning evaluation
ER  - 

TY  - CONF
TI  - A theory of auditability for allocation and social choice mechanisms
AU  - Grigoryan, Aram
AU  - Möller, Markus
T3  - FAccT '23
AB  - In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594017
SP  - 518
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594017
ER  - 

TY  - CONF
TI  - AI’s regimes of representation: A community-centered study of text-to-image models in south asia
AU  - Qadri, Rida
AU  - Shelby, Renee
AU  - Bennett, Cynthia L.
AU  - Denton, Emily
T3  - FAccT '23
AB  - This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants’ reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594016
SP  - 506
EP  - 517
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594016
KW  - human-centered AI
KW  - AI harms
KW  - cultural harms of AI
KW  - failure modes
KW  - generative AI
KW  - non-western AI fairness
KW  - qualitative research in AI
KW  - South Asia
KW  - text-to-image models
ER  - 

TY  - CONF
TI  - The privacy-bias tradeoff: Data minimization and racial disparity assessments in U.S. government
AU  - King, Jennifer
AU  - Ho, Daniel
AU  - Gupta, Arushi
AU  - Wu, Victor
AU  - Webley-Brown, Helen
T3  - FAccT '23
AB  - An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this “privacy-bias tradeoff” has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden’s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing “equity action plans,” with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by “visual observation” when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594015
SP  - 492
EP  - 505
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594015
ER  - 

TY  - CONF
TI  - Algorithms as social-ecological-technological systems: an environmental justice lens on algorithmic audits
AU  - Rakova, Bogdana
AU  - Dobbe, Roel
T3  - FAccT '23
AB  - This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594014
SP  - 491
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594014
ER  - 

TY  - CONF
TI  - Does AI-Assisted fact-checking disproportionately benefit majority groups online?
AU  - Neumann, Terrence
AU  - Wolczynski, Nicholas
T3  - FAccT '23
AB  - In recent years, algorithms have been incorporated into fact-checking pipelines. They are used not only to flag previously fact-checked misinformation, but also to provide suggestions about which trending claims should be prioritized for fact-checking - a paradigm called ‘check-worthiness.’ While several studies have examined the accuracy of these algorithms, none have investigated how the benefits from these algorithms (via reduction in exposure to misinformation) are distributed amongst various online communities. In this paper, we investigate how diverse representation across multiple stages of the AI development pipeline affects the distribution of benefits from AI-assisted fact-checking for different online communities. We simulate information propagation through the network using our novel Topic-Aware, Community-Impacted Twitter (TACIT) simulator on a large Twitter followers network, tuned to produce realistic cascades of true and false information across multiple topics. Finally, using simulated data as a test bed, we implement numerous algorithmic fact-checking interventions that explicitly account for notions of diversity. We find that both representative and egalitarian methods for sampling and labeling check-worthiness model training data can lead to network-wide benefit concentrated in majority communities, while incorporating diversity into how fact-checkers use algorithmic recommendations can actively reduce inequalities in benefits between majority and minority communities. These findings contribute to an important conversation around the responsible implementation of AI-assisted fact-checking by social media platforms and fact-checking organizations.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594013
SP  - 480
EP  - 490
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594013
KW  - machine learning
KW  - algorithmic fairness
KW  - socio-technical systems
KW  - agent-based modeling
KW  - misinformation detection
KW  - network science
ER  - 

TY  - CONF
TI  - It’s about power: What ethical concerns do software engineers have, and what do they (feel they can) do about them?
AU  - Widder, David Gray
AU  - Zhen, Derrick
AU  - Dabbish, Laura
AU  - Herbsleb, James
T3  - FAccT '23
AB  - How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as “fairness”, but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns – military, privacy, advertising, surveillance, and the scope of their concerns – from simple bugs to questioning their industry’s entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594012
SP  - 467
EP  - 479
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594012
ER  - 

TY  - CONF
TI  - On the praxes and politics of AI speech emotion recognition
AU  - Kang, Edward B.
T3  - FAccT '23
AB  - There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594011
SP  - 455
EP  - 466
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594011
ER  - 

TY  - CONF
TI  - Rethinking transparency as a communicative constellation
AU  - Eyert, Florian
AU  - Lopez, Paola
T3  - FAccT '23
AB  - In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594010
SP  - 444
EP  - 454
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594010
KW  - explainability
KW  - transparency
KW  - prediction
KW  - deliberation
KW  - science communication
ER  - 

TY  - CONF
TI  - Algorithmic transparency and accountability through crowdsourcing: A study of the NYC school admission lottery
AU  - Marian, Amelie
T3  - FAccT '23
AB  - Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency. In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many “wasted choices” on students’ ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594009
SP  - 434
EP  - 443
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594009
KW  - transparency
KW  - accountability
KW  - crowdsourcing
KW  - school matching
ER  - 

TY  - CONF
TI  - Domain adaptive decision trees: Implications for accuracy and fairness
AU  - Alvarez, Jose M.
AU  - Scott, Kristen M.
AU  - Berendt, Bettina
AU  - Ruggieri, Salvatore
T3  - FAccT '23
AB  - In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594008
SP  - 423
EP  - 433
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594008
KW  - Fairness
KW  - Decision Trees
KW  - Domain Adaptation
KW  - Covariate Shift
KW  - folktables
KW  - Information Gain
ER  - 

TY  - CONF
TI  - The possibility of fairness: Revisiting the impossibility theorem in practice
AU  - Bell, Andrew
AU  - Bynum, Lucius
AU  - Drushchak, Nazarii
AU  - Zakharchenko, Tetiana
AU  - Rosenblatt, Lucas
AU  - Stoyanovich, Julia
T3  - FAccT '23
AB  - The “impossibility theorem” — which is considered foundational in algorithmic fairness literature — asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner’s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when — and to what degree — fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594007
SP  - 400
EP  - 422
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594007
KW  - machine learning
KW  - fairness
KW  - responsible AI
KW  - public policy
ER  - 

TY  - CONF
TI  - Group-fair classification with strategic agents
AU  - Estornell, Andrew
AU  - Das, Sanmay
AU  - Liu, Yang
AU  - Vorobeychik, Yevgeniy
T3  - FAccT '23
AB  - The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these to be “fair” under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior can have on group-fair classification. We find that in many settings strategic behavior can lead to fairness reversal, with a conventional classifier exhibiting higher fairness than a classifier trained to satisfy group fairness. Further, we show that fairness reversal occurs as a result of a group-fair classifier becoming more selective, achieving fairness largely by excluding individuals from the advantaged group. In contrast, if group fairness is achieved by the classifier becoming more inclusive, fairness reversal does not occur.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594006
SP  - 389
EP  - 399
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594006
ER  - 

TY  - CONF
TI  - Envisioning equitable speech technologies for black older adults
AU  - Brewer, Robin N.
AU  - Harrington, Christina
AU  - Heldreth, Courtney
T3  - FAccT '23
AB  - There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594005
SP  - 379
EP  - 388
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594005
ER  - 

TY  - CONF
TI  - On the independence of association bias and empirical fairness in language models
AU  - Cabello, Laura
AU  - Jørgensen, Anna Katrine
AU  - Søgaard, Anders
T3  - FAccT '23
AB  - The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594004
SP  - 370
EP  - 378
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594004
KW  - Fairness
KW  - Natural Language Processing
KW  - Representational Bias
ER  - 

TY  - CONF
TI  - Simplicity bias leads to amplified performance disparities
AU  - Bell, Samuel James
AU  - Sagun, Levent
T3  - FAccT '23
AB  - Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594003
SP  - 355
EP  - 369
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594003
KW  - fairness
KW  - neural networks
KW  - performance disparities
KW  - simplicity bias
ER  - 

TY  - CONF
TI  - Stronger together: on the articulation of ethical charters, legal tools, and technical documentation in ML
AU  - Pistilli, Giada
AU  - Muñoz Ferrandis, Carlos
AU  - Jernite, Yacine
AU  - Mitchell, Margaret
T3  - FAccT '23
AB  - The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594002
SP  - 343
EP  - 354
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594002
KW  - Documentation
KW  - AI Governance
KW  - AI Policy
KW  - Applied Ethics
KW  - ML Licensing
ER  - 

TY  - CONF
TI  - Explainable AI is dead, long live explainable AI! Hypothesis-driven decision support using evaluative AI
AU  - Miller, Tim
T3  - FAccT '23
AB  - In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594001
SP  - 333
EP  - 342
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594001
ER  - 

TY  - CONF
TI  - ‘Affordances’ for machine learning
AU  - Davis, Jenny L
T3  - FAccT '23
AB  - The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. ‘Affordances’ are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power—and the systems power relations produce—to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3594000
SP  - 324
EP  - 332
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3594000
KW  - Machine Learning
KW  - Affordances
KW  - AI Alignment
KW  - Design Studies
KW  - Mechanisms and Conditions Framework
KW  - Principles-to-Practice
ER  - 

TY  - CONF
TI  - Ghosting the machine: Judicial resistance to a recidivism risk assessment instrument
AU  - Pruss, Dasha
T3  - FAccT '23
AB  - Recidivism risk assessment instruments are presented as an ‘evidence-based’ strategy for criminal justice reform – a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as “useless,” “worthless,” “boring,” “a waste of time,” “a non-thing,” and simply “not helpful.” I argue that this algorithm aversion cannot be accounted for by individuals’ distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument’s non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593999
SP  - 312
EP  - 323
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593999
KW  - human-AI interaction
KW  - criminal justice
KW  - algorithm aversion
KW  - community-informed
KW  - risk assessment instruments
ER  - 

TY  - CONF
TI  - Multi-target multiplicity: Flexibility and fairness in target specification under resource constraints
AU  - Watson-Daniels, Jamelle
AU  - Barocas, Solon
AU  - Hofman, Jake M.
AU  - Chouldechova, Alexandra
T3  - FAccT '23
AB  - Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals’ outcomes and selection rate disparities across groups. We call this multi-target multiplicity. Along the way, we refine the study of single-target multiplicity by introducing notions of multiplicity that respect resource constraints—a feature of many real-world tasks that isn’t captured by existing notions of predictive multiplicity. We apply our methods on a healthcare dataset, and show that the level of multiplicity that stems from target variable choice can be greater than that stemming from nearly-optimal models of a single target.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593998
SP  - 297
EP  - 311
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593998
KW  - fairness
KW  - multiplicity
KW  - predictive inconsistency
KW  - target specification
ER  - 

TY  - CONF
TI  - Saliency cards: A framework to characterize and compare saliency methods
AU  - Boggust, Angie
AU  - Suresh, Harini
AU  - Strobelt, Hendrik
AU  - Guttag, John
AU  - Satyanarayan, Arvind
T3  - FAccT '23
AB  - Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model’s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency: methodology, or how the saliency is calculated; sensitivity, or the relationship between the saliency and the underlying model and data; and, perceptibility, or how an end user ultimately interprets the result. By collating this information, saliency cards allow users to more holistically assess and compare the implications of different methods. Through nine semi-structured interviews with users from various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed vocabulary for discussing individual methods and allow for a more systematic selection of task-appropriate methods. Moreover, with saliency cards, we are able to analyze the research landscape in a more structured fashion to identify opportunities for new methods and evaluation metrics for unmet user needs.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593997
SP  - 285
EP  - 296
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593997
KW  - interpretability
KW  - transparency
KW  - documentation
KW  - saliency
KW  - saliency cards
ER  - 

TY  - CONF
TI  - Making intelligence: Ethical values in IQ and ML benchmarks
AU  - Blili-Hamelin, Borhane
AU  - Hancox-Li, Leif
T3  - FAccT '23
AB  - In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly “technical” or “scientific” decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence—standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating value-neutral benchmarks. Finally, we outline practical recommendations for ML benchmark research ethics and ethics review.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593996
SP  - 271
EP  - 284
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593996
KW  - feminist philosophy of science
KW  - IQ and ML benchmarks
KW  - thick concepts in science
KW  - value-neutrality
KW  - Values in ML benchmarks
ER  - 

TY  - CONF
TI  - The ethical ambiguity of AI data enrichment: Measuring gaps in research ethics norms and practices
AU  - Hawkins, Will
AU  - Mittelstadt, Brent
T3  - FAccT '23
AB  - The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or ‘data enrichment’, has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593995
SP  - 261
EP  - 270
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593995
KW  - artificial intelligence
KW  - research ethics
KW  - data enrichment
ER  - 

TY  - CONF
TI  - Certification labels for trustworthy AI: Insights from an empirical mixed-method study
AU  - Scharowski, Nicolas
AU  - Benk, Michaela
AU  - Kühne, Swen J.
AU  - Wettstein, Léane
AU  - Brühlmann, Florian
T3  - FAccT '23
AB  - Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users’ attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users’ trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users’ preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593994
SP  - 248
EP  - 260
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593994
KW  - Trust
KW  - AI
KW  - User study
KW  - Certification
KW  - Documentation
KW  - Trustworthy
KW  - Audit
KW  - Label
KW  - Seal
ER  - 

TY  - CONF
TI  - Diagnosing AI explanation methods with folk concepts of behavior
AU  - Jacovi, Alon
AU  - Bastings, Jasmijn
AU  - Gehrmann, Sebastian
AU  - Goldberg, Yoav
AU  - Filippova, Katja
T3  - FAccT '23
AB  - We investigate a formalism for the conditions of a successful explanation of AI. We consider “success” to depend not only on what information the explanation contains, but also on what information the human explainee understands from it. Theory of mind literature discusses the folk concepts that humans use to understand and generalize behavior. We posit that folk concepts of behavior provide us with a “language” that humans understand behavior with. We use these folk concepts as a framework of social attribution by the human explainee—the information constructs that humans are likely to comprehend from explanations—by introducing a blueprint for an explanatory narrative that explains AI behavior with these constructs. We then demonstrate that many XAI methods today can be mapped to folk concepts of behavior in a qualitative evaluation. This allows us to uncover their failure modes that prevent current methods from explaining successfully—i.e., the information constructs that are missing for any given XAI method, and whose inclusion can decrease the likelihood of misunderstanding AI behavior.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593993
SP  - 247
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593993
KW  - interpretability
KW  - explainability
KW  - xai
KW  - folk psychology
KW  - human-centered xai
KW  - theory of mind
ER  - 

TY  - CONF
TI  - Who should pay when machines cause harm? Laypeople’s expectations of legal damages for machine-caused harm
AU  - Lima, Gabriel
AU  - Grgic-Hlaca, Nina
AU  - Jeong, Jin Keun
AU  - Cha, Meeyoung
T3  - FAccT '23
AB  - The question of who should be held responsible when machines cause harm in high-risk environments is open to debate. Empirical research examining laypeople’s opinions has been largely restricted to the moral domain and has only inspected a limited set of negative outcomes. This study collects lay perceptions of legal responsibility for a wide range of machine-caused harms. We investigated how much people (N = 572) expect users and developers of machines to pay as legal damages in 37 diverse scenarios from the book “How Humans Judge Machines” by Hidalgo et al. [37]. Our results suggest that people’s expectations of legal damages for machine-caused harms are influenced by several factors, including perceived moral wrongness and the presence of victims. The scenarios exhibited substantial variation in how they were perceived and thus in the amount of legal damages they called for. People viewed both users and developers as legally responsible and expected the latter to pay higher damages. We discuss our findings in the context of future regulations of machines.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593992
SP  - 236
EP  - 246
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593992
KW  - artificial intelligence
KW  - responsibility
KW  - AI
KW  - robot
KW  - liability
KW  - algorithm
KW  - punishment
KW  - machine
KW  - damages
ER  - 

TY  - CONF
TI  - Algorithmic Transparency from the South: Examining the state of algorithmic transparency in Chile's public administration algorithms
AU  - Lapostol Piderit, José Pablo
AU  - Garrido Iglesias, Romina
AU  - Hermosilla Cornejo, María Paz
T3  - FAccT '23
AB  - This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ibáñez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593991
SP  - 227
EP  - 235
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593991
ER  - 

TY  - CONF
TI  - Walking the walk of AI ethics: Organizational challenges and the individualization of risk among ethics entrepreneurs
AU  - Ali, Sanna J.
AU  - Christin, Angèle
AU  - Smart, Andrew
AU  - Katila, Riitta
T3  - FAccT '23
AB  - Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate “ethics washing.” Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593990
SP  - 217
EP  - 226
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593990
KW  - AI ethics
KW  - responsible AI
KW  - organizations
KW  - decoupling
KW  - institutional change
KW  - neo-institutionalism
KW  - Science and Technology Studies
ER  - 

TY  - CONF
TI  - "I wouldn’t say offensive but...": Disability-centered perspectives on large language models
AU  - Gadiraju, Vinitha
AU  - Kane, Shaun
AU  - Dev, Sunipa
AU  - Taylor, Alex
AU  - Wang, Ding
AU  - Denton, Emily
AU  - Brewer, Robin
T3  - FAccT '23
AB  - Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593989
SP  - 205
EP  - 216
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593989
KW  - artificial intelligence
KW  - qualitative
KW  - chatbot
KW  - data annotation
KW  - algorithmic harms
KW  - dialog model
KW  - disability representation
KW  - large language models
ER  - 

TY  - CONF
TI  - The dataset multiplicity problem: How unreliable data impacts predictions
AU  - Meyer, Anna P.
AU  - Albarghouthi, Aws
AU  - D'Antoni, Loris
T3  - FAccT '23
AB  - We introduce dataset multiplicity, a way to study how inaccuracies, uncertainty, and social bias in training datasets impact test-time predictions. The dataset multiplicity framework asks a counterfactual question of what the set of resultant models (and associated test-time predictions) would be if we could somehow access all hypothetical, unbiased versions of the dataset. We discuss how to use this framework to encapsulate various sources of uncertainty in datasets’ factualness, including systemic social bias, data collection practices, and noisy labels or features. We show how to exactly analyze the impacts of dataset multiplicity for a specific model architecture and type of uncertainty: linear models with label errors. Our empirical analysis shows that real-world datasets, under reasonable assumptions, contain many test samples whose predictions are affected by dataset multiplicity. Furthermore, the choice of domain-specific dataset multiplicity definition determines what samples are affected, and whether different demographic groups are disparately impacted. Finally, we discuss implications of dataset multiplicity for machine learning practice and research, including considerations for when model outcomes should not be trusted.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593988
SP  - 193
EP  - 204
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593988
KW  - model robustness
KW  - procedural fairness
KW  - data bias
KW  - Dataset multiplicity
KW  - model multiplicity
ER  - 

TY  - CONF
TI  - In her shoes: Gendered labelling in crowdsourced safety perceptions data from india
AU  - Sengupta, Nandana
AU  - Vaidya, Ashwini
AU  - Evans, James
T3  - FAccT '23
AB  - In recent years, a proliferation of women’s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate ‘safety maps’ used by policy makers for urban design and academics for studying mobility patterns. Men and women’s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents’ data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593987
SP  - 183
EP  - 192
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593987
KW  - safety
KW  - India
KW  - algorithmic bias
KW  - gender
KW  - crowdsourced ratings
ER  - 

TY  - CONF
TI  - Trustworthy AI and the logics of intersectional resistance
AU  - Knowles, Bran
AU  - Fledderjohann, Jasmine
AU  - Richards, John T.
AU  - Varshney, Kush R.
T3  - FAccT '23
AB  - Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for ‘trustworthy AI’ and, from these, objective indicators of ‘trustworthiness’ for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly ‘trustworthy’ AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as ‘undeserving’, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI—fairness, accountability, and transparency—that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593986
SP  - 172
EP  - 182
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593986
KW  - Trust
KW  - artificial intelligence
KW  - transparency
KW  - accountability
KW  - fairness
KW  - bias
KW  - inequality
KW  - distrust
KW  - intersectionality
ER  - 

TY  - CONF
TI  - WEIRD FAccTs: How western, educated, industrialized, rich, and democratic is FAccT?
AU  - Septiandri, Ali Akbar
AU  - Constantinides, Marios
AU  - Tahaei, Mohammad
AU  - Quercia, Daniele
T3  - FAccT '23
AB  - Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world’s population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems’ fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593985
SP  - 160
EP  - 171
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593985
ER  - 

TY  - CONF
TI  - Preventing discriminatory decision-making in evolving data streams
AU  - Wang, Zichong
AU  - Saxena, Nripsuta
AU  - Yu, Tongjia
AU  - Karki, Sneha
AU  - Zetty, Tyler
AU  - Haque, Israat
AU  - Zhou, Shan
AU  - Kc, Dukka
AU  - Stockwell, Ian
AU  - Wang, Xuyu
AU  - Bifet, Albert
AU  - Zhang, Wenbin
T3  - FAccT '23
AB  - Bias in machine learning has rightly received significant attention over the past decade. However, most fair machine learning (fair-ML) works to address bias in decision-making systems has focused solely on the offline setting. Despite the wide prevalence of online systems in the real world, work on identifying and correcting bias in the online setting is severely lacking. The unique challenges of the online environment make addressing bias more difficult than in the offline setting. First, Streaming Machine Learning (SML) algorithms must deal with the constantly evolving real-time data stream. Secondly, they need to adapt to changing data distributions (concept drift) to make accurate predictions on new incoming data. Incorporating fairness constraints into this already intricate task is not straightforward. In this work, we focus on the challenges of achieving fairness in biased data streams while accounting for the presence of concept drift, accessing one sample at a time. We present Fair Sampling over Stream (FS2), a novel fair rebalancing approach capable of being integrated with SML classification algorithms. Furthermore, we devise the first unified performance-fairness metric, Fairness Bonded Utility (FBU), to efficiently evaluate and compare the trade-offs between performance and fairness across various bias mitigation methods. FBU simplifies the comparison of fairness-performance trade-offs of multiple techniques through one unified and intuitive evaluation, allowing model designers to easily choose a technique. Overall, extensive evaluations show our measures surpass those of other fair online techniques previously reported in the literature.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593984
SP  - 149
EP  - 159
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593984
KW  - Fairness
KW  - Concept Drift
KW  - Data Stream
KW  - Fairness Drift
ER  - 

TY  - CONF
TI  - “How biased are your features?”: Computing fairness influence functions with global sensitivity analysis
AU  - Ghosh, Bishwamittra
AU  - Basu, Debabrota
AU  - Meel, Kuldeep S.
T3  - FAccT '23
AB  - Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier’s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier’s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier. The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593983
SP  - 138
EP  - 148
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593983
KW  - Explainability
KW  - Bias
KW  - Fair Machine Learning
KW  - Global Sensitivity Analysis
KW  - Influence Functions.
KW  - Variance Decomposition
ER  - 

TY  - CONF
TI  - In the name of fairness: Assessing the bias in clinical record de-identification
AU  - Xiao, Yuxin
AU  - Lim, Shulammite
AU  - Pollard, Tom Joseph
AU  - Ghassemi, Marzyeh
T3  - FAccT '23
AB  - Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593982
SP  - 123
EP  - 137
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593982
KW  - Fairness
KW  - Clinical De-identification
KW  - Named Entity Recognition
ER  - 

TY  - CONF
TI  - The gradient of generative AI release: Methods and considerations
AU  - Solaiman, Irene
T3  - FAccT '23
AB  - As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593981
SP  - 111
EP  - 122
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593981
ER  - 

TY  - CONF
TI  - Reconciling individual probability Forecasts
AU  - Roth, Aaron
AU  - Tolbert, Alexander
AU  - Weinstein, Scott
T3  - FAccT '23
AB  - Individual probabilities refer to the probabilities of outcomes that are realized only once: the probability that it will rain tomorrow, the probability that Alice will die within the next 12 months, the probability that Bob will be arrested for a violent crime in the next 18 months, etc. Individual probabilities are fundamentally unknowable. Nevertheless, we show that two parties who agree on the data—or on how to sample from a data distribution—cannot agree to disagree on how to model individual probabilities. This is because any two models of individual probabilities that substantially disagree can together be used to empirically falsify and improve at least one of the two models. This can be efficiently iterated in a process of “reconciliation” that results in models that both parties agree are superior to the models they started with, and which themselves (almost) agree on the forecasts of individual probabilities (almost) everywhere. We conclude that although individual probabilities are unknowable, they are contestable via a computationally and data efficient process that must lead to agreement. Thus we cannot find ourselves in a situation in which we have two equally accurate and unimprovable models that disagree substantially in their predictions—providing an answer to what is sometimes called the predictive or model multiplicity problem.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593980
SP  - 101
EP  - 110
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593980
ER  - 

TY  - CONF
TI  - Multi-dimensional discrimination in law and machine learning - a comparative overview
AU  - Roy, Arjun
AU  - Horstmann, Jan
AU  - Ntoutsi, Eirini
T3  - FAccT '23
AB  - AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called “multi-dimensional discrimination” or “multi-dimensional fairness” problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593979
SP  - 89
EP  - 100
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593979
KW  - additive fairness
KW  - intersectional fairness
KW  - multi-discrimination
KW  - multi-fairness
KW  - sequential fairness
ER  - 

TY  - CONF
TI  - Humans, AI, and context: Understanding end-users’ trust in a real-world computer vision application
AU  - Kim, Sunnie S. Y.
AU  - Watkins, Elizabeth Anne
AU  - Russakovsky, Olga
AU  - Fong, Ruth
AU  - Monroy-Hernández, Andrés
T3  - FAccT '23
AB  - Trust is an important factor in people’s interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593978
SP  - 77
EP  - 88
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593978
KW  - Computer Vision
KW  - Human-AI Interaction
KW  - Trust in AI
KW  - Case Study
ER  - 

TY  - CONF
TI  - Welfarist moral grounding for transparent AI
AU  - Narayanan, Devesh
T3  - FAccT '23
AB  - As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple – even simplistic – move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593977
SP  - 64
EP  - 76
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593977
KW  - Transparency
KW  - AI Ethics
KW  - Moral Theory
KW  - Welfarism
ER  - 

TY  - CONF
TI  - Optimization’s neglected normative commitments
AU  - Laufer, Benjamin
AU  - Gilbert, Thomas
AU  - Nissenbaum, Helen
T3  - FAccT '23
AB  - Optimization is offered as an objective approach to resolving complex, real-world decisions involving uncertainty and conflicting interests. It drives business strategies as well as public policies and, increasingly, lies at the heart of sophisticated machine learning systems. A paradigm used to approach potentially high-stakes decisions, optimization relies on abstracting the real world to a set of decision(s), objective(s) and constraint(s). Drawing from the modeling process and a range of actual cases, this paper describes the normative choices and assumptions that are necessarily part of using optimization. It then identifies six emergent problems that may be neglected: 1) Misspecified values can yield optimizations that omit certain imperatives altogether or incorporate them incorrectly as a constraint or as part of the objective, 2) Problematic decision boundaries can lead to faulty modularity assumptions and feedback loops, 3) Failing to account for multiple agents’ divergent goals and decisions can lead to policies that serve only certain narrow interests, 4) Mislabeling and mismeasurement can introduce bias and imprecision, 5) Faulty use of relaxation and approximation methods, unaccompanied by formal characterizations and guarantees, can severely impede applicability, and 6) Treating optimization as a justification for action, without specifying the necessary contextual information, can lead to ethically dubious or faulty decisions. Suggestions are given to further understand and curb the harms that can arise when optimization is used wrongfully.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593976
SP  - 50
EP  - 63
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593976
KW  - ethics
KW  - Optimization
KW  - values
KW  - modeling assumptions
ER  - 

TY  - CONF
TI  - Two reasons for subjecting medical AI systems to lower standards than humans
AU  - Mainz, Jakob
AU  - Munch, Lauritz
AU  - Bjerring, Jens Christian
T3  - FAccT '23
AB  - This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593975
SP  - 44
EP  - 49
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593975
KW  - Speed
KW  - Opacity
KW  - Cost-effectiveness
KW  - Double Standard
KW  - Predictive Accuracy
ER  - 

TY  - CONF
TI  - Fairness in machine learning from the perspective of sociology of statistics: How machine learning is becoming scientific by turning its back on metrological realism
AU  - Benbouzid, Bilel
T3  - FAccT '23
AB  - We argue in this article that the integration of fairness into machine learning, or FairML, is a valuable exemplar of the politics of statistics and their ongoing transformations. Classically, statisticians sought to eliminate any trace of politics from their measurement tools. But data scientists who are developing predictive machines for social applications – are inevitably confronted with the problem of fairness. They thus face two difficult and often distinct types of demands: first, for reliable computational techniques, and second, for transparency, given the constructed, politically situated nature of quantification operations. We begin by socially localizing the formation of FairML as a field of research and describing the associated epistemological framework. We then examine how researchers simultaneously think the mathematical and social construction of approaches to machine learning, following controversies around fairness metrics and their status. Thirdly and finally, we show that FairML approaches tend towards a specific form of objectivity, “trained judgement,” which is based on a reasonably partial justification from the designer of the machine – which itself comes to be politically situated as a result.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593974
SP  - 35
EP  - 43
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593974
KW  - fairness in machine learning
KW  - objectivity
KW  - situated knowledge
KW  - controversy mapping
KW  - epistemic virtues
KW  - social epistemology
KW  - sociology of quantification
KW  - sociology of sciences and technologies
ER  - 

TY  - CONF
TI  - ‘We are adults and deserve control of our phones’: Examining the risks and opportunities of a right to repair for mobile apps
AU  - Kollnig, Konrad
AU  - Datta, Siddhartha
AU  - Serban Von Davier, Thomas
AU  - Van Kleek, Max
AU  - Binns, Reuben
AU  - Lyngs, Ulrik
AU  - Shadbolt, Nigel
T3  - FAccT '23
AB  - Many mobile apps are designed not just to support end-users’ needs, but also commercial aims. This can result in app designs that compromise end-user privacy, safety, and well-being. Since apps nowadays provide vital digital information and services, users often have no choice but to accept potentially harmful or manipulative app designs. What if, instead, individuals could customise their apps to make them safer and better suit their needs? This exploratory work examines this question through a multi-faceted approach; first, to understand user needs, we conducted a survey (n = 100) of changes users wanted in their apps, and of perceptions of risks in app repair. Second, to identify technical challenges, we developed a prototype that enables end-users to change their apps, and realised several modifications suggested by survey participants. Finally, we conduct a set of expert interviews (n = 8) to delve into the ethical and legal aspects of such a tool, and synthesise a framework of risks and opportunities of app repair.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593973
SP  - 22
EP  - 34
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593973
KW  - privacy
KW  - mobile apps
KW  - dark patterns
KW  - digital harms
KW  - right to repair
ER  - 

TY  - CONF
TI  - How to explain and justify almost any decision: Potential pitfalls for accountability in AI decision-making
AU  - Zhou, Joyce
AU  - Joachims, Thorsten
T3  - FAccT '23
AB  - Discussion of the “right to an explanation” has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593972
SP  - 12
EP  - 21
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593972
KW  - explainable AI
KW  - right to an explanation
KW  - adversarial explanations
ER  - 

TY  - CONF
TI  - Broadening AI ethics narratives: An indic art view
AU  - Divakaran, Ajay
AU  - Sridhar, Aparna
AU  - Srinivasan, Ramya
T3  - FAccT '23
AB  - Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction–that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the ‘Natyashastra’), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593971
SP  - 2
EP  - 11
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593971
KW  - AI ethics
KW  - Indian arts
ER  - 

TY  - CONF
TI  - Machine explanations and human understanding
AU  - Chen, Chacha
AU  - Feng, Shi
AU  - Sharma, Amit
AU  - Tan, Chenhao
T3  - FAccT '23
AB  - Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. To address this question, we first identify three core concepts that cover most existing quantitative measures of understanding: task decision boundary, model decision boundary, and model error. Using adapted causal diagrams, we provide a formal characterization of the relationship between these concepts and human approximations (i.e., understanding) of them. The relationship varies by the level of human intuition in different task types, such as emulation and discovery, which are often ignored when building or evaluating explanation methods. Our key result is that human intuitions are necessary for generating and evaluating machine explanations in human-AI decision making: without assumptions about human intuitions, explanations may improve human understanding of model decision boundary, but cannot improve human understanding of task decision boundary or model error. To validate our theoretical claims, we conduct human subject studies to show the importance of human intuitions. Together with our theoretical contributions, we provide a new paradigm for designing behavioral studies towards a rigorous view of the role of machine explanations across different tasks of human-AI decision making.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM conference on fairness, accountability, and transparency
DA  - 2023///
PY  - 2023
DO  - 10.1145/3593013.3593970
SP  - 1
PB  - Association for Computing Machinery
SN  - 9798400701924
UR  - https://doi.org/10.1145/3593013.3593970
ER  - 

TY  - CONF
TI  - Against Predictive Optimization: On the Legitimacy of Decision-Making Algorithms that Optimize Predictive Accuracy
AU  - Wang, Angelina
AU  - Kapoor, Sayash
AU  - Barocas, Solon
AU  - Narayanan, Arvind
T3  - FAccT '23
AB  - We formalize predictive optimization, a category of decision-making algorithms that use machine learning (ML) to predict future outcomes of interest about \ individuals}. For example, pre-trial risk prediction algorithms such as COMPAS use ML to predict whether an individual will re-offend in the future. Our thesis is that predictive optimization raises a distinctive and serious set of normative concerns that cause it to fail on its own terms. To test this, we review 387 reports, articles, and web pages from academia, industry, non-profits, governments, and modeling contests, and find many real-world examples of predictive optimization. We select eight particularly consequential examples as case studies. Simultaneously, we develop a set of normative and technical critiques that challenge the claims made by the developers of these applications—in particular, claims of increased accuracy, efficiency, and fairness. Our key finding is that these critiques apply to each of the applications, are not easily evaded by redesigning the systems, and thus challenge whether these applications should be deployed. We argue that the burden of evidence for justifying why the deployment of predictive optimization is not harmful should rest with the developers of the tools. Based on our analysis, we provide a rubric of critical questions that can be used to deliberate or contest specific predictive optimization applications.
C1  - New York, NY, USA
C3  - Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency
DA  - 2023/06/12/
PY  - 2023
DO  - 10.1145/3593013.3594030
DP  - ACM Digital Library
SP  - 626
PB  - Association for Computing Machinery
SN  - 9798400701924
ST  - Against Predictive Optimization
UR  - https://dl.acm.org/doi/10.1145/3593013.3594030
Y2  - 2024/03/20/
L1  - https://dl.acm.org/doi/pdf/10.1145/3593013.3594030
KW  - machine learning
KW  - algorithms
KW  - Predictive optimization
ER  - 

