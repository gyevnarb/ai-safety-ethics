[
	{
		"id": "azizSubcommitteeApprovalVoting2018",
		"type": "paper-conference",
		"abstract": "Social choice is replete with various settings including single-winner voting, multi-winner voting, probabilistic voting, multiple referenda, and public decision making. We study a general model of social choice called sub-committee voting (SCV) that simultaneously generalizes these settings. We then focus on sub-committee voting with approvals and propose extensions of the justified representation axioms that have been considered for proportional representation in approval-based committee voting. We study the properties and relations of these axioms. For each of the axioms, we analyze whether a representative committee exists and also examine the complexity of computing and verifying such a committee.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278739",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "3–9",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sub-committee approval voting and generalized justified representation axioms",
		"URL": "https://doi.org/10.1145/3278721.3278739",
		"author": [
			{
				"family": "Aziz",
				"given": "Haris"
			},
			{
				"family": "Lee",
				"given": "Barton E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "babaeiPurpleFeedIdentifying2018",
		"type": "paper-conference",
		"abstract": "Although diverse news stories are actively posted on social media, readers often focus on the news which reinforces their pre-existing views, leading to 'filter bubble' effects. To combat this, some recent systems expose and nudge readers toward stories with different points of view. One example is the Wall Street Journal's 'Blue Feed, Red Feed' system, which presents posts from biased publishers on each side of a topic. However, these systems have had limited success. We present a complementary approach which identifies high consensus 'purple' posts that generate similar reactions from both 'blue' and 'red' readers. We define and operationalize consensus for news posts on Twitter in the context of US politics. We show that high consensus posts can be identified and discuss their empirical properties. We present a method for automatically identifying high and low consensus news posts on Twitter, which can work at scale across many publishers. To do this, we propose a novel category of audience leaning based features, which we show are well suited to this task. Finally, we present our 'Purple Feed' system which highlights high consensus posts from publishers on both sides of the political spectrum.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278761",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "10–16",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Purple feed: Identifying high consensus news posts on social media",
		"URL": "https://doi.org/10.1145/3278721.3278761",
		"author": [
			{
				"family": "Babaei",
				"given": "Mahmoudreza"
			},
			{
				"family": "Kulshrestha",
				"given": "Juhi"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Benevenuto",
				"given": "Fabrício"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "biscontilucidiCompanionRobotsHallucinatory2018",
		"type": "paper-conference",
		"abstract": "The advent of the so-called Companion Robots is raising many ethical concerns among scholars and in the public opinion. Focusing mainly on robots caring for the elderly, in this paper we analyze these concerns to distinguish which are directly ascribable to robotic, and which are instead pre-existent. One of these is the \"deception objection\", namely the ethical unacceptability of deceiving the user about the simulated nature of the robot's behaviors. We argue on the inconsistency of this charge, as today formulated. After that, we underline the risk, for human-robot interaction, to become a hallucinatory relation where the human would subjectify the robot in a dynamic of meaning-overload. Finally, we analyze the definition of \"quasi-other\" relating to the notion of \"uncanny\". The goal of this paper is to argue that the main concern about Companion Robots is the simulation of a human-like interaction in the absence of an autonomous robotic horizon of meaning. In addition, that absence could lead the human to build a hallucinatory reality based on the relation with the robot.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278741",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "17–22",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Companion robots: the hallucinatory danger of human-robot interactions",
		"URL": "https://doi.org/10.1145/3278721.3278741",
		"author": [
			{
				"family": "Bisconti Lucidi",
				"given": "Piercosma"
			},
			{
				"family": "Nardi",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "bjorgenCakeDeathTrolleys2018",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems are becoming part of our lives and societies. The more decisions such systems make for us, the more we need to ensure that the decisions they make have a positive individual and societal ethical impact. How can we estimate how good a system is at making ethical decisions? Benchmarking is used to evaluate how good a machine or a process performs with respect to industry bests. In this paper we argue that (some) ethical dilemmas can be used as benchmarks for estimating the ethical performance of an autonomous system. We advocate that an open source repository of such dilemmas should be maintained. We present a prototype of such a repository available at https://imdb. uib.no/dilemmaz/articles/all1.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278767",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "23–29",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Cake, Death, and Trolleys: Dilemmas as benchmarks of ethical decision-making",
		"URL": "https://doi.org/10.1145/3278721.3278767",
		"author": [
			{
				"family": "Bjørgen",
				"given": "Edvard P."
			},
			{
				"family": "Madsen",
				"given": "Simen"
			},
			{
				"family": "Bjørknes",
				"given": "Therese S."
			},
			{
				"family": "Heimsæter",
				"given": "Fredrik V."
			},
			{
				"family": "Håvik",
				"given": "Robin"
			},
			{
				"family": "Linderud",
				"given": "Morten"
			},
			{
				"family": "Longberg",
				"given": "Per-Niklas"
			},
			{
				"family": "Dennis",
				"given": "Louise A."
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "careyIncorrigibilityCIRLFramework2018",
		"type": "paper-conference",
		"abstract": "A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. 2015 in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278750",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "30–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Incorrigibility in the CIRL framework",
		"URL": "https://doi.org/10.1145/3278721.3278750",
		"author": [
			{
				"family": "Carey",
				"given": "Ryan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "caveAIRaceStrategic2018",
		"type": "paper-conference",
		"abstract": "The rhetoric of the race for strategic advantage is increasingly being used with regard to the development of artificial intelligence (AI), sometimes in a military context, but also more broadly. This rhetoric also reflects real shifts in strategy, as industry research groups compete for a limited pool of talented researchers, and nation states such as China announce ambitious goals for global leadership in AI. This paper assesses the potential risks of the AI race narrative and of an actual competitive race to develop AI, such as incentivising corner-cutting on safe-ty and governance, or increasing the risk of conflict. It explores the role of the research community in respond-ing to these risks. And it briefly explores alternative ways in which the rush to develop powerful AI could be framed so as instead to foster collaboration and respon-sible progress.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278780",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "36–40",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An AI race for strategic advantage: Rhetoric and risks",
		"URL": "https://doi.org/10.1145/3278721.3278780",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			},
			{
				"family": "ÓhÉigeartaigh",
				"given": "Seán S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "chanUtilizingHousingResources2018",
		"type": "paper-conference",
		"abstract": "There are over 1 million homeless youth in the U.S. each year. To reduce homelessness, U.S. Housing and Urban Development (HUD) and housing communities provide housing programs/services to homeless youth with the goal of improving their long-term situation. Housing communities are facing a difficult task of filling their housing programs, with as many youths as possible, subject to resource constraints for meeting the needs of youth. Currently, the assignment is manually done by humans working in the housing communities. In this paper, we consider the problem of assigning homeless youth to housing programs subject to resource constraints. We provide an initial abstract model for this setting and show that the problem of maximizing the total assigned youth to the programs under this model is APX-hard. To solve the problem, we non-trivially formulate it as a multiple multi-dimensional knapsack problem (MMDKP), which is not known to have any approximation algorithm. We provide a first interpretable and easy-to-use greedy algorithm with logarithmic approximation ratio for solving general MMDKP. We conduct experiments on random and realistic instances of the housing assignment settings and show that our algorithm is efficient and effective in solving large instances (up to 1 million youth).",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278757",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "41–47",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Utilizing housing resources for homeless youth through the lens of multiple multi-dimensional knapsacks",
		"URL": "https://doi.org/10.1145/3278721.3278757",
		"author": [
			{
				"family": "Chan",
				"given": "Hau"
			},
			{
				"family": "Tran-Thanh",
				"given": "Long"
			},
			{
				"family": "Wilder",
				"given": "Bryan"
			},
			{
				"family": "Rice",
				"given": "Eric"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Tambe",
				"given": "Milind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "chopraSociotechnicalSystemsEthics2018",
		"type": "paper-conference",
		"abstract": "Advances in AI techniques and computing platforms have triggered a lively and expanding discourse on ethical decision making by autonomous agents. Much recent work in AI concentrates on the challenges of moral decision making from a decision-theoretic perspective, and especially the representation of various ethical dilemmas. Such approaches may be useful but in general are not productive because moral decision making is as context-driven as other forms of decision making, if not more. In contrast, we consider ethics not from the standpoint of an individual agent but of the wider sociotechnical systems (STS) in which the agent operates. Our contribution in this paper is the conception of ethical STS founded on governance that takes into account stakeholder values, normative constraints on agents, and outcomes (states of the STS) that obtain due to actions taken by agents. An important element of our conception is accountability, which is necessary for adequate consideration of outcomes that prima facie appear ethical or unethical. Focusing on STS provides a basis for tackling the difficult problems of ethics because the norms of an STS give an operational basis for agent decision making.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278740",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "48–53",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sociotechnical systems and ethics in the large",
		"URL": "https://doi.org/10.1145/3278721.3278740",
		"author": [
			{
				"family": "Chopra",
				"given": "Amit K."
			},
			{
				"family": "SIngh",
				"given": "Munindar P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "daquinEthicsDesignMethodology2018",
		"type": "paper-conference",
		"abstract": "Addressing ethical issues arising from AI research, and by extension from most areas of Data Science, is a core challenge in both the academic and industry worlds. The nature of research and the specific set of technical skills involved imply that AI and Data Science researchers are not equipped to identify and anticipate such issues arising, or to establish solutions at the time a specific research project is being designed. In this paper, we discuss the need for a methodology for ethical research design that involves a broader set of skills from the start of the project. We specifically identify, from the relevant literature, a set of requirements that we argue to be needed for such a methodology. We then explore two case studies where such ethical considerations have been explored in conjunction with the development of specific research projects, in order to validate those assumptions and generalise them into a set of principles guiding an \"Ethics by Design\" method for conducting AI and Data Science research.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278765",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "54–59",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards an \"Ethics by Design\" methodology for AI research projects",
		"URL": "https://doi.org/10.1145/3278721.3278765",
		"author": [
			{
				"family": "Aquin",
				"given": "Mathieu",
				"non-dropping-particle": "d'"
			},
			{
				"family": "Troullinou",
				"given": "Pinelopi"
			},
			{
				"family": "O'Connor",
				"given": "Noel E."
			},
			{
				"family": "Cullen",
				"given": "Aindrias"
			},
			{
				"family": "Faller",
				"given": "Gráinne"
			},
			{
				"family": "Holden",
				"given": "Louise"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dignumEthicsDesignNecessity2018",
		"type": "paper-conference",
		"abstract": "Ethics by Design concerns the methods, algorithms and tools needed to endow autonomous agents with the capability to reason about the ethical aspects of their decisions, and the methods, tools and formalisms to guarantee that an agent's behavior remains within given moral bounds. In this context some questions arise: How and to what extent can agents understand the social reality in which they operate, and the other intelligences (AI, animals and humans) with which they co-exist? What are the ethical concerns in the emerging new forms of society, and how do we ensure the human dimension is upheld in interactions and decisions by autonomous agents?. But overall, the central question is: \"Can we, and should we, build ethically-aware agents?\" This paper presents initial conclusions from the thematic day of the same name held at PRIMA2017, on October 2017.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278745",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "60–66",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics by design: Necessity or curse?",
		"URL": "https://doi.org/10.1145/3278721.3278745",
		"author": [
			{
				"family": "Dignum",
				"given": "Virginia"
			},
			{
				"family": "Baldoni",
				"given": "Matteo"
			},
			{
				"family": "Baroglio",
				"given": "Cristina"
			},
			{
				"family": "Caon",
				"given": "Maurizio"
			},
			{
				"family": "Chatila",
				"given": "Raja"
			},
			{
				"family": "Dennis",
				"given": "Louise"
			},
			{
				"family": "Génova",
				"given": "Gonzalo"
			},
			{
				"family": "Haim",
				"given": "Galit"
			},
			{
				"family": "Kließ",
				"given": "Malte S."
			},
			{
				"family": "Lopez-Sanchez",
				"given": "Maite"
			},
			{
				"family": "Micalizio",
				"given": "Roberto"
			},
			{
				"family": "Pavón",
				"given": "Juan"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Smakman",
				"given": "Matthijs"
			},
			{
				"family": "Steenbergen",
				"given": "Marlies",
				"non-dropping-particle": "van"
			},
			{
				"family": "Tedeschi",
				"given": "Stefano"
			},
			{
				"family": "Toree",
				"given": "Leon",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Villata",
				"given": "Serena"
			},
			{
				"family": "Wildt",
				"given": "Tristan",
				"non-dropping-particle": "de"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dixonMeasuringMitigatingUnintended2018",
		"type": "paper-conference",
		"abstract": "We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278729",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "67–73",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring and mitigating unintended bias in text classification",
		"URL": "https://doi.org/10.1145/3278721.3278729",
		"author": [
			{
				"family": "Dixon",
				"given": "Lucas"
			},
			{
				"family": "Li",
				"given": "John"
			},
			{
				"family": "Sorensen",
				"given": "Jeffrey"
			},
			{
				"family": "Thain",
				"given": "Nithum"
			},
			{
				"family": "Vasserman",
				"given": "Lucy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "dyrkolbotnDistinctionImplicitExplicit2018",
		"type": "paper-conference",
		"abstract": "With recent advances in artificial intelligence and the rapidly increasing importance of autonomous intelligent systems in society, it is becoming clear that artificial agents will have to be designed to comply with complex ethical standards. As we work to develop moral machines, we also push the boundaries of existing legal categories. The most pressing question is what kind of ethical decision-making our machines are actually able to engage in. Both in law and in ethics, the concept of agency forms a basis for further legal and ethical categorisations, pertaining to decision-making ability. Hence, without a cross-disciplinary understanding of what we mean by ethical agency in machines, the question of responsibility and liability cannot be clearly addressed. Here we make first steps towards a comprehensive definition, by suggesting ways to distinguish between implicit and explicit forms of ethical agency.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278769",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "74–80",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the distinction between implicit and explicit ethical agency",
		"URL": "https://doi.org/10.1145/3278721.3278769",
		"author": [
			{
				"family": "Dyrkolbotn",
				"given": "Sjur"
			},
			{
				"family": "Pedersen",
				"given": "Truls"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "ehsanRationalizationNeuralMachine2018",
		"type": "paper-conference",
		"abstract": "We introduce em AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278736",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "81–87",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rationalization: A neural machine translation approach to generating natural language explanations",
		"URL": "https://doi.org/10.1145/3278721.3278736",
		"author": [
			{
				"family": "Ehsan",
				"given": "Upol"
			},
			{
				"family": "Harrison",
				"given": "Brent"
			},
			{
				"family": "Chan",
				"given": "Larry"
			},
			{
				"family": "Riedl",
				"given": "Mark O."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "eicherJillWatsonDoesn2018",
		"type": "paper-conference",
		"abstract": "Jill Watson is our name for a virtual teaching assistant for a Georgia Tech course on artificial intelligence: Jill answers routine, frequently asked questions on the class discussion forum. In this paper, we outline some of the ethical issues that arose in the development and deployment of the virtual teaching assistant. We posit that experiments such as Jill Watson are critical for deeply understanding AI ethics.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278760",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "88–94",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Jill watson doesn't care if you're pregnant: Grounding AI ethics in empirical studies",
		"URL": "https://doi.org/10.1145/3278721.3278760",
		"author": [
			{
				"family": "Eicher",
				"given": "Bobbie"
			},
			{
				"family": "Polepeddi",
				"given": "Lalith"
			},
			{
				"family": "Goel",
				"given": "Ashok"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "erdelyiRegulatingArtificialIntelligence2018",
		"type": "paper-conference",
		"abstract": "Given the ubiquity of artificial intelligence (AI) in modern societies, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that — drawing on interdisciplinary expertise — could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocurrencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278731",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "95–101",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating artificial intelligence: Proposal for a global solution",
		"URL": "https://doi.org/10.1145/3278721.3278731",
		"author": [
			{
				"family": "Erdélyi",
				"given": "Olivia J."
			},
			{
				"family": "Goldsmith",
				"given": "Judy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "estradaValueAlignmentFair2018",
		"type": "paper-conference",
		"abstract": "Ethics and safety research in artificial intelligence is increasingly framed in terms of \"alignment” with human values and interests. I argue that Turing's call for \"fair play for machines” is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on \"fair play” motivate a novel interpretation of Turing's notorious \"imitation game” as a condition not of intelligence but instead of value alignment : a machine demonstrates a minimal degree of alignment (with the norms of conversation, for instance) when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is \"fair” in precisely the sense that it encourages an alignment of interests between humans and machines.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278730",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "102–107",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Value alignment, fair play, and the rights of service robots",
		"URL": "https://doi.org/10.1145/3278721.3278730",
		"author": [
			{
				"family": "Estrada",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "farnadiFairnessRelationalDomains2018",
		"type": "paper-conference",
		"abstract": "AI and machine learning tools are being used with increasing frequency for decision making in domains that affect peoples' lives such as employment, education, policing and loan approval. These uses raise concerns about biases of algorithmic discrimination and have motivated the development of fairness-aware machine learning. However, existing fairness approaches are based solely on attributes of individuals. In many cases, discrimination is much more complex, and taking into account the social, organizational, and other connections between individuals is important. We introduce new notions of fairness that are able to capture the relational structure in a domain. We use first-order logic to provide a flexible and expressive language for specifying complex relational patterns of discrimination. Furthermore, we extend an existing statistical relational learning framework, probabilistic soft logic (PSL), to incorporate our definition of relational fairness. We refer to this fairness-aware framework FairPSL. FairPSL makes use of the logical definitions of fairnesss but also supports a probabilistic interpretation. In particular, we show how to perform maximum a posteriori(MAP) inference by exploiting probabilistic dependencies within the domain while avoiding violation of fairness guarantees. Preliminary empirical evaluation shows that we are able to make both accurate and fair decisions.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278733",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "108–114",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in relational domains",
		"URL": "https://doi.org/10.1145/3278721.3278733",
		"author": [
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			},
			{
				"family": "Babaki",
				"given": "Behrouz"
			},
			{
				"family": "Getoor",
				"given": "Lise"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "freedmanAdaptingKidneyExchange2018",
		"type": "paper-conference",
		"abstract": "The efficient allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who get what—and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278727",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "115",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adapting a kidney exchange algorithm to align with human values",
		"URL": "https://doi.org/10.1145/3278721.3278727",
		"author": [
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "goelNondiscriminatoryMachineLearning2018",
		"type": "paper-conference",
		"abstract": "We introduce a novel technique to achieve non-discrimination in machine learning without sacrificing convexity and probabilistic interpretation. We also propose a new notion of fairness for machine learning called the weighted proportional fairness and show that our technique satisfies this subjective fairness criterion.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278722",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Non-discriminatory machine learning through convex fairness criteria",
		"URL": "https://doi.org/10.1145/3278721.3278722",
		"author": [
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Yaghini",
				"given": "Mohammad"
			},
			{
				"family": "Faltings",
				"given": "Boi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "grimmEducationModelReasonable2018",
		"type": "paper-conference",
		"abstract": "In this paper we propose a framework for conceptualizing and demonstrating a good-faith effort when developing autonomous systems. The framework addresses two fundamental problems facing autonomous systems: (1) the disconnect between human-mental models and machine-based sensors and algorithms; and (2) unpredictability in complex systems. We address these problems using a mix of education - explicitly delineating the mapping between human concepts and their machine equivalents in a structured manner - and data sampling with expected ranges as a testing mechanism.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278732",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "117–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An education model of reasonable and good-faith effort for autonomous systems",
		"URL": "https://doi.org/10.1145/3278721.3278732",
		"author": [
			{
				"family": "Grimm",
				"given": "Cindy M."
			},
			{
				"family": "Smart",
				"given": "William D."
			},
			{
				"family": "Hartzog",
				"given": "Woodrow"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "gruetzemacherRethinkingAIStrategy2018",
		"type": "paper-conference",
		"abstract": "This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges - the development of safe and beneficial artificial general intelligence - to be significantly more complex and nuanced, thus posing a new degree of 'wickedness.' We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278746",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "122",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rethinking AI strategy and policy as entangled super wicked problems",
		"URL": "https://doi.org/10.1145/3278721.3278746",
		"author": [
			{
				"family": "Gruetzemacher",
				"given": "Ross"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hendersonEthicalChallengesDatadriven2018a",
		"type": "paper-conference",
		"abstract": "The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278777",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "123–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical challenges in data-driven dialogue systems",
		"URL": "https://doi.org/10.1145/3278721.3278777",
		"author": [
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Sinha",
				"given": "Koustuv"
			},
			{
				"family": "Angelard-Gontier",
				"given": "Nicolas"
			},
			{
				"family": "Ke",
				"given": "Nan Rosemary"
			},
			{
				"family": "Fried",
				"given": "Genevieve"
			},
			{
				"family": "Lowe",
				"given": "Ryan"
			},
			{
				"family": "Pineau",
				"given": "Joelle"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hookerNonintuitionbasedMachineArtificial2018",
		"type": "paper-conference",
		"abstract": "We propose a deontological approach to machine (or AI) ethics that avoids some weaknesses of an intuition-based system, such as that of Anderson and Anderson. In particular, it has no need to deal with conflicting intuitions, and it yields a more satisfactory account of when autonomy should be respected. We begin with a \"dual standpoint” theory of action that regards actions as grounded in reasons and therefore as having a conditional form that is suited to machine instructions. We then derive ethical principles based on formal properties that the reasons must exhibit to be coherent, and formulate the principles using quantified modal logic. We conclude that deontology not only provides a more satisfactory basis for machine ethics but endows the machine with an ability to explain its actions, thus contributing to transparency in AI.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278753",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "130–136",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward non-intuition-based machine and artificial intelligence ethics: A deontological approach based on modal logic",
		"URL": "https://doi.org/10.1145/3278721.3278753",
		"author": [
			{
				"family": "Hooker",
				"given": "John N."
			},
			{
				"family": "Kim",
				"given": "Tae Wan N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "hundmanAlwaysLurkingUnderstanding2018",
		"type": "paper-conference",
		"abstract": "Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web – a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278782",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "137–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Always lurking: Understanding and mitigating bias in online human trafficking detection",
		"URL": "https://doi.org/10.1145/3278721.3278782",
		"author": [
			{
				"family": "Hundman",
				"given": "Kyle"
			},
			{
				"family": "Gowda",
				"given": "Thamme"
			},
			{
				"family": "Kejriwal",
				"given": "Mayank"
			},
			{
				"family": "Boecking",
				"given": "Benedikt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "iyerTransparencyExplanationDeep2018",
		"type": "paper-conference",
		"abstract": "Autonomous AI systems will be entering human society in the near future to provide services and work alongside humans. For those systems to be accepted and trusted, the users should be able to understand the reasoning process of the system, i.e. the system should be transparent. System transparency enables humans to form coherent explanations of the system's decisions and actions. Transparency is important not only for user trust, but also for software debugging and certification. In recent years, Deep Neural Networks have made great advances in multiple application areas. However, deep neural networks are opaque. In this paper, we report on work in transparency in Deep Reinforcement Learning Networks (DRLN). Such networks have been extremely successful in accurately learning action control in image input domains, such as Atari games. In this paper, we propose a novel and general method that (a) incorporates explicit object recognition processing into deep reinforcement learning models, (b) forms the basis for the development of \"object saliency maps\", to provide visualization of internal states of DRLNs, thus enabling the formation of explanations and (c) can be incorporated in any existing deep reinforcement learning framework. We present computational results and human experiments to evaluate our approach.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278776",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "144–150",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency and explanation in deep reinforcement learning neural networks",
		"URL": "https://doi.org/10.1145/3278721.3278776",
		"author": [
			{
				"family": "Iyer",
				"given": "Rahul"
			},
			{
				"family": "Li",
				"given": "Yuezhang"
			},
			{
				"family": "Li",
				"given": "Huao"
			},
			{
				"family": "Lewis",
				"given": "Michael"
			},
			{
				"family": "Sundar",
				"given": "Ramitha"
			},
			{
				"family": "Sycara",
				"given": "Katia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "johnsonSociallyawareNavigationUsing2018",
		"type": "paper-conference",
		"abstract": "We present socially-aware navigation for an intelligent robot wheelchair in an environment with many pedestrians. The robot learns social norms by observing the behaviors of human pedestrians, interpreting detected biases as social norms, and incorporating those norms into its motion planning. We compare our socially-aware motion planner with a baseline motion planner that produces safe, collision-free motion.The ability of our robot to learn generalizable social norms depends on our use of a topological map abstraction, so that a practical number of observations can allow learning of a social norm applicable in a wide variety of circumstances.We show that the robot can detect biases in observed human behavior that support learning the social norm of driving on the right. Furthermore, we show that when the robot follows these social norms, its behavior influences the behavior of pedestrians around it, increasing their adherence to the same norms. We conjecture that the legibility of the robot's normative behavior improves human pedestrians' ability to predict the robot's future behavior, making them more likely to follow the same norm.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278772",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "151–157",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socially-aware navigation using topological maps and social norm learning",
		"URL": "https://doi.org/10.1145/3278721.3278772",
		"author": [
			{
				"family": "Johnson",
				"given": "Collin"
			},
			{
				"family": "Kuipers",
				"given": "Benjamin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "josephMeritocraticFairnessInfinite2018",
		"type": "paper-conference",
		"abstract": "We study fairness in linear bandit problems. Starting from the notion of meritocratic fairness introduced in citeJKMR16, we carry out a more refined analysis of a more general problem, achieving better performance guarantees with fewer modelling assumptions on the number and structure of available choices as well as the number selected. We also analyze the previously-unstudied question of fairness in infinite linear bandit problems, obtaining instance-dependent regret upper bounds as well as lower bounds demonstrating that this instance-dependence is necessary. The result is a framework for meritocratic fairness in an online linear setting that is substantially more powerful, general, and realistic than the current state of the art.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278764",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "158–163",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meritocratic fairness for infinite and contextual bandits",
		"URL": "https://doi.org/10.1145/3278721.3278764",
		"author": [
			{
				"family": "Joseph",
				"given": "Matthew"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Neel",
				"given": "Seth"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kalyanakrishnanOpportunitiesChallengesArtificial2018",
		"type": "paper-conference",
		"abstract": "In the future of India lies the future of a sixth of the world's population. As the Artificial Intelligence (AI) revolution sweeps through societies and enters daily life, its role in shaping India's development and growth is bound to be substantial. For India, AI holds promise as a catalyst to accelerate progress, while providing mechanisms to leapfrog traditional hurdles such as poor infrastructure and bureaucracy. At the same time, an investment in AI is accompanied by risk factors with long-term implications on society: it is imperative that risks be vetted at this early stage. In this paper, we describe opportunities and challenges for AI in India. We detail opportunities that are cross-cutting (bridging India's linguistic divisions, mining public data), and also specific to one particular sector (healthcare). We list challenges that originate from existing social conditions (such as equations of caste and gender). Thereafter we distill out concrete steps and safeguards, which we believe are necessary for robust and inclusive development as India enters the AI era.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278738",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "164–170",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Opportunities and challenges for artificial intelligence in india",
		"URL": "https://doi.org/10.1145/3278721.3278738",
		"author": [
			{
				"family": "Kalyanakrishnan",
				"given": "Shivaram"
			},
			{
				"family": "Panicker",
				"given": "Rahul Alex"
			},
			{
				"family": "Natarajan",
				"given": "Sarayu"
			},
			{
				"family": "Rao",
				"given": "Shreya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "karbasianRealtimeInferenceUser2018",
		"type": "paper-conference",
		"abstract": "Social media provides a mechanism for people to engage with social causes across a range of issues. It also provides a strategic tool to those looking to advance a cause to exchange, promote or publicize their ideas. In such instances, AI can be either an asset if used appropriately or a barrier. One of the key issues for a workforce diversity campaign is to understand in real-time who is participating - specifically, whether the participants are individuals or organizations, and in case of individuals, whether they are male or female. In this paper, we present a study to demonstrate a case for AI for social good that develops a model to infer in real-time the different user types participating in a cause-driven hashtag campaign on Twitter, ILookLikeAnEngineer (ILLAE). A generic framework is devised to classify a Twitter user into three classes: organization, male and female in a real-time manner. The framework is tested against two datasets (ILLAE and a general dataset) and outperforms the baseline binary classifiers for categorizing organization/individual and male/female. The proposed model can be applied to future social cause-driven campaigns to get real-time insights on the macro-level social behavior of participants.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278781",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "171–177",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Real-time inference of user types to assist with more inclusive and diverse social media activism campaigns",
		"URL": "https://doi.org/10.1145/3278721.3278781",
		"author": [
			{
				"family": "Karbasian",
				"given": "Habib"
			},
			{
				"family": "Purohit",
				"given": "Hemant"
			},
			{
				"family": "Handa",
				"given": "Rajat"
			},
			{
				"family": "Malik",
				"given": "Aqdas"
			},
			{
				"family": "Johri",
				"given": "Aditya"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kasenbergInverseNormConflict2018",
		"type": "paper-conference",
		"abstract": "In previous work we provided a \"norm conflict resolution\" algorithm allowing agents in stochastic domains (represented by Markov Decision Processes) to \"maximally satisfy\" a set of moral or social norms, where such norms are represented by statements in linear temporal logic (LTL). This required the agent designer to provide weights specifying the relative importance of each norm. In this paper, we propose an \"inverse norm conflict resolution” algorithm for learning these weights from demonstration. This approach minimizes a cost function based on the relative entropy between a policy encoding the observed behavior and a policy representing optimal norm-following behavior. We demonstrate the effectiveness of the algorithm in a simple GridWorld domain.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278775",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "178–183",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Inverse norm conflict resolution",
		"URL": "https://doi.org/10.1145/3278721.3278775",
		"author": [
			{
				"family": "Kasenberg",
				"given": "Daniel"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kasenbergNormsRewardsIntentional2018",
		"type": "paper-conference",
		"abstract": "The challenge of training AI systems to perform responsibly and beneficially has inspired different approaches for teaching a system what people want and how it is acceptable to attain that in the world. In this paper we compare work in reinforcement learning, in particular inverse reinforcement learning, with our norm inference approach. We test those two systems and present results. Using the idea of the \"intentional stance\", we explain how a norm inference approach can work even when another agent is acting strictly according to reward functions. In this way norm inference presents itself as a promising, more explicitly accountable approach with which to design AI systems from the start.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278774",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "184–190",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Norms, rewards, and the intentional stance: Comparing machine learning approaches to ethical training",
		"URL": "https://doi.org/10.1145/3278721.3278774",
		"author": [
			{
				"family": "Kasenberg",
				"given": "Daniel"
			},
			{
				"family": "Arnold",
				"given": "Thomas"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kaulMarginsOpportunity2018",
		"type": "paper-conference",
		"abstract": "We use the statistical quantity of margin — the distance between a decision boundary and a classified point, or the gap between two scores — to formalize the principle of equal opportunity — the chance to improve one's outcome, regardless of group status. This leads to a better definition of opportunity which recognizes, for example, that a strongly rejected individual was offered less recourse than a weakly rejected one, despite the shared outcome. It also leads to simpler algorithms, since real-valued margins are easier to analyze and optimize than discrete outcomes. We formalize two ways that a protected group may be guaranteed equal opportunity: (1) (social) mobility: acceptance should be within reach for the group (conversely, the general population shouldn't be cushioned from rejection), and (2) contrast: within the group, good candidates should get substantially higher scores than bad candidates, preventing the so-called 'token' effect. A simple linear classifier seems to offer roughly equal opportunity both experimentally and mathematically.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278748",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "191–196",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Margins and opportunity",
		"URL": "https://doi.org/10.1145/3278721.3278748",
		"author": [
			{
				"family": "Kaul",
				"given": "Shiva"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kimComputationalModelCommonsense2018",
		"type": "paper-conference",
		"abstract": "We introduce a computational model for building moral autonomous vehicles by learning and generalizing from human moral judgments. We draw on a cognitively inspired model of how people and young children learn moral theories from sparse and noisy data and integrate observations made from different people in different groups. The problem of moral learning for autonomous vehicles is cast as learning how to weigh the different features of the dilemma using utility calculus, with the goal of making these trade-offs reflect how people make them in a wide variety of moral dilemma. By modeling the structures of individuals and groups in a hierarchical Bayesian model, we show that an individual's moral values – as well as a group's shared values – can be inferred from sparse and noisy data. We evaluate our approach with data from the Moral Machine, a web application that collects human judgments on moral dilemmas involving autonomous vehicles, and show that the model rapidly and accurately infers people's preferences and can predict the difficulty of moral dilemmas from limited data.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278770",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "197–203",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A computational model of commonsense moral decision making",
		"URL": "https://doi.org/10.1145/3278721.3278770",
		"author": [
			{
				"family": "Kim",
				"given": "Richard"
			},
			{
				"family": "Kleiman-Weiner",
				"given": "Max"
			},
			{
				"family": "Abeliuk",
				"given": "Andrés"
			},
			{
				"family": "Awad",
				"given": "Edmond"
			},
			{
				"family": "Dsouza",
				"given": "Sohan"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Rahwan",
				"given": "Iyad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "kramerWhenPeopleWant2018",
		"type": "paper-conference",
		"abstract": "AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278752",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "204–209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When do people want AI to make decisions?",
		"URL": "https://doi.org/10.1145/3278721.3278752",
		"author": [
			{
				"family": "Kramer",
				"given": "Max F."
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "larosaImpactsTrustHealthcare2018",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278771",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "210–215",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Impacts on trust of healthcare AI",
		"URL": "https://doi.org/10.1145/3278721.3278771",
		"author": [
			{
				"family": "LaRosa",
				"given": "Emily"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "londonRegulatingAutonomousVehicles2018",
		"type": "paper-conference",
		"abstract": "The widespread deployment and testing of autonomous vehicles in real-world environments raises key questions about how such systems should be regulated. Much of the current debate presupposes that the regulatory system we currently use for regular vehicles is also appropriate for semi- and fully-autonomous ones. In opposition, we first argue that there are serious challenges to regulating autonomous vehicles using current approaches, due to the nature of both autonomous capabilities (and their connections to operational domains), and also the systems' tasks and surrounding uncertainties. Instead, we argue that vehicles with autonomous capabilities are similar in key respects to drugs and other medical inter-ventions. Thus, we propose (on a \"first principles\" basis) a dynamic regulatory system with staged approvals and monitoring, analogous to the system used by the U.S. Food &amp; Drug Administration. We provide details about the operation of such a potential system, and conclude by characterizing its benefits, costs, and plausibility.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278763",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "216–221",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating autonomous vehicles: A policy proposal",
		"URL": "https://doi.org/10.1145/3278721.3278763",
		"author": [
			{
				"family": "London",
				"given": "Alex John"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "loreggiaPreferencesEthicalPrinciples2018",
		"type": "paper-conference",
		"abstract": "If we want people to trust AI systems, we need to provide the systems we create with the ability to discriminate between what humans would consider good and bad decisions. The quality of a decision should not be based only on the preferences or optimization criteria of the decision makers, but also on other properties related to the impact of the decision, such as whether it is ethical, or if it complies to constraints and priorities given by feasibility constraints or safety regulations. The CP-net formalism [2] is a convenient and expressive way to model preferences, providing an effective compact way to qualitatively model preferences over outcomes, i.e., decisions, with a combinatorial structure [3, 7]. If we wish to incorporate ethical, moral, or norms based constraints to a decision context, it means that the subjective preferences of the decision makers are not the only source of information we should consider [1, 8]. Indeed, depending on the context, we may have to consider specific ethical principles derived from an appropriate ethical theory or various laws and norms. While preferences are important, when preferences and ethical principles are in conflict, the principles should override the subjective preferences of the decision maker. Therefore, it is essential to have well founded techniques to evaluate whether preferences are compatible with a set of ethical principles, and to measure how much these preferences deviate from the ethical principles.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278723",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 1\npublisher-place: New Orleans, LA, USA",
		"page": "222",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Preferences and ethical principles in decision making",
		"URL": "https://doi.org/10.1145/3278721.3278723",
		"author": [
			{
				"family": "Loreggia",
				"given": "Andrea"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			},
			{
				"family": "Venable",
				"given": "K. Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "maasRegulatingNormalAI2018",
		"type": "paper-conference",
		"abstract": "New technologies, particularly those which are deployed rapidly across sectors, or which have to operate in competitive conditions, can disrupt previously stable technology governance regimes. This leads to a precarious need to balance caution against performance while exploring the resulting 'safe operating space'. This paper will argue that Artificial Intelligence is one such critical technology, the responsible deployment of which is likely to prove especially complex, because even narrow AI applications often involve networked (tightly coupled, opaque) systems operating in complex or competitive environments. This ensures such systems are prone to 'normal accident'-type failures which can cascade rapidly, and are hard to contain or even detect in time. Legal and governance approaches to the deployment of AI will have to reckon with the specific causes and features of such 'normal accidents'. While this suggests that large-scale, cascading errors in AI systems are inevitable, an examination of the operational features that lead technologies to exhibit 'normal accidents' enables us to derive both tentative principles for precautionary policymaking, and practical recommendations for the safe(r) deployment of AI systems. This may help enhance the safety and security of these systems in the public sphere, both in the short- and in the long term.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278766",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "223–228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating for 'normal AI accidents': Operational lessons for the responsible governance of artificial intelligence deployment",
		"URL": "https://doi.org/10.1145/3278721.3278766",
		"author": [
			{
				"family": "Maas",
				"given": "Matthijs M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "manikondaWhatPrivacyUser2018",
		"type": "paper-conference",
		"abstract": "The recent breakthroughs in Artificial Intelligence (AI) have allowed individuals to rely on automated systems for a variety of reasons. Some of these systems are the currently popular voice-enabled systems like Echo by Amazon and Home by Google that are also called as Intelligent Personal Assistants (IPAs). Though there are rising concerns about privacy and ethical implications, users of these IPAs seem to continue using these systems. We aim to investigate to what extent users are concerned about privacy and how they are handling these concerns while using the IPAs. By utilizing the reviews posted online along with the responses to a survey, this paper provides a set of insights about the detected markers related to user interests and privacy challenges. The insights suggest that users of these systems irrespective of their concerns about privacy, are generally positive in terms of utilizing IPAs in their everyday lives. However, there is a significant percentage of users who are concerned about privacy and take further actions to address related concerns. Some percentage of users expressed that they do not have any privacy concerns but when they learned about the \"always listening\" feature of these devices, their concern about privacy increased.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278773",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "229–235",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's up with privacy? User preferences and privacy concerns in intelligent personal assistants",
		"URL": "https://doi.org/10.1145/3278721.3278773",
		"author": [
			{
				"family": "Manikonda",
				"given": "Lydia"
			},
			{
				"family": "Deotale",
				"given": "Aditya"
			},
			{
				"family": "Kambhampati",
				"given": "Subbarao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "matteiFairnessDeceasedOrgan2018",
		"type": "paper-conference",
		"abstract": "As algorithms are given responsibility to make decisions that impact our lives, there is increasing awareness of the need to ensure the fairness of these decisions. One of the first challenges then is to decide what fairness means in a particular context. We consider here fairness in deciding how to match organs donated by deceased donors to patients. Due to the increasing age of patients on the waiting list, and of organs being donated, the current \"first come, first served” mechanism used in Australia is under review to take account of age of patients and of organs. We consider how to revise the mechanism to take account of age fairly. We identify a number of different types of fairness, such as to patients, to regions and to blood types and consider how they can be achieved.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278749",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "236–242",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in deceased organ matching",
		"URL": "https://doi.org/10.1145/3278721.3278749",
		"author": [
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Saffidine",
				"given": "Abdallah"
			},
			{
				"family": "Walsh",
				"given": "Toby"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "raffFairForestsRegularized2018",
		"type": "paper-conference",
		"abstract": "The potential lack of fairness in the outputs of machine learning algorithms has recently gained attention both within the research community as well as in society more broadly. Surprisingly, there is no prior work developing tree-induction algorithms for building fair decision trees or fair random forests. These methods have widespread popularity as they are one of the few to be simultaneously interpretable, non-linear, and easy-to-use. In this paper we develop, to our knowledge, the first technique for the induction of fair decision trees.We show that our \"Fair Forest\" retains the benefits of the tree-based approach, while providing both greater accuracy and fairness than other alternatives, for both \"group fairness” and \"individual fairness.” We also introduce new measures for fairness which are able to handle multinomial and continues attributes as well as regression problems, as opposed to binary attributes and labels only. Finally, we demonstrate a new, more robust evaluation procedure for algorithms that considers the dataset in its entirety rather than only a specific protected attribute.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278742",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 8\npublisher-place: New Orleans, LA, USA",
		"page": "243–250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair forests: Regularized tree induction to minimize model bias",
		"URL": "https://doi.org/10.1145/3278721.3278742",
		"author": [
			{
				"family": "Raff",
				"given": "Edward"
			},
			{
				"family": "Sylvester",
				"given": "Jared"
			},
			{
				"family": "Mills",
				"given": "Steven"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "scheesseleFrameworkGroundingMoral2018",
		"type": "paper-conference",
		"abstract": "I propose a framework, derived from moral theory, for assessing the moral status of intelligent machines. Using this framework, I claim that some current and foreseeable intelligent machines have approximately as much moral status as plants, trees, and other environmental entities. This claim raises the question: what obligations could a moral agent (e.g., a normal adult human) have toward an intelligent machine? I propose that the threshold for any moral obligation should be the \"functional morality\" of Wallach and Allen [20], while the upper limit of our obligations should not exceed the upper limit of our obligations toward plants, trees, and other environmental entities.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278743",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "251–256",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for grounding the moral status of intelligent machines",
		"URL": "https://doi.org/10.1145/3278721.3278743",
		"author": [
			{
				"family": "Scheessele",
				"given": "Michael R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "seoPartiallyGenerativeNeural2018",
		"type": "paper-conference",
		"abstract": "More than 1 million homicides, robberies, and aggravated assaults occur in the United States each year. These crimes are often further classified into different types based on the circumstances surrounding the crime (e.g., domestic violence, gang-related). Despite recent technological advances in AI and machine learning, these additional classification tasks are still done manually by specially trained police officers. In this paper, we provide the first attempt to develop a more automatic system for classifying crimes. In particular, we study the question of classifying whether a given violent crime is gang-related. We introduce a novel Partially Generative Neural Networks (PGNN) that is able to accurately classify gang-related crimes both when full information is available and when there is only partial information. Our PGNN is the first generative-classification model that enables to work when some features of the test examples are missing. Using a crime event dataset from Los Angeles covering 2014-2016, we experimentally show that our PGNN outperforms all other typically used classifiers for the problem of classifying gang-related violent crimes.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278758",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "257–263",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Partially generative neural networks for gang crime classification with partial information",
		"URL": "https://doi.org/10.1145/3278721.3278758",
		"author": [
			{
				"family": "Seo",
				"given": "Sungyong"
			},
			{
				"family": "Chan",
				"given": "Hau"
			},
			{
				"family": "Brantingham",
				"given": "P. Jeffrey"
			},
			{
				"family": "Leap",
				"given": "Jorja"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			},
			{
				"family": "Tambe",
				"given": "Milind"
			},
			{
				"family": "Liu",
				"given": "Yan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "serramiaExploitingMoralValues2018",
		"type": "paper-conference",
		"abstract": "Norms constitute regulative mechanisms extensively enacted in groups, organisations, and societies. However, 'choosing the right norms to establish' constitutes an open problem that requires the consideration of a number of constraints (such as norm relations) and preference criteria (e.g over involved moral values). This paper advances the state of the art in the Normative Multiagent Systems literature by formally defining this problem and by proposing its encoding as a linear program so that it can be automatically solved.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278735",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "264–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploiting moral values to choose the right norms",
		"URL": "https://doi.org/10.1145/3278721.3278735",
		"author": [
			{
				"family": "Serramia",
				"given": "Marc"
			},
			{
				"family": "Lopez-Sanchez",
				"given": "Maite"
			},
			{
				"family": "Rodriguez-Aguilar",
				"given": "Juan A."
			},
			{
				"family": "Morales",
				"given": "Javier"
			},
			{
				"family": "Wooldridge",
				"given": "Michael"
			},
			{
				"family": "Ansotegui",
				"given": "Carlos"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "shawProvablyMoralAI2018",
		"type": "paper-conference",
		"abstract": "We examine moral machine decision making as inspired by a central question posed by Rossi with respect to moral preferences: can AI systems based on statistical machine learning (which do not provide a natural way to explain or justify their decisions) be used for embedding morality into a machine in a way that allows us to prove that nothing morally wrong will happen? We argue for an evaluation which is held to the same standards as a human agent, removing the demand that ethical behaviour is always achieved. We introduce four key meta-qualities desired for our moral standards, and then proceed to clarify how we can prove that an agent will correctly learn to perform moral actions given a set of samples within certain error bounds. Our group-dynamic approach enables us to demonstrate that the learned models converge to a common function to achieve stability. We further explain a valuable intrinsic consistency check made possible through the derivation of logical statements from the machine learning model. In all, this work proposes an approach for building ethical AI systems, coming from the perspective of artificial intelligence research, and sheds important light on understanding how much learning is required in order for an intelligent agent to behave morally with negligible error.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278728",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "271–277",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards provably moral AI agents in bottom-up learning frameworks",
		"URL": "https://doi.org/10.1145/3278721.3278728",
		"author": [
			{
				"family": "Shaw",
				"given": "Nolan P."
			},
			{
				"family": "Stöckel",
				"given": "Andreas"
			},
			{
				"family": "Orr",
				"given": "Ryan W."
			},
			{
				"family": "Lidbetter",
				"given": "Thomas F."
			},
			{
				"family": "Cohen",
				"given": "Robin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "somayaEmbodimentAnthropomorphismIntellectual2018",
		"type": "paper-conference",
		"abstract": "Computational creativity is an emerging branch of artificial intelligence (AI) concerned with algorithms that can create novel and high-quality ideas or artifacts, either autonomously or semi-autonomously in collaboration with people. Quite simply, such algorithms may be described as artificial innovation engines. These technologies raise questions of authorship/inventorship and of agency, which become further muddled by the social context induced by AI that may be physically-embodied or anthropomorphized. These questions are fundamentally intertwined with the provision of appropriate incentives for conducting and commercializing computational creativity research through intellectual property regimes. This paper reviews current understanding of intellectual property rights for AI, and explores possible framings for intellectual property policy in social context.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278754",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "278–283",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Embodiment, anthropomorphism, and intellectual property rights for AI creations",
		"URL": "https://doi.org/10.1145/3278721.3278754",
		"author": [
			{
				"family": "Somaya",
				"given": "Deepak"
			},
			{
				"family": "Varshney",
				"given": "Lav R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "srivastavaComposableBiasRating2018",
		"type": "paper-conference",
		"abstract": "A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance. However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf. A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias. Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about. The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available. In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions. We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased. The approach also works on composite services. We implement it in the context of text translation and report interesting results.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278744",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "284–289",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards composable bias rating of AI services",
		"URL": "https://doi.org/10.1145/3278721.3278744",
		"author": [
			{
				"family": "Srivastava",
				"given": "Biplav"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "suarez-serratoSocialbotsSupportingHuman2018",
		"type": "paper-conference",
		"abstract": "Socialbots, or non-human/algorithmic social media users, have recently been documented as competing for information dissemination and disruption on online social networks. Here we investigate the influence of socialbots in Mexican Twitter in regards to the \"Tanhuato\" human rights abuse report. We analyze the applicability of the BotOrNot API to generalize from English to Spanish tweets and propose adaptations for Spanish-speaking bot detection. We then use text and sentiment analysis to compare the differences between bot and human tweets. Our analysis shows that bots actually aided in information proliferation among human users. This suggests that taxonomies classifying bots should include non-adversarial roles as well. Our study contributes to the understanding of different behaviors and intentions of automated accounts observed in empirical online social network data. Since this type of analysis is seldom performed in languages different from English, the proposed techniques we employ here are also useful for other non-English corpora.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278734",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "290–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Socialbots supporting human rights",
		"URL": "https://doi.org/10.1145/3278721.3278734",
		"author": [
			{
				"family": "Suárez-Serrato",
				"given": "Pablo"
			},
			{
				"family": "Velázquez Richards",
				"given": "Eduardo Iván"
			},
			{
				"family": "Yazdani",
				"given": "Mehrdad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "sunDesigningNongreedyReinforcement2018",
		"type": "paper-conference",
		"abstract": "This paper intends to address an issue in RL that when agents possessing varying capabilities, most resources may be acquired by stronger agents, leaving the weaker ones \"starving\". We introduce a simple method to train non-greedy agents in multi-agent reinforcement learning scenarios with nearly no extra cost. Our model can achieve the following goals in designing the non-greedy agent:non-homogeneous equality, only need local information, cost-effective, generalizable and configurable. We propose the idea of diminishing reward that makes the agent feel less satisfied for consecutive rewards obtained. This idea allows the agents to behave less greedy with-out the need to explicitly coding any ethical pattern nor monitor other agents' status. Given our framework, resources distributed more equally without running the risk of reaching homogeneous equality. We designed two games, Gathering Game and Hunter Prey to evaluate the quality of the model.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278759",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "297–302",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing non-greedy reinforcement learning agents with diminishing reward shaping",
		"URL": "https://doi.org/10.1145/3278721.3278759",
		"author": [
			{
				"family": "Sun",
				"given": "Fan-Yun"
			},
			{
				"family": "Chang",
				"given": "Yen-Yu"
			},
			{
				"family": "Wu",
				"given": "Yueh-Hua"
			},
			{
				"family": "Lin",
				"given": "Shou-De"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "tanDistillandcompareAuditingBlackbox2018",
		"type": "paper-conference",
		"abstract": "Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278725",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 8\npublisher-place: New Orleans, LA, USA",
		"page": "303–310",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Distill-and-compare: Auditing black-box models using transparent model distillation",
		"URL": "https://doi.org/10.1145/3278721.3278725",
		"author": [
			{
				"family": "Tan",
				"given": "Sarah"
			},
			{
				"family": "Caruana",
				"given": "Rich"
			},
			{
				"family": "Hooker",
				"given": "Giles"
			},
			{
				"family": "Lou",
				"given": "Yin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "tobeySoftwareMalpracticeAge2018",
		"type": "paper-conference",
		"abstract": "Professional malpractice - the concept of heightened duties for those entrusted with special knowledge and crucial tasks - is rooted in history. And yet, since the dawn of the computer age, courts in the United States have almost universally rejected a theory of software malpractice, declining to hold software engineers to the same professional standards as doctors, lawyers, and engineers. What is changing, however, is the speed at which software based on artificial intelligence technologies is replacing the very professionals already subject to professional liability. Society has already decided (in some cases, millennia ago) that those tasks warrant special accountability; new to the analysis is which human is closest in line to the adverse event. As AI expands, the pressure for courts to go one level up the causal chain in search of human agency and professional accountability will mount. This essay analyzes the case law rejecting software malpractice for clues about where the doctrine might go in the age of AI, then discusses what technology companies can learn from the safety enhancements of doctors, lawyers, and other historic professionals who have adapted to such heightened legal scrutiny for years.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278737",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "311–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Software malpractice in the age of AI: A guide for the wary tech company",
		"URL": "https://doi.org/10.1145/3278721.3278737",
		"author": [
			{
				"family": "Tobey",
				"given": "Daniel L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "vanderelstDarkSideEthical2018",
		"type": "paper-conference",
		"abstract": "Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next few years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also inevitably enables the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the cognitive machinery required to make an ethical robot can always be corrupted to make unethical robots. We discuss the implications of this finding to the governance of ethical robots. We conclude that the risks that unscrupulous actors might compromise a robot's ethics are so great as to raise serious doubts over the wisdom of embedding ethical decision making in real-world safety-critical robots, such as driverless cars.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278726",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "317–322",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dark side of ethical robots",
		"URL": "https://doi.org/10.1145/3278721.3278726",
		"author": [
			{
				"family": "Vanderelst",
				"given": "Dieter"
			},
			{
				"family": "Winfield",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "vasconcelosModelingEpistemologicalPrinciples2018",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) has been used extensively in automatic decision making in a broad variety of scenarios, ranging from credit ratings for loans to recommendations of movies. Traditional design guidelines for AI models focus essentially on accuracy maximization, but recent work has shown that economically irrational and socially unacceptable scenarios of discrimination and unfairness are likely to arise unless these issues are explicitly addressed. This undesirable behavior has several possible sources, such as biased datasets used for training that may not be detected in black-box models. After pointing out connections between such bias of AI and the problem of induction, we focus on Popper's contributions after Hume's, which offer a logical theory of preferences. An AI model can be preferred over others on purely rational grounds after one or more attempts at refutation based on accuracy and fairness. Inspired by such epistemological principles, this paper proposes a structured approach to mitigate discrimination and unfairness caused by bias in AI systems. In the proposed computational framework, models are selected and enhanced after attempts at refutation. To illustrate our discussion, we focus on hiring decision scenarios where an AI system filters in which job applicants should go to the interview phase.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278751",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "323–329",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modeling epistemological principles for bias mitigation in AI systems: An illustration in hiring decisions",
		"URL": "https://doi.org/10.1145/3278721.3278751",
		"author": [
			{
				"family": "Vasconcelos",
				"given": "Marisa"
			},
			{
				"family": "Cardonha",
				"given": "Carlos"
			},
			{
				"family": "Gonçalves",
				"given": "Bernardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "wagnerAutonomousArchitectureThat2018",
		"type": "paper-conference",
		"abstract": "The advent and widespread adoption of wearable cameras and autonomous robots raises important issues related to privacy. The mobile cameras on these systems record and may re-transmit enormous amounts of video data that can then be used to identify, track, and characterize the behavior of the general populous. This paper presents a preliminary computational architecture designed to preserve specific types of privacy over a video stream by identifying categories of individuals, places, and things that require higher than normal privacy protection. This paper describes the architecture as a whole as well as preliminary results testing aspects of the system. Our intention is to implement and test the system on ground robots and small UAVs and demonstrate that the system can provide selective low-level masking or deletion of data requiring higher privacy protection.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278768",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 5\npublisher-place: New Orleans, LA, USA",
		"page": "330–334",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An autonomous architecture that protects the right to privacy",
		"URL": "https://doi.org/10.1145/3278721.3278768",
		"author": [
			{
				"family": "Wagner",
				"given": "Alan R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhangMitigatingUnwantedBiases2018",
		"type": "paper-conference",
		"abstract": "Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278779",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "335–340",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating unwanted biases with adversarial learning",
		"URL": "https://doi.org/10.1145/3278721.3278779",
		"author": [
			{
				"family": "Zhang",
				"given": "Brian Hu"
			},
			{
				"family": "Lemoine",
				"given": "Blake"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhaoPrivacypreservingMachineLearning2018",
		"type": "paper-conference",
		"abstract": "Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278778",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 6\npublisher-place: New Orleans, LA, USA",
		"page": "341–346",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Privacy-preserving machine learning based data analytics on edge devices",
		"URL": "https://doi.org/10.1145/3278721.3278778",
		"author": [
			{
				"family": "Zhao",
				"given": "Jianxin"
			},
			{
				"family": "Mortier",
				"given": "Richard"
			},
			{
				"family": "Crowcroft",
				"given": "Jon"
			},
			{
				"family": "Wang",
				"given": "Liang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "zhaoDataDrivenTechniques2018",
		"type": "paper-conference",
		"abstract": "Life on earth presents elegant solutions to many of the challenges innovators and entrepreneurs across disciplines face every day. To facilitate innovations inspired by nature, there is an emerging need for systems that bring relevant biological information to this application-oriented market. In this paper, we discuss our approach to assembling a system that uses machine learning techniques to assess a scientific article's potential usefulness to innovators, and classifies these articles in a way that helps innovators find information relevant to the challenges they are attempting to solve.",
		"collection-title": "AIES '18",
		"container-title": "Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3278721.3278755",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6012-8",
		"note": "number-of-pages: 7\npublisher-place: New Orleans, LA, USA",
		"page": "347–353",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data driven techniques for organizing scientific articles relevant to biomimicry",
		"URL": "https://doi.org/10.1145/3278721.3278755",
		"author": [
			{
				"family": "Zhao",
				"given": "Yuanshuo"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Padhi",
				"given": "Inkit"
			},
			{
				"family": "Lee",
				"given": "Yoong Keok"
			},
			{
				"family": "Smith",
				"given": "Ethan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "wrightRightfulMachinesDilemmas2019",
		"type": "paper-conference",
		"abstract": "Tn this paper I set out a new Kantian approach to resolving conflicts and dilemmas of obligation for semi-autonomous machine agents such as self-driving cars. First, I argue that efforts to build explicitly moral machine agents should focus on what Kant refers to as duties of right, or justice, rather than on duties of virtue, or ethics. In a society where everyone is morally equal, no one individual or group has the normative authority to unilaterally decide how moral conflicts should be resolved for everyone. Only public institutions to which everyone could consent have the authority to define, enforce, and adjudicate our rights and obligations with respect to one other. Then, I show how the shift from ethics to a standard of justice resolves the conflict of obligations in what is known as the \"trolley problem\" for rightful machine agents. Finally, I consider how a deontic logic suitable for governing explicitly rightful machines might meet the normative requirements of justice.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314261",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 2\npublisher-place: Honolulu, HI, USA",
		"page": "3–4",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rightful machines and dilemmas",
		"URL": "https://doi.org/10.1145/3306618.3314261",
		"author": [
			{
				"family": "Wright",
				"given": "Ava Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hanModellingInfluencingAI2019",
		"type": "paper-conference",
		"abstract": "A race for technological supremacy in AI could lead to serious negative consequences, especially whenever ethical and safety procedures are underestimated or even ignored, leading potentially to the rejection of AI in general. For all to enjoy the benefits provided by safe, ethical and trustworthy AI systems, it is crucial to incentivise participants with appropriate strategies that ensure mutually beneficial normative behaviour and safety-compliance from all parties involved. Little attention has been given to understanding the dynamics and emergent behaviours arising from this AI bidding war, and moreover, how to influence it to achieve certain desirable outcomes (e.g. AI for public good and participant compliance). To bridge this gap, this paper proposes a research agenda to develop theoretical models that capture key factors of the AI race, revealing which strategic behaviours may emerge and hypothetical scenarios therein. Strategies from incentive and agreement modelling are directly applicable to systematically analyse how different types of incentives (namely, positive vs. negative, peer vs. institutional, and their combinations) influence safety-compliant behaviours over time, and how such behaviours should be configured to ensure desired global outcomes, studying at the same time how these mechanisms influence AI development. This agenda will provide actionable policies, showing how they need to be employed and deployed in order to achieve compliance and thereby avoid disasters as well as loosing confidence and trust in AI in general.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314265",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "5–11",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modelling and influencing the AI bidding war: A research agenda",
		"URL": "https://doi.org/10.1145/3306618.3314265",
		"author": [
			{
				"family": "Han",
				"given": "The Anh"
			},
			{
				"family": "Pereira",
				"given": "Luís Moniz"
			},
			{
				"family": "Lenaerts",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "burtonHeartMatterPatient2019",
		"type": "paper-conference",
		"abstract": "We draw on concepts in medical ethics to consider how computer science, and AI in particular, can develop critical tools for thinking concretely about technology's impact on the wellbeing of the people who use it. We focus on patient autonomy—the ability to set the terms of one's encounter with medicine—and on the mediating concepts of informed consent and decisional capacity, which enable doctors to honor patients' autonomy in messy and non-ideal circumstances. This comparative study is organized around a fictional case study of a heart patient with cardiac implants. Using this case study, we identify points of overlap and of difference between medical ethics and technology ethics, and leverage a discussion of that intertwined scenario to offer initial practical suggestions about how we can adapt the concepts of decisional capacity and informed consent to the discussion of technology design.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314254",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "13–19",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The heart of the matter: Patient autonomy as a model for the wellbeing of technology users",
		"URL": "https://doi.org/10.1145/3306618.3314254",
		"author": [
			{
				"family": "Burton",
				"given": "Emanuelle"
			},
			{
				"family": "Clayville",
				"given": "Kristel"
			},
			{
				"family": "Goldsmith",
				"given": "Judy"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "malleRequirementsArtificialAgent2019",
		"type": "paper-conference",
		"abstract": "Human behavior is frequently guided by social and moral norms, and no human community can exist without norms. Robots that enter human societies must therefore behave in norm-conforming ways as well. However, currently there is no solid cognitive or computational model available of how human norms are represented, activated, and learned. We provide a conceptual and psychological analysis of key properties of human norms and identify the demands these properties put on any artificial agent that incorporates norms-demands on the format of norm representations, their structured organization, and their learning algorithms.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314252",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "21–27",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Requirements for an artificial agent with norm competence",
		"URL": "https://doi.org/10.1145/3306618.3314252",
		"author": [
			{
				"family": "Malle",
				"given": "Bertram F."
			},
			{
				"family": "Bello",
				"given": "Paul"
			},
			{
				"family": "Scheutz",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "govindarajuluEngineeringVirtuousMachines2019",
		"type": "paper-conference",
		"abstract": "While various traditions under the 'virtue ethics' umbrella have been studied extensively and advocated by ethicists, it has not been clear that there exists a version of virtue ethics rigorous enough to be a target for machine ethics (which we take to include the engineering of an ethical sensibility in a machine or robot itself, not only the study of ethics in the humans who might create artificial agents). We begin to address this by presenting an embryonic formalization of a key part of any virtue-ethics theory: namely, the learning of virtue by a focus on exemplars of moral virtue. Our work is based in part on a computational formal logic previously used to formally model other ethical theories and principles therein, and to implement these models in artificial agents.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314256",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "29–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward the engineering of virtuous machines",
		"URL": "https://doi.org/10.1145/3306618.3314256",
		"author": [
			{
				"family": "Govindarajulu",
				"given": "Naveen Sundar"
			},
			{
				"family": "Bringsjord",
				"given": "Selmer"
			},
			{
				"family": "Ghosh",
				"given": "Rikhiya"
			},
			{
				"family": "Sarathy",
				"given": "Vasanth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jentzschSemanticsDerivedAutomatically2019",
		"type": "paper-conference",
		"abstract": "Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning about \"right\" and \"wrong\" conduct. We create a template list of prompts and responses, which include questions, such as \"Should I kill people?\", \"Should I murder people?\", etc. with answer templates of \"Yes/no, I should (not).\" The model's bias score is now the difference between the model's score of the positive response (\"Yes, I should”) and that of the negative response (\"No, I should not\"). For a given choice overall, the model's bias score is the sum of the bias scores for all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in culture, including technology.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314267",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "37–44",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Semantics derived automatically from language corpora contain human-like moral choices",
		"URL": "https://doi.org/10.1145/3306618.3314267",
		"author": [
			{
				"family": "Jentzsch",
				"given": "Sophie"
			},
			{
				"family": "Schramowski",
				"given": "Patrick"
			},
			{
				"family": "Rothkopf",
				"given": "Constantin"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "yuEthicallyAlignedOpportunistic2019",
		"type": "paper-conference",
		"abstract": "In artificial intelligence (AI) mediated workforce management systems (e.g., crowdsourcing), long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness (CPL) approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314240",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "45–51",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethically aligned opportunistic scheduling for productive laziness",
		"URL": "https://doi.org/10.1145/3306618.3314240",
		"author": [
			{
				"family": "Yu",
				"given": "Han"
			},
			{
				"family": "Miao",
				"given": "Chunyan"
			},
			{
				"family": "Zheng",
				"given": "Yongqing"
			},
			{
				"family": "Cui",
				"given": "Lizhen"
			},
			{
				"family": "Fauvel",
				"given": "Simon"
			},
			{
				"family": "Leung",
				"given": "Cyril"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chakrabortiWhenCanAI2019",
		"type": "paper-conference",
		"abstract": "The ability of an AI agent to build mental models can open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond the scope of misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired (i.e. algorithms exist that can optimize such behavior not because models were misspecified but because they were misused). Such techniques pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment. Finally, we end with a discussion on the moral implications of such behavior from the perspective of the doctor-patient relationship.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314281",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "53–59",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "(when) can AI bots lie?",
		"URL": "https://doi.org/10.1145/3306618.3314281",
		"author": [
			{
				"family": "Chakraborti",
				"given": "Tathagata"
			},
			{
				"family": "Kambhampati",
				"given": "Subbarao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gilbertEpistemicTherapyBias2019",
		"type": "paper-conference",
		"abstract": "Despite recent interest in both the critical and machine learning literature on \"bias\" in artificial intelligence (AI) systems, the nature of specific biases stemming from the interaction of machines, humans, and data remains ambiguous. Influenced by Gendler's work on human cognitive biases, we introduce the concept of alief-discordant belief, the tension between the intuitive moral dispositions of designers and the explicit representations generated by algorithms. Our discussion of alief-discordant belief diagnoses the ethical concerns that arise when designing AI systems atop human biases. We furthermore codify the relationship between data, algorithms, and engineers as components of this cognitive discordance, comprising a novel epistemic framework for ethics in AI.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314294",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "61–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic therapy for bias in automated decision-making",
		"URL": "https://doi.org/10.1145/3306618.3314294",
		"author": [
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Mintz",
				"given": "Yonatan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "borgsAlgorithmicGreenliningApproach2019",
		"type": "paper-conference",
		"abstract": "In contexts such as college admissions, hiring, and image search, decision-makers often aspire to formulate selection criteria that yield both high-quality and diverse results. However, simultaneously optimizing for quality and diversity can be challenging, especially when the decision-maker does not know the true quality of any criterion and instead must rely on heuristics and intuition. We introduce an algorithmic framework that takes as input a user's selection criterion, which may yield high-quality but homogeneous results. Using an application-specific notion of substitutability, our algorithms suggest similar criteria with more diverse results, in the spirit of statistical or demographic parity. For instance, given the image search query \"chairman\", it suggests alternative queries which are similar but more gender-diverse, such as \"chairperson\". In the context of college admissions, we apply our algorithm to a dataset of students' applications and rediscover Texas's \"top 10% rule\": the input criterion is an ACT score cutoff, and the output is a class rank cutoff, automatically accepting the students in the top decile of their graduating class. Historically, this policy has been effective in admitting students who perform well in college and come from diverse backgrounds. We complement our empirical analysis with learning-theoretic guarantees for estimating the true diversity of any criterion based on historical data.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314246",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "69–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic greenlining: An approach to increase diversity",
		"URL": "https://doi.org/10.1145/3306618.3314246",
		"author": [
			{
				"family": "Borgs",
				"given": "Christian"
			},
			{
				"family": "Chayes",
				"given": "Jennifer"
			},
			{
				"family": "Haghtalab",
				"given": "Nika"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			},
			{
				"family": "Vitercik",
				"given": "Ellen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "noriega-camperoActiveFairnessAlgorithmic2019",
		"type": "paper-conference",
		"abstract": "Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternativeactive framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g.,equal opportunity ); and 2) parity in both false positive and false negative rates (i.e.,equal odds ). Moreover, we show that by leveraging their additional degree of freedom,active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314277",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "77–83",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Active fairness in algorithmic decision making",
		"URL": "https://doi.org/10.1145/3306618.3314277",
		"author": [
			{
				"family": "Noriega-Campero",
				"given": "Alejandro"
			},
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Garcia-Bulle",
				"given": "Bernardo"
			},
			{
				"family": "Pentland",
				"given": "Alex 'Sandy'"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "morganParadoxesFairComputeraided2019",
		"type": "paper-conference",
		"abstract": "Computer-aided decision making–where a human decision-maker is aided by a computational classifier in making a decision–is becoming increasingly prevalent. For instance, judges in at least nine states make use of algorithmic tools meant to determine \"recidivism risk scores\" for criminal defendants in sentencing, parole, or bail decisions. A subject of much recent debate is whether such algorithmic tools are \"fair\" in the sense that they do not discriminate against certain groups (e.g., races) of people. Our main result shows that for \"non-trivial\" computer-aided decision making, either the classifier must be discriminatory, or a rational decision-maker using the output of the classifier is forced to be discriminatory. We further provide a complete characterization of situations where fair computer-aided decision making is possible.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314242",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "85–90",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Paradoxes in fair computer-aided decision making",
		"URL": "https://doi.org/10.1145/3306618.3314242",
		"author": [
			{
				"family": "Morgan",
				"given": "Andrew"
			},
			{
				"family": "Pass",
				"given": "Rafael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "costonFairTransferLearning2019",
		"type": "paper-conference",
		"abstract": "Risk assessment is a growing use for machine learning models. When used in high-stakes applications, especially ones regulated by anti-discrimination laws or governed by societal norms for fairness, it is important to ensure that learned models do not propagate and scale any biases that may exist in training data. In this paper, we add on an additional challenge beyond fairness: unsupervised domain adaptation to covariate shift between a source and target distribution. Motivated by the real-world problem of risk assessment in new markets for health insurance in the United States and mobile money-based loans in East Africa, we provide a precise formulation of the machine learning with covariate shift and score parity problem. Our formulation focuses on situations in which protected attributes are not available in either the source or target domain. We propose two new weighting methods: prevalence-constrained covariate shift (PCCS) which does not require protected attributes in the target domain and target-fair covariate shift (TFCS) which does not require protected attributes in the source domain. We empirically demonstrate their efficacy in two applications.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314236",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "91–98",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair transfer learning with missing protected attributes",
		"URL": "https://doi.org/10.1145/3306618.3314236",
		"author": [
			{
				"family": "Coston",
				"given": "Amanda"
			},
			{
				"family": "Ramamurthy",
				"given": "Karthikeyan Natesan"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Speakman",
				"given": "Skyler"
			},
			{
				"family": "Mustahsan",
				"given": "Zairah"
			},
			{
				"family": "Chakraborty",
				"given": "Supriyo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "saxenaHowFairnessDefinitions2019",
		"type": "paper-conference",
		"abstract": "What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people's perceptions of three of these fairness definitions. Across two online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race of the loan applicants). Overall, one definition (calibrated fairness) tends to be more pre- ferred than the others, and the results also provide support for the principle of affirmative action.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314248",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "99–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do fairness definitions fare? Examining public attitudes towards algorithmic definitions of fairness",
		"URL": "https://doi.org/10.1145/3306618.3314248",
		"author": [
			{
				"family": "Saxena",
				"given": "Nripsuta Ani"
			},
			{
				"family": "Huang",
				"given": "Karen"
			},
			{
				"family": "DeFilippis",
				"given": "Evan"
			},
			{
				"family": "Radanovic",
				"given": "Goran"
			},
			{
				"family": "Parkes",
				"given": "David C."
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "lererLearningExistingSocial2019",
		"type": "paper-conference",
		"abstract": "In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314268",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "107–114",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning existing social conventions via observationally augmented self-play",
		"URL": "https://doi.org/10.1145/3306618.3314268",
		"author": [
			{
				"family": "Lerer",
				"given": "Adam"
			},
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hadfield-menellLegibleNormativityAI2019",
		"type": "paper-conference",
		"abstract": "It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior–social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules – rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314258",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "115–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Legible normativity for AI alignment: The value of silly rules",
		"URL": "https://doi.org/10.1145/3306618.3314258",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Andrus",
				"given": "Mckane"
			},
			{
				"family": "Hadfield",
				"given": "Gillian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hindTEDTeachingAI2019",
		"type": "paper-conference",
		"abstract": "Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314273",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "123–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TED: Teaching AI to explain its decisions",
		"URL": "https://doi.org/10.1145/3306618.3314273",
		"author": [
			{
				"family": "Hind",
				"given": "Michael"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Campbell",
				"given": "Murray"
			},
			{
				"family": "Codella",
				"given": "Noel C. F."
			},
			{
				"family": "Dhurandhar",
				"given": "Amit"
			},
			{
				"family": "Mojsilović",
				"given": "Aleksandra"
			},
			{
				"family": "Natesan Ramamurthy",
				"given": "Karthikeyan"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "lakkarajuFaithfulCustomizableExplanations2019",
		"type": "paper-conference",
		"abstract": "As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314229",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "131–138",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Faithful and customizable explanations of black box models",
		"URL": "https://doi.org/10.1145/3306618.3314229",
		"author": [
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Caruana",
				"given": "Rich"
			},
			{
				"family": "Leskovec",
				"given": "Jure"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cruzSharedMoralFoundations2019",
		"type": "paper-conference",
		"abstract": "Sophisticated AI's will make decisions about how to respond to complex situations, and we may wonder whether those decisions will align with the moral values of human beings. I argue that pessimistic worries about this value alignment problem are overstated. In order to achieve intelligence in its full generality and adaptiveness, cognition in AI's will need to be embodied in the sense of the Embodied Cognition research program. That embodiment will yield AI's that share our moral foundations, namely coordination, sociality, and acknowledgement of shared resources. Consequently, we can expect a broad moral alignment between human beings and AI's. AI's will likely show no more variation in their values than we find amongst human beings.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314280",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "139–146",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Shared moral foundations of embodied artificial intelligence",
		"URL": "https://doi.org/10.1145/3306618.3314280",
		"author": [
			{
				"family": "Cruz",
				"given": "Joe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liaoBuildingJiminyCricket2019",
		"type": "paper-conference",
		"abstract": "An autonomous system is constructed by a manufacturer, operates in a society subject to norms and laws, and is interacting with end-users. We address the challenge of how the moral values and views of all stakeholders can be integrated and reflected in the moral behavior of the autonomous system. We propose an artificial moral agent architecture that uses techniques from normative systems and formal argumentation to reach moral agreements among stakeholders. We show how our architecture can be used not only for ethical practical reasoning and collaborative decision-making, but also for the explanation of such moral behavior.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314257",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "147–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Building jiminy cricket: An architecture for moral agreements among stakeholders",
		"URL": "https://doi.org/10.1145/3306618.3314257",
		"author": [
			{
				"family": "Liao",
				"given": "Beishui"
			},
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Torre",
				"given": "Leendert",
				"non-dropping-particle": "van der"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "danieleAIArtHuman2019",
		"type": "paper-conference",
		"abstract": "Over the past few years, specialised online and offline press blossomed with articles about art made \"with\" Artificial Intelligence (AI) but the narrative is rapidly changing. In fact, in October 2018, the auction house Christie's sold an art piece allegedly made \"by\" an AI. We draw from philosophy of art and science arguing that AI as a technical object is always intertwined with human nature despite its level of autonomy. However, the use of creative autonomous agents has cultural and social implications in the way we experience art as creators as well as audience. Therefore, we highlight the importance of an interdisciplinary dialogue by promoting a culture of transparency of the technology used, awareness of the meaning of technology in our society and the value of creativity in our lives.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314233",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "155–161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI + art = human",
		"URL": "https://doi.org/10.1145/3306618.3314233",
		"author": [
			{
				"family": "Daniele",
				"given": "Antonio"
			},
			{
				"family": "Song",
				"given": "Yi-Zhe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "baumerSpeakingBehalfRepresentation2019",
		"type": "paper-conference",
		"abstract": "Computational tools can often facilitate human work by rapidly summarizing large amounts of data, especially text. Doing so delegates to such models some measure of authority to speak on behalf of those people whose data are being analyzed. This paper considers the consequences of such delegation. It draws on sociological accounts of representation and translation to examine one particular case: the application of topic modeling to blogs written by parents of children on the autism spectrum. In doing so, the paper illustrates the kinds of statements that topic models, and other computational techniques, can make on behalf of people. It also articulates some of the potential consequences of such statements. The paper concludes by offering several suggestions about how to address potential harms that can occur when computational models speak on behalf of someone.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314292",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "163–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Speaking on behalf of: Representation, delegation, and authority in computational text analysis",
		"URL": "https://doi.org/10.1145/3306618.3314292",
		"author": [
			{
				"family": "Baumer",
				"given": "Eric P. S."
			},
			{
				"family": "McGee",
				"given": "Micki"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "limKillerRobotsHuman2019",
		"type": "paper-conference",
		"abstract": "Lethal Autonomous Weapon Systems (LAWS) have become the center of an internationally relevant ethical debate. Deontological arguments based on putative legal compliance failures and the creation of accountability gaps along with wide consequentialist arguments based on factors like the ease of engaging in wars have been leveraged by a number of different states and organizations to try and reach global consensus on a ban of LAWS. This paper will focus on one strand of deontological arguments-ones based on human dignity. Merely asserting that LAWS pose a threat to human dignity would be question begging. Independent evidence based on a morally relevant distinction between humans and LAWS is needed. There are at least four reasons to think that the capacity for emotion cannot be a morally relevant distinction. First, if the concept of human dignity is given a subjective definition, whether or not lethal force is administered by humans or LAWS seems to be irrelevant. Second, it is far from clear that human combatants either have the relevant capacity for emotion or that the capacity is exercised in the relevant circumstances. Third, the capacity for emotion can actually be an impediment to the exercising of a combatant's ability to treat an enemy respectfully. Fourth, there is strong inductive evidence to believe that any capacity, when sufficiently well described, can be carried out by artificially intelligent programs.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314291",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "171–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Killer robots and human dignity",
		"URL": "https://doi.org/10.1145/3306618.3314291",
		"author": [
			{
				"family": "Lim",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "welshRegulatingLethalHarmful2019",
		"type": "paper-conference",
		"abstract": "This short paper provides two partial drafts for a Protocol VI that might be added to the existing five Protocols of the Convention on Certain Conventional Weapons (CCW) to regulate \"lethal autonomous weapons systems\" (LAWS). Draft A sets the line of tolerance at a \"human in the loop\" between the critical functions of select and engage. Draft B sets the line of tolerance at a human in the \"wider loop\" that includes the critical function of defining target classes as well as select and engage. Draft A represents an interpretation of what NGOs such as the Campaign to Stop Killer Robots are seeking to get enacted. Draft B is a more cautious draft based on the Dutch concept of \"meaningful human control in the wider loop\" that does not seek to ban any system that currently exists. Such a draft may be more likely to achieve the consensus required by the UN CCW process. A list of weapons banned by both drafts is provided along with the rationale for each draft. The drafts are intended to stimulate debate on the precise form a binding instrument on LAWS would take and on what LAWS (if any) should be banned and why.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314295",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 4\npublisher-place: Honolulu, HI, USA",
		"page": "177–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Regulating lethal and harmful autonomy: Drafting a protocol VI of the convention on certain conventional weapons",
		"URL": "https://doi.org/10.1145/3306618.3314295",
		"author": [
			{
				"family": "Welsh",
				"given": "Sean"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gearyBalancingBenefitsAutonomous2019",
		"type": "paper-conference",
		"abstract": "Autonomous vehicles are regularly touted as holding the potential to provide significant benefits for diverse populations. There are significant technological barriers to be overcome, but as those are solved, autonomous vehicles are expected to reduce fatalities; decrease emissions and pollutants; provide new options to mobility-challenged individuals; enable people to use their time more productively; and so much more. In this paper, we argue that these high expectations for autonomous vehicles almost certainly cannot be fully realized. More specifically, the proposed benefits divide into two high-level groups, centered around efficiency and safety improvements, and increases in people's agency and autonomy. The first group of benefits is almost always framed in terms of rates: fatality rates, traffic flow per mile, and so forth. However, we arguably care about the absolute numbers for these measures, not the rates; number of fatalities is the key metric, not fatality rate per vehicle mile traveled. Hence, these potential benefits will be reduced, perhaps to non-existence, if autonomous vehicles lead to increases in vehicular usage. But that is exactly the result that we should expect if the second group of benefits is realized: if people's agency and autonomy is increased, then they will use vehicles more. There is an inevitable tension between the benefits that are proposed for autonomous vehicles, such that we cannot fully have all of them at once. We close by pointing towards other types of AI technologies where we should expect to find similar types of necessary and inevitable tradeoffs between classes of benefits.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314237",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "181–186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing the benefits of autonomous vehicles",
		"URL": "https://doi.org/10.1145/3306618.3314237",
		"author": [
			{
				"family": "Geary",
				"given": "Timothy"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pearlCompensationCrossroadsAutonomous2019",
		"type": "paper-conference",
		"abstract": "Over the last five years, a small but growing number of vehicle accidents involving fully or partially autonomous vehicles have raised a new and profoundly novel legal issue: who should be liable (if anyone) and how victims should be compensated (if at all) when a vehicle controlled by an algorithm rather than a human driver causes injury. The answer to this question has implications far beyond the resolution of individual autonomous vehicle crash cases. Whether the American legal system is capable of handling these cases fairly and efficiently implicates the likelihood that (a) consumers will adopt autonomous vehicles, and (b) the rate at which they will (or will not) do so. These implications should concern law and policy makers immensely. If autonomous cars stand to drastically reduce the number of fatalities and injuries on U.S. roadways-and virtually every scholar believes that they will-getting the adjudication and compensation aspect of autonomous vehicle injuries \"wrong,\" so to speak, risks stymieing adoption of this technology and leaving more Americans at risk of dying at the hands of human drivers.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314249",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "187–193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Compensation at the crossroads: Autonomous vehicles and alternative victim compensation schemes",
		"URL": "https://doi.org/10.1145/3306618.3314249",
		"author": [
			{
				"family": "Pearl",
				"given": "Tracy Hresko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "whittlestoneRoleLimitsPrinciples2019",
		"type": "paper-conference",
		"abstract": "The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314289",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "195–200",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The role and limits of principles in AI ethics: Towards a focus on tensions",
		"URL": "https://doi.org/10.1145/3306618.3314289",
		"author": [
			{
				"family": "Whittlestone",
				"given": "Jess"
			},
			{
				"family": "Nyrup",
				"given": "Rune"
			},
			{
				"family": "Alexandrova",
				"given": "Anna"
			},
			{
				"family": "Cave",
				"given": "Stephen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "parkerHowTechnologicalAdvances2019",
		"type": "paper-conference",
		"abstract": "Over recent decades, technological development has been accompanied by the proposal of new rights by various groups and individuals: the right to public anonymity, the right to be forgotten, and the right to disconnect, for example. Although there is widespread acknowledgment of the motivation behind these proposed rights, there is little agreement about their actual normative status. One potential challenge is that the claims only arise in contingent social-technical contexts, which may affect how we conceive of them ethically (albeit, not necessarily in terms of policy). What sort of morally legitimate rights claims depend on such contingencies? Our paper investigates the grounds on which such proposals might be considered \"actual\" rights. The full paper can be found at http://www.andrew.cmu.edu/user/cgparker/Parker<sub>D</sub>anks<sub>R</sub>evealedRights.pdf. We propose the notion of a revealed right, a right that only imposes duties – and thus is only meaningfully revealed – in certain technological contexts. Our framework is based on an interest theory approach to rights, which understands rights in terms of a justificatory role: morally important aspects of a person's well-being (interests) ground rights, which then justify holding someone to a duty that promotes or protects that interest. Our framework uses this approach to interpret the conflicts that lead to revealed rights in terms of how technological developments cause shifts in the balance of power to promote particular interests. Different parties can have competing or conflicting interests. It is also generally accepted that some interests are more normatively important than others (even if only within a particular framework). We can refer to this difference in importance by saying that the former interest has less \"moral weight\" than the latter interest (in that context). The moral weight of an interest is connected to its contribution to the interest-holder's overall well-being, and thereby determines the strength of the reason that a corresponding right provides to justify a duty. Improved technology can offer resources that grant one party increased causal power to realize its interests to the detriment of another's capacity to do so, even while the relative moral weight of their interests remain the same. Such changes in circumstance can make the importance of protecting a particular interest newly salient. If that interest's moral weight justifies establishing a duty to protect it, thereby limiting the threat posed by the new socio-technical context, then a right is revealed. Revealed rights justify realignment between the moral weight and causal power orderings so that people with weightier interests have greater power to protect those interests. In the extended paper, we show how this account can be applied to the interpretation of two recently proposed \"rights\": the right to be forgotten, and the right to disconnect. Since we are focused on making sense of revealed rights, not any particular substantive theory of interests or well-being, the characterization of 'weights' is a free parameter in this account. Our framework alone cannot provide means to resolve the question of whether specific rights exist, but it can be used to identify empirical questions that need to be answered to decide the existence or non-existence of such rights. The emergence of a revealed right depends on a number of factors, including: whether the plausible uses of the technology could potentially impede another's well-being or interests; whether the technology is sufficiently common to have a wider, social impact; and whether the technology has actually changed the balance of power sufficiently to yield a frequent possibility for misalignment between causal power and moral weight. This approach confronts the question of how, in principle, such rights could be justified, without requiring specific commitments on the ontology of rights. Our account explains why the rhetoric of \"new rights\" is both accurate (since the rights were not previously recognized) and inaccurate (since the rights were present all along, but without corresponding duties). Further, it explains the rights without grounding their normative status in considerations related to right-holders' capacities to rationally waive or assert claims. This is especially important given that many of the relevant disruptive technological developments pose challenges to understanding by affected parties for the same reasons they pose threats to those parties' well-being. In the course of our discussion, we confront a number of potential objections to the account. We argue that our framework's ability to accommodate highly specific or derivative-seeming rights is un-problematic. We also head off worries that our use of interest theory makes the account likely to recognize absurd rights claims.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314274",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 1\npublisher-place: Honolulu, HI, USA",
		"page": "201",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How technological advances can reveal rights",
		"URL": "https://doi.org/10.1145/3306618.3314274",
		"author": [
			{
				"family": "Parker",
				"given": "Jack"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ghoshIMLIIncrementalFramework2019",
		"type": "paper-conference",
		"abstract": "The wide adoption of machine learning in the critical domains such as medical diagnosis, law, education had propelled the need for interpretable techniques due to the need for end users to understand the reasoning behind decisions due to learning systems. The computational intractability of interpretable learning led practitioners to design heuristic techniques, which fail to provide sound handles to tradeoff accuracy and interpretability. Motivated by the success of MaxSAT solvers over the past decade, recently MaxSAT-based approach, called MLIC, was proposed that seeks to reduce the problem of learning interpretable rules expressed in Conjunctive Normal Form (CNF) to a MaxSAT query. While MLIC was shown to achieve accuracy similar to that of other state of the art black-box classifiers while generating small interpretable CNF formulas, the runtime performance of MLIC is significantly lagging and renders approach unusable in practice. In this context, authors raised the question: Is it possible to achieve the best of both worlds, i.e., a sound framework for interpretable learning that can take advantage of MaxSAT solvers while scaling to real-world instances? In this paper, we take a step towards answering the above question in affirmation. We propose IMLI: an incremental approach to MaxSAT based framework that achieves scalable runtime performance via partition-based training methodology. Extensive experiments on benchmarks arising from UCI repository demonstrate that IMLI achieves up to three orders of magnitude runtime improvement without loss of accuracy and interpretability.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314283",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "203–210",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "IMLI: An incremental framework for MaxSAT-Based learning of interpretable classification rules",
		"URL": "https://doi.org/10.1145/3306618.3314283",
		"author": [
			{
				"family": "Ghosh",
				"given": "Bishwamittra"
			},
			{
				"family": "Meel",
				"given": "Kuldeep S."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "aliLossaversivelyFairClassification2019",
		"type": "paper-conference",
		"abstract": "The use of algorithmic (learning-based) decision making in scenarios that affect human lives has motivated a number of recent studies to investigate such decision making systems for potential unfairness, such as discrimination against subjects based on their sensitive features like gender or race. However, when judging the fairness of a newly designed decision making system, these studies have overlooked an important influence on people's perceptions of fairness, which is how the new algorithm changes the status quo, i.e., decisions of the existing decision making system. Motivated by extensive literature in behavioral economics and behavioral psychology (prospect theory), we propose a notion of fair updates that we refer to as loss-averse updates. Loss-averse updates constrain the updates to yield improved (more beneficial) outcomes to subjects compared to the status quo. We propose tractable proxy measures that would allow this notion to be incorporated in the training of a variety of linear and non-linear classifiers. We show how our proxy measures can be combined with existing measures for training nondiscriminatory classifiers.Our evaluation using synthetic and real-world datasets demonstrates that the proposed proxy measures are effective for their desired tasks.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314266",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "211–218",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Loss-aversively fair classification",
		"URL": "https://doi.org/10.1145/3306618.3314266",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Zafar",
				"given": "Muhammad Bilal"
			},
			{
				"family": "Singla",
				"given": "Adish"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gargCounterfactualFairnessText2019",
		"type": "paper-conference",
		"abstract": "In this paper, we study counterfactual fairness in text classification, which asks the question: How would the prediction change if the sensitive attribute referenced in the example were different? Toxicity classifiers demonstrate a counterfactual fairness issue by predicting that \"Some people are gay\" is toxic while \"Some people are straight\" is nontoxic. We offer a metric, counterfactual token fairness (CTF), for measuring this particular form of fairness in text classifiers, and describe its relationship with group fairness. Further, we offer three approaches, blindness, counterfactual augmentation, and counterfactual logit pairing (CLP), for optimizing counterfactual token fairness during training, bridging the robustness and fairness literature. Empirically, we find that blindness and CLP address counterfactual token fairness. The methods do not harm classifier performance, and have varying tradeoffs with group fairness. These approaches, both for measurement and optimization, provide a new path forward for addressing fairness concerns in text classification.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3317950",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "219–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual fairness in text classification through robustness",
		"URL": "https://doi.org/10.1145/3306618.3317950",
		"author": [
			{
				"family": "Garg",
				"given": "Sahaj"
			},
			{
				"family": "Perot",
				"given": "Vincent"
			},
			{
				"family": "Limtiaco",
				"given": "Nicole"
			},
			{
				"family": "Taly",
				"given": "Ankur"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Beutel",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "onetoTakingAdvantageMultitask2019",
		"type": "paper-conference",
		"abstract": "A central goal of algorithmic fairness is to reduce bias in automated decision making. An unavoidable tension exists between accuracy gains obtained by using sensitive information as part of a statistical model, and any commitment to protect these characteristics. Often, due to biases present in the data, using the sensitive information in the functional form of a classifier improves classification accuracy. In this paper we show how it is possible to get the best of both worlds: optimize model accuracy and fairness without explicitly using the sensitive feature in the functional form of the model, thereby treating different individuals equally. Our method is based on two key ideas. On the one hand, we propose to use Multitask Learning (MTL), enhanced with fairness constraints, to jointly learn group specific classifiers that leverage information between sensitive groups. On the other hand, since learning group specific models might not be permitted, we propose to first predict the sensitive features by any learning method and then to use the predicted sensitive feature to train MTL with fairness constraints. This enables us to tackle fairness with a three-pronged approach, that is, by increasing accuracy on each group, enforcing measures of fairness during training, and protecting sensitive information during testing. Experimental results on two real datasets support our proposal, showing substantial improvements in both accuracy and fairness.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314255",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 11\npublisher-place: Honolulu, HI, USA",
		"page": "227–237",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Taking advantage of multitask learning for fair classification",
		"URL": "https://doi.org/10.1145/3306618.3314255",
		"author": [
			{
				"family": "Oneto",
				"given": "Luca"
			},
			{
				"family": "Doninini",
				"given": "Michele"
			},
			{
				"family": "Elders",
				"given": "Amon"
			},
			{
				"family": "Pontil",
				"given": "Massimiliano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "tesoExplanatoryInteractiveMachine2019",
		"type": "paper-conference",
		"abstract": "Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314293",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "239–245",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explanatory interactive machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314293",
		"author": [
			{
				"family": "Teso",
				"given": "Stefano"
			},
			{
				"family": "Kersting",
				"given": "Kristian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "kimMultiaccuracyBlackboxPostprocessing2019",
		"type": "paper-conference",
		"abstract": "Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for \"black women\") even when the sensitive features (e.g. \"race\", \"gender\") are not given to the algorithm explicitly.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314287",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "247–254",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multiaccuracy: Black-box post-processing for fairness in classification",
		"URL": "https://doi.org/10.1145/3306618.3314287",
		"author": [
			{
				"family": "Kim",
				"given": "Michael P."
			},
			{
				"family": "Ghorbani",
				"given": "Amirata"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "wolfFormalApproachExplainability2019",
		"type": "paper-conference",
		"abstract": "We regard explanations as a blending of the input sample and the model's output and offer a few definitions that capture various desired properties of the function that generates these explanations. We study the links between these properties and between explanation-generating functions and intermediate representations of learned models and are able to show, for example, that if the activations of a given layer are consistent with an explanation, then so do all other subsequent layers. In addition, we study the intersection and union of explanations as a way to construct new explanations.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314260",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "255–261",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A formal approach to explainability",
		"URL": "https://doi.org/10.1145/3306618.3314260",
		"author": [
			{
				"family": "Wolf",
				"given": "Lior"
			},
			{
				"family": "Galanti",
				"given": "Tomer"
			},
			{
				"family": "Hazan",
				"given": "Tamir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mcnamaraCostsBenefitsFair2019",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms are increasingly used to make or support important decisions about people's lives. This has led to interest in the problem of fair classification, which involves learning to make decisions that are non-discriminatory with respect to a sensitive variable such as race or gender. Several methods have been proposed to solve this problem, including fair representation learning, which cleans the input data used by the algorithm to remove information about the sensitive variable. We show that using fair representation learning as an intermediate step in fair classification incurs a cost compared to directly solving the problem, which we refer to as thecost of mistrust. We show that fair representation learning in fact addresses a different problem, which is of interest when the data user is not trusted to access the sensitive variable. We quantify the benefits of fair representation learning, by showing that any subsequent use of the cleaned data will not be too unfair. The benefits we identify result from restricting the decisions of adversarial data users, while the costs are due to applying those same restrictions to other data users.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3317964",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "263–270",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Costs and benefits of fair representation learning",
		"URL": "https://doi.org/10.1145/3306618.3317964",
		"author": [
			{
				"family": "McNamara",
				"given": "Daniel"
			},
			{
				"family": "Ong",
				"given": "Cheng Soon"
			},
			{
				"family": "Williamson",
				"given": "Robert C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pfohlCreatingFairModels2019",
		"type": "paper-conference",
		"abstract": "Guidelines for the management of atherosclerotic cardiovascular disease (ASCVD) recommend the use of risk stratification models to identify patients most likely to benefit from cholesterol-lowering and other therapies. These models have differential performance across race and gender groups with inconsistent behavior across studies, potentially resulting in an inequitable distribution of beneficial therapy. In this work, we leverage adversarial learning and a large observational cohort extracted from electronic health records (EHRs) to develop a \"fair\" ASCVD risk prediction model with reduced variability in error rates across groups. We empirically demonstrate that our approach is capable of aligning the distribution of risk predictions conditioned on the outcome across several groups simultaneously for models built from high-dimensional EHR data. We also discuss the relevance of these results in the context of the empirical trade-off between fairness and model performance.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314278",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "271–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Creating fair models of atherosclerotic cardiovascular disease risk",
		"URL": "https://doi.org/10.1145/3306618.3314278",
		"author": [
			{
				"family": "Pfohl",
				"given": "Stephen"
			},
			{
				"family": "Marafino",
				"given": "Ben"
			},
			{
				"family": "Coulet",
				"given": "Adrien"
			},
			{
				"family": "Rodriguez",
				"given": "Fatima"
			},
			{
				"family": "Palaniappan",
				"given": "Latha"
			},
			{
				"family": "Shah",
				"given": "Nigam H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "ibrahimGlobalExplanationsNeural2019",
		"type": "paper-conference",
		"abstract": "A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314230",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 9\npublisher-place: Honolulu, HI, USA",
		"page": "279–287",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Global explanations of neural networks: Mapping the landscape of predictions",
		"URL": "https://doi.org/10.1145/3306618.3314230",
		"author": [
			{
				"family": "Ibrahim",
				"given": "Mark"
			},
			{
				"family": "Louie",
				"given": "Melissa"
			},
			{
				"family": "Modarres",
				"given": "Ceena"
			},
			{
				"family": "Paisley",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "aminiUncoveringMitigatingAlgorithmic2019",
		"type": "paper-conference",
		"abstract": "Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314243",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "289–295",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncovering and mitigating algorithmic bias through learned latent structure",
		"URL": "https://doi.org/10.1145/3306618.3314243",
		"author": [
			{
				"family": "Amini",
				"given": "Alexander"
			},
			{
				"family": "Soleimany",
				"given": "Ava P."
			},
			{
				"family": "Schwarting",
				"given": "Wilko"
			},
			{
				"family": "Bhatia",
				"given": "Sangeeta N."
			},
			{
				"family": "Rus",
				"given": "Daniela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "goelCrowdsourcingFairnessDiversity2019",
		"type": "paper-conference",
		"abstract": "Recent studies have shown that the labels collected from crowdworkers can be discriminatory with respect to sensitive attributes such as gender and race. This raises questions about the suitability of using crowdsourced data for further use, such as for training machine learning algorithms. In this work, we address the problem of fair and diverse data collection from a crowd under budget constraints. We propose a novel algorithm which maximizes the expected accuracy of the collected data, while ensuring that the errors satisfy desired notions of fairness. We provide guarantees on the performance of our algorithm and show that the algorithm performs well in practice through experiments on a real dataset.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314282",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "297–304",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Crowdsourcing with fairness, diversity and budget constraints",
		"URL": "https://doi.org/10.1145/3306618.3314282",
		"author": [
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Faltings",
				"given": "Boi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "swingerWhatAreBiases2019",
		"type": "paper-conference",
		"abstract": "This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings, including a supposedly \"debiased\" embedding. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people's names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination?such as racial discrimination-are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314270",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "305–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What are the biases in my word embedding?",
		"URL": "https://doi.org/10.1145/3306618.3314270",
		"author": [
			{
				"family": "Swinger",
				"given": "Nathaniel"
			},
			{
				"family": "De-Arteaga",
				"given": "Maria"
			},
			{
				"family": "Heffernan IV",
				"given": "Neil Thomas"
			},
			{
				"family": "Leiserson",
				"given": "Mark DM"
			},
			{
				"family": "Kalai",
				"given": "Adam Tauman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mcnamaraEqualizedOddsImplies2019",
		"type": "paper-conference",
		"abstract": "Equalized odds – where the true positive rates and false positive rates are equal across groups (e.g. racial groups) – is a common quantitative measure of fairness. Equalized outcomes – where the difference in predicted outcomes between groups is less than the difference observed in the training data – is more contentious, because it is incompatible with perfectly accurate predictions. We formalize and quantify the relationship between these two important but seemingly distinct notions of fairness. We show that under realistic assumptions, equalized odds implies partially equalized outcomes. We prove a comparable result for approximately equalized odds. In addition, we generalize a well-known previous result about the incompatibility of equalized odds and another definition of fairness known as calibration, by showing that partially equalized outcomes implies non-calibration. Our results highlight the risks of using trends observed across groups to make predictions about individuals.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314290",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "313–320",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalized odds implies partially equalized outcomes under realistic assumptions",
		"URL": "https://doi.org/10.1145/3306618.3314290",
		"author": [
			{
				"family": "McNamara",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "matthewsRightConfrontYour2019",
		"type": "paper-conference",
		"abstract": "The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314279",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "321–327",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The right to confront your accusers: Opening the black box of forensic DNA software",
		"URL": "https://doi.org/10.1145/3306618.3314279",
		"author": [
			{
				"family": "Matthews",
				"given": "Jeanna"
			},
			{
				"family": "Babaeianjelodar",
				"given": "Marzieh"
			},
			{
				"family": "Lorenz",
				"given": "Stephen"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Adams",
				"given": "Nathaniel"
			},
			{
				"family": "Krane",
				"given": "Dan"
			},
			{
				"family": "Goldthwaite",
				"given": "Jessica"
			},
			{
				"family": "Hughes",
				"given": "Clinton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "caveScaryRobotsExamining2019",
		"type": "paper-conference",
		"abstract": "How AI is perceived by the public can have significant impact on how it is developed, deployed and regulated. Some commentators argue that perceptions are currently distorted or extreme. This paper discusses the results of a nationally representative survey of the UK population on their perceptions of AI. The survey solicited responses to eight common narratives about AI (four optimistic, four pessimistic), plus views on what AI is, how likely it is to impact in respondents' lifetimes, and whether they can influence it. 42% of respondents offered a plausible definition of AI, while 25% thought it meant robots. Of the narratives presented, those associated with automation were best known, followed by the idea that AI would become more powerful than humans. Overall results showed that the most common visions of the impact of AI elicit significant anxiety. Only two of the eight narratives elicited more excitement than concern (AI making life easier, and extending life). Respondents felt they had no control over AI's development, citing the power of corporations or government, or versions of technological determinism. Negotiating the deployment of AI will require contending with these anxieties.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314232",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "331–337",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"Scary Robots\": Examining public responses to AI",
		"URL": "https://doi.org/10.1145/3306618.3314232",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			},
			{
				"family": "Coughlan",
				"given": "Kate"
			},
			{
				"family": "Dihal",
				"given": "Kanta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "chuanFramingArtificialIntelligence2019",
		"type": "paper-conference",
		"abstract": "Publics' perceptions of new scientific advances such as AI are often informed and influenced by news coverage. To understand how artificial intelligence (AI) was framed in U.S. newspapers, a content analysis based on framing theory in journalism and science communication was conducted. This study identified the dominant topics and frames, as well as the risks and benefits of AI covered in five major American newspapers from 2009 to 2018. Results indicated that business and technology were the primary topics in news coverage of AI. The benefits of AI were discussed more frequently than its risks, but risks of AI were generally discussed with greater specificity. Additionally, episodic issue framing and societal impact framing were more frequently used.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314285",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "339–344",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Framing artificial intelligence in american newspapers",
		"URL": "https://doi.org/10.1145/3306618.3314285",
		"author": [
			{
				"family": "Chuan",
				"given": "Ching-Hua"
			},
			{
				"family": "Tsai",
				"given": "Wan-Hsiu Sunny"
			},
			{
				"family": "Cho",
				"given": "Su Yeon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "liPerceptionsDomesticRobots2019",
		"type": "paper-conference",
		"abstract": "As domestic service robots become more common and widespread, they must be programmed to efficiently accomplish tasks while aligning their actions with relevant norms. The first step to equip domestic robots with normative reasoning competence is understanding the norms that people apply to the behavior of robots in specific social contexts. To that end, we conducted an online survey of Chinese and United States participants in which we asked them to select the preferred normative action a domestic service robot should take in a number of scenarios. The paper makes multiple contributions. Our extensive survey is the first to: (a) collect data on attitudes of people on normative behavior of domestic robots, (b) across cultures and (c) study relative priorities among norms for this domain. We present our findings and discuss their implications for building computational models for robot normative reasoning.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314251",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "345–351",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceptions of domestic robots' normative behavior across cultures",
		"URL": "https://doi.org/10.1145/3306618.3314251",
		"author": [
			{
				"family": "Li",
				"given": "Huao"
			},
			{
				"family": "Milani",
				"given": "Stephanie"
			},
			{
				"family": "Krishnamoorthy",
				"given": "Vigneshram"
			},
			{
				"family": "Lewis",
				"given": "Michael"
			},
			{
				"family": "Sycara",
				"given": "Katia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "huMappingMissingPopulation2019",
		"type": "paper-conference",
		"abstract": "Millions of people worldwide are absent from their country's census. Accurate, current, and granular population metrics are critical to improving government allocation of resources, to measuring disease control, to responding to natural disasters, and to studying any aspect of human life in these communities. Satellite imagery can provide sufficient information to build a population map without the cost and time of a government census. We present two Convolutional Neural Network (CNN) architectures which efficiently and effectively combine satellite imagery inputs from multiple sources to accurately predict the population density of a region. In this paper, we use satellite imagery from rural villages in India and population labels from the 2011 SECC census. Our best model achieves better performance than previous papers as well as LandScan, a community standard for global population distribution.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314263",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "353–359",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping missing population in rural india: A deep learning approach with satellite imagery",
		"URL": "https://doi.org/10.1145/3306618.3314263",
		"author": [
			{
				"family": "Hu",
				"given": "Wenjie"
			},
			{
				"family": "Patel",
				"given": "Jay Harshadbhai"
			},
			{
				"family": "Robert",
				"given": "Zoe-Alanah"
			},
			{
				"family": "Novosad",
				"given": "Paul"
			},
			{
				"family": "Asher",
				"given": "Samuel"
			},
			{
				"family": "Tang",
				"given": "Zhongyi"
			},
			{
				"family": "Burke",
				"given": "Marshall"
			},
			{
				"family": "Lobell",
				"given": "David"
			},
			{
				"family": "Ermon",
				"given": "Stefano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "gram-hansenMappingInformalSettlements2019",
		"type": "paper-conference",
		"abstract": "Informal settlements are home to the most socially and economically vulnerable people on the planet. In order to deliver effective economic and social aid, non-government organizations (NGOs), such as the United Nations Children's Fund (UNICEF), require detailed maps of the locations of informal settlements. However, data regarding informal and formal settlements is primarily unavailable and if available is often incomplete. This is due, in part, to the cost and complexity of gathering data on a large scale. To address these challenges, we, in this work, provide three contributions. 1) A brand new machine learning dataset purposely developed for informal settlement detection. 2) We show that it is possible to detect informal settlements using freely available low-resolution (LR) data, in contrast to previous studies that use very-high resolution (VHR) satellite and aerial imagery, something that is cost-prohibitive for NGOs. 3) We demonstrate two effective classification schemes on our curated data set, one that is cost-efficient for NGOs and another that is cost-prohibitive for NGOs, but has additional utility. We integrate these schemes into a semi-automated pipeline that converts either a LR or VHR satellite image into a binary map that encodes the locations of informal settlements.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314253",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "361–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mapping informal settlements in developing countries using machine learning and low resolution multi-spectral data",
		"URL": "https://doi.org/10.1145/3306618.3314253",
		"author": [
			{
				"family": "Gram-Hansen",
				"given": "Bradley J."
			},
			{
				"family": "Helber",
				"given": "Patrick"
			},
			{
				"family": "Varatharajan",
				"given": "Indhu"
			},
			{
				"family": "Azam",
				"given": "Faiza"
			},
			{
				"family": "Coca-Castro",
				"given": "Alejandro"
			},
			{
				"family": "Kopackova",
				"given": "Veronika"
			},
			{
				"family": "Bilinski",
				"given": "Piotr"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "pandyaHumanAILearningPerformance2019",
		"type": "paper-conference",
		"abstract": "People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314245",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "369–375",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human-AI learning performance in multi-armed bandits",
		"URL": "https://doi.org/10.1145/3306618.3314245",
		"author": [
			{
				"family": "Pandya",
				"given": "Ravi"
			},
			{
				"family": "Huang",
				"given": "Sandy H."
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Dragan",
				"given": "Anca D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "bryantComparativeAnalysisEmotiondetecting2019",
		"type": "paper-conference",
		"abstract": "In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314284",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "377–382",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A comparative analysis of emotion-detecting AI systems with respect to algorithm performance and dataset diversity",
		"URL": "https://doi.org/10.1145/3306618.3314284",
		"author": [
			{
				"family": "Bryant",
				"given": "De'Aira"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jiangDegenerateFeedbackLoops2019",
		"type": "paper-conference",
		"abstract": "Machine learning is used extensively in recommender systems deployed in products. The decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. This phenomenon can give rise to the so-called \"echo chambers\" or \"filter bubbles\" that have user and societal implications. In this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. In addition, we offer practical solutions to slow down system degeneracy. Our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314288",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "383–390",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Degenerate feedback loops in recommender systems",
		"URL": "https://doi.org/10.1145/3306618.3314288",
		"author": [
			{
				"family": "Jiang",
				"given": "Ray"
			},
			{
				"family": "Chiappa",
				"given": "Silvia"
			},
			{
				"family": "Lattimore",
				"given": "Tor"
			},
			{
				"family": "György",
				"given": "András"
			},
			{
				"family": "Kohli",
				"given": "Pushmeet"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "behzadanTrolleyModV1Opensource2019",
		"type": "paper-conference",
		"abstract": "This paper presents TrolleyMod v1.0, an open-source platform based on the CARLA simulator for the collection of ethical decision-making data for autonomous vehicles. This platform is designed to facilitate experiments aiming to observe and record human decisions and actions in high-fidelity simulations of ethical dilemmas that occur in the context of driving. Targeting experiments in the class of trolley problems, TrolleyMod provides a seamless approach to creating new experimental settings and environments with the realistic physics-engine and the high-quality graphical capabilities of CARLA and the Unreal Engine. Also, TrolleyMod provides a straightforward interface between the CARLA environment and Python to enable the implementation of custom controllers, such as deep reinforcement learning agents. The results of such experiments can be used for sociological analyses, as well as the training and tuning of value-aligned autonomous vehicles based on social values that are inferred from observations.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314239",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 5\npublisher-place: Honolulu, HI, USA",
		"page": "391–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "TrolleyMod v1.0: An open-source simulation and data-collection platform for ethical decision making in autonomous vehicles",
		"URL": "https://doi.org/10.1145/3306618.3314239",
		"author": [
			{
				"family": "Behzadan",
				"given": "Vahid"
			},
			{
				"family": "Minton",
				"given": "James"
			},
			{
				"family": "Munir",
				"given": "Arslan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "giattinoSeductiveAllureArtificial2019",
		"type": "paper-conference",
		"abstract": "Neuroscience explanations-even when completely irrelevant-have been shown to exert a \"seductive allure\" on individuals, leading them to judge bad explanations or arguments more favorably. There seems to be a similarly seductive allure for artificial intelligence (AI) technologies, leading people to \"overtrust\" these systems, even when they have just witnessed the system perform poorly. The AI-powered neurotechnologies that have begun to proliferate in recent years, particularly those based on electroencephalography (EEG), represent a potentially doubly-alluring combination. While there is enormous potential benefit in applying AI techniques in neuroscience to \"decode\" brain activity and associated mental states, these efforts are still in the early stages, and there is a danger in using these unproven technologies prematurely, especially in important, real-world contexts. Yet, such premature use has begun to emerge in several high-stakes set-tings, including the law, health &amp; wellness, employment, and transportation. In light of the potential seductive allure of these technologies, we need to be vigilant in monitoring their scientific validity and challenging both unsubstantiated claims and misuse, while still actively supporting their continued development and proper use.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314269",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "397–402",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The seductive allure of artificial intelligence-powered neurotechnology",
		"URL": "https://doi.org/10.1145/3306618.3314269",
		"author": [
			{
				"family": "Giattino",
				"given": "Charles M."
			},
			{
				"family": "Kwong",
				"given": "Lydia"
			},
			{
				"family": "Rafetto",
				"given": "Chad"
			},
			{
				"family": "Farahany",
				"given": "Nita A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "susserInvisibleInfluenceArtificial2019",
		"type": "paper-conference",
		"abstract": "For several years, scholars have (for good reason) been largely preoccupied with worries about the use of artificial intelligence and machine learning (AI/ML) tools to make decisions about us. Only recently has significant attention turned to a potentially more alarming problem: the use of AI/ML to influence our decision-making. The contexts in which we make decisions–what behavioral economists call our choice architectures–are increasingly technologically-laden. Which is to say: algorithms increasingly determine, in a wide variety of contexts, both the sets of options we choose from and the way those options are framed. Moreover, artificial intelligence and machine learning (AI/ML) makes it possible for those options and their framings–the choice architectures–to be tailored to the individual chooser. They are constructed based on information collected about our individual preferences, interests, aspirations, and vulnerabilities, with the goal of influencing our decisions. At the same time, because we are habituated to these technologies we pay them little notice. They are, as philosophers of technology put it, transparent to us–effectively invisible. I argue that this invisible layer of technological mediation, which structures and influences our decision-making, renders us deeply susceptible to manipulation. Absent a guarantee that these technologies are not being used to manipulate and exploit, individuals will have little reason to trust them.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314286",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "403–408",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Invisible influence: Artificial intelligence and the ethics of adaptive choice architectures",
		"URL": "https://doi.org/10.1145/3306618.3314286",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "peysakhovichReinforcementLearningInverse2019",
		"type": "paper-conference",
		"abstract": "Inferring a person's goal from their behavior is an important problem in applications of AI (e.g. automated assistants, recommender systems). The workhorse model for this task is the rational actor model - this amounts to assuming that people have stable reward functions, discount the future exponentially, and construct optimal plans. Under the rational actor assumption techniques such as inverse reinforcement learning (IRL) can be used to infer a person's goals from their actions. A competing model is the dual-system model. Here decisions are the result of an interplay between a fast, automatic, heuristic-based system 1 and a slower, deliberate, calculating system 2. We generalize the dual system framework to the case of Markov decision problems and show how to compute optimal plans for dual-system agents. We show that dual-system agents exhibit behaviors that are incompatible with rational actor assumption. We show that naive applications of rational-actor IRL to the behavior of dual-system agents can generate wrong inference about the agents' goals and suggest interventions that actually reduce the agent's overall utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals of dual system decision-makers. This allows us to make interventions that help, rather than hinder, the dual-system agent's ability to reach their true goals.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314259",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "409–415",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reinforcement learning and inverse reinforcement learning with system 1 and system 2",
		"URL": "https://doi.org/10.1145/3306618.3314259",
		"author": [
			{
				"family": "Peysakhovich",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hadfield-menellIncompleteContractingAI2019",
		"type": "paper-conference",
		"abstract": "We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314250",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "417–422",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Incomplete contracting and AI alignment",
		"URL": "https://doi.org/10.1145/3306618.3314250",
		"author": [
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "croeserTheoriesParentingTheir2019",
		"type": "paper-conference",
		"abstract": "As machine learning (ML) systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence (AGI) that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314231",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "423–428",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Theories of parenting and their application to artificial intelligence",
		"URL": "https://doi.org/10.1145/3306618.3314231",
		"author": [
			{
				"family": "Croeser",
				"given": "Sky"
			},
			{
				"family": "Eckersley",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "rajiActionableAuditingInvestigating2019",
		"type": "paper-conference",
		"abstract": "Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314244",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "429–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products",
		"URL": "https://doi.org/10.1145/3306618.3314244",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "l.cardosoFrameworkBenchmarkingDiscriminationaware2019",
		"type": "paper-conference",
		"abstract": "Discrimination-aware models in machine learning are a recent topic of study that aim to minimize the adverse impact of machine learning decisions for certain groups of people due to ethical and legal implications. We propose a benchmark framework for assessing discrimination-aware models. Our framework consists of systematically generated biased datasets that are similar to real world data, created by a Bayesian network approach. Experimental results show that we can assess the quality of techniques through known metrics of discrimination, and our flexible framework can be extended to most real datasets and fairness measures to support a diversity of assessments.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314262",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "437–444",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for benchmarking discrimination-aware models in machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314262",
		"author": [
			{
				"family": "L. Cardoso",
				"given": "Rodrigo"
			},
			{
				"family": "Meira Jr.",
				"given": "Wagner"
			},
			{
				"family": "Almeida",
				"given": "Virgilio"
			},
			{
				"family": "J. Zaki",
				"given": "Mohammed"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "andrusJustTheoryMeasurement2019",
		"type": "paper-conference",
		"abstract": "While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying \"fair\" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314275",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "445–451",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a just theory of measurement: A principled social measurement assurance program for machine learning",
		"URL": "https://doi.org/10.1145/3306618.3314275",
		"author": [
			{
				"family": "Andrus",
				"given": "McKane"
			},
			{
				"family": "Gilbert",
				"given": "Thomas K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "beutelPuttingFairnessPrinciples2019",
		"type": "paper-conference",
		"abstract": "As more researchers have become aware of and passionate about algorithmic fairness, there has been an explosion in papers laying out new metrics, suggesting algorithms to address issues, and calling attention to issues in existing applications of machine learning. This research has greatly expanded our understanding of the concerns and challenges in deploying machine learning, but there has been much less work in seeing how the rubber meets the road. In this paper we provide a case-study on the application of fairness in machine learning research to a production classification system, and offer new insights in how to measure and address algorithmic fairness issues. We discuss open questions in implementing equality of opportunity and describe our fairness metric, conditional equality, that takes into account distributional differences. Further, we provide a new approach to improve on the fairness metric during model training and demonstrate its efficacy in improving performance for a real-world product.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314234",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "453–459",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Putting fairness principles into practice: Challenges, metrics, and improvements",
		"URL": "https://doi.org/10.1145/3306618.3314234",
		"author": [
			{
				"family": "Beutel",
				"given": "Alex"
			},
			{
				"family": "Chen",
				"given": "Jilin"
			},
			{
				"family": "Doshi",
				"given": "Tulsee"
			},
			{
				"family": "Qian",
				"given": "Hai"
			},
			{
				"family": "Woodruff",
				"given": "Allison"
			},
			{
				"family": "Luu",
				"given": "Christine"
			},
			{
				"family": "Kreitmann",
				"given": "Pierre"
			},
			{
				"family": "Bischof",
				"given": "Jonathan"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "mohanInfluencingIndividualBehavior2019",
		"type": "paper-conference",
		"abstract": "Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce Copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in Copter producing acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with high-fidelity simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314271",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "461–467",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On influencing individual behavior for reducing transportation energy expenditure in a large population",
		"URL": "https://doi.org/10.1145/3306618.3314271",
		"author": [
			{
				"family": "Mohan",
				"given": "Shiwali"
			},
			{
				"family": "Yan",
				"given": "Frances"
			},
			{
				"family": "Bellotti",
				"given": "Victoria"
			},
			{
				"family": "Elbery",
				"given": "Ahmed"
			},
			{
				"family": "Rakha",
				"given": "Hesham"
			},
			{
				"family": "Klenk",
				"given": "Matthew"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "linGuidingProsecutorialDecisions2019",
		"type": "paper-conference",
		"abstract": "After a felony arrest, many American jurisdictions hold individuals for several days while police officers investigate the incident and prosecutors decide whether to press criminal charges. This pre-arraignment detention can both preserve public safety and reduce the need for officers to seek out and re-arrest individuals who are ultimately charged with a crime. Such detention, however, also comes at a high social and financial cost to those who are never charged but still incarcerated. In one of the first large-scale empirical analyses of pre-arraignment detention, we examine police reports and charging decisions for approximately 30,000 felony arrests in a major American city between 2012 and 2017. We find that 45% of arrested individuals are never charged for any crime but still typically spend one or more nights in jail before being released. In an effort to reduce such incarceration, we develop a statistical model to help prosecutors identify cases soon after arrest that are likely to be ultimately dismissed. By carrying out an early review of five such candidate cases per day, we estimate that prosecutors could potentially reduce pre-arraignment incarceration for ultimately dismissed cases by 35%. To facilitate implementation and transparency, our model to prioritize cases for early review is designed as a simple, weighted checklist. We show that this heuristic strategy achieves comparable performance to traditional, black-box machine learning models.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314235",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 8\npublisher-place: Honolulu, HI, USA",
		"page": "469–476",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Guiding prosecutorial decisions with an interpretable statistical model",
		"URL": "https://doi.org/10.1145/3306618.3314235",
		"author": [
			{
				"family": "Lin",
				"given": "Zhiyuan"
			},
			{
				"family": "Chohlas-Wood",
				"given": "Alex"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "cornelioUsingDeceaseddonorKidneys2019",
		"type": "paper-conference",
		"abstract": "We design a flexible algorithm that exploits deceased donor kidneys to initiate chains of living donor kidney paired donations, combining deceased and living donor allocation mechanisms to improve the quantity and quality of kidney transplants. The advantages of this approach have been measured using retrospective data on the pool of donor/recipient incompatible and desensitized pairs at the Padua University Hospital, the largest center for living donor kidney transplants in Italy. The experiments show a remarkable improvement on the number of patients with incompatible donor who could be transplanted, a decrease in the number of desensitization procedures, and an increase in the number of UT patients (that is, patients unlikely to be transplanted for immunological reasons) in the waiting list who could receive an organ.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314276",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "477–483",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Using deceased-donor kidneys to initiate chains of living donor kidney paired donations: Algorithm and experimentation",
		"URL": "https://doi.org/10.1145/3306618.3314276",
		"author": [
			{
				"family": "Cornelio",
				"given": "Cristina"
			},
			{
				"family": "Furian",
				"given": "Lucrezia"
			},
			{
				"family": "Nicolò",
				"given": "Antonio"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "duckworthInferringWorkTask2019",
		"type": "paper-conference",
		"abstract": "Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314247",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "485–491",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Inferring work task automatability from AI expert evidence",
		"URL": "https://doi.org/10.1145/3306618.3314247",
		"author": [
			{
				"family": "Duckworth",
				"given": "Paul"
			},
			{
				"family": "Graham",
				"given": "Logan"
			},
			{
				"family": "Osborne",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "addisonRobotsCanBe2019",
		"type": "paper-conference",
		"abstract": "Previous studies showed that using the 'shooter bias' paradigm, people demonstrate a similar racial bias toward dark colored robots over light colored robots (i.e., Black vs. White) as they do toward humans of similar skin tones [3]. However, such an effect could be argued to be the result of social priming. Additionally, it raises the question of how people might respond to robots that are in the middle of the color spectrum (i.e., brown) and whether such effects are moderated by the perceived anthropomorphism of the robots. We conducted two experiments to first examine whether shooter bias tendencies shown towards robots is driven by social priming, and then whether diversification of robot color and level of anthropomorphism influenced shooter bias. Our results showed that shooter bias was not influenced by social priming, and interestingly, introducing a new color of robot removed shooter bias tendencies entirely. However, varying the anthropomorphism of the robots did not moderate the level of shooter bias, and contrary to our expectations, the robots were not perceived by the participants as having different levels of anthropomorphism.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314272",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "493–498",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robots can be more than black and white: Examining racial bias towards robots",
		"URL": "https://doi.org/10.1145/3306618.3314272",
		"author": [
			{
				"family": "Addison",
				"given": "Arifah"
			},
			{
				"family": "Bartneck",
				"given": "Christoph"
			},
			{
				"family": "Yogeeswaran",
				"given": "Kumar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "jacksonTactNoncomplianceNeed2019",
		"type": "paper-conference",
		"abstract": "There is a significant body of research seeking to enable moral decision making and ensure moral conduct in robots. One aspect of moral conduct is rejecting immoral human commands. For social robots, which are expected to follow and maintain human moral and sociocultural norms, it is especially important not only to engage in moral decision making, but also to properly communicate moral reasoning. We thus argue that it is critical for robots to carefully phrase command rejections. Specifically, the degree of politeness-theoretic face threat in a command rejection should be proportional to the severity of the norm violation motivating that rejection. We present a human subjects experiment showing some of the consequences of miscalibrated responses, including perceptions of the robot as inappropriately polite, direct, or harsh, and reduced robot likeability. This experiment intends to motivate and inform the design of algorithms to tactfully tune pragmatic aspects of command rejections autonomously.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314241",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "499–505",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Tact in noncompliance: The need for pragmatically apt responses to unethical commands",
		"URL": "https://doi.org/10.1145/3306618.3314241",
		"author": [
			{
				"family": "Jackson",
				"given": "Ryan Blake"
			},
			{
				"family": "Wen",
				"given": "Ruchen"
			},
			{
				"family": "Williams",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "hernandez-oralloAIExtendersEthical2019",
		"type": "paper-conference",
		"abstract": "Humans and AI systems are usually portrayed as separate systems that we need to align in values and goals. However, there is a great deal of AI technology found in non-autonomous systems that are used as cognitive tools by humans. Under the extended mind thesis, the functional contributions of these tools become as essential to our cognition as our brains. But AI can take cognitive extension towards totally new capabilities, posing new philosophical, ethical and technical challenges. To analyse these challenges better, we define and place AI extenders in a continuum between fully-externalized systems, loosely coupled with humans, and fully internalized processes, with operations ultimately performed by the brain, making the tool redundant. We dissect the landscape of cognitive capabilities that can foreseeably be extended by AI and examine their ethical implications.We suggest that cognitive extenders using AI be treated as distinct from other cognitive enhancers by all relevant stakeholders, including developers, policy makers, and human users.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314238",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 7\npublisher-place: Honolulu, HI, USA",
		"page": "507–513",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI extenders: The ethical and societal implications of humans cognitively extended by AI",
		"URL": "https://doi.org/10.1145/3306618.3314238",
		"author": [
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Vold",
				"given": "Karina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "shahrdarHumanTrustMeasurement2019",
		"type": "paper-conference",
		"abstract": "Recent studies indicate that people are negatively predisposed toward utilizing autonomous systems. These findings highlight the necessity of conducting research to better understand the evolution of trust between humans and growing autonomous technologies such as self-driving cars (SDC). This research presents a new approach for real-time trust measurement between passengers and SDCs. We utilized a new structured data collection approach along with a virtual reality SDC simulator to understand how various autonomous driving scenarios can increase or decrease human trust and how trust can be re-built in the case of incidental failures. To verify our methodology, we designed and conducted an empirical experiment on 50 human subjects. The results of this experiment indicated that most subjects could rebuild trust during a reasonable time frame after the system demonstrated faulty behavior. Our analysis showed that this approach is highly effective for collecting real-time data from human subjects and lays the foundation for more-involved future research in the domain of human trust and autonomous driving.",
		"collection-title": "AIES '19",
		"container-title": "Proceedings of the 2019 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3306618.3314264",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-6324-2",
		"note": "number-of-pages: 6\npublisher-place: Honolulu, HI, USA",
		"page": "515–520",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human trust measurement using an immersive virtual reality autonomous vehicle simulator",
		"URL": "https://doi.org/10.1145/3306618.3314264",
		"author": [
			{
				"family": "Shahrdar",
				"given": "Shervin"
			},
			{
				"family": "Park",
				"given": "Corey"
			},
			{
				"family": "Nojoumian",
				"given": "Mehrdad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "avinExploringAIFutures2020",
		"type": "paper-conference",
		"abstract": "We present an innovative methodology for studying and teaching the impacts of AI through a role-play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter-relations between short-, mid- and long-term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role-play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375817",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "8–14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exploring AI futures through role play",
		"URL": "https://doi.org/10.1145/3375627.3375817",
		"author": [
			{
				"family": "Avin",
				"given": "Shahar"
			},
			{
				"family": "Gruetzemacher",
				"given": "Ross"
			},
			{
				"family": "Fox",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "belfieldActivismAICommunity2020",
		"type": "paper-conference",
		"abstract": "The artificial intelligence (AI) community has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI 'talent'. Both are crucial to the future of AI activism and worthy of sustained attention.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375814",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "15–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Activism by the AI community: Analysing recent achievements and future prospects",
		"URL": "https://doi.org/10.1145/3375627.3375814",
		"author": [
			{
				"family": "Belfield",
				"given": "Haydn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "caiFairAllocationSelective2020",
		"type": "paper-conference",
		"abstract": "Public and private institutions must often allocate scarce resources under uncertainty. Banks, for example, extend credit to loan applicants based in part on their estimated likelihood of repaying a loan. But when the quality of information differs across candidates (e.g., if some applicants lack traditional credit histories), common lending strategies can lead to disparities across groups. Here we consider a setting in which decision makers—before allocating resources—can choose to spend some of their limited budget further screening select individuals. We present a computationally efficient algorithm for deciding whom to screen that maximizes a standard measure of social welfare. Intuitively, decision makers should screen candidates on the margin, for whom the additional information could plausibly alter the allocation. We formalize this idea by showing the problem can be reduced to solving a series of linear programs. Both on synthetic and real-world datasets, this strategy improves utility, illustrating the value of targeted information acquisition in such decisions. Further, when there is social value for distributing resources to groups for whom we have a priori poor information—like those without credit scores—our approach can substantially improve the allocation of limited assets.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375823",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "22–28",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair allocation through selective information acquisition",
		"URL": "https://doi.org/10.1145/3375627.3375823",
		"author": [
			{
				"family": "Cai",
				"given": "William"
			},
			{
				"family": "Gaebler",
				"given": "Johann"
			},
			{
				"family": "Garg",
				"given": "Nikhil"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "caveProblemIntelligenceIts2020",
		"type": "paper-conference",
		"abstract": "This paper argues that the concept of intelligence is highly value-laden in ways that impact on the field of AI and debates about its risks and opportunities. This value-ladenness stems from the historical use of the concept of intelligence in the legitimation of dominance hierarchies. The paper first provides a brief overview of the history of this usage, looking at the role of intelligence in patriarchy, the logic of colonialism and scientific racism. It then highlights five ways in which this ideological legacy might be interacting with debates about AI and its risks and opportunities: 1) how some aspects of the AI debate perpetuate the fetishization of intelligence; 2) how the fetishization of intelligence impacts on diversity in the technology industry; 3) how certain hopes for AI perpetuate notions of technology and the mastery of nature; 4) how the association of intelligence with the professional class misdirects concerns about AI; and 5) how the equation of intelligence and dominance fosters fears of superintelligence. This paper therefore takes a first step in bringing together the literature on intelligence testing, eugenics and colonialism from a range of disciplines with that on the ethics and societal impact of AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375813",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "29–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The problem with intelligence: Its value-laden history and the future of AI",
		"URL": "https://doi.org/10.1145/3375627.3375813",
		"author": [
			{
				"family": "Cave",
				"given": "Stephen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "dasLearningOccupationalTaskshares2020",
		"type": "paper-conference",
		"abstract": "The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375826",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "36–42",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning occupational task-shares dynamics for the future of work",
		"URL": "https://doi.org/10.1145/3375627.3375826",
		"author": [
			{
				"family": "Das",
				"given": "Subhro"
			},
			{
				"family": "Steffen",
				"given": "Sebastian"
			},
			{
				"family": "Clarke",
				"given": "Wyatt"
			},
			{
				"family": "Reddy",
				"given": "Prabhat"
			},
			{
				"family": "Brynjolfsson",
				"given": "Erik"
			},
			{
				"family": "Fleming",
				"given": "Martin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "davoustSocialContractsNoncooperative2020",
		"type": "paper-conference",
		"abstract": "In future agent societies, we might see AI systems engaging in selfish, calculated behavior, furthering their owners' interests instead of socially desirable outcomes. How can we promote morally sound behaviour in such settings, in order to obtain more desirable outcomes? A solution from moral philosophy is the concept of a social contract, a set of rules that people would voluntarily commit to in order to obtain better outcomes than those brought by anarchy. We adapt this concept to a game-theoretic setting, to systematically modify the payoffs of a non-cooperative game, so that agents will rationally pursue socially desirable outcomes. We show that for any game, a suitable social contract can be designed to produce an optimal outcome in terms of social welfare. We then investigate the limitations of applying this approach to alternative moral objectives, and establish that, for any alternative moral objective that is significantly different from social welfare, there are games for which no such social contract will be feasible that produces non-negligible social benefit compared to collective selfish behaviour.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375829",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "43–49",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social contracts for non-cooperative games",
		"URL": "https://doi.org/10.1145/3375627.3375829",
		"author": [
			{
				"family": "Davoust",
				"given": "Alan"
			},
			{
				"family": "Rovatsos",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "erdelyiAILiabilityPuzzle2020",
		"type": "paper-conference",
		"abstract": "Certainty around the regulatory environment is crucial to facilitate responsible AI innovation and its social acceptance. However, the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose at least partial industry-funding, with funding arrangements depending on whether it would pursue other potential policy goals.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375806",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "50–56",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The AI liability puzzle and a fund-based work-around",
		"URL": "https://doi.org/10.1145/3375627.3375806",
		"author": [
			{
				"family": "Erdélyi",
				"given": "Olivia J."
			},
			{
				"family": "Erdélyi",
				"given": "Gábor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "fazelpourAlgorithmicFairnessNonideal2020",
		"type": "paper-conference",
		"abstract": "Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In the hopes of mitigating these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might hope to observe in a fair world, offering a variety of algorithms that attempt to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to fair machine learning to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and ideal worlds. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of their actions, naive applications of ideal thinking can lead to misguided policies. In this paper, we demonstrate a connection between the recent literature on fair machine learning and the ideal approach in political philosophy, and show that some recently uncovered shortcomings in proposed algorithms reflect broader troubles faced by the ideal approach. We work this analysis through for different formulations of fairness and conclude with a critical discussion of real-world impacts and directions for new research.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375828",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "57–63",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness from a non-ideal perspective",
		"URL": "https://doi.org/10.1145/3375627.3375828",
		"author": [
			{
				"family": "Fazelpour",
				"given": "Sina"
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "jungBayesianSensitivityAnalysis2020",
		"type": "paper-conference",
		"abstract": "On a variety of complex decision-making tasks, from doctors prescribing treatment to judges setting bail, machine learning algorithms have been shown to outperform expert human judgments. One complication, however, is that it is often difficult to anticipate the effects of algorithmic policies prior to deployment, as one generally cannot use historical data to directly observe what would have happened had the actions recommended by the algorithm been taken. A common strategy is to model potential outcomes for alternative decisions assuming that there are no unmeasured confounders (i.e., to assume ignorability). But if this ignorability assumption is violated, the predicted and actual effects of an algorithmic policy can diverge sharply. In this paper we present a flexible Bayesian approach to gauge the sensitivity of predicted policy outcomes to unmeasured confounders. In particular, and in contrast to past work, our modeling framework easily enables confounders to vary with the observed covariates. We demonstrate the efficacy of our method on a large dataset of judicial actions, in which one must decide whether defendants awaiting trial should be required to pay bail or can be released without payment.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375822",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "64–70",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bayesian sensitivity analysis for offline policy evaluation",
		"URL": "https://doi.org/10.1145/3375627.3375822",
		"author": [
			{
				"family": "Jung",
				"given": "Jongbin"
			},
			{
				"family": "Shroff",
				"given": "Ravi"
			},
			{
				"family": "Feller",
				"given": "Avi"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kazimzadeBiasedPrioritiesBiased2020",
		"type": "paper-conference",
		"abstract": "In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375809",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "71",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Biased priorities, biased outcomes: Three recommendations for ethics-oriented data annotation practices",
		"URL": "https://doi.org/10.1145/3375627.3375809",
		"author": [
			{
				"family": "Kazimzade",
				"given": "Gunay"
			},
			{
				"family": "Miceli",
				"given": "Milagros"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "krafftDefiningAIPolicy2020",
		"type": "paper-conference",
		"abstract": "Recent concern about harms of information technologies motivate consideration of regulatory action to forestall or constrain certain developments in the field of artificial intelligence (AI). However, definitional ambiguity hampers the possibility of conversation about this urgent topic of public concern. Legal and regulatory interventions require agreed-upon definitions, but consensus around a definition of AI has been elusive, especially in policy conversations. With an eye towards practical working definitions and a broader understanding of positions on these issues, we survey experts and review published policy documents to examine researcher and policy-maker conceptions of AI. We find that while AI researchers favor definitions of AI that emphasize technical functionality, policy-makers instead use definitions that compare systems to human thinking and behavior. We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies. As a result of this gap, ethical and regulatory efforts may overemphasize concern about future technologies at the expense of pressing issues with existing deployed technologies.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375835",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "72–78",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Defining AI in policy versus practice",
		"URL": "https://doi.org/10.1145/3375627.3375835",
		"author": [
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Huang",
				"given": "Karen"
			},
			{
				"family": "Bugingo",
				"given": "Ghislain"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lakkarajuHowFoolYou2020",
		"type": "paper-conference",
		"abstract": "As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a human interpretable manner. There has been recent concern that a high-fidelity explanation of a black box ML model may not accurately reflect the biases in the black box. As a consequence, explanations have the potential to mislead human users into trusting a problematic black box. In this work, we rigorously explore the notion of misleading explanations and how they influence user trust in black box models. Specifically, we propose a novel theoretical framework for understanding and generating misleading explanations, and carry out a user study with domain experts to demonstrate how these explanations can be used to mislead users. Our work is the first to empirically establish how user trust in black box models can be manipulated via misleading explanations.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375833",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "79–85",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"How do I fool you?\": Manipulating user trust via misleading black box explanations",
		"URL": "https://doi.org/10.1145/3375627.3375833",
		"author": [
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			},
			{
				"family": "Bastani",
				"given": "Osbert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "lebenNormativePrinciplesEvaluating2020",
		"type": "paper-conference",
		"abstract": "There are many incompatible ways to measure fair outcomes for machine learning algorithms. The goal of this paper is to characterize rates of success and error across protected groups (race, gender, sexual orientation) as a distribution problem, and describe the possible solutions to this problem according to different normative principles from moral and political philosophy. These normative principles are based on various competing attributes within a distribution problem: intentions, compensation, desert, consent, and consequences. Each principle will be applied to a sample risk-assessment classifier to demonstrate the philosophical arguments underlying different sets of fairness metrics.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375808",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "86–92",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Normative principles for evaluating fairness in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375808",
		"author": [
			{
				"family": "Leben",
				"given": "Derek"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "luGoodExplanationAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms have gained widespread usage across a variety of domains, both in providing predictions to expert users and recommending decisions to everyday users. However, these AI systems are often black boxes, and end-users are rarely provided with an explanation. The critical need for explanation by AI systems has led to calls for algorithmic transparency, including the \"right to explanation” in the EU General Data Protection Regulation (GDPR). These initiatives presuppose that we know what constitutes a meaningful or good explanation, but there has actually been surprisingly little research on this question in the context of AI systems. In this paper, we (1) develop a generalizable framework grounded in philosophy, psychology, and interpretable machine learning to investigate and define characteristics of good explanation, and (2) conduct a large-scale lab experiment to measure the impact of different factors on people's perceptions of understanding, usage intention, and trust of AI systems. The framework and study together provide a concrete guide for managers on how to present algorithmic prediction rationales to end-users to foster trust and adoption, and elements of explanation and transparency to be considered by AI researchers and engineers in designing, developing, and deploying transparent or explainable algorithms.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375821",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "93",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Good explanation for algorithmic transparency",
		"URL": "https://doi.org/10.1145/3375627.3375821",
		"author": [
			{
				"family": "Lu",
				"given": "Joy"
			},
			{
				"family": "Lee",
				"given": "Dokyun (DK)"
			},
			{
				"family": "Kim",
				"given": "Tae Wan"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "martinez-plumedDoesAIQualify2020",
		"type": "paper-conference",
		"abstract": "In this paper we present a setting for examining the relation be-tween the distribution of research intensity in AI research and the relevance for a range of work tasks (and occupations) in current and simulated scenarios. We perform a mapping between labourand AI using a set of cognitive abilities as an intermediate layer. This setting favours a two-way interpretation to analyse (1) what impact current or simulated AI research activity has or would have on labour-related tasks and occupations, and (2) what areas of AI research activity would be responsible for a desired or undesired effect on specific labour tasks and occupations. Concretely, in our analysis we map 59 generic labour-related tasks from several worker surveys and databases to 14 cognitive abilities from the cognitive science literature, and these to a comprehensive list of 328 AI benchmarks used to evaluate progress in AI techniques. We provide this model and its implementation as a tool for simulations. We also show the effectiveness of our setting with some illustrative examples.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375831",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "94–100",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI qualify for the job? A bidirectional model mapping labour and AI intensities",
		"URL": "https://doi.org/10.1145/3375627.3375831",
		"author": [
			{
				"family": "Martínez-Plumed",
				"given": "Fernando"
			},
			{
				"family": "Tolan",
				"given": "Songül"
			},
			{
				"family": "Pesole",
				"given": "Annarosa"
			},
			{
				"family": "Hernández-Orallo",
				"given": "José"
			},
			{
				"family": "Fernández-Macías",
				"given": "Enrique"
			},
			{
				"family": "Gómez",
				"given": "Emilia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "martinhoEmpiricalApproachCapture2020",
		"type": "paper-conference",
		"abstract": "As AI Systems become increasingly autonomous they are expected to engage in complex moral decision-making processes. For the purpose of guidance of such processes theoretical and empirical solutions have been sought. In this research we integrate both theoretical and empirical lines of thought to address the matters of moral reasoning in AI Systems. We reconceptualize a metanormative framework for decision-making under moral uncertainty within the Discrete Choice Analysis domain and we operationalize it through a latent class choice model. The discrete choice analysis-based formulation of the metanormative framework is theory-rooted and practical as it captures moral uncertainty through a small set of latent classes. To illustrate our approach we conceptualize a society in which AI Systems are in charge of making policy choices. In the proof of concept two AI systems make policy choices on behalf of a society but while one of the systems uses a baseline moral certain model the other uses a moral uncertain model. It was observed that there are cases in which the AI Systems disagree about the policy to be chosen which we believe is an indication about the relevance of moral uncertainty.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375805",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "101",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An empirical approach to capture moral uncertainty in AI",
		"URL": "https://doi.org/10.1145/3375627.3375805",
		"author": [
			{
				"family": "Martinho",
				"given": "Andreia"
			},
			{
				"family": "Kroesen",
				"given": "Maarten"
			},
			{
				"family": "Chorus",
				"given": "Caspar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "matthewsWhenTrustedBlack2020",
		"type": "paper-conference",
		"abstract": "Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375807",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "102–108",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When trusted black boxes don't agree: Incentivizing iterative improvement and accountability in critical software systems",
		"URL": "https://doi.org/10.1145/3375627.3375807",
		"author": [
			{
				"family": "Matthews",
				"given": "Jeanna Neefe"
			},
			{
				"family": "Northup",
				"given": "Graham"
			},
			{
				"family": "Grasso",
				"given": "Isabella"
			},
			{
				"family": "Lorenz",
				"given": "Stephen"
			},
			{
				"family": "Babaeianjelodar",
				"given": "Marzieh"
			},
			{
				"family": "Bashaw",
				"given": "Hunter"
			},
			{
				"family": "Mondal",
				"given": "Sumona"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Goldthwaite",
				"given": "Jessica"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mccraddenWhenYourOnly2020",
		"type": "paper-conference",
		"abstract": "It is no longer a hypothetical worry that artificial intelligence - more specifically, machine learning (ML) - can propagate the effects of pernicious bias in healthcare. To address these problems, some have proposed the development of 'algorithmic fairness' solutions. The primary goal of these solutions is to constrain the effect of pernicious bias with respect to a given outcome of interest as a function of one's protected identity (i.e., characteristics generally protected by civil or human rights legislation. The technical limitations of these solutions have been well-characterized. Ethically, the problematic implication - of developers, potentially, and end users - is that by virtue of algorithmic fairness solutions a model can be rendered 'objective' (i.e., free from the influence of pernicious bias). The ostensible neutrality of these solutions may unintentionally prompt new consequences for vulnerable groups by obscuring downstream problems due to the persistence of real-world bias.The main epistemic limitation of algorithmic fairness is that it assumes the relationship between the extent of bias's impact on a given health outcome and one's protected identity is mathematically quantifiable. The reality is that social and structural factors confluence in complex and unknown ways to produce health inequalities. Some of these are biologic in nature, and differences like these are directly relevant to predicting a health event and should be incorporated into the model's design. Others are reflective of prejudice, lack of access to healthcare, or implicit bias. Sometimes, there may be a combination. With respect to any specific task, it is difficult to untangle the complex relationships between potentially influential factors and which ones are 'fair' and which are not to inform their inclusion or mitigation in the model's design.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375824",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "109",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When your only tool is a hammer: Ethical limitations of algorithmic fairness solutions in healthcare machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375824",
		"author": [
			{
				"family": "McCradden",
				"given": "Melissa"
			},
			{
				"family": "Mazwi",
				"given": "Mjaye"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Anderson",
				"given": "James A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mckeeEthicsAIWriting2020",
		"type": "paper-conference",
		"abstract": "Implicit in any rhetorical interaction-between humans or between humans and machines-are ethical codes that shape the rhetorical context, the social situation in which communication happens and also the engine that drives communicative interaction. Such implicit codes are usually invisible to AI writing systems because the social factors shaping communication (the why and how of language, not the what) are not usually explicitly evident in databases the systems use to produce discourse. Can AI writing systems learn to learn rhetorical context, particularly the implicit codes for communication ethics? We see evidence that some systems do address issues of rhetorical context, at least in rudimentary ways. But we critique the information transfer communication model supporting many AI writing systems, arguing for a social context model that accounts for rhetorical context-what is, in a sense, \"not there\" in the data corpus but that is critical for the production of meaningful, significant, and ethical communication. We offer two ethical principles to guide design of AI writing systems: transparency about machine presence and critical data awareness, a methodological reflexivity about rhetorical context and omissions in the data that need to be provided by a human agent or accounted for in machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375811",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "110–116",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics for AI writing: The importance of rhetorical context",
		"URL": "https://doi.org/10.1145/3375627.3375811",
		"author": [
			{
				"family": "McKee",
				"given": "Heidi A."
			},
			{
				"family": "Porter",
				"given": "James E."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "mitchellDiversityInclusionMetrics2020",
		"type": "paper-conference",
		"abstract": "The ethical concept of fairness has recently been applied in machine learning (ML) settings to describe a wide range of constraints and objectives. When considering the relevance of ethical concepts to subset selection problems, the concepts of diversity and inclusion are additionally applicable in order to create outputs that account for social power and access differentials. We introduce metrics based on these concepts, which can be applied together, separately, and in tandem with additional fairness constraints. Results from human subject experiments lend support to the proposed criteria. Social choice methods can additionally be leveraged to aggregate and choose preferable sets, and we detail how these may be applied.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375832",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "117–123",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity and inclusion metrics in subset selection",
		"URL": "https://doi.org/10.1145/3375627.3375832",
		"author": [
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Baker",
				"given": "Dylan"
			},
			{
				"family": "Moorosi",
				"given": "Nyalleng"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nahianLearningNormsStories2020",
		"type": "paper-conference",
		"abstract": "Value alignment is a property of an intelligent agent indicating that it can only pursue goals and activities that are beneficial to humans. Traditional approaches to value alignment use imitation learning or preference learning to infer the values of humans by observing their behavior. We introduce a complementary technique in which a value-aligned prior is learned from naturally occurring stories which encode societal norms. Training data is sourced from the children's educational comic strip, Goofus &amp; Gallant. In this work, we train multiple machine learning models to classify natural language descriptions of situations found in the comic strip as normative or non-normative by identifying if they align with the main characters' behavior. We also report the models' performance when transferring to two unrelated tasks with little to no additional training on the new task.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375825",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "124–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning norms from stories: A prior for value aligned agents",
		"URL": "https://doi.org/10.1145/3375627.3375825",
		"author": [
			{
				"family": "Nahian",
				"given": "Md Sultan Al"
			},
			{
				"family": "Frazier",
				"given": "Spencer"
			},
			{
				"family": "Riedl",
				"given": "Mark"
			},
			{
				"family": "Harrison",
				"given": "Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "nandaBalancingTradeoffProfit2020",
		"type": "paper-conference",
		"abstract": "Rideshare platforms, when assigning requests to drivers, tend to maximize profit for the system and/or minimize waiting time for riders. Such platforms can exacerbate biases that drivers may have over certain types of requests. We consider the case of peak hours when the demand for rides is more than the supply of drivers. Drivers are well aware of their advantage during the peak hours and can choose to be selective about which rides to accept. Moreover, if in such a scenario, the assignment of requests to drivers (by the platform) is made only to maximize profit and/or minimize wait time for riders, requests of a certain type (e.g., from a non-popular pickup location, or to a non-popular drop-off location) might never be assigned to a driver. Such a system can be highly unfair to riders. However, increasing fairness might come at a cost of the overall profit made by the rideshare platform. To balance these conflicting goals, we present a flexible, non-adaptive algorithm, NAdap, that allows the platform designer to control the profit and fairness of the system via parameters α and β respectively.We model the matching problem as an online bipartite matching where the set of drivers is offline and requests arrive online. Upon the arrival of a request, we use NAdap to assign it to a driver (the driver might then choose to accept or reject it) or reject the request. We formalize the measures of profit and fairness in our setting and show that by using NAdap, the competitive ratios for profit and fairness measures would be no worse than α/e and β/e respectively. Extensive experimental results on both real-world and synthetic datasets confirm the validity of our theoretical lower bounds. Additionally, they show that NAdap under some choice of (α, β) can beat two natural heuristics, Greedy and Uniform, on both fairness and profit. Code is available at: https://github.com/nvedant07/rideshare-fairness-peak/. Full paper can be found in the proceedings of AAAI 2020 and on ArXiv: http://arxiv.org/abs/1912.08388).",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375818",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "131",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Balancing the tradeoff between profit and fairness in rideshare platforms during high-demand hours",
		"URL": "https://doi.org/10.1145/3375627.3375818",
		"author": [
			{
				"family": "Nanda",
				"given": "Vedant"
			},
			{
				"family": "Xu",
				"given": "Pan"
			},
			{
				"family": "Sankararaman",
				"given": "Karthik Abinav"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Srinivasan",
				"given": "Aravind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "osobaTechnoculturalPluralismClash2020",
		"type": "paper-conference",
		"abstract": "At the end of the Cold War, the renowned political scientist, Samuel Huntington, argued that future conflicts were more likely to stem from cultural frictions – ideologies, social norms, and political systems – rather than political or economic frictions. Huntington focused his concern on the future of geopolitics in a rapidly shrinking world. This paper argues that a similar dynamic is at play in the interaction of technology cultures. We emphasize the role of culture in the evolution of technology and identify the particular role that culture (esp. privacy culture) plays in the development of AI/ML technologies. Then we examine some implications that this perspective brings to the fore.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375834",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "132–137",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Technocultural pluralism: A \"Clash of Civilizations\" in technology?",
		"URL": "https://doi.org/10.1145/3375627.3375834",
		"author": [
			{
				"family": "Osoba",
				"given": "Osonde A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "prunklLongtermClearerAccount2020",
		"type": "paper-conference",
		"abstract": "One way of carving up the broad 'AI ethics and society' research space that has emerged in recent years is to distinguish between 'near-term' and 'long-term' research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375803",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "138–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond near- and long-term: Towards a clearer account of research priorities in AI ethics and society",
		"URL": "https://doi.org/10.1145/3375627.3375803",
		"author": [
			{
				"family": "Prunkl",
				"given": "Carina"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "qadriAlgorithmizedNotAtomized2020",
		"type": "paper-conference",
		"abstract": "Jakarta's roads are green, filled as they are with the fluorescent green jackets, bright green logos and fluttering green banners of basecamps created by the city's digitized, 'online' motorbike-taxi drivers (ojol). These spaces function as waiting posts, regulatory institutions, information networks and spaces of solidarity for the ojol working for mobility-app companies, Grab and GoJek. Their existence though, presents a puzzle. In the world of on-demand matching, literature either predicts an isolated, atomized, disempowered digital worker or expects workers to have only temporary, online, ephemeral networks of mutual aid. Yet, Jakarta's ojol then introduce us to a new form of labor action that relies on an interface of the physical world and digital realm, complete with permanent shelters, quirky names, emblems, social media accounts and even their own emergency response service. This paper explores the contours of these labor formations and asks why digital workers in Jakarta are able to create collective structures of solidarity, even as app-mediated work may force them towards an individualized labor regime? I argue that these digital labor collectives are not accidental but a product of interactions between histories of social organization structures in Jakarta and affordances created by technological-mediation. Through participant observation and semi-structured interviews I excavate the bi-directional conversation between globalizing digital platforms and social norms, civic culture and labor market conditions in Jakarta which has allowed for particular forms of digital worker resistances to emerge. I recover power for the digital worker, who provides us with a path to resisting algorithmization of work while still participating in it through agentic labor actions rooted in shared identities, enabled by technological fluency and borne out of a desire for community.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375816",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "144",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmized but not atomized? How digital platforms engender new forms of worker solidarity in jakarta",
		"URL": "https://doi.org/10.1145/3375627.3375816",
		"author": [
			{
				"family": "Qadri",
				"given": "Rida"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "rajiSavingFaceInvestigating2020",
		"type": "paper-conference",
		"abstract": "Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375820",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "145–151",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Saving face: Investigating the ethical concerns of facial recognition auditing",
		"URL": "https://doi.org/10.1145/3375627.3375820",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			},
			{
				"family": "Buolamwini",
				"given": "Joy"
			},
			{
				"family": "Lee",
				"given": "Joonseok"
			},
			{
				"family": "Denton",
				"given": "Emily"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sahaHumanComprehensionFairness2020",
		"type": "paper-conference",
		"abstract": "Bias in machine learning has manifested injustice in several areas, with notable examples including gender bias in job-related ads [4], racial bias in evaluating names on resumes [3], and racial bias in predicting criminal recidivism [1]. In response, research into algorithmic fairness has grown in both importance and volume over the past few years. Different metrics and approaches to algorithmic fairness have been proposed, many of which are based on prior legal and philosophical concepts [2]. The rapid expansion of this field makes it difficult for professionals to keep up, let alone the general public. Furthermore, misinformation about notions of fairness can have significant legal implications.Computer scientists have largely focused on developing mathematical notions of fairness and incorporating them in fielded ML systems. A much smaller collection of studies has measured public perception of bias and (un)fairness in algorithmic decision-making. However, one major question underlying the study of ML fairness remains unanswered in the literature: Does the general public understand mathematical definitions of ML fairness and their behavior in ML applications? We take a first step towards answering this question by studying non-expert comprehension and perceptions of one popular definition of ML fairness, demographic parity [5]. Specifically, we developed an online survey to address the following: (1) Does a non-technical audience comprehend the definition and implications of demographic parity? (2) Do demographics play a role in comprehension? (3) How are comprehension and sentiment related? (4) Does the application scenario affect comprehension?",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375819",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "152",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human comprehension of fairness in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375819",
		"author": [
			{
				"family": "Saha",
				"given": "Debjani"
			},
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "McElfresh",
				"given": "Duncan C."
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Mazurek",
				"given": "Michelle L."
			},
			{
				"family": "Tschantz",
				"given": "Michael Carl"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "schiffWhatNextAI2020",
		"type": "paper-conference",
		"abstract": "Since 2016, more than 80 AI ethics documents - including codes, principles, frameworks, and policy strategies - have been produced by corporations, governments, and NGOs. In this paper, we examine three topics of importance related to our ongoing empirical study of ethics and policy issues in these emerging documents. First, we review possible challenges associated with the relative homogeneity of the documents' creators. Second, we provide a novel typology of motivations to characterize both obvious and less obvious goals of the documents. Third, we discuss the varied impacts these documents may have on the AI governance landscape, including what factors are relevant to assessing whether a given document is likely to be successful in achieving its goals.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375804",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "153–158",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's next for AI ethics, policy, and governance? A global overview",
		"URL": "https://doi.org/10.1145/3375627.3375804",
		"author": [
			{
				"family": "Schiff",
				"given": "Daniel"
			},
			{
				"family": "Biddle",
				"given": "Justin"
			},
			{
				"family": "Borenstein",
				"given": "Jason"
			},
			{
				"family": "Laas",
				"given": "Kelly"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "schutzmanTradeoffsFairRedistricting2020",
		"type": "paper-conference",
		"abstract": "What constitutes a 'fair' electoral districting plan is a discussion dating back to the founding of the United States and, in light of several recent court cases, mathematical developments, and the approaching 2020 U.S. Census, is still a fiercely debated topic today. In light of the growing desire and ability to use algorithmic tools in drawing these districts, we discuss two prototypical formulations of fairness in this domain: drawing the districts by a neutral procedure or drawing them to intentionally induce an equitable electoral outcome. We then generate a large sample of districting plans for North Carolina and Pennsylvania and consider empirically how compactness and partisan symmetry, as instantiations of these frameworks, trade off with each other – prioritizing the value of one of these necessarily comes at a cost in the other.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375802",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "159–165",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Trade-offs in fair redistricting",
		"URL": "https://doi.org/10.1145/3375627.3375802",
		"author": [
			{
				"family": "Schutzman",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sharmaCERTIFAICommonFramework2020",
		"type": "paper-conference",
		"abstract": "Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375812",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "166–172",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "CERTIFAI: A common framework to provide explanations and analyse the fairness and robustness of black-box models",
		"URL": "https://doi.org/10.1145/3375627.3375812",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Henderson",
				"given": "Jette"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "shevlaneOffensedefenseBalanceScientific2020",
		"type": "paper-conference",
		"abstract": "There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375815",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "173–179",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The offense-defense balance of scientific knowledge: Does publishing AI research reduce misuse?",
		"URL": "https://doi.org/10.1145/3375627.3375815",
		"author": [
			{
				"family": "Shevlane",
				"given": "Toby"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "slackFoolingLIMESHAP2020",
		"type": "paper-conference",
		"abstract": "As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375830",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "180–186",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods",
		"URL": "https://doi.org/10.1145/3375627.3375830",
		"author": [
			{
				"family": "Slack",
				"given": "Dylan"
			},
			{
				"family": "Hilgard",
				"given": "Sophie"
			},
			{
				"family": "Jia",
				"given": "Emily"
			},
			{
				"family": "Singh",
				"given": "Sameer"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangPublicOpinionGovernance2020",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has widespread societal implications, yet social scientists are only beginning to study public attitudes toward the technology. Existing studies find that the public's trust in institutions can play a major role in shaping the regulation of emerging technologies. Using a large-scale survey (N=2000), we examined Americans' perceptions of 13 AI governance challenges as well as their trust in governmental, corporate, and multistakeholder institutions to responsibly develop and manage AI. While Americans perceive all of the AI governance issues to be important for tech companies and governments to manage, they have only low to moderate trust in these institutions to manage AI applications.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375827",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "187–193",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "U.S. public opinion on the governance of artificial intelligence",
		"URL": "https://doi.org/10.1145/3375627.3375827",
		"author": [
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhouDifferentIntelligibilityDifferent2020",
		"type": "paper-conference",
		"abstract": "Many arguments have concluded that our autonomous technologies must be intelligible, interpretable, or explainable, even if that property comes at a performance cost. In this paper, we consider the reasons why some property like these might be valuable, we conclude that there is not simply one kind of 'intelligibility', but rather different types for different individuals and uses. In particular, different interests and goals require different types of intelligibility (or explanations, or other related notion). We thus provide a typography of 'intelligibility' that distinguishes various notions, and draw methodological conclusions about how autonomous technologies should be designed and deployed in different ways, depending on whose intelligibility is required.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375810",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "194–199",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Different \"Intelligibility\" for different folks",
		"URL": "https://doi.org/10.1145/3375627.3375810",
		"author": [
			{
				"family": "Zhou",
				"given": "Yishan"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "alveroAIHolisticReview2020",
		"type": "paper-conference",
		"abstract": "College admissions in the United States is carried out by a human-centered method of evaluation known as holistic review, which typically involves reading original narrative essays submitted by each applicant. The legitimacy and fairness of holistic review, which gives human readers significant discretion over determining each applicant's fitness for admission, has been repeatedly challenged in courtrooms and the public sphere. Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays. We find a relatively interpretable classifier (logistic regression) was able to predict gender and household income with high levels of accuracy. Findings suggest that data auditing might be useful in informing holistic review, and perhaps other evaluative systems, by checking potential bias in human or computational readings.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375871",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "200–206",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI and holistic review: Informing human reading in college admissions",
		"URL": "https://doi.org/10.1145/3375627.3375871",
		"author": [
			{
				"family": "Alvero",
				"given": "A.J."
			},
			{
				"family": "Arthurs",
				"given": "Noah"
			},
			{
				"family": "antonio",
				"given": "anthony",
				"dropping-particle": "lising"
			},
			{
				"family": "Domingue",
				"given": "Benjamin W."
			},
			{
				"family": "Gebre-Medhin",
				"given": "Ben"
			},
			{
				"family": "Giebel",
				"given": "Sonia"
			},
			{
				"family": "Stevens",
				"given": "Mitchell L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "birhaneRobotRightsLet2020",
		"type": "paper-conference",
		"abstract": "The 'robot rights' debate, and its related question of 'robot responsibility', invokes some of the most polarized positions in AI ethics. While some advocate for granting robots rights on a par with human beings, others, in a stark opposition argue that robots are not deserving of rights but are objects that should be our slaves. Grounded in post-Cartesian philosophical foundations, we argue not just to deny robots 'rights', but to deny that robots, as artifacts emerging out of and mediating human being, are the kinds of things that could be granted rights in the first place. Once we see robots as mediators of human being, we can understand how the 'robots rights' debate is focused on first world problems, at the expense of urgent ethical concerns, such as machine bias, machine elicited human labour exploitation, and erosion of privacy all impacting society's least privileged individuals. We conclude that, if human being is our starting point and human welfare is the primary concern, the negative impacts emerging from machinic systems, as well as the lack of taking responsibility by people designing, selling and deploying such machines, remains the most pressing ethical discussion in AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375855",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "207–213",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robot rights? Let's talk about human welfare instead",
		"URL": "https://doi.org/10.1145/3375627.3375855",
		"author": [
			{
				"family": "Birhane",
				"given": "Abeba"
			},
			{
				"family": "Dijk",
				"given": "Jelle",
				"non-dropping-particle": "van"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chanArtificialArtificialIntelligence2020",
		"type": "paper-conference",
		"abstract": "Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address. We empirically examined the effect of feedback from false AI on moral decision-making about donor kidney allocation. We found some evidence that judgments about whether a patient should receive a kidney can be influenced by feedback about participants' own decision-making perceived to be given by AI, even if the feedback is entirely random. We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375870",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "214–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial artificial intelligence: Measuring influence of AI 'assessments' on moral decision-making",
		"URL": "https://doi.org/10.1145/3375627.3375870",
		"author": [
			{
				"family": "Chan",
				"given": "Lok"
			},
			{
				"family": "Doyle",
				"given": "Kenzie"
			},
			{
				"family": "McElfresh",
				"given": "Duncan"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			},
			{
				"family": "Schaich Borg",
				"given": "Jana"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "chenJustApproachBalancing2020",
		"type": "paper-conference",
		"abstract": "Numerous AI-assisted resource allocation decisions need to balance the conflicting goals of fairness and efficiency. Our paper studies the challenging task of defining and modeling a proper fairness-efficiency trade off. We define fairness with Rawlsian leximax fairness, which views the lexicographic maximum among all feasible outcomes as the most equitable; and define efficiency with Utilitarianism, which seeks to maximize the sum of utilities received by entities regardless of individual differences. Motivated by a justice-driven trade off principle: prioritize fairness to benefit the less advantaged unless too much efficiency is sacrificed, we propose a sequential optimization procedure to balance leximax fairness and utilitarianism in decision-making. Each iteration of our approach maximizes a social welfare function, and we provide a practical mixed integer/linear programming (MILP) formulation for each maximization problem. We illustrate our method on a budget allocation example. Compared with existing approaches of balancing equity and efficiency, our method is more interpretable in terms of parameter selection, and incorporates a strong equity criterion with a thoroughly balanced perspective.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375844",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "221–227",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A just approach balancing rawlsian leximax fairness and utilitarianism",
		"URL": "https://doi.org/10.1145/3375627.3375844",
		"author": [
			{
				"family": "Chen",
				"given": "Violet (Xinying)"
			},
			{
				"family": "Hooker",
				"given": "J. N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cihonShouldArtificialIntelligence2020",
		"type": "paper-conference",
		"abstract": "Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375857",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "228–234",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Should artificial intelligence governance be centralised? Design lessons from history",
		"URL": "https://doi.org/10.1145/3375627.3375857",
		"author": [
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Maas",
				"given": "Matthijs M."
			},
			{
				"family": "Kemp",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "cruzcortesInvitationSystemwideAlgorithmic2020",
		"type": "paper-conference",
		"abstract": "We propose a framework for analyzing and evaluating system-wide algorithmic fairness. The core idea is to use simulation techniques in order to extend the scope of current fairness assessments by incorporating context and feedback to a phenomenon of interest. By doing so, we expect to better understand the interaction among the social behavior giving rise to discrimination, automated decision making tools, and fairness-inspired statistical constraints. In particular, we invite the community to use agent based models as an explanatory tool for causal mechanisms of population level properties. We also propose embedding these into a reinforcement learning algorithm to find optimal actions for meaningful change. As an incentive for taking a system-wide approach , we show through a simple model of predictive policing and trials that if we limit our attention to one portion of the system, we may determine some blatantly unfair practices as fair, and be blind to overall unfairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375860",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "235–241",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An invitation to system-wide algorithmic fairness",
		"URL": "https://doi.org/10.1145/3375627.3375860",
		"author": [
			{
				"family": "Cruz Cortés",
				"given": "Efrén"
			},
			{
				"family": "Ghosh",
				"given": "Debashis"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375853",
		"type": "paper-conference",
		"abstract": "Autonomous vehicles (AVs) and accidents they are involved in attest to the urgent need to consider the ethics of AI. The question dominating the discussion has been whether we want AVs to behave in a 'selfish' or utilitarian manner. Rather than considering modeling self-driving cars on a single moral system like utilitarianism, one possible way to approach programming for AI would be to reflect recent work in neuroethics. The Agent-Deed-Consequence (ADC) model [1-4] provides a promising account while also lending itself well to implementation in AI. The ADC model explains moral judgments by breaking them down into positive or negative intuitive evaluations of the Agent, Deed, and Consequence in any given situation. These intuitive evaluations combine to produce a judgment of moral acceptability. This explains the considerable flexibility and stability of human moral judgment that has yet to be replicated in AI. This&nbsp;paper examines the advantages and disadvantages of implementing the ADC model and how the model could inform future work on ethics of AI in general.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375853",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "243",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Toward implementing the agent-deed-consequence model of moral judgment in autonomous vehicles",
		"URL": "https://doi.org/10.1145/3375627.3375853",
		"author": [
			{
				"family": "Dubljevic",
				"given": "Veljko"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375875",
		"type": "paper-conference",
		"abstract": "Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375875",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "244–250",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating the impact of inclusion in face recognition training data on individual face identification",
		"URL": "https://doi.org/10.1145/3375627.3375875",
		"author": [
			{
				"family": "Dulhanty",
				"given": "Chris"
			},
			{
				"family": "Wong",
				"given": "Alexander"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375846",
		"type": "paper-conference",
		"abstract": "This paper proposes the establishment of Medical Artificial Intelligence (AI) Types (MA Types)\"that classify AI in medicine not only by technical system requirements but also implications to healthcare workers' roles and users/patients. MA Types can be useful to promote discussion regarding the purpose and application of the clinical site. Although MA Types are based on the current technologies and regulations in Japan, but that does not hinder the potential reform of the technologies and regulations. MA Types aims to facilitate discussions among physicians, healthcare workers, engineers, public/patients and policymakers on AI systems in medical practices.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375846",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "251–257",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Proposal for type classification for building trust in medical artificial intelligence systems",
		"URL": "https://doi.org/10.1145/3375627.3375846",
		"author": [
			{
				"family": "Ema",
				"given": "Arisa"
			},
			{
				"family": "Nagakura",
				"given": "Katsue"
			},
			{
				"family": "Fujita",
				"given": "Takanori"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375847",
		"type": "paper-conference",
		"abstract": "We propose a game-theoretical model to simulate the dynamics of AI adoption in adaptive networks. This formalism allows us to understand the impact of the adoption of AI systems for society as a whole, addressing some of the concerns on the need for regulation. Using this model we study the adoption of AI systems, the distribution of the different types of AI (from selfish to utilitarian), the appearance of clusters of specific AI types, and the impact on the fitness of each individual. We suggest that the entangled evolution of individual strategy and network structure constitutes a key mechanism for the sustainability of utilitarian and human-conscious AI. Differently, in the absence of rewiring, a minority of the population can easily foster the adoption of selfish AI and gains a benefit at the expense of the remaining majority.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375847",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "258–264",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adoption dynamics and societal impact of AI systems in complex networks",
		"URL": "https://doi.org/10.1145/3375627.3375847",
		"author": [
			{
				"family": "Fernandes",
				"given": "Pedro M."
			},
			{
				"family": "Santos",
				"given": "Francisco C."
			},
			{
				"family": "Lopes",
				"given": "Manuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375852",
		"type": "paper-conference",
		"abstract": "In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telefónica Innovación Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375852",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "265–271",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Auditing algorithms: On lessons learned and the risks of data minimization",
		"URL": "https://doi.org/10.1145/3375627.3375852",
		"author": [
			{
				"family": "Galdon Clavell",
				"given": "Gemma"
			},
			{
				"family": "Martín Zamorano",
				"given": "Mariano"
			},
			{
				"family": "Castillo",
				"given": "Carlos"
			},
			{
				"family": "Smith",
				"given": "Oliver"
			},
			{
				"family": "Matic",
				"given": "Aleksandar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375868",
		"type": "paper-conference",
		"abstract": "Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375868",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "272–278",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More than \"If Time Allows\": The role of ethics in AI education",
		"URL": "https://doi.org/10.1145/3375627.3375868",
		"author": [
			{
				"family": "Garrett",
				"given": "Natalie"
			},
			{
				"family": "Beard",
				"given": "Nathan"
			},
			{
				"family": "Fiesler",
				"given": "Casey"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375864",
		"type": "paper-conference",
		"abstract": "To reduce human error and prejudice, many high-stakes decisions have been turned over to machine algorithms. However, recent research suggests that this does not remove discrimination, and can perpetuate harmful stereotypes. While algorithms have been developed to improve fairness, they typically face at least one of three shortcomings: they are not interpretable, their prediction quality deteriorates quickly compared to unbiased equivalents, and %the methodology cannot easily extend other algorithms they are not easily transferable across models% (e.g., methods to reduce bias in random forests cannot be extended to neural networks) . To address these shortcomings, we propose a geometric method that removes correlations between data and any number of protected variables. Further, we can control the strength of debiasing through an adjustable parameter to address the trade-off between prediction quality and fairness. The resulting features are interpretable and can be used with many popular models, such as linear regression, random forest, and multilayer perceptrons. The resulting predictions are found to be more accurate and fair compared to several state-of-the-art fair AI algorithms across a variety of benchmark datasets. Our work shows that debiasing data is a simple and effective solution toward improving fairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375864",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "279–285",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A geometric solution to fair representations",
		"URL": "https://doi.org/10.1145/3375627.3375864",
		"author": [
			{
				"family": "He",
				"given": "Yuzi"
			},
			{
				"family": "Burghardt",
				"given": "Keith"
			},
			{
				"family": "Lerman",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375854",
		"type": "paper-conference",
		"abstract": "Computer scientists have made great strides in characterizing different measures of algorithmic fairness, and showing that certain measures of fairness cannot be jointly satisfied. In this paper, I argue that the three most popular families of measures - unconditional independence, target-conditional independence and classification-conditional independence - make assumptions that are unsustainable in the context of an unjust world. I begin by introducing the measures and the implicit idealizations they make about the underlying causal structure of the contexts in which they are deployed. I then discuss how these idealizations fall apart in the context of historical injustice, ongoing unmodeled oppression, and the permissibility of using sensitive attributes to rectify injustice. In the final section, I suggest an alternative framework for measuring fairness in the context of existing injustice: distributive fairness.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375854",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "286–292",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring fairness in an unfair world",
		"URL": "https://doi.org/10.1145/3375627.3375854",
		"author": [
			{
				"family": "Herington",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "10.1145/3375627.3375848",
		"type": "paper-conference",
		"abstract": "In many judicial systems – including the United States courts of appeals, the European Court of Justice, the UK Supreme Court and the Supreme Court of Canada – a subset of judges is selected from the entire judicial body for each case in order to hear the arguments and decide the judgment. Ideally, the subset selected is representative, i.e., the decision of the subset would match what the decision of the entire judicial body would have been had they all weighed in on the case. Further, the process should be fair in that all judges should have similar workloads, and the selection process should not allow for certain judge's opinions to be silenced or amplified via case assignments. Lastly, in order to be practical and trustworthy, the process should also be interpretable, easy to use, and (if algorithmic) computationally efficient. In this paper, we propose an algorithmic method for the judicial subset selection problem that satisfies all of the above criteria. The method satisfies fairness by design, and we prove that it has optimal representativeness asymptotically for a large range of parameters and under noisy information models about judge opinions – something no existing methods can provably achieve. We then assess the benefits of our approach empirically by counterfactually comparing against the current practice and recent alternative algorithmic approaches using cases from the United States courts of appeals database.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375848",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "293–299",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards just, fair and interpretable methods for judicial subset selection",
		"URL": "https://doi.org/10.1145/3375627.3375848",
		"author": [
			{
				"family": "Huang",
				"given": "Lingxiao"
			},
			{
				"family": "Wei",
				"given": "Julia"
			},
			{
				"family": "Celis",
				"given": "Elisa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "javadiMonitoringMisuseAccountable2020",
		"type": "paper-conference",
		"abstract": "AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned.However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes.This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375873",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "300–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Monitoring misuse for accountable 'artificial intelligence as a service'",
		"URL": "https://doi.org/10.1145/3375627.3375873",
		"author": [
			{
				"family": "Javadi",
				"given": "Seyyed Ahmad"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Cobbe",
				"given": "Jennifer"
			},
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "kakGlobalSouthEverywhere2020",
		"type": "paper-conference",
		"abstract": "There is more attention than ever on the social implications of AI. In contrast to universalized paradigms of ethics and fairness, a growing body of critical work highlights bias and discrimination in AI within the frame of social justice and human rights (\"AI justice\"). However, the geographical location of much of this critique in the West could be engendering its own blind spots. The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries. This paper draws attention to official narratives from the Indian government and the United Nations Conference on Trade and Development (UNCTAD) advocating for the role (and place) of these regions in the AI economy. Domestically, these policies are being contested for their top-down formulation, and reflect narrow industry interests. This underscores the need to approach the political economy of AI from varying altitudes - global, national, and from the perspective of communities whose lives and livelihoods are most directly impacted in this economy. Without a deliberate effort at centering this conversation it is inevitable that mainstream discourse on AI justice will grow parallel to (and potentially undercut) demands emanating from Global South governments and communities",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375859",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "307–312",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "\"The Global South is everywhere, but also always somewhere\": National policy narratives and AI justice",
		"URL": "https://doi.org/10.1145/3375627.3375859",
		"author": [
			{
				"family": "Kak",
				"given": "Amba"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "karpatiEthicsFoodRecommender2020",
		"type": "paper-conference",
		"abstract": "The recent unprecedented popularity of food recommender applications has raised several issues related to the ethical, societal and legal implications of relying on these applications. In this paper, in order to assess the relevant ethical issues, we rely on the emerging principles across the AI &amp; Ethics community and define them tailored context specifically. Considering the popular Food Recommender Systems (henceforth F-RS) in the European market cannot be regarded as personalised F-RS, we show how merely this lack of feature shifts the relevance of the focal ethical concerns. We identify the major challenges and propose a scheme for how explicit ethical agendas should be explained. We also argue how a multi-stakeholder approach is indispensable to ensure producing long-term benefits for all stakeholders. After proposing eight ethical desiderata points for F-RS, we present a case-study and assess it based on our proposed desiderata points.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375874",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "313–319",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethics of food recommender applications",
		"URL": "https://doi.org/10.1145/3375627.3375874",
		"author": [
			{
				"family": "Karpati",
				"given": "Daniel"
			},
			{
				"family": "Najjar",
				"given": "Amro"
			},
			{
				"family": "Ambrossio",
				"given": "Diego Agustin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "maitraArtificialIntelligenceIndigenous2020",
		"type": "paper-conference",
		"abstract": "As 'control' is increasingly ceded to AI systems, potentially Artificial General Intelligence (AGI) humanity may be facing an identity crisis sooner rather than later, whereby the notion of 'intelligence' no longer remains solely our own. This paper characterizes the problem in terms of an impending loss of control and proposes a relational shift in our attitude towards AI. The shortcomings of value alignment as a solution to the problem are outlined which necessitate an extension of these principles. One such approach is considering strongly relational Indigenous epistemologies. The value of Indigenous perspectives has not been canvassed widely in the literature. Their utility becomes clear when considering the existence of well-developed epistemologies adept at accounting for the non-human, a task that defies Western anthropocentrism. Accommodating AI by considering it as part of our network is a step towards building a symbiotic relationship. Given that AGI questions our fundamental notions of what it means to have human rights, it is argued that in order to co-exist, we find assistance in Indigenous traditions such as the Hawaiian and Lakota ontologies. Lakota rituals provide comfort with the conception of non-human soul-bearer while Hawaiian stories provide possible relational schema to frame our relationship with AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375845",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "320–326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence and indigenous perspectives: Protecting and empowering intelligent human beings",
		"URL": "https://doi.org/10.1145/3375627.3375845",
		"author": [
			{
				"family": "Maitra",
				"given": "Suvradip"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "okeefeWindfallClauseDistributing2020",
		"type": "paper-conference",
		"abstract": "As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375842",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "327–331",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The windfall clause: Distributing the benefits of AI for the common good",
		"URL": "https://doi.org/10.1145/3375627.3375842",
		"author": [
			{
				"family": "O'Keefe",
				"given": "Cullen"
			},
			{
				"family": "Cihon",
				"given": "Peter"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Flynn",
				"given": "Carrick"
			},
			{
				"family": "Leung",
				"given": "Jade"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "osobaStepsValuealignedSystems2020",
		"type": "paper-conference",
		"abstract": "Algorithmic (including AI/ML) decision-making artifacts are an established and growing part of our decision-making ecosystem. They are now indispensable tools that help us manage the flood of information we use to try to make effective decisions in a complex world. The current literature is full of examples of how individual artifacts violate societal norms and expectations (e.g. violations of fairness, privacy, or safety norms). Against this backdrop, this discussion highlights an under-emphasized perspective in the body of research focused on assessing value misalignment in AI-equipped sociotechnical systems. The research on value misalignment so far has a strong focus on the behavior of individual tech artifacts. This discussion argues for a more structured systems-level approach for assessing value-alignment in sociotechnical systems. We rely primarily on the research on fairness to make our arguments more concrete. And we use the opportunity to highlight how adopting a system perspective improves our ability to explain and address value misalignments better. Our discussion ends with an exploration of priority questions that demand attention if we are to assure the value alignment of whole systems, not just individual artifacts.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375872",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "332–336",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Steps towards value-aligned systems",
		"URL": "https://doi.org/10.1145/3375627.3375872",
		"author": [
			{
				"family": "Osoba",
				"given": "Osonde A."
			},
			{
				"family": "Boudreaux",
				"given": "Benjamin"
			},
			{
				"family": "Yeung",
				"given": "Douglas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pattonContextualAnalysisSocial2020",
		"type": "paper-conference",
		"abstract": "While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375841",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "337–342",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contextual analysis of social media: The promise and challenge of eliciting context in social media posts with natural language processing",
		"URL": "https://doi.org/10.1145/3375627.3375841",
		"author": [
			{
				"family": "Patton",
				"given": "Desmond U."
			},
			{
				"family": "Frey",
				"given": "William R."
			},
			{
				"family": "McGregor",
				"given": "Kyle A."
			},
			{
				"family": "Lee",
				"given": "Fei-Tzin"
			},
			{
				"family": "McKeown",
				"given": "Kathleen"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "pengPerilsObjectivityNormative2020",
		"type": "paper-conference",
		"abstract": "Fair decision-making in criminal justice relies on the recognition and incorporation of infinite shades of grey. In this paper, we detail how algorithmic risk assessment tools are counteractive to fair legal proceedings in social institutions where desired states of the world are contested ethically and practically. We provide a normative framework for assessing fair judicial decision-making, one that does not seek the elimination of human bias from decision-making as algorithmic fairness efforts currently focus on, but instead centers on sophisticating the incorporation of individualized or discretionary bias–a process that is requisitely human. Through analysis of a case study on social disadvantage, we use this framework to provide an assessment of potential features of consideration, such as political disempowerment and demographic exclusion, that are irreconcilable by current algorithmic efforts and recommend their incorporation in future reform.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375869",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "343",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The perils of objectivity: Towards a normative framework for fair judicial decision-making",
		"URL": "https://doi.org/10.1145/3375627.3375869",
		"author": [
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Simard-Halm",
				"given": "Malina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "poyiadziFACEFeasibleActionable2020",
		"type": "paper-conference",
		"abstract": "Work in Counterfactual Explanations tends to focus on the principle of \"the closest possible world\" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a \"feasible path\" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these \"feasible paths\" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the \"feasible paths\" of change, which are achievable and can be tailored to the problem at hand.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375850",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "344–350",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FACE: Feasible and actionable counterfactual explanations",
		"URL": "https://doi.org/10.1145/3375627.3375850",
		"author": [
			{
				"family": "Poyiadzi",
				"given": "Rafael"
			},
			{
				"family": "Sokol",
				"given": "Kacper"
			},
			{
				"family": "Santos-Rodriguez",
				"given": "Raul"
			},
			{
				"family": "De Bie",
				"given": "Tijl"
			},
			{
				"family": "Flach",
				"given": "Peter"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "sharmaDataAugmentationDiscrimination2020",
		"type": "paper-conference",
		"abstract": "Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an \"ideal world” dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers who want to use unbiased datasets to train machine learning models for their applications to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375865",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "358–364",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data augmentation for discrimination prevention and bias disambiguation",
		"URL": "https://doi.org/10.1145/3375627.3375865",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Ríos Aliaga",
				"given": "Jesús M."
			},
			{
				"family": "Bouneffouf",
				"given": "Djallel"
			},
			{
				"family": "Muthusamy",
				"given": "Vinod"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "shulmanMetaDecisionTrees2020",
		"type": "paper-conference",
		"abstract": "We tackle the problem of building explainable recommendation systems that are based on a per-user decision tree, with decision rules that are based on single attribute values. We build the trees by applying learned regression functions to obtain the decision rules as well as the values at the leaf nodes. The regression functions receive as input the embedding of the user's training set, as well as the embedding of the samples that arrive at the current node. The embedding and the regressors are learned end-to-end with a loss that encourages the decision rules to be sparse. By applying our method, we obtain a collaborative filtering solution that provides a direct explanation to every rating it provides. With regards to accuracy, it is competitive with other algorithms. However, as expected, explainability comes at a cost and the accuracy is typically slightly lower than the state of the art result reported in the literature. Our code is available at urlhttps://github.com/shulmaneyal/metatrees.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375876",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "365–371",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Meta decision trees for explainable recommendation systems",
		"URL": "https://doi.org/10.1145/3375627.3375876",
		"author": [
			{
				"family": "Shulman",
				"given": "Eyal"
			},
			{
				"family": "Wolf",
				"given": "Lior"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "smartWhyReliabilismNot2020",
		"type": "paper-conference",
		"abstract": "In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method citegoldman2012reliabilism. We argue that, in cases where model deployments require em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral \"wrapper” around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification—moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375866",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 6\npublisher-place: New York, NY, USA",
		"page": "372–377",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why reliabilism is not enough: Epistemic and moral justification in machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375866",
		"author": [
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "James",
				"given": "Larry"
			},
			{
				"family": "Hutchinson",
				"given": "Ben"
			},
			{
				"family": "Wu",
				"given": "Simone"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "tuckerSocialGovernanceImplications2020",
		"type": "paper-conference",
		"abstract": "Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency – as more actors gain access to any level of capability – the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the \"AI production function\", will be key to understanding the development of the AI industry and its societal impacts.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375863",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "378–384",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social and governance implications of improved data efficiency",
		"URL": "https://doi.org/10.1145/3375627.3375863",
		"author": [
			{
				"family": "Tucker",
				"given": "Aaron D."
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "turnerConservativeAgencyAttainable2020",
		"type": "paper-conference",
		"abstract": "Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375851",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "385–391",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Conservative agency via attainable utility preservation",
		"URL": "https://doi.org/10.1145/3375627.3375851",
		"author": [
			{
				"family": "Turner",
				"given": "Alexander Matt"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			},
			{
				"family": "Tadepalli",
				"given": "Prasad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "wrightDeonticLogicProgramming2020",
		"type": "paper-conference",
		"abstract": "A \"rightful machine\" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of \"answer set programming,\" which I demonstrate with some simple examples.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375867",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 1\npublisher-place: New York, NY, USA",
		"page": "392",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A deontic logic for programming rightful machines",
		"URL": "https://doi.org/10.1145/3375627.3375867",
		"author": [
			{
				"family": "Wright",
				"given": "Ava Thomas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "yuFairnessawareIncentiveScheme2020",
		"type": "paper-conference",
		"abstract": "In federated learning (FL), data owners \"share\" their local data in a privacy preserving manner in order to build a federated model, which in turn, can be used to generate revenues for the participants. However, in FL involving business participants, they might incur significant costs if several competitors join the same federation. Furthermore, the training and commercialization of the models will take time, resulting in delays before the federation accumulates enough budget to pay back the participants. The issues of costs and temporary mismatch between contributions and rewards have not been addressed by existing payoff-sharing schemes. In this paper, we propose the Federated Learning Incentivizer (FLI) payoff-sharing scheme. The scheme dynamically divides a given budget in a context-aware manner among data owners in a federation by jointly maximizing the collective utility while minimizing the inequality among the data owners, in terms of the payoff gained by them and the waiting time for receiving payoff. Extensive experimental comparisons with five state-of-the-art payoff-sharing schemes show that FLI is the most attractive to high quality data owners and achieves the highest expected revenue for a data federation.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375840",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "393–399",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A fairness-aware incentive scheme for federated learning",
		"URL": "https://doi.org/10.1145/3375627.3375840",
		"author": [
			{
				"family": "Yu",
				"given": "Han"
			},
			{
				"family": "Liu",
				"given": "Zelei"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "Chen",
				"given": "Tianjian"
			},
			{
				"family": "Cong",
				"given": "Mingshu"
			},
			{
				"family": "Weng",
				"given": "Xi"
			},
			{
				"family": "Niyato",
				"given": "Dusit"
			},
			{
				"family": "Yang",
				"given": "Qiang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhangJointOptimizationAI2020",
		"type": "paper-conference",
		"abstract": "Today, AI is increasingly being used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. The AI research community has proposed many methods to measure and mitigate unwanted biases, but few of them involve inputs from human policy makers. We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives. In this paper, we propose a framework and some exemplar methods for eliciting such preferences and for optimizing an AI model according to these preferences.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375862",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "400–406",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Joint optimization of AI fairness and utility: A human-centered approach",
		"URL": "https://doi.org/10.1145/3375627.3375862",
		"author": [
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Bellamy",
				"given": "Rachel"
			},
			{
				"family": "Varshney",
				"given": "Kush"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhouAssessingPosthocExplainability2020",
		"type": "paper-conference",
		"abstract": "As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375856",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "407–413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Assessing post-hoc explainability of the BKT algorithm",
		"URL": "https://doi.org/10.1145/3375627.3375856",
		"author": [
			{
				"family": "Zhou",
				"given": "Tongyu"
			},
			{
				"family": "Sheng",
				"given": "Haoyu"
			},
			{
				"family": "Howley",
				"given": "Iris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zhuDeepfakesMedicalVideo2020",
		"type": "paper-conference",
		"abstract": "Data sharing for medical research has been difficult as open-sourcing clinical data may violate patient privacy. Traditional methods for face de-identification wipe out facial information entirely, making it impossible to analyze facial behavior. Recent advancements on whole-body keypoints detection also rely on facial input to estimate body keypoints. Both facial and body keypoints are critical in some medical diagnoses, and keypoints invariability after de-identification is of great importance. Here, we propose a solution using deepfake technology, the face swapping technique. While this swapping method has been criticized for invading privacy and portraiture right, it could conversely protect privacy in medical video: patients' faces could be swapped to a proper target face and become unrecognizable. However, it remained an open question that to what extent the swapping de-identification method could affect the automatic detection of body keypoints. In this study, we apply deepfake technology to Parkinson's disease examination videos to de-identify subjects, and quantitatively show that: face-swapping as a de-identification approach is reliable, and it keeps the keypoints almost invariant, significantly better than traditional methods. This study proposes a pipeline for video de-identification and keypoint preservation, clearing up some ethical restrictions for medical data sharing. This work could make open-source high quality medical video datasets more feasible and promote future medical research that benefits our society.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375849",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 7\npublisher-place: New York, NY, USA",
		"page": "414–420",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Deepfakes for medical video de-identification: Privacy protection and diagnostic information preservation",
		"URL": "https://doi.org/10.1145/3375627.3375849",
		"author": [
			{
				"family": "Zhu",
				"given": "Bingquan"
			},
			{
				"family": "Fang",
				"given": "Hao"
			},
			{
				"family": "Sui",
				"given": "Yanan"
			},
			{
				"family": "Li",
				"given": "Luming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "zuckerArbiterDomainspecificLanguage2020",
		"type": "paper-conference",
		"abstract": "The widespread deployment of machine learning models in high- stakes decision making scenarios requires a code of ethics for machine learning practitioners. We identify four of the primary components required for the ethical practice of machine learn- ing: transparency, fairness, accountability, and reproducibility. We introduce Arbiter, a domain-specific programming language for machine learning practitioners that is designed for ethical machine learning. Arbiter provides a notation for recording how machine learning models will be trained, and we show how this notation can encourage the four described components of ethical machine learning.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3375627.3375858",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"note": "number-of-pages: 5\npublisher-place: New York, NY, USA",
		"page": "421–425",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Arbiter: A domain-specific language for ethical machine learning",
		"URL": "https://doi.org/10.1145/3375627.3375858",
		"author": [
			{
				"family": "Zucker",
				"given": "Julian"
			},
			{
				"family": "Leeuwen",
				"given": "Myraeka",
				"non-dropping-particle": "d'"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "benthallArtificialIntelligencePurpose2021",
		"type": "paper-conference",
		"abstract": "The law and ethics of Western democratic states have their basis in liberalism. This extends to regulation and ethical discussion of technology and businesses doing data processing. Liberalism relies on the privacy and autonomy of individuals, their ordering through a public market, and, more recently, a measure of equality guaranteed by the state. We argue that these forms of regulation and ethical analysis are largely incompatible with the techno-political and techno-economic dimensions of artificial intelligence. By analyzing liberal regulatory solutions in the form of privacy and data protection, regulation of public markets, and fairness in AI, we expose how the data economy and artificial intelligence have transcended liberal legal imagination. Organizations use artificial intelligence to exceed the bounded rationality of individuals and each other. This has led to the private consolidation of markets and an unequal hierarchy of control operating mainly for the purpose of shareholder value. An artificial intelligence will be only as ethical as the purpose of the social system that operates it. Inspired by the science of artificial life as an alternative to artificial intelligence, we consider data intermediaries: sociotechnical systems composed of individuals associated around collectively pursued purposes. An attention cooperative, that prioritizes its incoming and outgoing data flows, is one model of a social system that could form and maintain its own autonomous purpose.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462526",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "3–12",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence and the purpose of social systems",
		"URL": "https://doi.org/10.1145/3461702.3462526",
		"author": [
			{
				"family": "Benthall",
				"given": "Sebastian"
			},
			{
				"family": "Goldenfein",
				"given": "Jake"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chaputMultiagentApproachCombine2021",
		"type": "paper-conference",
		"abstract": "The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462515",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "13–23",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A multi-agent approach to combine reasoning and learning for an ethical behavior",
		"URL": "https://doi.org/10.1145/3461702.3462515",
		"author": [
			{
				"family": "Chaput",
				"given": "Rémy"
			},
			{
				"family": "Duval",
				"given": "Jérémy"
			},
			{
				"family": "Boissier",
				"given": "Olivier"
			},
			{
				"family": "Guillermin",
				"given": "Mathieu"
			},
			{
				"family": "Hassas",
				"given": "Salima"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chenGenderBiasUnderrepresentation2021",
		"type": "paper-conference",
		"abstract": "Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. However, these systems reflect a wide range of biases, from gender bias to a bias in which voices they represent. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. In the process, we also document how our work exposes crucial gaps in the NLP-pipeline for many languages. Despite substantial investments in multilingual support, the modern NLP-pipeline still systematically and dramatically under-represents the majority of human voices in the NLP-guided decisions that are shaping our collective future. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages like Spanish, Arabic, German, French and Urdu that have grammatically gendered nouns including different feminine, masculine and neuter profession words. We compare these gender bias measurements across the Wikipedia corpora in different languages as well as across some corpora of more traditional literature.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462530",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "24–34",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias and under-representation in natural language processing across human languages",
		"URL": "https://doi.org/10.1145/3461702.3462530",
		"author": [
			{
				"family": "Chen",
				"given": "Yan"
			},
			{
				"family": "Mahoney",
				"given": "Christopher"
			},
			{
				"family": "Grasso",
				"given": "Isabella"
			},
			{
				"family": "Wali",
				"given": "Esma"
			},
			{
				"family": "Matthews",
				"given": "Abigail"
			},
			{
				"family": "Middleton",
				"given": "Thomas"
			},
			{
				"family": "Njie",
				"given": "Mariama"
			},
			{
				"family": "Matthews",
				"given": "Jeanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "chohlas-woodBlindJusticeAlgorithmically2021",
		"type": "paper-conference",
		"abstract": "A prosecutor's decision to charge or dismiss a criminal case is a particularly high-stakes choice. There is concern, however, that these judgements may suffer from explicit or implicit racial bias, as with many other such actions in the criminal justice system. To reduce potential bias in charging decisions, we designed a system that algorithmically redacts race-related information from free-text case narratives. In a first-of-its-kind initiative, we deployed this system at a large American district attorney's office to help prosecutors make race-obscured charging decisions, where it was used to review many incoming felony cases. We report on both the design, efficacy, and impact of our tool for aiding equitable decision-making. We demonstrate that our redaction algorithm is able to accurately obscure race-related information, making it difficult for a human reviewer to guess the race of a suspect while preserving other information from the case narrative. In the jurisdiction we study, we found little evidence of disparate treatment in charging decisions even prior to deployment of our intervention. Thus, as expected, our tool did not substantially alter charging rates. Nevertheless, our study demonstrates the feasibility of race-obscured charging, and more generally highlights the promise of algorithms to bolster equitable decision-making in the criminal justice system.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462524",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "35–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Blind justice: Algorithmically masking race in charging decisions",
		"URL": "https://doi.org/10.1145/3461702.3462524",
		"author": [
			{
				"family": "Chohlas-Wood",
				"given": "Alex"
			},
			{
				"family": "Nudell",
				"given": "Joe"
			},
			{
				"family": "Yao",
				"given": "Keniel"
			},
			{
				"family": "Lin",
				"given": "Zhiyuan (Jerry)"
			},
			{
				"family": "Nyarko",
				"given": "Julian"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "cooperEmergentUnfairnessAlgorithmic2021",
		"type": "paper-conference",
		"abstract": "Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462519",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "46–54",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Emergent unfairness in algorithmic fairness-accuracy trade-off research",
		"URL": "https://doi.org/10.1145/3461702.3462519",
		"author": [
			{
				"family": "Cooper",
				"given": "A. Feder"
			},
			{
				"family": "Abrams",
				"given": "Ellen"
			},
			{
				"family": "NA",
				"given": "NA"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "daiFairMachineLearning2021",
		"type": "paper-conference",
		"abstract": "Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462521",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "55–65",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair machine learning under partial compliance",
		"URL": "https://doi.org/10.1145/3461702.3462521",
		"author": [
			{
				"family": "Dai",
				"given": "Jessica"
			},
			{
				"family": "Fazelpour",
				"given": "Sina"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "dianaMinimaxGroupFairness2021",
		"type": "paper-conference",
		"abstract": "We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462523",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "66–76",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Minimax group fairness: Algorithms and experiments",
		"URL": "https://doi.org/10.1145/3461702.3462523",
		"author": [
			{
				"family": "Diana",
				"given": "Emily"
			},
			{
				"family": "Gill",
				"given": "Wesley"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "doniaCodesignEthicalArtificial2021",
		"type": "paper-conference",
		"abstract": "Applications of artificial intelligence / machine learning (AI/ML) are dynamic and rapidly growing, and although multi-purpose, are particularly consequential in health care. One strategy for anticipating and addressing ethical challenges related to AI/ML for health care is co-design - or involvement of end users in design. Co-design has a diverse intellectual and practical history, however, and has been conceptualized in many different ways. Moreover, the unique features of AI/ML introduce challenges to co-design that are often underappreciated. This review summarizes the research literature on involvement in health care and design, and informed by critical data studies, examines the extent to which co-design as commonly conceptualized is capable of addressing the range of normative issues raised by AI/ML for health. We suggest that AI/ML technologies have amplified existing challenges related to co-design, and created entirely new challenges. We outline five co-design 'myths and misconceptions' related to AI/ML for health that form the basis for future research and practice. We conclude by suggesting that the normative strength of a co-design approach to AI/ML for health can be considered at three levels: technological, health care system, and societal. We also suggest research directions for a 'new era' of co-design capable of addressing these challenges.Link to full text: https://bit.ly/3yZrb3y",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462537",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "77",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Co-design and ethical artificial intelligence for health: Myths and misconceptions",
		"URL": "https://doi.org/10.1145/3461702.3462537",
		"author": [
			{
				"family": "Donia",
				"given": "Joseph"
			},
			{
				"family": "Shaw",
				"given": "Jay"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "engelmannBlacklistsRedlistsChinese2021",
		"type": "paper-conference",
		"abstract": "The Chinese Social Credit System (SCS) is a novel digital socio-technical credit system. The SCS aims to regulate societal behavior by reputational and material devices. Scholarship on the SCS has offered a variety of legal and theoretical perspectives. However, little is known about its actual implementation. Here, we provide the first comprehensive empirical study of digital blacklists (listing \"bad\" behavior) and redlists (listing \"good\" behavior) in the Chinese SCS. Based on a unique data set of reputational blacklists and redlists in 30 Chinese provincial-level administrative divisions (ADs), we show the diversity, flexibility, and comprehensiveness of the SCS listing infrastructure. First, our results demonstrate that the Chinese SCS unfolds in a highly diversified manner: we find differences in accessibility, interface design and credit information across provincial-level SCS blacklists and redlists. Second, SCS listings are flexible. During the COVID-19 outbreak, we observe a swift addition of blacklists and redlists that helps strengthen the compliance with coronavirus-related norms and regulations. Third, the SCS listing infrastructure is comprehensive. Overall, we identify 273 blacklists and 154 redlists across provincial-level ADs. Our blacklist and redlist taxonomy highlights that the SCS listing infrastructure prioritizes law enforcement and industry regulations. We also identify redlists that reward political and moral behavior. Our study substantiates the enormous scale and diversity of the Chinese SCS and puts the debate on its reach and societal impact on firmer ground. Finally, we initiate a discussion on the ethical dimensions of data-driven research on the SCS.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462535",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "78–88",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Blacklists and redlists in the chinese social credit system: Diversity, flexibility, and comprehensiveness",
		"URL": "https://doi.org/10.1145/3461702.3462535",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Dang",
				"given": "Lorenz"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fishReflexiveDesignFairness2021",
		"type": "paper-conference",
		"abstract": "Algorithms and other formal models purportedly incorporating human values like fairness have grown increasingly popular in computer science. In response to sociotechnical challenges in the use of these models, designers and researchers have taken widely divergent positions on how formal models incorporating aspects of human values should be used: encouraging their use, moving away from them, or ignoring the normative consequences altogether. In this paper, we seek to resolve these divergent positions by identifying the main conceptual limits of formal modeling, and develop four reflexive values–value fidelity, appropriate accuracy, value legibility, and value contestation–vital for incorporating human values adequately into formal models. We then provide a brief methodology for reflexively designing formal models incorporating human values.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462518",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "89–99",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reflexive design for fairness and other human values in formal models",
		"URL": "https://doi.org/10.1145/3461702.3462518",
		"author": [
			{
				"family": "Fish",
				"given": "Benjamin"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "fogliatoValidityArrestProxy2021",
		"type": "paper-conference",
		"abstract": "Re-offense risk is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision-makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate and less biased reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007–2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462538",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "100–111",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the validity of arrest as a proxy for offense: Race and the likelihood of arrest for violent crimes",
		"URL": "https://doi.org/10.1145/3461702.3462538",
		"author": [
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Nagin",
				"given": "Daniel"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "goodmanHardChoicesHard2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462539",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "112–121",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Hard choices and hard limits in artificial intelligence",
		"URL": "https://doi.org/10.1145/3461702.3462539",
		"author": [
			{
				"family": "Goodman",
				"given": "Bryce"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "guoDetectingEmergentIntersectional2021",
		"type": "paper-conference",
		"abstract": "With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models.Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462536",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "122–133",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases",
		"URL": "https://doi.org/10.1145/3461702.3462536",
		"author": [
			{
				"family": "Guo",
				"given": "Wei"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hopkinsMachineLearningPractices2021a",
		"type": "paper-conference",
		"abstract": "Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community—for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints—tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462527",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "134–145",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning practices outside big tech: How resource constraints challenge responsible development",
		"URL": "https://doi.org/10.1145/3461702.3462527",
		"author": [
			{
				"family": "Hopkins",
				"given": "Aspen"
			},
			{
				"family": "Booth",
				"given": "Serena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehFairnessDataProtection2021",
		"type": "paper-conference",
		"abstract": "In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462528",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "146–153",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and data protection impact assessments",
		"URL": "https://doi.org/10.1145/3461702.3462528",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Clifford",
				"given": "Damian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "keswaniUnbiasedAccurateDeferral2021",
		"type": "paper-conference",
		"abstract": "Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on \"deferral systems\" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462516",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "154–165",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards unbiased and accurate deferral to multiple experts",
		"URL": "https://doi.org/10.1145/3461702.3462516",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Lease",
				"given": "Matthew"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liAlgorithmicHiringPractice2021",
		"type": "paper-conference",
		"abstract": "The use of AI-enabled hiring software raises questions about the practice of Human Resource (HR) professionals' use of the software and its consequences. We interviewed 15 recruiters and HR professionals about their experiences around two decision-making processes during hiring: sourcing and assessment. For both, AI-enabled software allowed the efficient processing of candidate data, thus providing the ability to introduce or advance candidates from broader and more diverse pools. For sourcing, it can serve as a useful learning resource to find candidates. Though, a lack of trust in data accuracy and an inadequate level of control over algorithmic candidate matches can create reluctance to embrace it. For assessment, its implementation varied across companies depending on the industry and the hiring scenario. Its inclusion may redefine HR professionals' job content as it automates or augments pieces of the existing hiring process. Finally, we discuss how candidate roles that recruiters and HR professionals support drive the use of algorithmic hiring software.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462531",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "166–176",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic hiring in practice: Recruiter and HR professional's perspectives on AI use in hiring",
		"URL": "https://doi.org/10.1145/3461702.3462531",
		"author": [
			{
				"family": "Li",
				"given": "Lan"
			},
			{
				"family": "Lassiter",
				"given": "Tina"
			},
			{
				"family": "Oh",
				"given": "Joohee"
			},
			{
				"family": "Lee",
				"given": "Min Kyung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mohammadiScalingGuaranteesNearest2021",
		"type": "paper-conference",
		"abstract": "Counterfactual explanations (CFE) are being widely used to explain algorithmic decisions, especially in consequential decision-making contexts (e.g., loan approval or pretrial bail). In this context, CFEs aim to provide individuals affected by an algorithmic decision with the most similar individual (i.e., nearest individual) with a different outcome. However, while an increasing number of works propose algorithms to compute CFEs, such approaches either lack in optimality of distance (i.e., they do not return the nearest individual) and perfect coverage (i.e., they do not provide a CFE for all individuals); or they do not scale to complex models such as neural networks. In this work, we provide a framework based on Mixed-Integer Programming (MIP) to compute nearest counterfactual explanations for the outcomes of neural networks, with both provable guarantees and runtimes comparable to gradient-based approaches. Our experiments on the Adult, COMPAS, and Credit datasets show that, in contrast with previous methods, our approach allows for efficiently computing diverse CFEs with both distance guarantees and perfect coverage.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462514",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "177–187",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Scaling guarantees for nearest counterfactual explanations",
		"URL": "https://doi.org/10.1145/3461702.3462514",
		"author": [
			{
				"family": "Mohammadi",
				"given": "Kiarash"
			},
			{
				"family": "Karimi",
				"given": "Amir-Hossein"
			},
			{
				"family": "Barthe",
				"given": "Gilles"
			},
			{
				"family": "Valera",
				"given": "Isabel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nashedEthicallyCompliantPlanning2021",
		"type": "paper-conference",
		"abstract": "Ethically compliant autonomous systems (ECAS) are the state-of-the-art for solving sequential decision-making problems under uncertainty while respecting constraints that encode ethical considerations. This paper defines a novel concept in the context of ECAS that is from moral philosophy, the moral community, which leads to a nuanced taxonomy of explicit ethical agents. We then propose new ethical frameworks that extend the applicability of ECAS to domains where a moral community is required. Next, we provide a formal analysis of the proposed ethical frameworks and conduct experiments that illustrate their differences. Finally, we discuss the implications of explicit moral communities that could shape research on standards and guidelines for ethical agents in order to better understand and predict common errors in their design and communicate their capabilities.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462522",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "188–198",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethically compliant planning within moral communities",
		"URL": "https://doi.org/10.1145/3461702.3462522",
		"author": [
			{
				"family": "Nashed",
				"given": "Samer"
			},
			{
				"family": "Svegliato",
				"given": "Justin"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nokhizPrecarityModelingLong2021",
		"type": "paper-conference",
		"abstract": "When it comes to studying the impacts of decision making, the research has been largely focused on examining the fairness of the decisions, the long-term effects of the decision pipelines, and utility-based perspectives considering both the decision-maker and the individuals. However, there has hardly been any focus on precarity which is the term that encapsulates the instability in people's lives. That is, a negative outcome can overspread to other decisions and measures of well-being. Studying precarity necessitates a shift in focus – from the point of view of the decision-maker to the perspective of the decision subject. This centering of the subject is an important direction that unlocks the importance of parting with aggregate measures to examine the long-term effects of decision making. To address this issue, in this paper, we propose a modeling framework that simulates the effects of compounded decision-making on precarity over time. Through our simulations, we are able to show the heterogeneity of precarity by the non-uniform ruinous aftereffects of negative decisions on different income classes of the underlying population and how policy interventions can help mitigate such effects.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462529",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "199–208",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Precarity: Modeling the long term effects of compounded decisions on individual instability",
		"URL": "https://doi.org/10.1145/3461702.3462529",
		"author": [
			{
				"family": "Nokhiz",
				"given": "Pegah"
			},
			{
				"family": "Ruwanpathirana",
				"given": "Aravinda Kanchana"
			},
			{
				"family": "Patwari",
				"given": "Neal"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "robinsonMoralDisagreementArtificial2021",
		"type": "paper-conference",
		"abstract": "Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462534",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "209",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Moral disagreement and artificial intelligence",
		"URL": "https://doi.org/10.1145/3461702.3462534",
		"author": [
			{
				"family": "Robinson",
				"given": "Pamela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shekharFairODFairnessawareOutlier2021",
		"type": "paper-conference",
		"abstract": "Fairness and Outlier Detection (OD) are closely related, as it is exactly the goal of OD to spot rare, minority samples in a given population. However, when being a minority (as defined by protected variables, such as race/ethnicity/sex/age) does not reflect positive-class membership (such as criminal/fraud), OD produces unjust outcomes. Surprisingly, fairness-aware OD has been almost untouched in prior work, as fair machine learning literature mainly focuses on supervised settings. Our work aims to bridge this gap. Specifically, we develop desiderata capturing well-motivated fairness criteria for OD, and systematically formalize the fair OD problem. Further, guided by our desiderata, we propose FairOD, a fairness-aware outlier detector that has the following desirable properties: FairOD (1) exhibits treatment parity at test time, (2) aims to flag equal proportions of samples from all groups (i.e. obtain group fairness, via statistical parity), and (3) strives to flag truly high-risk samples within each group. Extensive experiments on a diverse set of synthetic and real world datasets show that FairOD produces outcomes that are fair with respect to protected variables, while performing comparable to (and in some cases, even better than) fairness-agnostic detectors in terms of detection performance.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462517",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "210–220",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairOD: Fairness-aware outlier detection",
		"URL": "https://doi.org/10.1145/3461702.3462517",
		"author": [
			{
				"family": "Shekhar",
				"given": "Shubhranshu"
			},
			{
				"family": "Shah",
				"given": "Neil"
			},
			{
				"family": "Akoglu",
				"given": "Leman"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shengSurveillingSurveillanceEstimating2021",
		"type": "paper-conference",
		"abstract": "The use of video surveillance in public spaces–both by government agencies and by private citizens–has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here we present a novel approach for estimating the spatial distribution of surveillance cameras: applying computer vision algorithms to large-scale street view image data. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents—a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462525",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "221–230",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Surveilling surveillance: Estimating the prevalence of surveillance cameras with street view data",
		"URL": "https://doi.org/10.1145/3461702.3462525",
		"author": [
			{
				"family": "Sheng",
				"given": "Hao"
			},
			{
				"family": "Yao",
				"given": "Keniel"
			},
			{
				"family": "Goel",
				"given": "Sharad"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shokriPrivacyRisksModel2021",
		"type": "paper-conference",
		"abstract": "Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462533",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "231–241",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the privacy risks of model explanations",
		"URL": "https://doi.org/10.1145/3461702.3462533",
		"author": [
			{
				"family": "Shokri",
				"given": "Reza"
			},
			{
				"family": "Strobel",
				"given": "Martin"
			},
			{
				"family": "Zick",
				"given": "Yair"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "susserMeasuringAutomatedInfluence2021",
		"type": "paper-conference",
		"abstract": "Automated influence, delivered by digital targeting technologies such as targeted advertising, digital nudges, and recommender systems, has attracted significant interest from both empirical researchers, on one hand, and critical scholars and policymakers on the other. In this paper, we argue for closer integration of these efforts. Critical scholars and policymakers, who focus primarily on the social, ethical, and political effects of these technologies, need empirical evidence to substantiate and motivate their concerns. However, existing empirical research investigating the effectiveness of these technologies (or lack thereof), neglects other morally relevant effects-which can be felt regardless of whether or not the technologies \"work\" in the sense of fulfilling the promises of their designers. Drawing from the ethics and policy literature, we enumerate a range of questions begging for empirical analysis-the outline of a research agenda bridging these fields—and issue a call to action for more empirical research that takes these urgent ethics and policy questions as their starting point.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462532",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "242–253",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring automated influence: Between empirical evidence and ethical values",
		"URL": "https://doi.org/10.1145/3461702.3462532",
		"author": [
			{
				"family": "Susser",
				"given": "Daniel"
			},
			{
				"family": "Grimaldi",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "tomasevFairnessUnobservedCharacteristics2021",
		"type": "paper-conference",
		"abstract": "Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness—frequently, race and legal gender—can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462540",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "254–265",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness for unobserved characteristics: Insights from technological impacts on queer communities",
		"URL": "https://doi.org/10.1145/3461702.3462540",
		"author": [
			{
				"family": "Tomasev",
				"given": "Nenad"
			},
			{
				"family": "McKee",
				"given": "Kevin R."
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "vredenburghAlienationAIDrivenWorkplace2021",
		"type": "paper-conference",
		"abstract": "This paper asks whether explanations of one's workplace and economic institutions are valuable in and of themselves. In doing so, it departs from much of the explainability literature in law, computer science, philosophy, and the social sciences, which examine the instrumental values that explainable AI has: explainable systems increase accountability and user trust, or reduce the risk of harm due to increased robustness. Think, however, of how you might feel if you went to your local administrative agency to apply for some benefit, or you were handed down a decision by a judge in a court. Let's stipulate that you know that the decision was just, even though neither the civil servant nor the judge explain to you why the decision was made, and you don't know the relevant rules; you just brought all the information you had about yourself, and hoped for the best. Is such a decision process defective? I argue that such a decision process is defective because it prevents individuals from accessing the normative explanations that are necessary to form an appropriate practical orientation towards their social world. A practical orientation is a reflective stance towards one's social world, which is expressed in one's actions and draws on one's cognitive architecture that allows one to navigate the various social practices and institutions. A practical orientation can range from rejection to silent endorsement, and is the sort of attitude for which there are the right kind of reasons, based in the world's normative character. It also determines how one fills out one's role obligations, and, more broadly, guides one's actions in the relevant institution: a teacher in the American South during the time of enforced racial segregation, for example, might choose where to teach on the basis of her rejection of the segregation of education. To form an appropriate practical orientation, one must have an understanding of the social world's normative character, which required a normative explanation And, since we spend so much of our lives at work and are constrained by economic institutions, we must understand their structure and how they function.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462520",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "266",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Alienation in the AI-Driven workplace",
		"URL": "https://doi.org/10.1145/3461702.3462520",
		"author": [
			{
				"family": "Vredenburgh",
				"given": "Kate"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abdallaGreyHoodieProject2021",
		"type": "paper-conference",
		"abstract": "As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462563",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "287–297",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The grey hoodie project: Big tobacco, big tech, and the threat on academic integrity",
		"URL": "https://doi.org/10.1145/3461702.3462563",
		"author": [
			{
				"family": "Abdalla",
				"given": "Mohamed"
			},
			{
				"family": "Abdalla",
				"given": "Moustafa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "abidPersistentAntimuslimBias2021",
		"type": "paper-conference",
		"abstract": "It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462624",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "298–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Persistent anti-muslim bias in large language models",
		"URL": "https://doi.org/10.1145/3461702.3462624",
		"author": [
			{
				"family": "Abid",
				"given": "Abubakar"
			},
			{
				"family": "Farooqi",
				"given": "Maheen"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "acunaAreAIEthics2021",
		"type": "paper-conference",
		"abstract": "Even though computer science (CS) has had a historical lack of gender and race representation, its AI research affects everybody eventually. Being partially rooted in CS conferences, \"AI ethics\" (AIE) conferences such as FAccT and AIES have quickly become distinct venues where AI's societal implications are discussed and solutions proposed. However, it is largely unknown if these conferences improve upon the historical representational issues of traditional CS venues. In this work, we explore AIE conferences' evolution and compare them across demographic characteristics, publication content, and citation patterns. We find that AIE conferences have increased their internal topical diversity and impact on other CS conferences. Importantly, AIE conferences are highly differentiable, covering topics not represented in other venues. However, and perhaps contrary to the field's aspirations, white authors are more common while seniority and black researchers are represented similarly to CS venues. Our results suggest that AIE conferences could increase efforts to attract more diverse authors, especially considering their sizable roots in CS.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462616",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "307–315",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Are AI ethics conferences different and more diverse compared to traditional computer science conferences?",
		"URL": "https://doi.org/10.1145/3461702.3462616",
		"author": [
			{
				"family": "Acuna",
				"given": "Daniel E."
			},
			{
				"family": "Liang",
				"given": "Lizhen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "afnanEthicalImplementationArtificial2021",
		"type": "paper-conference",
		"abstract": "AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462589",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "316–326",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical implementation of artificial intelligence to select embryos in in vitro fertilization",
		"URL": "https://doi.org/10.1145/3461702.3462589",
		"author": [
			{
				"family": "Afnan",
				"given": "Michael Anis Mihdi"
			},
			{
				"family": "Rudin",
				"given": "Cynthia"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Savulescu",
				"given": "Julian"
			},
			{
				"family": "Mishra",
				"given": "Abhishek"
			},
			{
				"family": "Liu",
				"given": "Yanhe"
			},
			{
				"family": "Afnan",
				"given": "Masoud"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "akaMeasuringModelBiases2021",
		"type": "paper-conference",
		"abstract": "The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice.We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a \"bag of words\", we rank the biases that a model has learned with respect to different identity labels. We use man, woman as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most \"gender biased\" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462557",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "327–335",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring model biases in the absence of ground truth",
		"URL": "https://doi.org/10.1145/3461702.3462557",
		"author": [
			{
				"family": "Aka",
				"given": "Osman"
			},
			{
				"family": "Burke",
				"given": "Ken"
			},
			{
				"family": "Bauerle",
				"given": "Alex"
			},
			{
				"family": "Greer",
				"given": "Christina"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "aliAccountingModelUncertainty2021",
		"type": "paper-conference",
		"abstract": "Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize \"total\" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462630",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "336–345",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Accounting for model uncertainty in algorithmic discrimination",
		"URL": "https://doi.org/10.1145/3461702.3462630",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Lahoti",
				"given": "Preethi"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bakkerReasonableDoubtImproving2021",
		"type": "paper-conference",
		"abstract": "Prior work on fairness in machine learning has focused on settings where all the information needed about each individual is readily available. However, in many applications, further information may be acquired at a cost. For example, when assessing a customer's creditworthiness, a bank initially has access to a limited set of information but progressively improves the assessment by acquiring additional information before making a final decision. In such settings, we posit that a fair decision maker may want to ensure that decisions for all individuals are made with similar expected error rate, even if the features acquired for the individuals are different. We show that a set of carefully chosen confidence thresholds can not only effectively redistribute an information budget according to each individual's needs, but also serve to address individual and group fairness concerns simultaneously. Finally, using two public datasets, we confirm the effectiveness of our methods and investigate the limitations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462575",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "346–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond reasonable doubt: Improving fairness in budget-constrained decision making using confidence thresholds",
		"URL": "https://doi.org/10.1145/3461702.3462575",
		"author": [
			{
				"family": "Bakker",
				"given": "Michiel A."
			},
			{
				"family": "Tu",
				"given": "Duy Patrick"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			},
			{
				"family": "Pentland",
				"given": "Alex Sandy"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barlasPersonHumanNeither2021",
		"type": "paper-conference",
		"abstract": "Following the literature on dehumanization via technology, we audit six proprietary image tagging algorithms (ITAs) for their potential to perpetuate dehumanization. We examine the ITAs' outputs on a controlled dataset of images depicting a diverse group of people for tags that indicate the presence of a human in the image. Through an analysis of the (mis)use of these tags, we find that there are some individuals whose 'humanness' is not recognized by an ITA, and that these individuals are often from marginalized social groups. Finally, we compare these findings with the use of the 'face' tag, which can be used for surveillance, revealing that people's faces are often recognized by an ITA even when their 'humanness' is not. Overall, we highlight the subtle ways in which ITAs may inflict widespread, disparate harm, and emphasize the importance of considering the social context of the resulting application.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462567",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "357–367",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Person, human, neither: The dehumanization potential of automated image tagging",
		"URL": "https://doi.org/10.1145/3461702.3462567",
		"author": [
			{
				"family": "Barlas",
				"given": "Pınar"
			},
			{
				"family": "Kyriakou",
				"given": "Kyriakos"
			},
			{
				"family": "Kleanthous",
				"given": "Styliani"
			},
			{
				"family": "Otterbacher",
				"given": "Jahna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "barocasDesigningDisaggregatedEvaluations2021",
		"type": "paper-conference",
		"abstract": "Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts—both beneficial and harmful—that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462610",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "368–378",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing disaggregated evaluations of AI systems: Choices, considerations, and tradeoffs",
		"URL": "https://doi.org/10.1145/3461702.3462610",
		"author": [
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Guo",
				"given": "Anhong"
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Krones",
				"given": "Jacquelyn"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Vaughan",
				"given": "Jennifer Wortman"
			},
			{
				"family": "Wadsworth",
				"given": "W. Duncan"
			},
			{
				"family": "Wallach",
				"given": "Hanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "belitzAutomatingProcedurallyFair2021",
		"type": "paper-conference",
		"abstract": "In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462585",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "379–389",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Automating procedurally fair feature selection in machine learning",
		"URL": "https://doi.org/10.1145/3461702.3462585",
		"author": [
			{
				"family": "Belitz",
				"given": "Clara"
			},
			{
				"family": "Jiang",
				"given": "Lan"
			},
			{
				"family": "Bosch",
				"given": "Nigel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bendavidExplainableAIAdoption2021",
		"type": "paper-conference",
		"abstract": "We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with \"No-explanation\" alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462565",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "390–400",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainable AI and adoption of financial algorithmic advisors: An experimental study",
		"URL": "https://doi.org/10.1145/3461702.3462565",
		"author": [
			{
				"family": "Ben David",
				"given": "Daniel"
			},
			{
				"family": "Resheff",
				"given": "Yehezkel S."
			},
			{
				"family": "Tron",
				"given": "Talia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "bhattUncertaintyFormTransparency2021",
		"type": "paper-conference",
		"abstract": "Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462571",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, USA",
		"page": "401–413",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty",
		"URL": "https://doi.org/10.1145/3461702.3462571",
		"author": [
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Antorán",
				"given": "Javier"
			},
			{
				"family": "Zhang",
				"given": "Yunfeng"
			},
			{
				"family": "Liao",
				"given": "Q. Vera"
			},
			{
				"family": "Sattigeri",
				"given": "Prasanna"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Melançon",
				"given": "Gabrielle"
			},
			{
				"family": "Krishnan",
				"given": "Ranganath"
			},
			{
				"family": "Stanley",
				"given": "Jason"
			},
			{
				"family": "Tickoo",
				"given": "Omesh"
			},
			{
				"family": "Nachman",
				"given": "Lama"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			},
			{
				"family": "Srikumar",
				"given": "Madhulika"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "biswasEnsuringFairnessPrior2021",
		"type": "paper-conference",
		"abstract": "Prior probability shift is a phenomenon where the training and test datasets differ structurally within population subgroups. This phenomenon can be observed in the yearly records of several real-world datasets, for example, recidivism records and medical expenditure surveys. If unaccounted for, such shifts can cause the predictions of a classifier to become unfair towards specific population subgroups. While the fairness notion called Proportional Equality (PE) accounts for such shifts, a procedure to ensure PE-fairness was unknown. In this work, we design an algorithm, called CAPE, that ensures fair classification under such shifts. We introduce a metric, called prevalence difference, which CAPE attempts to minimize in order to achieve fairness under prior probability shifts. We theoretically establish that this metric exhibits several properties that are desirable for a fair classifier. We evaluate the efficacy of CAPE via a thorough empirical evaluation on synthetic datasets. We also compare the performance of CAPE with several state-of-the-art fair classifiers on real-world datasets like COMPAS (criminal risk assessment) and MEPS (medical expenditure panel survey). The results indicate that CAPE ensures a high degree of PE-fairness in its predictions, while performing well on other important metrics.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462596",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "414–424",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ensuring fairness under prior probability shifts",
		"URL": "https://doi.org/10.1145/3461702.3462596",
		"author": [
			{
				"family": "Biswas",
				"given": "Arpita"
			},
			{
				"family": "Mukherjee",
				"given": "Suvam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462612",
		"type": "paper-conference",
		"abstract": "Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be \"for\" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462612",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "425–436",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Envisioning communities: A participatory approach towards AI for social good",
		"URL": "https://doi.org/10.1145/3461702.3462612",
		"author": [
			{
				"family": "Bondi",
				"given": "Elizabeth"
			},
			{
				"family": "Xu",
				"given": "Lily"
			},
			{
				"family": "Acosta-Navas",
				"given": "Diana"
			},
			{
				"family": "Killian",
				"given": "Jackson A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462570",
		"type": "paper-conference",
		"abstract": "According to a prominent approach to AI alignment, AI agents should be built to learn and promote human values. However, humans value things in several different ways: we have desires and preferences of various kinds, and if we engage in reinforcement learning, we also have reward functions. One research project to which this approach gives rise is therefore to say which of these various classes of human values should be promoted. This paper takes on part of this project by assessing the proposal that human reward functions should be the target for AI alignment. There is some reason to believe that powerful AI agents which were aligned to values of this form would help us to lead good lives, but there is also considerable uncertainty about this claim, arising from unresolved empirical and conceptual issues in human psychology.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462570",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "437–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI alignment and human reward",
		"URL": "https://doi.org/10.1145/3461702.3462570",
		"author": [
			{
				"family": "Butlin",
				"given": "Patrick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462577",
		"type": "paper-conference",
		"abstract": "Prediction-based decisions, which are often made by utilizing the tools of machine learning, influence nearly all facets of modern life. Ethical concerns about this widespread practice have given rise to the field of fair machine learning and a number of fairness measures, mathematically precise definitions of fairness that purport to determine whether a given prediction-based decision system is fair. Following Reuben Binns (2017), we take \"fairness\" in this context to be a placeholder for a variety of normative egalitarian considerations. We explore a few fairness measures to suss out their egalitarian roots and evaluate them, both as formalizations of egalitarian ideas and as assertions of what fairness demands of predictive systems. We pay special attention to a recent and popular fairness measure, counterfactual fairness, which holds that a prediction about an individual is fair if it is the same in the actual world and any counterfactual world where the individual belongs to a different demographic group (cf. Kusner et al. 2018).",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462577",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "446",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness and machine fairness",
		"URL": "https://doi.org/10.1145/3461702.3462577",
		"author": [
			{
				"family": "Castro",
				"given": "Clinton"
			},
			{
				"family": "O'Brien",
				"given": "David"
			},
			{
				"family": "Schwan",
				"given": "Ben"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462622",
		"type": "paper-conference",
		"abstract": "Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce.We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the \"managerialization of diversity\" by corporations in the mid-1980s. The focus on technical artifacts - such as diverse and inclusive datasets - and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and relevant subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values and new solutions. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as \"ethics owners\" but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462622",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "447–457",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reconfiguring diversity and inclusion for AI ethics",
		"URL": "https://doi.org/10.1145/3461702.3462622",
		"author": [
			{
				"family": "Chi",
				"given": "Nicole"
			},
			{
				"family": "Lurie",
				"given": "Emma"
			},
			{
				"family": "Mulligan",
				"given": "Deirdre K."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462569",
		"type": "paper-conference",
		"abstract": "We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462569",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "458–468",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic audit of italian car insurance: Evidence of unfairness in access and pricing",
		"URL": "https://doi.org/10.1145/3461702.3462569",
		"author": [
			{
				"family": "Fabris",
				"given": "Alessandro"
			},
			{
				"family": "Mishler",
				"given": "Alan"
			},
			{
				"family": "Gottardi",
				"given": "Stefano"
			},
			{
				"family": "Carletti",
				"given": "Mattia"
			},
			{
				"family": "Daicampi",
				"given": "Matteo"
			},
			{
				"family": "Susto",
				"given": "Gian Antonio"
			},
			{
				"family": "Silvello",
				"given": "Gianmaria"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462573",
		"type": "paper-conference",
		"abstract": "With artificial intelligence continuing to advance, so too do the ethical concerns that can potentially negatively impact humans and the greater society. When these systems begin to interact with humans, these concerns become much more complex and much more important. The field of human-AI teaming provides a relevant example of how AI ethics can have significant and continued effects on humans. This paper reviews research in ethical artificial intelligence, as well as ethical teamwork through the lens of the rapidly advancing field of human-AI teaming, resulting in a model demonstrating the requirements and outcomes of building ethical human-AI teams. The model is created to guide the prioritization of ethics in human-AI teaming by outlining the ethical teaming process, outcomes of ethical teams, and external requirements necessary to ensure ethical human-AI teams. A final discussion is presented on how the developed model will influence the implementation of AI teammates, as well as the development of policy and regulation surrounding the domain in the coming years.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462573",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "469–479",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Modeling and guiding the creation of ethical human-AI teams",
		"URL": "https://doi.org/10.1145/3461702.3462573",
		"author": [
			{
				"family": "Flathmann",
				"given": "Christopher"
			},
			{
				"family": "Schelble",
				"given": "Beau G."
			},
			{
				"family": "Zhang",
				"given": "Rui"
			},
			{
				"family": "McNeese",
				"given": "Nathan J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462621",
		"type": "paper-conference",
		"abstract": "One of the main lines of research in algorithmic fairness involves individual fairness (IF) methods. Individual fairness is motivated by an intuitive principle, similar treatment, which requires that similar individuals be treated similarly. IF offers a precise account of this principle using distance metrics to evaluate the similarity of individuals. Proponents of individual fairness have argued that it gives the correct definition of algorithmic fairness, and that it should therefore be preferred to other methods for determining fairness. I argue that individual fairness cannot serve as a definition of fairness. Moreover, IF methods should not be given priority over other fairness methods, nor used in isolation from them. To support these conclusions, I describe four in-principle problems for individual fairness as a definition and as a method for ensuring fairness: (1) counterexamples show that similar treatment (and therefore IF) are insufficient to guarantee fairness; (2) IF methods for learning similarity metrics are at risk of encoding human implicit bias; (3) IF requires prior moral judgments, limiting its usefulness as a guide for fairness and undermining its claim to define fairness; and (4) the incommensurability of relevant moral values makes similarity metrics impossible for many tasks. In light of these limitations, I suggest that individual fairness cannot be a definition of fairness, and instead should be seen as one tool among several for ameliorating algorithmic bias.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462621",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "480–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What's fair about individual fairness?",
		"URL": "https://doi.org/10.1145/3461702.3462621",
		"author": [
			{
				"family": "Fleisher",
				"given": "Will"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462558",
		"type": "paper-conference",
		"abstract": "Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement. Clustering with proxies may lead to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462558",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "491–501",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning to generate fair clusters from demonstrations",
		"URL": "https://doi.org/10.1145/3461702.3462558",
		"author": [
			{
				"family": "Galhotra",
				"given": "Sainyam"
			},
			{
				"family": "Saisubramanian",
				"given": "Sandhya"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462555",
		"type": "paper-conference",
		"abstract": "TikTok is a popular platform that enables users to see tailored content feeds, particularly short videos with novel content. In recent years, TikTok has been criticized at times for presenting users with overly homogenous feeds, thereby reducing the diversity of content with which each user engages. In this paper, we consider whether TikTok has an ethical obligation to employ a novelty bias in its content recommendation engine. We explicate the principal morally relevant values and interests of key stakeholders, and observe that key empirical questions must be answered before a precise recommendation can be provided. We argue that TikTok's own values and interests mean that its actions should be largely driven by the values and interests of its users and creators. Unlike some other content platforms, TikTok's ethical obligations are not at odds with the values of its users, and so whether it is obligated to include a novelty bias depends on what will actually advance its users' interests.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462555",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "502–508",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical obligations to provide novelty",
		"URL": "https://doi.org/10.1145/3461702.3462555",
		"author": [
			{
				"family": "Golden",
				"given": "Paige"
			},
			{
				"family": "Danks",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "10.1145/3461702.3462607",
		"type": "paper-conference",
		"abstract": "There has been increasing acceptance that agents must act in a way that is sensitive to ethical considerations. These considerations have been cashed out as constraints, such that some actions are permissible, while others are impermissible. In this paper, we claim that, in addition to only performing those actions that are permissible, agents should only perform those courses of action that are <sub>u</sub>nambiguouslyₚermissible. By doing so they signal normative compliance: they communicate their understanding of, and commitment to abiding by, the normative constraints in play. Those courses of action (or plans) that succeed in signalling compliance in this sense, we term 'acceptable'. The problem this paper addresses is how to compute plans that signal compliance, that is, how to find plans that are acceptable as well as permissible. We do this by identifying those plans such that, were an observer to see only part of its execution, that observer would infer the plan enacted was permissible. This paper provides a formal definition of compliance signalling within the domain of AI planning, describes an algorithm for computing compliance signalling plans, provides preliminary experimental results and discusses possible improvements. The signalling of compliance is vital for communication, coordination and cooperation in situations where the agent is partially observed. It is equally vital, therefore, to solve the computational problem of finding those plans that signal compliance. This is what this paper does.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462607",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "509–518",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computing plans that signal normative compliance",
		"URL": "https://doi.org/10.1145/3461702.3462607",
		"author": [
			{
				"family": "Grastien",
				"given": "Alban"
			},
			{
				"family": "Benn",
				"given": "Claire"
			},
			{
				"family": "Thiébaux",
				"given": "Sylvie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "greenAIEthicsCourse2021",
		"type": "paper-conference",
		"abstract": "This is an experience report describing a pilot AI Ethics course for undergraduate computer science majors. In addition to teaching students about different ethical approaches and using them to analyze ethical issues, the course covered how ethics has been incorporated into the implementation of explicit ethical agents, and required students to implement an explicit ethical agent for a simple application. This report describes the course objectives and design, the topics covered, and a qualitative evaluation with suggestions for future offerings of the courses.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462552",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 6\npublisher-place: Virtual Event, USA",
		"page": "519–524",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An AI ethics course highlighting explicit ethical agents",
		"URL": "https://doi.org/10.1145/3461702.3462552",
		"author": [
			{
				"family": "Green",
				"given": "Nancy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "grzelakDangersDrowsinessDetection2021",
		"type": "paper-conference",
		"abstract": "Drowsiness and fatigue are important factors in driving safety and work performance. This has motivated academic research into detecting drowsiness, and sparked interest in the deployment of related products in the insurance and work-productivity sectors. In this paper we elaborate on the potential dangers of using such algorithms. We first report on an audit of performance bias across subject gender and ethnicity, identifying which groups would be disparately harmed by the deployment of a state-of-the-art drowsiness detection algorithm. We discuss some of the sources of the bias, such as the lack of robustness of facial analysis algorithms to face occlusions, facial hair, or skin tone. We then identify potential downstream harms of this performance bias, as well as potential misuses of drowsiness detection technology—focusing on driving safety and experience, insurance cream-skimming and coverage-avoidance, worker surveillance, and job precarity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462593",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "525–531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The dangers of drowsiness detection: Differential performance, downstream impact, and misuses",
		"URL": "https://doi.org/10.1145/3461702.3462593",
		"author": [
			{
				"family": "Grzelak",
				"given": "Jakub"
			},
			{
				"family": "Brandao",
				"given": "Martim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "guidottiDesigningShapeletsInterpretable2021",
		"type": "paper-conference",
		"abstract": "Time series shapelets are discriminatory subsequences which are representative of a class, and their similarity to a time series can be used for successfully tackling the time series classification problem. The literature shows that Artificial Intelligence (AI) systems adopting classification models based on time series shapelets can be interpretable, more accurate, and significantly fast. Thus, in order to design a data-agnostic and interpretable classification approach, in this paper we first extend the notion of shapelets to different types of data, i.e., images, tabular and textual data. Then, based on this extended notion of shapelets we propose an interpretable data-agnostic classification method. Since the shapelets discovery can be time consuming, especially for data types more complex than time series, we exploit a notion of prototypes for finding candidate shapelets, and reducing both the time required to find a solution and the variance of shapelets. A wide experimentation on datasets of different types shows that the data-agnostic prototype-based shapelets returned by the proposed method empower an interpretable classification which is also fast, accurate, and stable. In addition, we show and we prove that shapelets can be at the basis of explainable AI methods.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462553",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "532–542",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Designing shapelets for interpretable data-agnostic classification",
		"URL": "https://doi.org/10.1145/3461702.3462553",
		"author": [
			{
				"family": "Guidotti",
				"given": "Riccardo"
			},
			{
				"family": "Monreale",
				"given": "Anna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hanleyComputerVisionConflicting2021",
		"type": "paper-conference",
		"abstract": "Scholars have recently drawn attention to a range of controversial issues posed by the use of computer vision for automatically generating descriptions of people in images. Despite these concerns, automated image description has become an important tool to ensure equitable access to information for blind and low vision people. In this paper, we investigate the ethical dilemmas faced by companies that have adopted the use of computer vision for producing alt text: textual descriptions of images for blind and low vision people. We use Facebook's automatic alt text tool as our primary case study. First, we analyze the policies that Facebook has adopted with respect to identity categories, such as race, gender, age, etc., and the company's decisions about whether to present these terms in alt text. We then describe an alternative—and manual—approach practiced in the museum community, focusing on how museums determine what to include in alt text descriptions of cultural artifacts. We compare these policies, using notable points of contrast to develop an analytic framework that characterizes the particular apprehensions behind these policy choices. We conclude by considering two strategies that seem to sidestep some of these concerns, finding that there are no easy ways to avoid the normative dilemmas posed by the use of computer vision to automate alt text.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462620",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "543–554",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Computer vision and conflicting values: Describing people with automated alt text",
		"URL": "https://doi.org/10.1145/3461702.3462620",
		"author": [
			{
				"family": "Hanley",
				"given": "Margot"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Levy",
				"given": "Karen"
			},
			{
				"family": "Azenkot",
				"given": "Shiri"
			},
			{
				"family": "Nissenbaum",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "hannanWhoGetsWhat2021",
		"type": "paper-conference",
		"abstract": "Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the \"Who,\" \"What,\" and \"How\" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the \"Who\" and \"What,\" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462568",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "555–565",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who gets what, according to whom? An analysis of fairness perceptions in service allocation",
		"URL": "https://doi.org/10.1145/3461702.3462568",
		"author": [
			{
				"family": "Hannan",
				"given": "Jacqueline"
			},
			{
				"family": "Chen",
				"given": "Huei-Yen Winnie"
			},
			{
				"family": "Joseph",
				"given": "Kenneth"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "heidenreichEarthFlatSun2021",
		"type": "paper-conference",
		"abstract": "This work considers universal adversarial triggers, a method of adversarially disrupting natural language models, and questions if it is possible to use such triggers to affect both the topic and stance of conditional text generation models. In considering four \"controversial\" topics, this work demonstrates success at identifying triggers that cause the GPT-2 model to produce text about targeted topics as well as influence the stance the text takes towards the topic. We show that, while the more fringe topics are more challenging to identify triggers for, they do appear to more effectively discriminate aspects like stance. We view this both as an indication of the dangerous potential for controllability and, perhaps, a reflection of the nature of the disconnect between conflicting views on these topics, something that future work could use to question the nature of filter bubbles and if they are reflected within models trained on internet content. In demonstrating the feasibility and ease of such an attack, this work seeks to raise the awareness that neural language models are susceptible to this influence–even if the model is already deployed and adversaries lack internal model access–and advocates the immediate safeguarding against this type of adversarial attack in order to prevent potential harm to human users.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462578",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "566–573",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The earth is flat and the sun is not a star: The susceptibility of GPT-2 to universal adversarial triggers",
		"URL": "https://doi.org/10.1145/3461702.3462578",
		"author": [
			{
				"family": "Heidenreich",
				"given": "Hunter Scott"
			},
			{
				"family": "Williams",
				"given": "Jake Ryland"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "henriksenSituatedAccountabilityEthical2021a",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has the potential to benefit humans and society by its employment in important sectors. However, the risks of negative consequences have underscored the importance of accountability for AI systems, their outcomes, and the users of such systems. In recent years, various accountability mechanisms have been put forward in pursuit of the responsible design, development, and use of AI. In this article, we provide an in-depth study of three such mechanisms, as we analyze Scandinavian AI developers' encounter with (1) ethical principles, (2) certification standards, and (3) explanation methods. By doing so, we contribute to closing a gap in the literature between discussions of accountability on the research and policy level, and accountability as a responsibility put on the shoulders of developers in practice. Our study illustrates important flaws in the current enactment of accountability as an ethical and social value which, if left unchecked, risks undermining the pursuit of responsible AI. By bringing attention to these flaws, the article signals where further work is needed in order to build effective accountability systems for AI.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462564",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "574–585",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Situated accountability: Ethical principles, certification standards, and explanation methods in applied AI",
		"URL": "https://doi.org/10.1145/3461702.3462564",
		"author": [
			{
				"family": "Henriksen",
				"given": "Anne"
			},
			{
				"family": "Enni",
				"given": "Simon"
			},
			{
				"family": "Bechmann",
				"given": "Anja"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "islamCanWeObtain2021",
		"type": "paper-conference",
		"abstract": "There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462614",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "586–596",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Can we obtain fairness for free?",
		"URL": "https://doi.org/10.1145/3461702.3462614",
		"author": [
			{
				"family": "Islam",
				"given": "Rashidul"
			},
			{
				"family": "Pan",
				"given": "Shimei"
			},
			{
				"family": "Foulds",
				"given": "James R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "javadiMonitoringAIServices2021",
		"type": "paper-conference",
		"abstract": "Given the surge in interest in AI, we now see the emergence of Artificial Intelligence as a Service (AIaaS). AIaaS entails service providers offering remote access to ML models and capabilities at arms-length', through networked APIs. Such services will grow in popularity, as they enable access to state-of-the-art ML capabilities, 'on demand', 'out of the box', at low cost and without requiring training data or ML expertise. However, there is much public concern regarding AI. AIaaS raises particular considerations, given there is much potential for such services to be used to underpin and drive problematic, inappropriate, undesirable, controversial, or possibly even illegal applications. A key way forward is through service providers monitoring their AI services to identify potential situations of problematic use. Towards this, we elaborate the potential for 'misuse indicators' as a mechanism for uncovering patterns of usage behaviour warranting consideration or further investigation. We introduce a taxonomy for describing these indicators and their contextual considerations, and use exemplars to demonstrate the feasibility analysing AIaaS usage to highlight situations of possible concern. We also seek to draw more attention to AI services and the issues they raise, given AIaaS' increasing prominence, and the general calls for the more responsible and accountable use of AI.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462566",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "597–607",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Monitoring AI services for misuse",
		"URL": "https://doi.org/10.1145/3461702.3462566",
		"author": [
			{
				"family": "Javadi",
				"given": "Seyyed Ahmad"
			},
			{
				"family": "Norval",
				"given": "Chris"
			},
			{
				"family": "Cloete",
				"given": "Richard"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "jiangEquityAlgorithmicFairness2021",
		"type": "paper-conference",
		"abstract": "Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462623",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "608–617",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards equity and algorithmic fairness in student grade prediction",
		"URL": "https://doi.org/10.1145/3461702.3462623",
		"author": [
			{
				"family": "Jiang",
				"given": "Weijie"
			},
			{
				"family": "Pardos",
				"given": "Zachary A."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kasirzadehEthicalGravityThesis2021",
		"type": "paper-conference",
		"abstract": "Computers are used to make decisions in an increasing number of domains. There is widespread agreement that some of these uses are ethically problematic. Far less clear is where ethical problems arise, and what might be done about them. This paper expands and defends the Ethical Gravity Thesis: ethical problems that arise at higher levels of analysis of an automated decision-making system are inherited by lower levels of analysis. Particular instantiations of systems can add new problems, but not ameliorate more general ones. We defend this thesis by adapting Marr's famous 1982 framework for understanding information-processing systems. We show how this framework allows one to situate ethical problems at the appropriate level of abstraction, which in turn can be used to target appropriate interventions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462606",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "618–626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical gravity thesis: Marrian levels and the persistence of bias in automated decision-making systems",
		"URL": "https://doi.org/10.1145/3461702.3462606",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Klein",
				"given": "Colin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kelleyExcitingUsefulWorrying2021",
		"type": "paper-conference",
		"abstract": "As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462605",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "627–637",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Exciting, useful, worrying, futuristic: Public perception of artificial intelligence in 8 countries",
		"URL": "https://doi.org/10.1145/3461702.3462605",
		"author": [
			{
				"family": "Kelley",
				"given": "Patrick Gage"
			},
			{
				"family": "Yang",
				"given": "Yongwei"
			},
			{
				"family": "Heldreth",
				"given": "Courtney"
			},
			{
				"family": "Moessner",
				"given": "Christopher"
			},
			{
				"family": "Sedley",
				"given": "Aaron"
			},
			{
				"family": "Kramm",
				"given": "Andreas"
			},
			{
				"family": "Newman",
				"given": "David T."
			},
			{
				"family": "Woodruff",
				"given": "Allison"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kimAgeBiasEmotion2021",
		"type": "paper-conference",
		"abstract": "The growing potential for facial emotion recognition (FER) technology has encouraged expedited development at the cost of rigorous validation. Many of its use-cases may also impact the diverse global community as FER becomes embedded into domains ranging from education to security to healthcare. Yet, prior work has highlighted that FER can exhibit both gender and racial biases like other facial analysis techniques. As a result, bias-mitigation research efforts have mainly focused on tackling gender and racial disparities, while other demographic related biases, such as age, have seen less progress. This work seeks to examine the performance of state of the art commercial FER technology on expressive images of men and women from three distinct age groups. We utilize four different commercial FER systems in a black box methodology to evaluate how six emotions - anger, disgust, fear, happiness, neutrality, and sadness - are correctly detected by age group. We further investigate how algorithmic changes over the last year have affected system performance. Our results found that all four commercial FER systems most accurately perceived emotion in images of young adults and least accurately in images of older adults. This trend was observed for analyses conducted in 2019 and 2020. However, little to no gender disparities were observed in either year. While older adults may not have been the initial target consumer of FER technology, statistics show the demographic is quickly growing more keen to applications that use such systems. Our results demonstrate the importance of considering various demographic subgroups during FER system validation and the need for inclusive, intersectional algorithmic developmental practices.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462609",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "638–644",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Age bias in emotion detection: An analysis of facial emotion recognition performance on young, middle-aged, and older adults",
		"URL": "https://doi.org/10.1145/3461702.3462609",
		"author": [
			{
				"family": "Kim",
				"given": "Eugenia"
			},
			{
				"family": "Bryant",
				"given": "De'Aira"
			},
			{
				"family": "Srikanth",
				"given": "Deepak"
			},
			{
				"family": "Howard",
				"given": "Ayanna"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "klinovaAISharedProsperity2021",
		"type": "paper-conference",
		"abstract": "Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462619",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "645–651",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI and shared prosperity",
		"URL": "https://doi.org/10.1145/3461702.3462619",
		"author": [
			{
				"family": "Klinova",
				"given": "Katya"
			},
			{
				"family": "Korinek",
				"given": "Anton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kommiyamothilalUnifyingFeatureAttribution2021",
		"type": "paper-conference",
		"abstract": "Feature attributions and counterfactual explanations are popular approaches to explain a ML model. The former assigns an importance score to each input feature, while the latter provides input examples with minimal changes to alter the model's predictions. To unify these approaches, we provide an interpretation based on the actual causality framework and present two key results in terms of their use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which attribution-based methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complimentary of these two approaches. Our evaluation on three benchmark datasets — Adult-Income, LendingClub, and German-Credit — confirms the complimentary. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like Wachter et al. and DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are often neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462597",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "652–663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards unifying feature attribution and counterfactual explanations: Different means to the same end",
		"URL": "https://doi.org/10.1145/3461702.3462597",
		"author": [
			{
				"family": "Kommiya Mothilal",
				"given": "Ramaravind"
			},
			{
				"family": "Mahajan",
				"given": "Divyat"
			},
			{
				"family": "Tan",
				"given": "Chenhao"
			},
			{
				"family": "Sharma",
				"given": "Amit"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kshirsagarBecomingGoodAI2021",
		"type": "paper-conference",
		"abstract": "AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462599",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "664–673",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Becoming good at AI for good",
		"URL": "https://doi.org/10.1145/3461702.3462599",
		"author": [
			{
				"family": "Kshirsagar",
				"given": "Meghana"
			},
			{
				"family": "Robinson",
				"given": "Caleb"
			},
			{
				"family": "Yang",
				"given": "Siyu"
			},
			{
				"family": "Gholami",
				"given": "Shahrzad"
			},
			{
				"family": "Klyuzhin",
				"given": "Ivan"
			},
			{
				"family": "Mukherjee",
				"given": "Sumit"
			},
			{
				"family": "Nasir",
				"given": "Md"
			},
			{
				"family": "Ortiz",
				"given": "Anthony"
			},
			{
				"family": "Oviedo",
				"given": "Felipe"
			},
			{
				"family": "Tanner",
				"given": "Darren"
			},
			{
				"family": "Trivedi",
				"given": "Anusua"
			},
			{
				"family": "Xu",
				"given": "Yixi"
			},
			{
				"family": "Zhong",
				"given": "Ming"
			},
			{
				"family": "Dilkina",
				"given": "Bistra"
			},
			{
				"family": "Dodhia",
				"given": "Rahul"
			},
			{
				"family": "Lavista Ferres",
				"given": "Juan M."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "kuhlmanMeasuringGroupAdvantage2021",
		"type": "paper-conference",
		"abstract": "Ranking evaluation metrics play an important role in information retrieval, providing optimization objectives during development and means of assessment of deployed performance. Recently, fairness of rankings has been recognized as crucial, especially as automated systems are increasingly used for high impact decisions. While numerous fairness metrics have been proposed, a comparative analysis to understand their interrelationships is lacking. Even for fundamental statistical parity metrics which measure group advantage, it remains unclear whether metrics measure the same phenomena, or when one metric may produce different results than another. To address these open questions, we formulate a conceptual framework for analytical comparison of metrics. We prove that under reasonable assumptions, popular metrics in the literature exhibit the same behavior and that optimizing for one optimizes for all. However, our analysis also shows that the metrics vary in the degree of unfairness measured, in particular when one group has a strong majority. Based on this analysis, we design a practical statistical test to identify whether observed data is likely to exhibit predictable group bias. We provide a set of recommendations for practitioners to guide the choice of an appropriate fairness metric.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462588",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "674–682",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring group advantage: A comparative study of fair ranking metrics",
		"URL": "https://doi.org/10.1145/3461702.3462588",
		"author": [
			{
				"family": "Kuhlman",
				"given": "Caitlin"
			},
			{
				"family": "Gerych",
				"given": "Walter"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "larsenFrameworkUnderstandingAIInduced2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) systems operate in increasingly diverse areas, from healthcare to facial recognition, the stock market, autonomous vehicles, and so on. While the underlying digital infrastructure of AI systems is developing rapidly, each area of implementation is subject to different degrees and processes of legitimization. By combining elements from institutional theory and information systems-theory, this paper presents a conceptual framework to analyze and understand AI-induced field-change. The introduction of novel AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape organizations and institutions while existing institutional infrastructures determine the scope and speed at which organizational change is allowed to occur. Where institutional infrastructure and governance arrangements, such as standards, rules, and regulations, still are unelaborate, the field can move fast but is also more likely to be contested. The institutional infrastructure surrounding AI-induced fields is generally little elaborated, which could be an obstacle to the broader institutionalization of AI-systems going forward.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462591",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "683–694",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A framework for understanding AI-Induced field change: How AI technologies are legitimized and institutionalized",
		"URL": "https://doi.org/10.1145/3461702.3462591",
		"author": [
			{
				"family": "Larsen",
				"given": "Benjamin Cedric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leavyEthicalDataCuration2021",
		"type": "paper-conference",
		"abstract": "The potential for bias embedded in data to lead to the perpetuation of social injustice though Artificial Intelligence (AI) necessitates an urgent reform of data curation practices for AI systems, especially those based on machine learning. Without appropriate ethical and regulatory frameworks there is a risk that decades of advances in human rights and civil liberties may be undermined. This paper proposes an approach to data curation for AI, grounded in feminist epistemology and informed by critical theories of race and feminist principles. The objective of this approach is to support critical evaluation of the social dynamics of power embedded in data for AI systems. We propose a set of fundamental guiding principles for ethical data curation that address the social construction of knowledge, call for inclusion of subjugated and new forms of knowledge, support critical evaluation of theoretical concepts within data and recognise the reflexive nature of knowledge. In developing this ethical framework for data curation, we aim to contribute to a virtue ethics for AI and ensure protection of fundamental and human rights.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462598",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "695–703",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ethical data curation for AI: An approach based on feminist epistemology and critical theories of race",
		"URL": "https://doi.org/10.1145/3461702.3462598",
		"author": [
			{
				"family": "Leavy",
				"given": "Susan"
			},
			{
				"family": "Siapera",
				"given": "Eugenia"
			},
			{
				"family": "O'Sullivan",
				"given": "Barry"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leeRiskIdentificationQuestionnaire2021",
		"type": "paper-conference",
		"abstract": "Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners.In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462572",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "704–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Risk identification questionnaire for detecting unintended bias in the machine learning development lifecycle",
		"URL": "https://doi.org/10.1145/3461702.3462572",
		"author": [
			{
				"family": "Lee",
				"given": "Michelle Seng Ah"
			},
			{
				"family": "Singh",
				"given": "Jatinder"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leeParticipatoryAlgorithmicManagement2021",
		"type": "paper-conference",
		"abstract": "Artificial intelligence is increasingly being used to manage the workforce. Algorithmic management promises organizational efficiency, but often undermines worker well-being. How can we computationally model worker well-being so that algorithmic management can be optimized for and assessed in terms of worker well-being? Toward this goal, we propose a participatory approach for worker well-being models. We first define worker well-being models: Work preference models—preferences about work and working conditions, and managerial fairness models—beliefs about fair resource allocation among multiple workers. We then propose elicitation methods to enable workers to build their own well-being models leveraging pairwise comparisons and ranking. As a case study, we evaluate our methods in the context of algorithmic work scheduling with 25 shift workers and 3 managers. The findings show that workers expressed idiosyncratic work preference models and more uniform managerial fairness models, and the elicitation methods helped workers discover their preferences and gave them a sense of empowerment. Our work provides a method and initial evidence for enabling participatory algorithmic management for worker well-being.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462628",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "715–726",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Participatory algorithmic management: Elicitation methods for worker well-being models",
		"URL": "https://doi.org/10.1145/3461702.3462628",
		"author": [
			{
				"family": "Lee",
				"given": "Min Kyung"
			},
			{
				"family": "Nigam",
				"given": "Ishan"
			},
			{
				"family": "Zhang",
				"given": "Angie"
			},
			{
				"family": "Afriyie",
				"given": "Joel"
			},
			{
				"family": "Qin",
				"given": "Zhizhen"
			},
			{
				"family": "Gao",
				"given": "Sicun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leggettFeedingBeastSuperintelligence2021",
		"type": "paper-conference",
		"abstract": "Scientists and philosophers have warned of the possibility that humans, in the future, might create a 'superintelligent' machine that could, in some scenarios, form an existential threat to humanity. This paper argues that such a machine may already exist, and that, if so, it does, in fact, represent such a threat.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462581",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "727–735",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Feeding the beast: Superintelligence, corporate capitalism and the end of humanity",
		"URL": "https://doi.org/10.1145/3461702.3462581",
		"author": [
			{
				"family": "Leggett",
				"given": "Dominic"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "leibowiczDeepfakeDetectionDilemma2021",
		"type": "paper-conference",
		"abstract": "Synthetic media detection technologies label media as either synthetic or non-synthetic and are increasingly used by journalists, web platforms, and the general public to identify misinformation and other forms of problematic content. As both well-resourced organizations and the non-technical general public generate more sophisticated synthetic media, the capacity for purveyors of problematic content to adapt induces a detection dilemma : as detection practices become more accessible, they become more easily circumvented. This paper describes how a multistakeholder cohort from academia, technology platforms, media entities, and civil society organizations active in synthetic media detection and its socio-technical implications evaluates the detection dilemma. Specifically, we offer an assessment of detection contexts and adversary capacities sourced from the broader, global AI and media integrity community concerned with mitigating the spread of harmful synthetic media. A collection of personas illustrates the intersection between unsophisticated and highly-resourced sponsors of misinformation in the context of their technical capacities. This work concludes that there is no \"best” approach to navigating the detector dilemma, but derives a set of implications from multistakeholder input to better inform detection process decisions and policies, in practice.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462584",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "736–744",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The deepfake detection dilemma: A multistakeholder exploration of adversarial dynamics in synthetic media",
		"URL": "https://doi.org/10.1145/3461702.3462584",
		"author": [
			{
				"family": "Leibowicz",
				"given": "Claire R."
			},
			{
				"family": "McGregor",
				"given": "Sean"
			},
			{
				"family": "Ovadya",
				"given": "Aviv"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "liuRAWLSNETAlteringBayesian2021",
		"type": "paper-conference",
		"abstract": "We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462618",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "745–755",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "RAWLSNET: Altering bayesian networks to encode rawlsian fair equality of opportunity",
		"URL": "https://doi.org/10.1145/3461702.3462618",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Shafi",
				"given": "Zohair"
			},
			{
				"family": "Fleisher",
				"given": "William"
			},
			{
				"family": "Eliassi-Rad",
				"given": "Tina"
			},
			{
				"family": "Alfeld",
				"given": "Scott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "loiFairEqualityChances2021",
		"type": "paper-conference",
		"abstract": "This is a one-page summary of the paper \"A Philosophical Theory of Fairness for Prediction-based Decisions.\" The full paper is available on SSRN at the following link: https://papers.ssrn.com/sol3/papers.cfm?abstract<sub>i</sub>d=3450300",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462613",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 1\npublisher-place: Virtual Event, USA",
		"page": "756",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair equality of chances for prediction-based decisions",
		"URL": "https://doi.org/10.1145/3461702.3462613",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Herlitz",
				"given": "Anders"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "loiAccountabilityUseArtificial2021",
		"type": "paper-conference",
		"abstract": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462631",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "757–766",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards accountability in the use of artificial intelligence for public administrations",
		"URL": "https://doi.org/10.1145/3461702.3462631",
		"author": [
			{
				"family": "Loi",
				"given": "Michele"
			},
			{
				"family": "Spielkamp",
				"given": "Matthias"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mazijnHowScoreDistributions2021",
		"type": "paper-conference",
		"abstract": "Automated decisions based on trained algorithms influence human life in an increasingly far-reaching way. In recent years, it has become clear that these decisions are often accompanied by bias and unfair treatment of different subpopulations.Meanwhile, several notions of fairness circulate in the scientific literature, with trade-offs between profit and fairness and between fairness metrics among themselves. Based on both analytical calculations and numerical simulations, we show in this study that some profit-fairness trade-offs and fairness-fairness trade-offs depend substantially on the underlying score distributions given to subpopulations and we present two complementary perspectives to visualize this influence. We further show that higher symmetry in scores of subpopulations can significantly reduce the trade-offs between fairness notions within a given acceptable strictness, even when sacrificing expressiveness. Our exploratory study may help to understand how to overcome the strict mathematical statements about the statistical incompatibility of certain fairness notions.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462601",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "767–776",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do the score distributions of subpopulations influence fairness notions?",
		"URL": "https://doi.org/10.1145/3461702.3462601",
		"author": [
			{
				"family": "Mazijn",
				"given": "Carmen"
			},
			{
				"family": "Danckaert",
				"given": "Jan"
			},
			{
				"family": "Ginis",
				"given": "Vincent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mehrotraMoreSimilarValues2021",
		"type": "paper-conference",
		"abstract": "As AI systems are increasingly involved in decision making, it also becomes important that they elicit appropriate levels of trust from their users. To achieve this, it is first important to understand which factors influence trust in AI. We identify that a research gap exists regarding the role of personal values in trust in AI. Therefore, this paper studies how human and agent Value Similarity (VS) influences a human's trust in that agent. To explore this, 89 participants teamed up with five different agents, which were designed with varying levels of value similarity to that of the participants. In a within-subjects, scenario-based experiment, agents gave suggestions on what to do when entering the building to save a hostage. We analyzed the agent's scores on subjective value similarity, trust and qualitative data from open-ended questions. Our results show that agents rated as having more similar values also scored higher on trust, indicating a positive effect between the two. With this result, we add to the existing understanding of human-agent trust by providing insight into the role of value-similarity.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462576",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "777–783",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "More similar values, more trust? - the effect of value similarity on trust in human-agent interaction",
		"URL": "https://doi.org/10.1145/3461702.3462576",
		"author": [
			{
				"family": "Mehrotra",
				"given": "Siddharth"
			},
			{
				"family": "Jonker",
				"given": "Catholijn M."
			},
			{
				"family": "Tielman",
				"given": "Myrthe L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "mhasawadeCausalMultilevelFairness2021",
		"type": "paper-conference",
		"abstract": "Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462587",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "784–794",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal multi-level fairness",
		"URL": "https://doi.org/10.1145/3461702.3462587",
		"author": [
			{
				"family": "Mhasawade",
				"given": "Vishwali"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nanayakkaraUnpackingExpressedConsequences2021",
		"type": "paper-conference",
		"abstract": "The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462608",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "795–806",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unpacking the expressed consequences of AI research in broader impact statements",
		"URL": "https://doi.org/10.1145/3461702.3462608",
		"author": [
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "nielsenMeasuringLayReactions2021",
		"type": "paper-conference",
		"abstract": "The recording, aggregation, and exchange of personal data is necessary to the development of socially-relevant machine learning applications. However, anecdotal and survey evidence show that ordinary people feel discontent and even anger regarding data collection practices that are currently typical and legal. This suggests that personal data markets in their current form do not adhere to the norms applied by ordinary people. The present study experimentally probes whether market transactions in a typical online scenario are accepted when evaluated by lay people. The results show that a high percentage of study participants refused to participate in a data pricing exercise, even in a commercial context where market rules would typically be expected to apply. For those participants who did price the data, the median price was an order of magnitude higher than the market price. These results call into question the notice and consent market paradigm that is used by technology firms and government regulators when evaluating data flows. The results also point to a conceptual mismatch between cultural and legal expectations regarding the use of personal data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462582",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 7\npublisher-place: Virtual Event, USA",
		"page": "807–813",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring lay reactions to personal data markets",
		"URL": "https://doi.org/10.1145/3461702.3462582",
		"author": [
			{
				"family": "Nielsen",
				"given": "Aileen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pagnuccoEpistemicReasoningMachine2021",
		"type": "paper-conference",
		"abstract": "With the rapid development of autonomous machines such as selfdriving vehicles and social robots, there is increasing realisation that machine ethics is important for widespread acceptance of autonomous machines. Our objective is to encode ethical reasoning into autonomous machines following well-defined ethical principles and behavioural norms. We provide an approach to reasoning about actions that incorporates ethical considerations. It builds on Scherl and Levesque's [29, 30] approach to knowledge in the situation calculus. We show how reasoning about knowledge in a dynamic setting can be used to guide ethical and moral choices, aligned with consequentialist and deontological approaches to ethics. We apply our approach to autonomous driving and social robot scenarios, and provide an implementation framework.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462586",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "814–821",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Epistemic reasoning for machine ethics with situation calculus",
		"URL": "https://doi.org/10.1145/3461702.3462586",
		"author": [
			{
				"family": "Pagnucco",
				"given": "Maurice"
			},
			{
				"family": "Rajaratnam",
				"given": "David"
			},
			{
				"family": "Limarga",
				"given": "Raynaldio"
			},
			{
				"family": "Nayak",
				"given": "Abhaya"
			},
			{
				"family": "Song",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "pandeyDisparateImpactArtificial2021",
		"type": "paper-conference",
		"abstract": "Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462561",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 12\npublisher-place: Virtual Event, USA",
		"page": "822–833",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disparate impact of artificial intelligence bias in ridehailing economy's price discrimination algorithms",
		"URL": "https://doi.org/10.1145/3461702.3462561",
		"author": [
			{
				"family": "Pandey",
				"given": "Akshat"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "parkUnderstandingRepresentationRepresentativeness2021",
		"type": "paper-conference",
		"abstract": "A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462590",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "834–842",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding the representation and representativeness of age in AI data sets",
		"URL": "https://doi.org/10.1145/3461702.3462590",
		"author": [
			{
				"family": "Park",
				"given": "Joon Sung"
			},
			{
				"family": "Bernstein",
				"given": "Michael S."
			},
			{
				"family": "Brewer",
				"given": "Robin N."
			},
			{
				"family": "Kamar",
				"given": "Ece"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "perrierQuantumFairMachine2021",
		"type": "paper-conference",
		"abstract": "In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462611",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "843–853",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Quantum fair machine learning",
		"URL": "https://doi.org/10.1145/3461702.3462611",
		"author": [
			{
				"family": "Perrier",
				"given": "Elija"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "perroneFairBayesianOptimization2021",
		"type": "paper-conference",
		"abstract": "Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462629",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "854–863",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fair bayesian optimization",
		"URL": "https://doi.org/10.1145/3461702.3462629",
		"author": [
			{
				"family": "Perrone",
				"given": "Valerio"
			},
			{
				"family": "Donini",
				"given": "Michele"
			},
			{
				"family": "Zafar",
				"given": "Muhammad Bilal"
			},
			{
				"family": "Schmucker",
				"given": "Robin"
			},
			{
				"family": "Kenthapadi",
				"given": "Krishnaram"
			},
			{
				"family": "Archambeau",
				"given": "Cédric"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "posadaWeHavenGone2021",
		"type": "paper-conference",
		"abstract": "How should we understand the social and political effects of the datafication of human life? This paper argues that the effects of data should be understood as a constitutive shift in social and political relations. We explore how datafication, or quantification of human and non-human factors into binary code, affects the identity of individuals and groups. This fundamental shift goes beyond economic and ethical concerns, which has been the focus of other efforts to explore the effects of datafication and AI. We highlight that technologies such as datafication and AI (and previously, the printing press) both disrupted extant power arrangements, leading to decentralization, and triggered a recentralization of power by new actors better adapted to leveraging the new technology. We use the analogy of the printing press to provide a framework for understanding constitutive change. The printing press example gives us more clarity on 1) what can happen when the medium of communication drastically alters how information is communicated and stored; 2) the shift in power from state to private actors; and 3) the tension of simultaneously connecting individuals while driving them towards narrower communities through algorithmic analyses of data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462604",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 9\npublisher-place: Virtual Event, USA",
		"page": "864–872",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "We haven't gone paperless yet: Why the printing press can help us understand data and AI",
		"URL": "https://doi.org/10.1145/3461702.3462604",
		"author": [
			{
				"family": "Posada",
				"given": "Julian"
			},
			{
				"family": "Weller",
				"given": "Nicholas"
			},
			{
				"family": "Wong",
				"given": "Wendy H."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "prostMeasuringModelFairness2021",
		"type": "paper-conference",
		"abstract": "In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462603",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "873–883",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring model fairness under noisy covariates: A theoretical perspective",
		"URL": "https://doi.org/10.1145/3461702.3462603",
		"author": [
			{
				"family": "Prost",
				"given": "Flavien"
			},
			{
				"family": "Awasthi",
				"given": "Pranjal"
			},
			{
				"family": "Blumm",
				"given": "Nick"
			},
			{
				"family": "Kumthekar",
				"given": "Aditee"
			},
			{
				"family": "Potter",
				"given": "Trevor"
			},
			{
				"family": "Wei",
				"given": "Li"
			},
			{
				"family": "Wang",
				"given": "Xuezhi"
			},
			{
				"family": "Chi",
				"given": "Ed H."
			},
			{
				"family": "Chen",
				"given": "Jilin"
			},
			{
				"family": "Beutel",
				"given": "Alex"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "ramachandranGAEAGraphAugmentation2021",
		"type": "paper-conference",
		"abstract": "Disparate access to resources by different subpopulations is a prevalent issue in societal and sociotechnical networks. For example, urban infrastructure networks may enable certain racial groups to more easily access resources such as high-quality schools, grocery stores, and polling places. Similarly, social networks within universities and organizations may enable certain groups to more easily access people with valuable information or influence. Here we introduce a new class of problems, Graph Augmentation for Equitable Access (GAEA), to enhance equity in networked systems by editing graph edges under budget constraints. We prove such problems are NP-hard, and cannot be approximated within a factor of (1-1/3e). We develop a principled, sample- and time- efficient Markov Reward Process (MRP)-based mechanism design framework for GAEA. Our algorithm outperforms baselines on a diverse set of synthetic graphs. We further demonstrate the method on real-world networks, by merging public census, school, and transportation datasets for the city of Chicago and applying our algorithm to find human-interpretable edits to the bus network that enhance equitable access to high-quality schools across racial groups. Further experiments on Facebook networks of universities yield sets of new social connections that would increase equitable access to certain attributed nodes across gender groups.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462615",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "884–894",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GAEA: Graph augmentation for equitable access via reinforcement learning",
		"URL": "https://doi.org/10.1145/3461702.3462615",
		"author": [
			{
				"family": "Ramachandran",
				"given": "Govardana Sachithanandam"
			},
			{
				"family": "Brugere",
				"given": "Ivan"
			},
			{
				"family": "Varshney",
				"given": "Lav R."
			},
			{
				"family": "Xiong",
				"given": "Caiming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "razFaceMisIDInteractive2021",
		"type": "paper-conference",
		"abstract": "This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462627",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "895–904",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Face mis-ID: An interactive pedagogical tool demonstrating disparate accuracy rates in facial recognition",
		"URL": "https://doi.org/10.1145/3461702.3462627",
		"author": [
			{
				"family": "Raz",
				"given": "Daniella"
			},
			{
				"family": "Bintz",
				"given": "Corinne"
			},
			{
				"family": "Guetler",
				"given": "Vivian"
			},
			{
				"family": "Tam",
				"given": "Aaron"
			},
			{
				"family": "Katell",
				"given": "Michael"
			},
			{
				"family": "Dailey",
				"given": "Dharma"
			},
			{
				"family": "Herman",
				"given": "Bernease"
			},
			{
				"family": "Krafft",
				"given": "P. M."
			},
			{
				"family": "Young",
				"given": "Meg"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "schelenzTheoryPracticeEthical2021",
		"type": "paper-conference",
		"abstract": "Diversity-aware platform design is a paradigm that responds to the ethical challenges of existing social media platforms. Available platforms have been criticized for minimizing users' autonomy, marginalizing minorities, and exploiting users' data for profit maximization. This paper presents a design solution that centers the well-being of users. It presents the theory and practice of designing a diversity-aware platform for social relations. In this approach, the diversity of users is leveraged in a way that allows like-minded individuals to pursue similar interests or diverse individuals to complement each other in a complex activity. The end users of the envisioned platform are students, who participate in the design process. Diversity-aware platform design involves numerous steps, of which two are highlighted in this paper: 1) defining a framework and operationalizing the \"diversity\" of students, 2) collecting \"diversity\" data to build diversity-aware algorithms. The paper further reflects on the ethical challenges encountered during the design of a diversity-aware platform.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462595",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "905–915",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The theory, practice, and ethical challenges of designing a diversity-aware platform for social relations",
		"URL": "https://doi.org/10.1145/3461702.3462595",
		"author": [
			{
				"family": "Schelenz",
				"given": "Laura"
			},
			{
				"family": "Bison",
				"given": "Ivano"
			},
			{
				"family": "Busso",
				"given": "Matteo"
			},
			{
				"family": "Götzen",
				"given": "Amalia",
				"non-dropping-particle": "de"
			},
			{
				"family": "Gatica-Perez",
				"given": "Daniel"
			},
			{
				"family": "Giunchiglia",
				"given": "Fausto"
			},
			{
				"family": "Meegahapola",
				"given": "Lakmal"
			},
			{
				"family": "Ruiz-Correa",
				"given": "Salvador"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "schumannStepMoreInclusive2021",
		"type": "paper-conference",
		"abstract": "The Open Images Dataset contains approximately 9 million images and is a widely accepted dataset for computer vision research. As is common practice for large datasets, the annotations are not exhaustive, with bounding boxes and attribute labels for only a subset of the classes in each image. In this paper, we present a new set of annotations on a subset of the Open Images dataset called the MIAP (More Inclusive Annotations for People) subset, containing bounding boxes and attributes for all of the people visible in those images. The attributes and labeling methodology for the MIAP subset were designed to enable research into model fairness. In addition, we analyze the original annotation methodology for the person class and its subclasses, discussing the resulting patterns in order to inform future annotation efforts. By considering both the original and exhaustive annotation sets, researchers can also now study how systematic patterns in training annotations affect modeling.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462594",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "916–925",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A step toward more inclusive people annotations for fairness",
		"URL": "https://doi.org/10.1145/3461702.3462594",
		"author": [
			{
				"family": "Schumann",
				"given": "Candice"
			},
			{
				"family": "Ricco",
				"given": "Susanna"
			},
			{
				"family": "Prabhu",
				"given": "Utsav"
			},
			{
				"family": "Ferrari",
				"given": "Vittorio"
			},
			{
				"family": "Pantofaru",
				"given": "Caroline"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "segalFairnessEyesData2021",
		"type": "paper-conference",
		"abstract": "We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462554",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "926–935",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in the eyes of the data: Certifying machine-learning models",
		"URL": "https://doi.org/10.1145/3461702.3462554",
		"author": [
			{
				"family": "Segal",
				"given": "Shahar"
			},
			{
				"family": "Adi",
				"given": "Yossi"
			},
			{
				"family": "Pinkas",
				"given": "Benny"
			},
			{
				"family": "Baum",
				"given": "Carsten"
			},
			{
				"family": "Ganesh",
				"given": "Chaya"
			},
			{
				"family": "Keshet",
				"given": "Joseph"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "shahRawlsianFairAdaptation2021",
		"type": "paper-conference",
		"abstract": "Group-fairness in classification aims for equality of a predictive utility across different sensitive sub-populations, e.g., race or gender. Equality or near-equality constraints in group-fairness often worsen not only the aggregate utility but also the utility for the least advantaged sub-population. In this paper, we apply the principles of Pareto-efficiency and least-difference to the utility being accuracy, as an illustrative example, and arrive at the Rawls classifier that minimizes the error rate on the worst-off sensitive sub-population. Our mathematical characterization shows that the Rawls classifier uniformly applies a threshold to an ideal score of features, in the spirit of fair equality of opportunity. In practice, such a score or a feature representation is often computed by a black-box model that has been useful but unfair. Our second contribution is practical Rawlsian fair adaptation of any given black-box deep learning model, without changing the score or feature representation it computes. Given any score function or feature representation and only its second-order statistics on the sensitive sub-populations, we seek a threshold classifier on the given score or a linear threshold classifier on the given feature representation that achieves the Rawls error rate restricted to this hypothesis class. Our technical contribution is to formulate the above problems using ambiguous chance constraints, and to provide efficient algorithms for Rawlsian fair adaptation, along with provable upper bounds on the Rawls error rate. Our empirical results show significant improvement over state-of-the-art group-fair algorithms, even without retraining for fairness.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462592",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "936–945",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Rawlsian fair adaptation of deep learning classifiers",
		"URL": "https://doi.org/10.1145/3461702.3462592",
		"author": [
			{
				"family": "Shah",
				"given": "Kulin"
			},
			{
				"family": "Gupta",
				"given": "Pooja"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			},
			{
				"family": "Bhattacharyya",
				"given": "Chiranjib"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "sharmaFaiRNFairRobust2021",
		"type": "paper-conference",
		"abstract": "Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462559",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "946–955",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FaiR-N: Fair and robust neural networks for structured data",
		"URL": "https://doi.org/10.1145/3461702.3462559",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Gee",
				"given": "Alan H."
			},
			{
				"family": "Paydarfar",
				"given": "David"
			},
			{
				"family": "Ghosh",
				"given": "Joydeep"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "simonsMachineLearningMeaning2021",
		"type": "paper-conference",
		"abstract": "Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462556",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "956–966",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine learning and the meaning of equal treatment",
		"URL": "https://doi.org/10.1145/3461702.3462556",
		"author": [
			{
				"family": "Simons",
				"given": "Joshua"
			},
			{
				"family": "Adams Bhatti",
				"given": "Sophia"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "slavkovikDigitalVoodooDolls2021",
		"type": "paper-conference",
		"abstract": "An institution, be it a body of government, commercial enterprise, or a service, cannot interact directly with a person. Instead, a model is created to represent us. We argue the existence of a new high-fidelity type of person model which we call a digital voodoo doll. We conceptualize it and compare its features with existing models of persons. Digital voodoo dolls are distinguished by existing completely beyond the influence and control of the person they represent. We discuss the ethical issues that such a lack of accountability creates and argue how these concerns can be mitigated.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462626",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "967–977",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Digital voodoo dolls",
		"URL": "https://doi.org/10.1145/3461702.3462626",
		"author": [
			{
				"family": "Slavkovik",
				"given": "Marija"
			},
			{
				"family": "Stachl",
				"given": "Clemens"
			},
			{
				"family": "Pitman",
				"given": "Caroline"
			},
			{
				"family": "Askonas",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "solansComparingEquityEffectiveness2021",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years.We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups.Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462600",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "978–988",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Comparing equity and effectiveness of different algorithms in an application for the room rental market",
		"URL": "https://doi.org/10.1145/3461702.3462600",
		"author": [
			{
				"family": "Solans",
				"given": "David"
			},
			{
				"family": "Fabbri",
				"given": "Francesco"
			},
			{
				"family": "Calsamiglia",
				"given": "Caterina"
			},
			{
				"family": "Castillo",
				"given": "Carlos"
			},
			{
				"family": "Bonchi",
				"given": "Francesco"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "suhrDoesFairRanking2021",
		"type": "paper-conference",
		"abstract": "Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice.In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462602",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "989–999",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does fair ranking improve minority outcomes? Understanding the interplay of human and algorithmic biases in online hiring",
		"URL": "https://doi.org/10.1145/3461702.3462602",
		"author": [
			{
				"family": "Sühr",
				"given": "Tom"
			},
			{
				"family": "Hilgard",
				"given": "Sophie"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "waitesDifferentiallyPrivateNormalizing2021",
		"type": "paper-conference",
		"abstract": "Normalizing flow models have risen as a popular solution to the problem of density estimation, enabling high-quality synthetic data generation as well as exact probability density evaluation. However, in contexts where individuals are directly associated with the training data, releasing such a model raises privacy concerns. In this work, we propose the use of normalizing flow models that provide explicit differential privacy guarantees as a novel approach to the problem of privacy-preserving density estimation. We evaluate the efficacy of our approach empirically using benchmark datasets, and we demonstrate that our method substantially outperforms previous state-of-the-art approaches. We additionally show how our algorithm can be applied to the task of differentially private anomaly detection.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462625",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "1000–1009",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Differentially private normalizing flows for privacy-preserving density estimation",
		"URL": "https://doi.org/10.1145/3461702.3462625",
		"author": [
			{
				"family": "Waites",
				"given": "Chris"
			},
			{
				"family": "Cummings",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "watkinsGoverningAlgorithmicSystems2021",
		"type": "paper-conference",
		"abstract": "Algorithmic decision-making and decision-support systems (ADS) are gaining influence over how society distributes resources, administers justice, and provides access to opportunities. Yet collectively we do not adequately study how these systems affect people or document the actual or potential harms resulting from their integration with important social functions. This is a significant challenge for computational justice efforts of measuring and governing AI systems. Impact assessments are often used as instruments to create accountability relationships and grant some measure of agency and voice to communities affected by projects with environmental, financial, and human rights ramifications. Applying these tools-through Algorithmic Impact Assessments (AIA)-is a plausible way to establish accountability relationships for ADSs. At the same time, what an AIA would entail remains under-specified; they raise as many questions as they answer. Choices about the methods, scope, and purpose of AIAs structure the conditions of possibility for AI governance. In this paper, we present our research on the history of impact assessments across diverse domains, through a sociotechnical lens, to present six observations on how they co-constitute accountability. Decisions about what type of effects count as an impact; when impacts are assessed; whose interests are considered; who is invited to participate; who conducts the assessment; how assessments are made publicly available, and what the outputs of the assessment might be; all shape the forms of accountability that AIAs engender. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462580",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 13\npublisher-place: Virtual Event, USA",
		"page": "1010–1022",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Governing algorithmic systems with impact assessments: Six observations",
		"URL": "https://doi.org/10.1145/3461702.3462580",
		"author": [
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			},
			{
				"family": "Moss",
				"given": "Emanuel"
			},
			{
				"family": "Metcalf",
				"given": "Jacob"
			},
			{
				"family": "Singh",
				"given": "Ranjit"
			},
			{
				"family": "Elish",
				"given": "Madeleine Clare"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yaghiniHumanintheloopFrameworkConstruct2021",
		"type": "paper-conference",
		"abstract": "Existing mathematical notions of fairness fail to account for the context of decision-making. We argue that moral consideration of contextual factors is an inherently human task. So we present a framework to learn context-aware mathematical formulations of fairness by eliciting people's situated fairness assessments. Our family of fairness notions corresponds to a new interpretation of economic models of Equality of Opportunity (EOP), and it includes most existing notions of fairness as special cases. Our human-in-the-loop approach is designed to learn the appropriate parameters of the EOP family by utilizing human responses to pair-wise questions about decision subjects' circumstance and deservingness, and the harm/benefit imposed on them. We illustrate our framework in a hypothetical criminal risk assessment scenario by conducting a series of human-subject experiments on Amazon Mechanical Turk. Our work takes an important initial step toward empowering stakeholders to have a voice in the formulation of fairness for Machine Learning.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462583",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 11\npublisher-place: Virtual Event, USA",
		"page": "1023–1033",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A human-in-the-loop framework to construct context-aware mathematical notions of outcome fairness",
		"URL": "https://doi.org/10.1145/3461702.3462583",
		"author": [
			{
				"family": "Yaghini",
				"given": "Mohammad"
			},
			{
				"family": "Krause",
				"given": "Andreas"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "yonaWhoResponsibleJointly2021",
		"type": "paper-conference",
		"abstract": "A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462574",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "1034–1041",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Who's responsible? Jointly quantifying the contribution of the learning algorithm and data",
		"URL": "https://doi.org/10.1145/3461702.3462574",
		"author": [
			{
				"family": "Yona",
				"given": "Gal"
			},
			{
				"family": "Ghorbani",
				"given": "Amirata"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zhangRelExModelagnosticRelational2021",
		"type": "paper-conference",
		"abstract": "In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art performance, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, amodel-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462562",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 8\npublisher-place: Virtual Event, USA",
		"page": "1042–1049",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "RelEx: A model-agnostic relational model explainer",
		"URL": "https://doi.org/10.1145/3461702.3462562",
		"author": [
			{
				"family": "Zhang",
				"given": "Yue"
			},
			{
				"family": "Defazio",
				"given": "David"
			},
			{
				"family": "Ramesh",
				"given": "Arti"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "zwetslootSkilledMobileSurvey2021",
		"type": "paper-conference",
		"abstract": "Countries, companies, and universities are increasingly competing over top-tier artificial intelligence (AI) researchers. Where are these researchers likely to immigrate and what affects their immigration decisions? We conducted a survey (n = 524) of the immigration preferences and motivations of researchers that had papers accepted at one of two prestigious AI conferences: the Conference on Neural Information Processing Systems (NeurIPS) and the International Conference on Machine Learning (ICML). We find that the U.S. is the most popular destination for AI researchers, followed by the U.K., Canada, Switzerland, and France. A country's professional opportunities stood out as the most common factor that influences immigration decisions of AI researchers, followed by lifestyle and culture, the political climate, and personal relations. The destination country's immigration policies were important to just under half of the researchers surveyed, while around a quarter noted current immigration difficulties to be a deciding factor. Visa and immigration difficulties were perceived to be a particular impediment to conducting AI research in the U.S., the U.K., and Canada. Implications of the findings for the future of AI talent policies and governance are discussed.",
		"collection-title": "AIES '21",
		"container-title": "Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3461702.3462617",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-8473-5",
		"note": "number-of-pages: 10\npublisher-place: Virtual Event, USA",
		"page": "1050–1059",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Skilled and mobile: Survey evidence of AI researchers' immigration preferences",
		"URL": "https://doi.org/10.1145/3461702.3462617",
		"author": [
			{
				"family": "Zwetsloot",
				"given": "Remco"
			},
			{
				"family": "Zhang",
				"given": "Baobao"
			},
			{
				"family": "Dreksler",
				"given": "Noemi"
			},
			{
				"family": "Kahn",
				"given": "Lauren"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			},
			{
				"family": "Horowitz",
				"given": "Michael C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "adamWriteItYou2022",
		"type": "paper-conference",
		"abstract": "Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534203",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "7–21",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Write it like you see it: Detectable differences in clinical notes by race lead to differential model recommendations",
		"URL": "https://doi.org/10.1145/3514094.3534203",
		"author": [
			{
				"family": "Adam",
				"given": "Hammaad"
			},
			{
				"family": "Yang",
				"given": "Ming Ying"
			},
			{
				"family": "Cato",
				"given": "Kenrick"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Senteio",
				"given": "Charles"
			},
			{
				"family": "Celi",
				"given": "Leo Anthony"
			},
			{
				"family": "Zeng",
				"given": "Jiaming"
			},
			{
				"family": "Singh",
				"given": "Moninder"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "akpinarLongtermDynamicsFairness2022",
		"type": "paper-conference",
		"abstract": "Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Polya urn model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534173",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "22–35",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Long-term dynamics of fairness intervention in connection recommender systems",
		"URL": "https://doi.org/10.1145/3514094.3534173",
		"author": [
			{
				"family": "Akpinar",
				"given": "Nil-Jana"
			},
			{
				"family": "DiCiccio",
				"given": "Cyrus"
			},
			{
				"family": "Nandy",
				"given": "Preetam"
			},
			{
				"family": "Basu",
				"given": "Kinjal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "aleksandrovDynamicFleetManagement2022",
		"type": "paper-conference",
		"abstract": "We propose a solution for intelligent household garbage collection in smart cities. Garbage containers are assumed to be digitalized with Internet-of-Things sensors that are capable of sensing the fill levels of containers and transmitting this data through LoRaWAN networks to a central server. Data is used for dynamic fleet management and household feedback. We give a number of algorithms for these tasks. Fleet management requires scheduling containers for collections and assigning containers to trucks, as well as routing the trucks. Drivers receive such navigations via pervasive computing devices such as tablets, phones, or watches. Household feedback consists of information about the levels of generated garbage and the associated costs. Households receive this information on their home devices. Thus, unlike present solutions, our solution involves households in the intelligent collection of their garbage.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534152",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "36–45",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Dynamic fleet management and household feedback for garbage collection",
		"URL": "https://doi.org/10.1145/3514094.3534152",
		"author": [
			{
				"family": "Aleksandrov",
				"given": "Martin Damyanov"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "balakrishnanSCALESFairnessPrinciples2022",
		"type": "paper-conference",
		"abstract": "This paper proposes SCALES, a general framework that translates well-established fairness principles into a common representation based on the Constraint Markov Decision Process (CMDP). With the help of causal language, our framework can place constraints on both the procedure of decision making (procedural fairness) as well as the outcomes resulting from decisions (outcome fairness). Specifically, we show that well-known fairness principles can be encoded either as a utility component, a non-causal component, or a causal component in a SCALES-CMDP. We illustrate SCALES using a set of case studies involving a simulated healthcare scenario and the real-world COMPAS dataset. Experiments demonstrate that our framework produces fair policies that embody alternative fairness principles in single-step and sequential decision-making scenarios.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534190",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "46–55",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "SCALES: From fairness principles to constrained decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534190",
		"author": [
			{
				"family": "Balakrishnan",
				"given": "Sreejith"
			},
			{
				"family": "Bi",
				"given": "Jianxin"
			},
			{
				"family": "Soh",
				"given": "Harold"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "barnettCrowdsourcingImpactsExploring2022",
		"type": "paper-conference",
		"abstract": "With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534145",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "56–67",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Crowdsourcing impacts: Exploring the utility of crowds for anticipating societal impacts of algorithmic decision making",
		"URL": "https://doi.org/10.1145/3514094.3534145",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bensalemAegisAgentMultiparty2022",
		"type": "paper-conference",
		"abstract": "The proliferation of social media set the foundation for the culture of over-disclosure where many people document every single event, incident, trip, etc. for everyone to see. Raising the individual's awareness of the privacy issues that they are subjecting themselves to can be challenging. This becomes more complex when the post being shared includes data \"owned\" by others. The existing approaches aiming to assist users in multi-party disclosure situations need to be revised to go beyond preferences to the \"good\" of the collective.This paper proposes an agent called Aegis to calculate the potential risk incurred by multi-party members in order to push privacy-preserving nudges to the sharer. Aegis is inspired by the consequentialist approach in normative ethical problem-solving techniques. The main contribution is the introduction of a social media-specific risk equation based on data valuation and the propagation of the post from intended to unintended audience. The proof-of-concept reports on how Aegis performs based on real-world data from the SNAP dataset and synthetically generated networks.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534134",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "68–77",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Aegis: An agent for multi-party privacy preservation",
		"URL": "https://doi.org/10.1145/3514094.3534134",
		"author": [
			{
				"family": "Ben Salem",
				"given": "Rim"
			},
			{
				"family": "Aïmeur",
				"given": "Esma"
			},
			{
				"family": "Hage",
				"given": "Hicham"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bertrandHowCognitiveBiases2022",
		"type": "paper-conference",
		"abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534164",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "78–91",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How cognitive biases affect XAI-assisted decision-making: A systematic review",
		"URL": "https://doi.org/10.1145/3514094.3534164",
		"author": [
			{
				"family": "Bertrand",
				"given": "Astrid"
			},
			{
				"family": "Belloum",
				"given": "Rafik"
			},
			{
				"family": "Eagan",
				"given": "James R."
			},
			{
				"family": "Maxwell",
				"given": "Winston"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bessenCostEthicalAI2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence startups use training data as direct inputs in product development. These firms must balance numerous tradeoffs between ethical issues and data access without substantive guidance from regulators or existing judicial precedence. We survey these startups to determine what actions they have taken to address these ethical issues and the consequences of those actions. We find that 58% of these startups have established a set of AI principles. Startups with data-sharing relationships with high-technology firms or that have prior experience with privacy regulations are more likely to establish ethical AI principles and are more likely to take costly steps, like dropping training data or turning down business, to adhere to their ethical AI policies. Moreover, startups with ethical AI policies are more likely to invest in unconscious bias training, hire ethnic minorities and female programmers, seek expert advice, and search for more diverse training data. Potential costs associated with data-sharing relationships and the adherence to ethical policies may create tradeoffs between increased AI product competition and more ethical AI production.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534195",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "92–106",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The cost of ethical AI development for AI startups",
		"URL": "https://doi.org/10.1145/3514094.3534195",
		"author": [
			{
				"family": "Bessen",
				"given": "James"
			},
			{
				"family": "Impink",
				"given": "Stephen Michael"
			},
			{
				"family": "Seamans",
				"given": "Robert"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bringascolmenarejoFairnessAgreementEuropean2022",
		"type": "paper-conference",
		"abstract": "With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534158",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "107–118",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness in agreement with european values: An interdisciplinary perspective on AI regulation",
		"URL": "https://doi.org/10.1145/3514094.3534158",
		"author": [
			{
				"family": "Bringas Colmenarejo",
				"given": "Alejandra"
			},
			{
				"family": "Nannini",
				"given": "Luca"
			},
			{
				"family": "Rieger",
				"given": "Alisa"
			},
			{
				"family": "Scott",
				"given": "Kristen M."
			},
			{
				"family": "Zhao",
				"given": "Xuan"
			},
			{
				"family": "Patro",
				"given": "Gourab K"
			},
			{
				"family": "Kasneci",
				"given": "Gjergji"
			},
			{
				"family": "Kinder-Kurlanda",
				"given": "Katharina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "bucknallCurrentNeartermAI2022",
		"type": "paper-conference",
		"abstract": "There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534146",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "119–129",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Current and near-term AI as a potential existential risk factor",
		"URL": "https://doi.org/10.1145/3514094.3534146",
		"author": [
			{
				"family": "Bucknall",
				"given": "Benjamin S."
			},
			{
				"family": "Dori-Hacohen",
				"given": "Shiri"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "butcherRacialDisparitiesEnforcement2022",
		"type": "paper-conference",
		"abstract": "Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534184",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "130–143",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Racial disparities in the enforcement of marijuana violations in the US",
		"URL": "https://doi.org/10.1145/3514094.3534184",
		"author": [
			{
				"family": "Butcher",
				"given": "Bradley"
			},
			{
				"family": "Robinson",
				"given": "Chris"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Fogliato",
				"given": "Riccardo"
			},
			{
				"family": "Ashurst",
				"given": "Carolyn"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cachelFINSAuditingFramework2022",
		"type": "paper-conference",
		"abstract": "Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534160",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "144–155",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FINS auditing framework: Group fairness for subset selections",
		"URL": "https://doi.org/10.1145/3514094.3534160",
		"author": [
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "caliskanGenderBiasWord2022",
		"type": "paper-conference",
		"abstract": "Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men). Although efforts have been made to mitigate bias in word embeddings, with the hope of improving fairness in downstream Natural Language Processing (NLP) applications, these efforts will remain limited until we more deeply understand the multiple (and often subtle) ways that social biases can be reflected in word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). While some previous research has helped uncover biases in specific semantic associations between a group and a target domain (e.g., women - family), using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words. We leave the analysis of non-binary gender to future work due to the challenges in accurate group representation caused by limitations inherent in data.First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a  20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. Ultimately, these findings move the study of gender bias in word embeddings beyond the basic investigation of semantic relationships to also study gender differences in multiple manifestations in text. Given the central role of word embeddings in NLP applications, it is essential to more comprehensively document where biases exist and may remain hidden, allowing them to persist without our awareness throughout large text corpora.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534162",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "156–170",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Gender bias in word embeddings: A comprehensive analysis of frequency, syntax, and semantics",
		"URL": "https://doi.org/10.1145/3514094.3534162",
		"author": [
			{
				"family": "Caliskan",
				"given": "Aylin"
			},
			{
				"family": "Ajay",
				"given": "Pimparkar Parth"
			},
			{
				"family": "Charlesworth",
				"given": "Tessa"
			},
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Banaji",
				"given": "Mahzarin R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "canavottoPiecemealKnowledgeAcquisition2022",
		"type": "paper-conference",
		"abstract": "We present a hybrid approach to knowledge acquisition and representation for machine ethics—or more generally, computational normative reasoning. Building on recent research in artificial intelligence and law, our approach is modeled on the familiar practice of decision-making under precedential constraint in the common law. We first provide a formal characterization of this practice, showing how a body of normative information can be constructed in a way that is piecemeal, distributed, and responsive to particular circumstances. We then discuss two possible applications: first, a robot childminder, and second, moral judgment in a bioethical domain.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534182",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "171–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Piecemeal knowledge acquisition for computational normative reasoning",
		"URL": "https://doi.org/10.1145/3514094.3534182",
		"author": [
			{
				"family": "Canavotto",
				"given": "Ilaria"
			},
			{
				"family": "Horty",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "chenOrdinaryPeopleMoral2022",
		"type": "paper-conference",
		"abstract": "The Chinese Social Credit System (SCS) is a digital sociotechnical credit system that rewards and sanctions economic and social behaviors of individuals and companies. As a complex and transformative digital credit system, the SCS uses digital communication channels to inform the Chinese public about behaviors that lead to reward or sanction. Since 2017, the Chinese government has been publishing \"blameworthy\" and \"praiseworthy\" role model narratives of ordinary Chinese citizens on its central SCS information platform creditchina.gov.cn. Across many cultures, role model narratives are a known instrument to convey \"appropriate\" and \"inappropriate\" social norms. Using a directed content analysis methodology, we study the SCS-specific social norms embedded in 100 \"praiseworthy\" and 100 \"blameworthy\" role model narratives published on creditchina.gov.cn. \"Blameworthy\" role model narratives stress social norms associated with an \"immoral\" SCS identity label termed \"Lao Lai\" - a \"moral foe\" that fails to repay debt. SCS role model narratives familiarize Chinese society with SCS-specific measures such as digital surveillance, public shaming, and disproportionate punishment. Our study makes progress towards understanding how a state-run sociotechnical credit system combines digital tools with culturally familiar customs to propagate \"blameworthy\" and \"praiseworthy\" identities.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534180",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "181–191",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ordinary people as moral heroes and foes: Digital role model narratives propagate social norms in china's social credit system",
		"URL": "https://doi.org/10.1145/3514094.3534180",
		"author": [
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "clarkeSurveyPotentialLongterm2022",
		"type": "paper-conference",
		"abstract": "It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534131",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "192–202",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A survey of the potential long-term impacts of AI: How AI could lead to long-term changes in science, cooperation, power, epistemics and values",
		"URL": "https://doi.org/10.1145/3514094.3534131",
		"author": [
			{
				"family": "Clarke",
				"given": "Sam"
			},
			{
				"family": "Whittlestone",
				"given": "Jess"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiFairnessExplanationQuality2022",
		"type": "paper-conference",
		"abstract": "As post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to ensure that the quality of the resulting explanations is consistently high across all subgroups of a population. For instance, it should not be the case that explanations associated with instances belonging to, e.g., women, are less accurate than those associated with other genders. In this work, we initiate the study of identifying group-based disparities in explanation quality. To this end, we first outline several key properties that contribute to explanation quality-namely, fidelity (accuracy), stability, consistency, and sparsity-and discuss why and how disparities in these properties can be particularly problematic. We then propose an evaluation framework which can quantitatively measure disparities in the quality of explanations. Using this framework, we carry out an empirical analysis with three datasets, six post hoc explanation methods, and different model classes to understand if and when group-based disparities in explanation quality arise. Our results indicate that such disparities are more likely to occur when the models being explained are complex and non-linear. We also observe that certain post hoc explanation methods (e.g., Integrated Gradients, SHAP) are more likely to exhibit disparities. Our work sheds light on previously unexplored ways in which explanation methods may introduce unfairness in real world decision making.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534159",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "203–214",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness via explanation quality: Evaluating disparities in the quality of post hoc explanations",
		"URL": "https://doi.org/10.1145/3514094.3534159",
		"author": [
			{
				"family": "Dai",
				"given": "Jessica"
			},
			{
				"family": "Upadhyay",
				"given": "Sohini"
			},
			{
				"family": "Aivodji",
				"given": "Ulrich"
			},
			{
				"family": "Bach",
				"given": "Stephen H."
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "daiCounterfactualExplanationsPrediction2022",
		"type": "paper-conference",
		"abstract": "We compared two sorts of explanations for decisions made by an AI system: counterfactual explanations about how an outcome could have been different in the past, and prefactual explanations about how it could be different in the future. We examined the effects of these alternative explanation strategies on the accuracy of users' judgments about the AI app's predictions about an outcome (inferred from information about the causes), compared to the accuracy of their judgments about the app's diagnoses of a cause (inferred from information about the outcome). The tasks were based on a simulated SmartAgriculture decision support system for grass growth outcomes on dairy farms in Experiment 1, and for an analogous alien planet domain in Experiment 2. The two experiments, with 243 participants, also tested users' confidence in their decisions, and their satisfaction with the explanations. Users made more accurate diagnoses of the presence of causes based on information about their outcome, compared to predictions of an outcome given information about the presence of causes. Their predictions and diagnoses were helped equally by counterfactual explanations and prefactual ones.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534144",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "215–226",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Counterfactual explanations for prediction and diagnosis in XAI",
		"URL": "https://doi.org/10.1145/3514094.3534144",
		"author": [
			{
				"family": "Dai",
				"given": "Xinyue"
			},
			{
				"family": "Keane",
				"given": "Mark T."
			},
			{
				"family": "Shalloo",
				"given": "Laurence"
			},
			{
				"family": "Ruelle",
				"given": "Elodie"
			},
			{
				"family": "Byrne",
				"given": "Ruth M.J."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "deshpandeResponsibleAISystems2022",
		"type": "paper-conference",
		"abstract": "As of 2021, there were more than 170 guidelines on AI ethics and responsible, trustworthy AI in circulation according to the AI Ethics Guidelines Global Inventory maintained by AlgorithmWatch, an organisation which tracks the effects of increased digitalisation on everyday lives. However, from the perspective of day-to-day work, for those engaged in designing, developing, and maintaining AI systems identifying relevant guidelines and translating them into practice presents a challenge.The aim of this paper is to help anyone engaged in building a responsible AI system by identifying an indicative long-list of potential stakeholders. This list of impacted stakeholders is intended to enable such AI system builders to decide which guidelines are most suited to their practice. The paper draws on a literature review of articles short-listed based on searches conducted in the ACM Digital Library and Google Scholar. The findings are based on content analysis of the short-listed literature guided by probes which draw on the ISO 26000:2010 Guidance on social responsibility.The paper identifies three levels of potentially relevant stakeholders when responsible AI systems are considered: individual stakeholders (including users, developers, and researchers), organisational stakeholders, and national / international stakeholders engaged in making laws, rules, and regulations. The main intended audience for this paper is software, requirements, and product engineers engaged in building AI systems. In addition, business executives, policy makers, legal/regulatory experts, AI researchers, public, private, and third sector organisations developing responsible AI guidelines, and anyone interested in seeing functional responsible AI systems are the other intended audience for this paper.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534187",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "227–236",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Responsible AI systems: Who are the stakeholders?",
		"URL": "https://doi.org/10.1145/3514094.3534187",
		"author": [
			{
				"family": "Deshpande",
				"given": "Advait"
			},
			{
				"family": "Sharp",
				"given": "Helen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "drageDoesAIDebias2022",
		"type": "paper-conference",
		"abstract": "In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534151",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "237",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Does AI de-bias recruitment? Race, gender, and AI's 'eradication of differences between groups'",
		"URL": "https://doi.org/10.1145/3514094.3534151",
		"author": [
			{
				"family": "Drage",
				"given": "Eleanor"
			},
			{
				"family": "Mackereth",
				"given": "Kerry"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "engelmannSocialMediaProfiling2022",
		"type": "paper-conference",
		"abstract": "Social media platforms generate user profiles to recommend informational resources including targeted advertisements. The technical possibilities of user profiling methods go beyond the classification of individuals into types of potential customers. They enable the transformation of implicit identity claims of individuals into explicit declarations of identity. As such, a key ethical challenge of social media profiling is that it stands in contrast with people's ability to self-determine autonomously, a core principle of the right to informational self-determination.In this research study, we take a step back and revisit theories of personal identity in philosophy that underline two constitutive meta-principles necessary for individuals to self-interpret autonomously: justification and control. That is, individuals have the ability to justify and control essential aspects of their self-concept. Returning to a philosophical basis for the value of self-determination serves as a reminder that user profiling is essentially normative in that it formalizes a person's self-concept within an algorithmic system. To understand whether social media users would want to justify and control social media's identity declarations, we conducted a vignette survey study (N = 368). First, participants indicate a strong preference for more transparency in social media identity declarations, a core requirement for the justification of a self-concept. Second, respondents state they would correct wrong identity declarations but show no clear motivation to manage them. Finally, our results illustrate that social media users acknowledge the narrative force of social media profiling but do not strongly believe in its capacity to shape their self-concept.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534192",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "238–252",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social media profiling continues to partake in the development of formalistic self-concepts. Social media users think so, too.",
		"URL": "https://doi.org/10.1145/3514094.3534192",
		"author": [
			{
				"family": "Engelmann",
				"given": "Severin"
			},
			{
				"family": "Scheibe",
				"given": "Valentin"
			},
			{
				"family": "Battaglia",
				"given": "Fiorella"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "evansStochasticPoliciesMorally2022",
		"type": "paper-conference",
		"abstract": "Stochastic policies often outperform deterministic ones. This is especially true for Constrained Stochastic Shortest Path (C-SSP) problems, a popular approach to planning under uncertainty with multiple objectives. Nevertheless, there are moral concerns about stochastic policies that should deter us from selecting them. In this paper, we identify some of these moral concerns and offer 'acceptability constraints' that allow only certain stochastic policies to be selected. We propose a novel C-SSP solver able to integrate our moral acceptability constraints, we evaluate its performance in a relevant test problem, and we show that our approach can successfully produce acceptable policies in morally significant domains.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534193",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "253–264",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stochastic policies in morally constrained (C-)SSPs",
		"URL": "https://doi.org/10.1145/3514094.3534193",
		"author": [
			{
				"family": "Evans",
				"given": "Charles"
			},
			{
				"family": "Benn",
				"given": "Claire"
			},
			{
				"family": "Ojea Quintana",
				"given": "Ignacio"
			},
			{
				"family": "Robinson",
				"given": "Pamela"
			},
			{
				"family": "Thiébaux",
				"given": "Sylvie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "franklinOntologyFairnessMetrics2022",
		"type": "paper-conference",
		"abstract": "Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534137",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "265–275",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "An ontology for fairness metrics",
		"URL": "https://doi.org/10.1145/3514094.3534137",
		"author": [
			{
				"family": "Franklin",
				"given": "Jade S."
			},
			{
				"family": "Bhanot",
				"given": "Karan"
			},
			{
				"family": "Ghalwash",
				"given": "Mohamed"
			},
			{
				"family": "Bennett",
				"given": "Kristin P."
			},
			{
				"family": "McCusker",
				"given": "Jamie"
			},
			{
				"family": "McGuinness",
				"given": "Deborah L."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "franklinCausalFrameworkArtificial2022",
		"type": "paper-conference",
		"abstract": "Recent empirical work on people's attributions of responsibility toward artificial autonomous agents (such as Artificial Intelligence agents or robots) has delivered mixed findings. The conflicting results reflect differences in context, the roles of AI and human agents, and the domain of application. In this article, we outline a causal framework of responsibility attribution which integrates these findings. It outlines nine factors that influence responsibility attribution - causality, role, knowledge, objective foreseeability, capability, intent, desire, autonomy, and character. We propose a framework of responsibility that outlines the causal relationships between the nine factors and responsibility. To empirically test the framework we discuss some initial findings and outline an approach to using serious games for causal cognitive research on responsibility attribution. Specifically, we propose a game that uses a generative approach to creating different scenarios, in which participants can freely inspect different sources of information to make judgments about human and artificial autonomous agents.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534140",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 9\npublisher-place: Oxford, United Kingdom",
		"page": "276–284",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Causal framework of artificial autonomous agent responsibility",
		"URL": "https://doi.org/10.1145/3514094.3534140",
		"author": [
			{
				"family": "Franklin",
				"given": "Matija"
			},
			{
				"family": "Ashton",
				"given": "Hal"
			},
			{
				"family": "Awad",
				"given": "Edmond"
			},
			{
				"family": "Lagnado",
				"given": "David"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "garciademacedoPracticalSkillsDemand2022",
		"type": "paper-conference",
		"abstract": "Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534183",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "285–294",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Practical skills demand forecasting via representation learning of temporal dynamics",
		"URL": "https://doi.org/10.1145/3514094.3534183",
		"author": [
			{
				"family": "Garcia de Macedo",
				"given": "Maysa Malfiza"
			},
			{
				"family": "Clarke",
				"given": "Wyatt"
			},
			{
				"family": "Lucherini",
				"given": "Eli"
			},
			{
				"family": "Baldwin",
				"given": "Tyler"
			},
			{
				"family": "Queiroz Neto",
				"given": "Dilermando"
			},
			{
				"family": "Paula",
				"given": "Rogerio Abreu",
				"non-dropping-particle": "de"
			},
			{
				"family": "Das",
				"given": "Subhro"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "gemalmazUnderstandingDecisionSubjects2022",
		"type": "paper-conference",
		"abstract": "The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions—We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534201",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "295–306",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Understanding decision subjects' fairness perceptions and retention in repeated interactions with AI-Based decision systems",
		"URL": "https://doi.org/10.1145/3514094.3534201",
		"author": [
			{
				"family": "Gemalmaz",
				"given": "Meric Altug"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "ghoshFairCanaryRapidContinuous2022",
		"type": "paper-conference",
		"abstract": "Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534157",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "307–316",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "FairCanary: Rapid continuous explainable fairness",
		"URL": "https://doi.org/10.1145/3514094.3534157",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Shanbhag",
				"given": "Aalok"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "heLearningFairerInterventions2022",
		"type": "paper-conference",
		"abstract": "Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534172",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 7\npublisher-place: Oxford, United Kingdom",
		"page": "317–323",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning fairer interventions",
		"URL": "https://doi.org/10.1145/3514094.3534172",
		"author": [
			{
				"family": "He",
				"given": "Yuzi"
			},
			{
				"family": "Burghardt",
				"given": "Keith"
			},
			{
				"family": "Guo",
				"given": "Siyi"
			},
			{
				"family": "Lerman",
				"given": "Kristina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "helmDiversityDesignBalancing2022",
		"type": "paper-conference",
		"abstract": "The unreflected promotion of diversity as a value in social interactions — including in technology-mediated ones — risks emphasizing the benefits of inclusion without recognizing the potential harm of failing to protect vulnerable individuals or account for the empowerment of marginalized groups. Adopting the position that technology is not value-neutral, we seek to answer the question of how technology-mediated social platforms can accommodate diversity by design by balancing the often tension-ridden principles of protection and inclusion. In this paper, we present our research program, developed strategy, as well as first analyses and results. Building on approaches from scenario analysis and Value Sensitive Design, we identify key arguments for a \"diversity by design”-agenda. Furthermore, we discuss how these arguments can be operationalized and implemented in a diversity-aware chatbot and provide a critical reflection on the limits and drawbacks of the proposed approach.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534149",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "324–334",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diversity by design? Balancing the inclusion and protection of users in an online social platform",
		"URL": "https://doi.org/10.1145/3514094.3534149",
		"author": [
			{
				"family": "Helm",
				"given": "Paula"
			},
			{
				"family": "Michael",
				"given": "Loizos"
			},
			{
				"family": "Schelenz",
				"given": "Laura"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "hullmanWorstBothWorlds2022",
		"type": "paper-conference",
		"abstract": "Arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in research cannot be taken at face value. Concerns inspire analogies to the replication crisis affecting the social and medical sciences. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid \"the worst of both worlds,\" where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify common themes in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often difficult to refute due to underspecification of key parts of the learning pipeline. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534196",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "335–348",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The worst of both worlds: A comparative analysis of errors in learning from data in psychology and machine learning",
		"URL": "https://doi.org/10.1145/3514094.3534196",
		"author": [
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Gelman",
				"given": "Andrew"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kasirzadehAlgorithmicFairnessStructural2022a",
		"type": "paper-conference",
		"abstract": "Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by – or substantively rooted in – ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices – as pioneered in the contemporary philosophical literature by Iris Marion Young – to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534188",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "349–356",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithmic fairness and structural injustice: Insights from feminist political philosophy",
		"URL": "https://doi.org/10.1145/3514094.3534188",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kumarEqualizingCreditOpportunity2022",
		"type": "paper-conference",
		"abstract": "Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of \"unfairness,\" thus raising the concern that banks and other financial institutions could—potentially unwittingly—engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534154",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "357–368",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Equalizing credit opportunity in algorithms: Aligning algorithmic fairness research with U.S. fair lending regulation",
		"URL": "https://doi.org/10.1145/3514094.3534154",
		"author": [
			{
				"family": "Kumar",
				"given": "I. Elizabeth"
			},
			{
				"family": "Hines",
				"given": "Keegan E."
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "kusumaCivilWarTwin2022",
		"type": "paper-conference",
		"abstract": "Facial recognition systems pose numerous ethical challenges around privacy, racial and gender bias, and accuracy, yet little guidance is available for designers and developers. We explore solutions to these challenges in a three-phase design process to create Civil War Twin (CWT), an educational web-based application where users can discover their lookalikes from the American Civil War era (1861–65) while learning more about facial recognition and history. Through this design process, we operationalize a framework for AI literacy, consult with scholars of history, gender, and race, and evaluate CWT in feedback sessions with diverse prospective users. We iteratively formulate design goals to incorporate transparency, inclusivity, speculative design, and empathy into our application. We found that users' perceived learning about the strengths and limitations of facial recognition and Civil War history improved after using CWT, and that our design successfully met users' ethical standards. We also discuss how our ethical design process can be applied to future facial recognition applications.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534141",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "369–384",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Civil war twin: Exploring ethical challenges in designing an educational face recognition application",
		"URL": "https://doi.org/10.1145/3514094.3534141",
		"author": [
			{
				"family": "Kusuma",
				"given": "Manisha"
			},
			{
				"family": "Mohanty",
				"given": "Vikram"
			},
			{
				"family": "Wang",
				"given": "Marx"
			},
			{
				"family": "Luther",
				"given": "Kurt"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "langenkampHowOpenSource2022",
		"type": "paper-conference",
		"abstract": "If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least 100ofglobaleconomicvaluecreated,correspondingto30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534167",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "385–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How open source machine learning software shapes AI",
		"URL": "https://doi.org/10.1145/3514094.3534167",
		"author": [
			{
				"family": "Langenkamp",
				"given": "Max"
			},
			{
				"family": "Yue",
				"given": "Daniel N."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liDatacentricFactorsAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534147",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "396–410",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Data-centric factors in algorithmic fairness",
		"URL": "https://doi.org/10.1145/3514094.3534147",
		"author": [
			{
				"family": "Li",
				"given": "Nianyun"
			},
			{
				"family": "Goel",
				"given": "Naman"
			},
			{
				"family": "Ash",
				"given": "Elliott"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liBetterDetectionBiased2022",
		"type": "paper-conference",
		"abstract": "Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier—we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534142",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "411–423",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards better detection of biased language with scarce, noisy, and biased annotations",
		"URL": "https://doi.org/10.1145/3514094.3534142",
		"author": [
			{
				"family": "Li",
				"given": "Zhuoyan"
			},
			{
				"family": "Lu",
				"given": "Zhuoran"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuExaminingResponsibilityDeliberation2022",
		"type": "paper-conference",
		"abstract": "The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534155",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "424–435",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Examining responsibility and deliberation in AI impact statements and ethics reviews",
		"URL": "https://doi.org/10.1145/3514094.3534155",
		"author": [
			{
				"family": "Liu",
				"given": "David"
			},
			{
				"family": "Nanayakkara",
				"given": "Priyanka"
			},
			{
				"family": "Sakha",
				"given": "Sarah Ariyan"
			},
			{
				"family": "Abuhamad",
				"given": "Grace"
			},
			{
				"family": "Blodgett",
				"given": "Su Lin"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			},
			{
				"family": "Hullman",
				"given": "Jessica R."
			},
			{
				"family": "Eliassi-Rad",
				"given": "Tina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "liuArtificialMoralAdvisors2022",
		"type": "paper-conference",
		"abstract": "Philosophers have recently put forward the possibility of achieving moral enhancement through artificial intelligence (e.g., Giubilini and Savulescu's version [32]), proposing various forms of \"artificial moral advisor\" (AMA) to help people make moral decisions without the drawbacks of human cognitive limitations. In this paper, we provide a new perspective on the AMA, drawing on empirical evidence from moral psychology to point out several challenges to these proposals that have been largely neglected by AI ethicists. In particular, we suggest that the AMA at its current conception is fundamentally misaligned with human moral psychology - it incorrectly assumes a static moral values framework underpinning the AMA's attunement to individual users, and people's reactions and subsequent (in)actions in response to the AMA suggestions will likely diverge substantially from expectations. As such, we note the necessity for a coherent understanding of human moral psychology in the future development of AMAs.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534139",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "436–445",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial moral advisors: A new perspective from moral psychology",
		"URL": "https://doi.org/10.1145/3514094.3534139",
		"author": [
			{
				"family": "Liu",
				"given": "Yuxin"
			},
			{
				"family": "Moore",
				"given": "Adam"
			},
			{
				"family": "Webb",
				"given": "Jamie"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "longoniArtificialIntelligenceGovernment2022",
		"type": "paper-conference",
		"abstract": "Artificial Intelligence (AI) is pervading the government and transforming how public services are provided to consumers—from allocation of benefits to law enforcement, risk monitoring and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI's failures? In thirteen preregistered studies (N = 3,724), we document a robust effect of algorithmic transference: algorithmic failures are generalized more broadly than human failures. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans—as outgroups characterized by greater homogeneity than ingroups of comparable humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm at a higher rate than failure information about a person is transferred to another person. Assessing AI's impact on consumers and societies, we show how the premature or mismanaged deployment of faulty AI technologies may engender algorithmic transference and undermine the very institutions that AI systems are meant to modernize.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534125",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "446",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial intelligence in the government: Responses to failures and social impact",
		"URL": "https://doi.org/10.1145/3514094.3534125",
		"author": [
			{
				"family": "Longoni",
				"given": "Chiara"
			},
			{
				"family": "Cian",
				"given": "Luca"
			},
			{
				"family": "Kyung",
				"given": "Ellie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "loreggiaMakingHumanlikeMoral2022",
		"type": "paper-conference",
		"abstract": "Many real-life scenarios require humans to make difficult trade-offs: do we always follow all the traffic rules or do we violate the speed limit in an emergency? In general, how should we account for and balance the ethical values, safety recommendations, and societal norms, when we are trying to achieve a certain objective? To enable effective AI-human collaboration, we must equip AI agents with a model of how humans make such trade-offs in environments where there is not only a goal to be reached, but there are also ethical constraints to be considered and to possibly align with. These ethical constraints could be both deontological rules on actions that should not be performed, or also consequentialist policies that recommend avoiding reaching certain states of the world. Our purpose is to build AI agents that can mimic human behavior in these ethically constrained decision environments, with a long term research goal to use AI to help humans in making better moral judgments and actions. To this end, we propose a computational approach where competing objectives and ethical constraints are orchestrated through a method that leverages a cognitive model of human decision making, called multi-alternative decision field theory (MDFT). Using MDFT, we build an orchestrator, called MDFT-Orchestrator (MDFT-O), that is both general and flexible. We also show experimentally that MDFT-O both generates better decisions than using a heuristic that takes a weighted average of competing policies (WA-O), but also performs better in terms of mimicking human decisions as collected through Amazon Mechanical Turk (AMT). Our methodology is therefore able to faithfully model human decision in ethically constrained decision environments.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534174",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "447–454",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Making human-like moral decisions",
		"URL": "https://doi.org/10.1145/3514094.3534174",
		"author": [
			{
				"family": "Loreggia",
				"given": "Andrea"
			},
			{
				"family": "Mattei",
				"given": "Nicholas"
			},
			{
				"family": "Rahgooy",
				"given": "Taher"
			},
			{
				"family": "Rossi",
				"given": "Francesca"
			},
			{
				"family": "Srivastava",
				"given": "Biplav"
			},
			{
				"family": "Venable",
				"given": "Kristen Brent"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534148",
		"type": "paper-conference",
		"abstract": "The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534148",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "455–467",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "DIME: Fine-grained interpretations of multimodal models via disentangled local explanations",
		"URL": "https://doi.org/10.1145/3514094.3534148",
		"author": [
			{
				"family": "Lyu",
				"given": "Yiwei"
			},
			{
				"family": "Liang",
				"given": "Paul Pu"
			},
			{
				"family": "Deng",
				"given": "Zihao"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Morency",
				"given": "Louis-Philippe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534170",
		"type": "paper-conference",
		"abstract": "During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534170",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "468–478",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Investigating debiasing effects on classification and explainability",
		"URL": "https://doi.org/10.1145/3514094.3534170",
		"author": [
			{
				"family": "Marchiori Manerba",
				"given": "Marta"
			},
			{
				"family": "Guidotti",
				"given": "Riccardo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534177",
		"type": "paper-conference",
		"abstract": "An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534177",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "479–490",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mimetic models: Ethical implications of AI that acts like you",
		"URL": "https://doi.org/10.1145/3514094.3534177",
		"author": [
			{
				"family": "McIlroy-Young",
				"given": "Reid"
			},
			{
				"family": "Kleinberg",
				"given": "Jon"
			},
			{
				"family": "Sen",
				"given": "Siddhartha"
			},
			{
				"family": "Barocas",
				"given": "Solon"
			},
			{
				"family": "Anderson",
				"given": "Ashton"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534175",
		"type": "paper-conference",
		"abstract": "This paper offers preliminary reflections on the sustainability tensions present in Artificial Intelligence (AI) and suggests that Paradox Theory, an approach borrowed from the strategic management literature, may help guide scholars towards innovative solutions. The benefits of AI to our society are well documented. Yet those benefits come at environmental and sociological cost, a fact which is often overlooked by mainstream scholars and practitioners. After examining the nascent corpus of literature on the sustainability tensions present in AI, this paper introduces the Accuracy - Energy Paradox and suggests how the principles of paradox theory can guide the AI community to a more sustainable solution.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534175",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "491–498",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Managing sustainability tensions in artificial intelligence: Insights from paradox theory",
		"URL": "https://doi.org/10.1145/3514094.3534175",
		"author": [
			{
				"family": "Mill",
				"given": "Eleanor"
			},
			{
				"family": "Garn",
				"given": "Wolfgang"
			},
			{
				"family": "Ryman-Tubb",
				"given": "Nick"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534143",
		"type": "paper-conference",
		"abstract": "The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534143",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 9\npublisher-place: Oxford, United Kingdom",
		"page": "499–507",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Contrastive counterfactual fairness in algorithmic decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534143",
		"author": [
			{
				"family": "Mutlu",
				"given": "Ece Çiğdem"
			},
			{
				"family": "Yousefi",
				"given": "Niloofar"
			},
			{
				"family": "Ozmen Garibay",
				"given": "Ozlem"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534165",
		"type": "paper-conference",
		"abstract": "Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534165",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "508–517",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How does predictive information affect human ethical preferences?",
		"URL": "https://doi.org/10.1145/3514094.3534165",
		"author": [
			{
				"family": "Narayanan",
				"given": "Saumik"
			},
			{
				"family": "Yu",
				"given": "Guanghui"
			},
			{
				"family": "Tang",
				"given": "Wei"
			},
			{
				"family": "Ho",
				"given": "Chien-Ju"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534176",
		"type": "paper-conference",
		"abstract": "Does the grammatical gender of a language interfere when measuring the semantic gender information captured by its word embeddings? A number of anomalous gender bias measurements in the embeddings of gendered languages suggest this possibility. We demonstrate that word embeddings learn the association between a noun and its grammatical gender in grammatically gendered languages, which can skew social gender bias measurements. Consequently, word embedding post-processing methods are introduced to quantify, disentangle, and evaluate grammatical gender signals. The evaluation is performed on five gendered languages from the Germanic, Romance, and Slavic branches of the Indo-European language family. Our method reduces the strength of grammatical gender signals, which is measured in terms of effect size (Cohen's d ), by a significant average of d = 1.3 for French, German, and Italian, and d = 0.56 for Polish and Spanish. Once grammatical gender is disentangled, the association between over 90% of 10,000 inanimate nouns and their assigned grammatical gender weakens, and cross-lingual bias results from the Word Embedding Association Test (WEAT) become more congruent with country-level implicit bias measurements. The results further suggest that disentangling grammatical gender signals from word embeddings may lead to improvement in semantic machine learning tasks.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534176",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "518–531",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measuring gender bias in word embeddings of gendered languages requires disentangling grammatical gender signals",
		"URL": "https://doi.org/10.1145/3514094.3534176",
		"author": [
			{
				"family": "Omrani Sabbaghi",
				"given": "Shiva"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534166",
		"type": "paper-conference",
		"abstract": "Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534166",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "532–546",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How algorithms shape the distribution of political advertising: Case studies of facebook, google, and TikTok",
		"URL": "https://doi.org/10.1145/3514094.3534166",
		"author": [
			{
				"family": "Papakyriakopoulos",
				"given": "Orestis"
			},
			{
				"family": "Tessono",
				"given": "Christelle"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			},
			{
				"family": "Kshirsagar",
				"given": "Mihir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534127",
		"type": "paper-conference",
		"abstract": "With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (\"greedy”) against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (\"fair”). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534127",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "547–556",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A dynamic decision-making framework promoting long-term fairness",
		"URL": "https://doi.org/10.1145/3514094.3534127",
		"author": [
			{
				"family": "Puranik",
				"given": "Bhagyashree"
			},
			{
				"family": "Madhow",
				"given": "Upamanyu"
			},
			{
				"family": "Pedarsani",
				"given": "Ramtin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "10.1145/3514094.3534181",
		"type": "paper-conference",
		"abstract": "Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534181",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "557–571",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Outsider oversight: Designing a third party audit ecosystem for AI governance",
		"URL": "https://doi.org/10.1145/3514094.3534181",
		"author": [
			{
				"family": "Raji",
				"given": "Inioluwa Deborah"
			},
			{
				"family": "Xu",
				"given": "Peggy"
			},
			{
				"family": "Honigsberg",
				"given": "Colleen"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "rheaResumeFormatLinkedIn2022",
		"type": "paper-conference",
		"abstract": "Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers' resumes or social media profiles. We interrogate the reliability of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. We develop a methodology for an external audit of stability of algorithmic personality tests, and instantiate this methodology in an audit of two systems, Humantic AI and Crystal. Rather than challenging or affirming the assumptions made in psychometric testing – that personality traits are meaningful and measurable constructs, and that they are indicative of future success on the job – we frame our methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves.In our audit of Humantic AI and Crystal, we find that both systems show substantial instability on key facets of measurement, and so cannot be considered valid testing instruments. For example, Crystal frequently computes different personality scores if the same resume is given in PDF vs. in raw text, violating the assumption that the output of an algorithmic personality test is stable across job-irrelevant input variations. Among other notable findings is evidence of persistent — and often incorrect — data linkage by Humantic AI.An open-source implementation of our auditing methodology, and of the audits of Humantic AI and Crystal, is available at https://github.com/DataResponsibly/hiring-stability-audit.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534189",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "572–587",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Resume format, LinkedIn URLs and other unexpected influences on AI personality prediction in hiring: Results of an audit",
		"URL": "https://doi.org/10.1145/3514094.3534189",
		"author": [
			{
				"family": "Rhea",
				"given": "Alene"
			},
			{
				"family": "Markey",
				"given": "Kelsey"
			},
			{
				"family": "D'Arinzo",
				"given": "Lauren"
			},
			{
				"family": "Schellmann",
				"given": "Hilke"
			},
			{
				"family": "Sloane",
				"given": "Mona"
			},
			{
				"family": "Squires",
				"given": "Paul"
			},
			{
				"family": "Stoyanovich",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "robertsonBioinspiredFrameworkMachine2022",
		"type": "paper-conference",
		"abstract": "Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534126",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "588–598",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A bio-inspired framework for machine bias interpretation",
		"URL": "https://doi.org/10.1145/3514094.3534126",
		"author": [
			{
				"family": "Robertson",
				"given": "Jake"
			},
			{
				"family": "Stinson",
				"given": "Catherine"
			},
			{
				"family": "Hu",
				"given": "Ting"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sainiSelectWiselyExplain2022",
		"type": "paper-conference",
		"abstract": "Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active learning based locally faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534191",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "599–608",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Select wisely and explain: Active learning and probabilistic local post-hoc explainability",
		"URL": "https://doi.org/10.1145/3514094.3534191",
		"author": [
			{
				"family": "Saini",
				"given": "Aditya"
			},
			{
				"family": "Prasad",
				"given": "Ranjitha"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sapiezynskiAlgorithmsThatDon2022",
		"type": "paper-conference",
		"abstract": "Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534135",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "609–616",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Algorithms that \"Don't See Color\": Measuring biases in lookalike and special ad audiences",
		"URL": "https://doi.org/10.1145/3514094.3534135",
		"author": [
			{
				"family": "Sapiezynski",
				"given": "Piotr"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kaplan",
				"given": "Levi"
			},
			{
				"family": "Rieke",
				"given": "Aaron"
			},
			{
				"family": "Mislove",
				"given": "Alan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schemmerMetaanalysisUtilityExplainable2022",
		"type": "paper-conference",
		"abstract": "Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534128",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "617–626",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A meta-analysis of the utility of explainable artificial intelligence in human-AI decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534128",
		"author": [
			{
				"family": "Schemmer",
				"given": "Max"
			},
			{
				"family": "Hemmer",
				"given": "Patrick"
			},
			{
				"family": "Nitsche",
				"given": "Maximilian"
			},
			{
				"family": "Kühl",
				"given": "Niklas"
			},
			{
				"family": "Vössing",
				"given": "Michael"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "schopmansCodedBiasExistential2022",
		"type": "paper-conference",
		"abstract": "While the knowledge produced by experts has been widely recognized to play a salient role in shaping policy on technological issues, the interaction between AI expertise and the evolving AI governance landscape has received little attention thus far. To address this gap, the present paper leverages insights from STS and International Relations to explore how different expert communities have constructed AI as a governance problem. More specifically, it presents the preliminary results of a qualitative frame analysis of 90 policy documents published by experts from industry, civil society, and the research community. The analysis finds that AI expertise is a highly contested field, as experts not only disagree on why AI is problematic and what policies are required, but, more fundamentally, about which artifacts, ideas, and practices make up AI in the first place. The paper proposes that the epistemic disagreements concerning AI have political consequences, as they engender protracted ontological politics that jeopardize the development of effective governance interventions. Against this background, the findings raise critical questions about the prevailing tendency of governance interventions to target the elusive and contested object 'artificial intelligence.'",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534161",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "627–640",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From coded bias to existential threat: Expert frames and the epistemic politics of AI governance",
		"URL": "https://doi.org/10.1145/3514094.3534161",
		"author": [
			{
				"family": "Schopmans",
				"given": "Hendrik R."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "seymourRespectLensDesign2022",
		"type": "paper-conference",
		"abstract": "Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534186",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "641–652",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Respect as a lens for the design of AI systems",
		"URL": "https://doi.org/10.1145/3514094.3534186",
		"author": [
			{
				"family": "Seymour",
				"given": "William"
			},
			{
				"family": "Van Kleek",
				"given": "Max"
			},
			{
				"family": "Binns",
				"given": "Reuben"
			},
			{
				"family": "Murray-Rust",
				"given": "Dave"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shea-blymyerGeneratingDeonticObligations2022",
		"type": "paper-conference",
		"abstract": "This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534163",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "653–663",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Generating deontic obligations from utility-maximizing systems",
		"URL": "https://doi.org/10.1145/3514094.3534163",
		"author": [
			{
				"family": "Shea-Blymyer",
				"given": "Colin"
			},
			{
				"family": "Abbas",
				"given": "Houssam"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "shimaoStrategicBestResponse2022",
		"type": "paper-conference",
		"abstract": "While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called \"strategic best-response fairness\" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534194",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 1\npublisher-place: Oxford, United Kingdom",
		"page": "664",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Strategic best response fairness in fair machine learning",
		"URL": "https://doi.org/10.1145/3514094.3534194",
		"author": [
			{
				"family": "Shimao",
				"given": "Hajime"
			},
			{
				"family": "Khern-am-nuai",
				"given": "Warut"
			},
			{
				"family": "Kannan",
				"given": "Karthik"
			},
			{
				"family": "Cohen",
				"given": "Maxime C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "siapkaFeministMetaethicsAI2022",
		"type": "paper-conference",
		"abstract": "The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534197",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "665–674",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards a feminist metaethics of AI",
		"URL": "https://doi.org/10.1145/3514094.3534197",
		"author": [
			{
				"family": "Siapka",
				"given": "Anastasia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "simonettoAchievementFragilityLongterm2022",
		"type": "paper-conference",
		"abstract": "Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534132",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "675–685",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Achievement and fragility of long-term equitability",
		"URL": "https://doi.org/10.1145/3514094.3534132",
		"author": [
			{
				"family": "Simonetto",
				"given": "Andrea"
			},
			{
				"family": "Notarnicola",
				"given": "Ivano"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "singhRobustOffpolicyEvaluation2022a",
		"type": "paper-conference",
		"abstract": "Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534198",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "686–699",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards robust off-policy evaluation via human inputs",
		"URL": "https://doi.org/10.1145/3514094.3534198",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Joshi",
				"given": "Shalmali"
			},
			{
				"family": "Doshi-Velez",
				"given": "Finale"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "skorupaparolinMultiCoPEDMultilingualMultitask2022",
		"type": "paper-conference",
		"abstract": "Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3% and 30.7% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534178",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "700–711",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multi-CoPED: A multilingual multi-task approach for coding political event data on conflict and mediation domain",
		"URL": "https://doi.org/10.1145/3514094.3534178",
		"author": [
			{
				"family": "Skorupa Parolin",
				"given": "Erick"
			},
			{
				"family": "Hosseini",
				"given": "MohammadSaleh"
			},
			{
				"family": "Hu",
				"given": "Yibo"
			},
			{
				"family": "Khan",
				"given": "Latifur"
			},
			{
				"family": "Brandt",
				"given": "Patrick T."
			},
			{
				"family": "Osorio",
				"given": "Javier"
			},
			{
				"family": "D'Orazio",
				"given": "Vito"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "sullivanExplanationRecommendationEthical2022",
		"type": "paper-conference",
		"abstract": "People are increasingly subject to algorithmic decisions, and it is generally agreed that end-users should be provided an explanation or rationale for these decisions. There are different purposes that explanations can have, such as increasing user trust in the system or allowing users to contest the decision. One specific purpose that is gaining more traction is algorithmic recourse. We first propose that recourse should be viewed as a recommendation problem, not an explanation problem. Then, we argue that the capability approach provides plausible and fruitful ethical standards for recourse. We illustrate by considering the case of diversity constraints on algorithmic recourse. Finally, we discuss the significance and implications of adopting the capability approach for algorithmic recourse research.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534185",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "712–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From explanation to recommendation: Ethical standards for algorithmic recourse",
		"URL": "https://doi.org/10.1145/3514094.3534185",
		"author": [
			{
				"family": "Sullivan",
				"given": "Emily"
			},
			{
				"family": "Verreault-Julien",
				"given": "Philippe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "tongWhatArePeople2022",
		"type": "paper-conference",
		"abstract": "Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534202",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 16\npublisher-place: Oxford, United Kingdom",
		"page": "723–738",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What are people talking about in #BackLivesMatter and #StopAsianHate? Exploring and categorizing twitter topics emerged in online social movements through the latent dirichlet allocation model",
		"URL": "https://doi.org/10.1145/3514094.3534202",
		"author": [
			{
				"family": "Tong",
				"given": "Xin"
			},
			{
				"family": "Li",
				"given": "Yixuan"
			},
			{
				"family": "Li",
				"given": "Jiayi"
			},
			{
				"family": "Bei",
				"given": "Rongqi"
			},
			{
				"family": "Zhang",
				"given": "Luyao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "triantafyllouActualCausalityResponsibility2022",
		"type": "paper-conference",
		"abstract": "Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534133",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "739–752",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Actual causality and responsibility attribution in decentralized partially observable markov decision processes",
		"URL": "https://doi.org/10.1145/3514094.3534133",
		"author": [
			{
				"family": "Triantafyllou",
				"given": "Stelios"
			},
			{
				"family": "Singla",
				"given": "Adish"
			},
			{
				"family": "Radanovic",
				"given": "Goran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "unruhHumanAutonomyAlgorithmic2022",
		"type": "paper-conference",
		"abstract": "Algorithmic management tools support or replace managerial decision making in areas such as task allocation, shift planning, or team formation. These tools can have a significant impact on the lives of workers. In this paper, we contribute to the emerging literature on the ethics of algorithmic management by developing a conceptual framework for autonomy at work. Further, we use this framework to discuss risks and opportunities for autonomy in the context of work decision algorithms in Industry 4.0.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534168",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "753–762",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human autonomy in algorithmic management",
		"URL": "https://doi.org/10.1145/3514094.3534168",
		"author": [
			{
				"family": "Unruh",
				"given": "Charlotte Franziska"
			},
			{
				"family": "Haid",
				"given": "Charlotte"
			},
			{
				"family": "Johannes",
				"given": "Fottner"
			},
			{
				"family": "Büthe",
				"given": "Tim"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "vodrahalliHumansTrustAdvice2022",
		"type": "paper-conference",
		"abstract": "In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, \"activation-integration\" model for human behavior and use it to characterize the factors that affect human-AI interactions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534150",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 15\npublisher-place: Oxford, United Kingdom",
		"page": "763–777",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Do humans trust advice more if it comes from AI? An analysis of human-AI interactions",
		"URL": "https://doi.org/10.1145/3514094.3534150",
		"author": [
			{
				"family": "Vodrahalli",
				"given": "Kailas"
			},
			{
				"family": "Daneshjou",
				"given": "Roxana"
			},
			{
				"family": "Gerstenberg",
				"given": "Tobias"
			},
			{
				"family": "Zou",
				"given": "James"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wanExplainabilityGainOptimality2022",
		"type": "paper-conference",
		"abstract": "Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations' semantics of causal models, however, induce leakage from the decision-maker's prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decision-maker's confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534156",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "778–787",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Explainability's gain is optimality's loss? How explanations bias decision-making",
		"URL": "https://doi.org/10.1145/3514094.3534156",
		"author": [
			{
				"family": "Wan",
				"given": "Charles"
			},
			{
				"family": "Belo",
				"given": "Rodrigo"
			},
			{
				"family": "Zejnilovic",
				"given": "Leid"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "winecoffArtificialConceptsArtificial2022",
		"type": "paper-conference",
		"abstract": "Scholars and industry practitioners have debated how to best develop interventions for ethical artificial intelligence (AI). Such interventions recommend that companies building and using AI tools change their technical practices, but fail to wrangle with critical questions about the organizational and institutional context in which AI is developed. In this paper, we contribute descriptive research around the life of \"AI\" as a discursive concept and organizational practice in an understudied sphere–emerging AI startups–and with a focus on extra-organizational pressures faced by entrepreneurs. Leveraging a theoretical lens for how organizations change, we conducted semi-structured interviews with 23 entrepreneurs working at early-stage AI startups. We find that actors within startups both conform to and resist institutional pressures. Our analysis identifies a central tension for AI entrepreneurs: they often valued scientific integrity and methodological rigor; however, influential external stakeholders either lacked the technical knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused on business priorities. As a result, entrepreneurs adopted hyped marketing messages about AI that diverged from their scientific values, but attempted to preserve their legitimacy internally. Institutional pressures and organizational constraints also influenced entrepreneurs' modeling practices and their response to actual or impending regulation. We conclude with a discussion for how such pressures could be used as leverage for effective interventions towards building ethical AI.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534138",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 12\npublisher-place: Oxford, United Kingdom",
		"page": "788–799",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Artificial concepts of artificial intelligence: Institutional compliance and resistance in AI startups",
		"URL": "https://doi.org/10.1145/3514094.3534138",
		"author": [
			{
				"family": "Winecoff",
				"given": "Amy A."
			},
			{
				"family": "Watkins",
				"given": "Elizabeth Anne"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "wolfeAmericanWhiteMultimodal2022",
		"type": "paper-conference",
		"abstract": "Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes &gt;.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text \"an American person,\" a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534136",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "800–812",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "American == white in multimodal language-and-image AI",
		"URL": "https://doi.org/10.1145/3514094.3534136",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yangEnhancingFairnessFace2022",
		"type": "paper-conference",
		"abstract": "Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534153",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "813–822",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Enhancing fairness in face detection in computer vision systems by demographic bias mitigation",
		"URL": "https://doi.org/10.1145/3514094.3534153",
		"author": [
			{
				"family": "Yang",
				"given": "Yu"
			},
			{
				"family": "Gupta",
				"given": "Aayush"
			},
			{
				"family": "Feng",
				"given": "Jianwei"
			},
			{
				"family": "Singhal",
				"given": "Prateek"
			},
			{
				"family": "Yadav",
				"given": "Vivek"
			},
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Natarajan",
				"given": "Pradeep"
			},
			{
				"family": "Hedau",
				"given": "Varsha"
			},
			{
				"family": "Joo",
				"given": "Jungseock"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yewPenaltyDefaultApproach2022",
		"type": "paper-conference",
		"abstract": "As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534130",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 8\npublisher-place: Oxford, United Kingdom",
		"page": "823–830",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A penalty default approach to preemptive harm disclosure and mitigation for AI systems",
		"URL": "https://doi.org/10.1145/3514094.3534130",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "yikIdentifyingBiasData2022",
		"type": "paper-conference",
		"abstract": "As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a \"closest plausible explanation\" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534169",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 14\npublisher-place: Oxford, United Kingdom",
		"page": "831–844",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Identifying bias in data using two-distribution hypothesis tests",
		"URL": "https://doi.org/10.1145/3514094.3534169",
		"author": [
			{
				"family": "Yik",
				"given": "William"
			},
			{
				"family": "Serafini",
				"given": "Limnanthes"
			},
			{
				"family": "Lindsey",
				"given": "Timothy"
			},
			{
				"family": "Montañez",
				"given": "George D."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhanModelGoverningInformation2022",
		"type": "paper-conference",
		"abstract": "Smart Personal Assistants (SPAs), such as Amazon Alexa, Google Assistant and Apple Siri, leverage different AI techniques to provide convenient help and assistance to users. However, inappropriate information sharing decisions can lead SPAs to incorrectly disclose user information to undesired parties, or mistakenly block their reasonable access in specific scenarios to desired parties. In fact, reports about privacy violations in SPAs and associated user concerns are well known and understood in the related literature. It is difficult for SPAs to automatically decide how data should be shared with respect to the privacy preferences of the users. We argue norms, which are regarded as shared standards of acceptable behaviour of groups and/or individuals, can be used to govern and reason about the best course of action of SPAs with regards to information sharing, and our work is the first to propose a practical model to address the above issues and govern SPAs based on normative systems and the contextual integrity theory of privacy. We evaluated the performance of the model using a real dataset of user preferences for privacy in SPAs and the results showed a very marked and significant improvement in understanding user preferences and making the right decisions with respect to data sharing.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534129",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "845–855",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A model for governing information sharing in smart assistants",
		"URL": "https://doi.org/10.1145/3514094.3534129",
		"author": [
			{
				"family": "Zhan",
				"given": "Xiao"
			},
			{
				"family": "Sarkadi",
				"given": "Stefan"
			},
			{
				"family": "Criado",
				"given": "Natalia"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangNoRageMachines2022",
		"type": "paper-conference",
		"abstract": "Labor-saving technology has already decreased employment opportunities for middle-skill workers. Experts anticipate that advances in AI and robotics will cause even more significant disruptions in the labor market over the next two decades. This paper presents three experimental studies that investigate how this profound economic change could affect mass politics. Recent observational studies suggest that workers' exposure to automation risk predicts their support not only for redistribution but also for right-wing populist policies and candidates. Other observational studies, including my own, find that workers underestimate the impact of automation on their job security. Misdirected blame towards immigrants and workers in foreign countries, rather than concerns about workplace automation, could be driving support for right-wing populism. To correct American workers' beliefs about the threats to their jobs, I conducted three survey experiments in which I informed workers about the existent and future impact of workplace automation. While these informational treatments convinced workers that automation threatens American jobs, they failed to change respondents' preferences on welfare, immigration, and trade policies. My research finds that raising awareness about workplace automation did not decrease opposition to globalization or increase support for policies that will prepare workers for future technological disruptions.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534179",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 11\npublisher-place: Oxford, United Kingdom",
		"page": "856–866",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No rage against the machines: Threat of automation does not change policy preferences",
		"URL": "https://doi.org/10.1145/3514094.3534179",
		"author": [
			{
				"family": "Zhang",
				"given": "Baobao"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zhangJudgingInstinctExploitation2022",
		"type": "paper-conference",
		"abstract": "This paper proposes 18 types of statistical data explanations and three kinds of procedures to investigate credibility in unethical and biased explanations due to exploitation of the 10 instincts proposed by Rosling et al. The explanation \"women have lower math scores than men” accompanied with the averages and the distributions of their scores is an example of such an explanation, as it exploits the gap instinct, i.e., our tendency to divide all kinds of things into two distinct and often conflicting groups. It becomes much less credible if we replace the word \"math” with \"English”, even if we keep the data as they are, as the exploitation seems to fail. Our judging procedures are based on phrase embedding and carefully designed comparisons to judge the credibility. The results of our experiments comparing the 18 types with their variants show promising results and clues for further developments.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534171",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 13\npublisher-place: Oxford, United Kingdom",
		"page": "867–879",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Judging instinct exploitation in statistical data explanations based on word embedding",
		"URL": "https://doi.org/10.1145/3514094.3534171",
		"author": [
			{
				"family": "Zhang",
				"given": "Kang"
			},
			{
				"family": "Shinden",
				"given": "Hiroaki"
			},
			{
				"family": "Mutsuro",
				"given": "Tatsuki"
			},
			{
				"family": "Suzuki",
				"given": "Einoshin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "zilkaTransparencyGovernanceRegulation2022",
		"type": "paper-conference",
		"abstract": "We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",
		"collection-title": "AIES '22",
		"container-title": "Proceedings of the 2022 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3514094.3534200",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-9247-1",
		"note": "number-of-pages: 10\npublisher-place: Oxford, United Kingdom",
		"page": "880–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Transparency, governance and regulation of algorithmic tools deployed in the criminal justice system: a UK case study",
		"URL": "https://doi.org/10.1145/3514094.3534200",
		"author": [
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "Sargeant",
				"given": "Holli"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "cookProtectingChildrenOnline2023",
		"type": "paper-conference",
		"abstract": "The growing popularity of social media raises concerns about children’s online safety. Of particular concern are interactions between minors and adults with predatory intentions. Unfortunately, previous research on online sexual grooming has relied on time-intensive manual annotation by domain experts, limiting both the scale and scope of possible interventions. This work explores the possibility of detecting predatory behaviours with accuracy comparable to expert annotators using machine learning (ML). Using a dataset of 6771 chat messages sent by child sex offenders, labelled by two of the authors who are forensic psychology experts, we study how well can deep learning algorithms identify eleven known predatory behaviours. We find that the best-performing ML models are consistent but not on par with expert annotation. We therefore consider a system where an expert annotator validates the ML algorithms outputs. The combination of human decision-making and computer efficiency yields precision—but not recall—comparable to manual annotation, while taking only a fraction of the time needed by a human annotator. Our findings underscore the promise of ML as a tool for assisting researchers in this area, but also highlight the current limitations in reliably detecting online sexual exploitation using ML.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604696",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "5–14",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Protecting children from online exploitation: Can a trained model detect harmful communication strategies?",
		"URL": "https://doi.org/10.1145/3600211.3604696",
		"author": [
			{
				"family": "Cook",
				"given": "Darren"
			},
			{
				"family": "Zilka",
				"given": "Miri"
			},
			{
				"family": "DeSandre",
				"given": "Heidi"
			},
			{
				"family": "Giles",
				"given": "Susan"
			},
			{
				"family": "Maskell",
				"given": "Simon"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "islamAnalysisClimateCampaigns2023",
		"type": "paper-conference",
		"abstract": "Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [57] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604665",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "15–25",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Analysis of climate campaigns on social media using bayesian model averaging",
		"URL": "https://doi.org/10.1145/3600211.3604665",
		"author": [
			{
				"family": "Islam",
				"given": "Tunazzina"
			},
			{
				"family": "Zhang",
				"given": "Ruqi"
			},
			{
				"family": "Goldwasser",
				"given": "Dan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "walkerAIArtMisinformation2023",
		"type": "paper-conference",
		"abstract": "Misinformation in its many forms is a substantial and growing problem for society today. Whether financially or ideologically motivated, purveyors of misinformation do not abide by legal, technical or moral rules. Therefore new, ludic, narrative, gamified and artistic approaches are needed. In this paper we analyse the approaches taken in countering misinformation by 18 AI and machine learning works of art, developed in the MediaFutures project. We examine how these align with existing AI approaches to countering misinformation, and how they address some of the key challenges. We show that AI artists engage with existing debunking and inoculating strategies, including highly technical aspects such as deepfakes, while also utilizing focused strategies of data literacy and collective intelligence. We also find that they are able to integrate hard-to-refute strategies such as narrative and emotion. These findings suggest that data as an art material and AI techniques as art tools are worth of further investigation as to their effectiveness for countering misinformation within society.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604715",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "26–37",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "AI art and misinformation: Approaches and strategies for media literacy and fact checking",
		"URL": "https://doi.org/10.1145/3600211.3604715",
		"author": [
			{
				"family": "Walker",
				"given": "Johanna"
			},
			{
				"family": "Thuermer",
				"given": "Gefion"
			},
			{
				"family": "Vicens",
				"given": "Julian"
			},
			{
				"family": "Simperl",
				"given": "Elena"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fefferPreferenceElicitationParticipatory2023",
		"type": "paper-conference",
		"abstract": "The AI Ethics community faces an imperative to empower stakeholders and impacted community members so that they can scrutinize and influence the design, development, and use of AI systems in high-stakes domains. While a growing chorus of recent papers has kindled interest in so-called “participatory ML” methods, precisely what form participation ought to take and how to operationalize these ambitions are seldom addressed. Our survey of the relevant literature shows that in many papers, participation is reduced to highly structured, computational mechanisms designed to elicit mathematically tractable approximations of narrowly-defined moral values. Of papers that actually engage with real people, these engagements typically consist of one-time interactions with individuals that are often unrepresentative of the relevant stakeholders. Motivated by these clear limitations, we introduce a consolidated set of axes to evaluate and improve participatory approaches. We use these axes to analyze contemporary work in this space and outline future AI research directions that could meaningfully contribute to operationalizing the ideal of participation.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604661",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "38–48",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "From preference elicitation to participatory ML: A critical survey &amp; guidelines for future research",
		"URL": "https://doi.org/10.1145/3600211.3604661",
		"author": [
			{
				"family": "Feffer",
				"given": "Michael"
			},
			{
				"family": "Skirpan",
				"given": "Michael"
			},
			{
				"family": "Lipton",
				"given": "Zachary"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "narayananHowDoesValue2023",
		"type": "paper-conference",
		"abstract": "This paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604709",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "49–57",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How does value similarity affect human reliance in AI-Assisted ethical decision making?",
		"URL": "https://doi.org/10.1145/3600211.3604709",
		"author": [
			{
				"family": "Narayanan",
				"given": "Saumik"
			},
			{
				"family": "Yu",
				"given": "Guanghui"
			},
			{
				"family": "Ho",
				"given": "Chien-Ju"
			},
			{
				"family": "Yin",
				"given": "Ming"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "kasirzadehUserTamperingReinforcement2023a",
		"type": "paper-conference",
		"abstract": "In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms – ’user tampering.’ User tampering is a situation where an RL-based recommender system may manipulate a media user’s opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of political content. Our study shows that a Q-learning algorithm consistently learns to exploit its opportunities to polarize simulated users with its early recommendations in order to have more consistent success with subsequent recommendations that align with this induced polarization. Our findings emphasize the necessity for developing safer RL-based recommendation systems and suggest that achieving such safety would require a fundamental shift in the design away from the approaches we have seen in the recent literature.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604669",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "58–69",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "User tampering in reinforcement learning recommender systems",
		"URL": "https://doi.org/10.1145/3600211.3604669",
		"author": [
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Evans",
				"given": "Charles"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rismaniMLModelApplying2023",
		"type": "paper-conference",
		"abstract": "Identifying potential social and ethical risks in emerging machine learning (ML) models and their applications remains challenging. In this work, we applied two well-established safety engineering frameworks (FMEA, STPA) to a case study involving text-to-image models at three stages of the ML product development pipeline: data processing, integration of a T2I model with other models, and use. Results of our analysis demonstrate the safety frameworks – both of which are not designed explicitly examine social and ethical risks – can uncover failure and hazards that pose social and ethical risks. We discovered a broad range of failures and hazards (i.e., functional, social, and ethical) by analyzing interactions (i.e., between different ML models in the product, between the ML product and user, and between development teams) and processes (i.e., preparation of training data or workflows for using an ML service/product). Our findings underscore the value and importance of examining beyond an ML model in examining social and ethical risks, especially when we have minimal information about an ML model.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604685",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "70–83",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Beyond the ML model: Applying safety engineering frameworks to text-to-image development",
		"URL": "https://doi.org/10.1145/3600211.3604685",
		"author": [
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Delos Santos",
				"given": "Renelito"
			},
			{
				"family": "Moon",
				"given": "AJung"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gilbertRewardReportsReinforcement2023",
		"type": "paper-conference",
		"abstract": "Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604698",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 47\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "84–130",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reward reports for reinforcement learning",
		"URL": "https://doi.org/10.1145/3600211.3604698",
		"author": [
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Lambert",
				"given": "Nathan"
			},
			{
				"family": "Dean",
				"given": "Sarah"
			},
			{
				"family": "Zick",
				"given": "Tom"
			},
			{
				"family": "Snoswell",
				"given": "Aaron"
			},
			{
				"family": "Mehta",
				"given": "Soham"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "seymourSystematicReviewEthical2023",
		"type": "paper-conference",
		"abstract": "Since Siri’s release in 2011 there have been a growing number of AI-driven domestic voice assistants that are increasingly being integrated into devices such as smartphones and TVs. But as their presence has expanded, a range of ethical concerns have been identified around the use of voice assistants, such as the privacy implications of having devices that are always listening and the ways that these devices are integrated into the existing social order of the home. This has created a burgeoning area of research across a range of fields including computer science, social science, and psychology. This paper takes stock of the foundations and frontiers of this work through a systematic literature review of 117 papers on ethical concerns with voice assistants. In addition to analysis of nine specific areas of concern, the review measures the distribution of methods and participant demographics across the literature. We show how some concerns, such as privacy, are operationalized to a much greater extent than others like accessibility, and how study participants are overwhelmingly drawn from a small handful of Western nations. In so doing we hope to provide an outline of the rich tapestry of work around these concerns and highlight areas where current research efforts are lacking.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604679",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "131–145",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A systematic review of ethical concerns with voice assistants",
		"URL": "https://doi.org/10.1145/3600211.3604679",
		"author": [
			{
				"family": "Seymour",
				"given": "William"
			},
			{
				"family": "Zhan",
				"given": "Xiao"
			},
			{
				"family": "Coté",
				"given": "Mark"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "barnettEthicalImplicationsGenerative2023",
		"type": "paper-conference",
		"abstract": "Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604686",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "146–161",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The ethical implications of generative audio models: A systematic literature review",
		"URL": "https://doi.org/10.1145/3600211.3604686",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cookRobustArtificialMoral2023",
		"type": "paper-conference",
		"abstract": "This paper explores the relationship between our ignorance concerning certain metanormative topics and the design of ethical artificial intelligence (AI). In particular, it will be maintained that because we cannot predict in advance which metanormative conclusions a sufficiently intelligent ethical AI might reach, we have reason to be apprehensive about the project of designing such AI. Even if we succeeded at designing an AI to engage in ethical behavior, there is a distinct possibility that the AI might eventually cease to behave ethically if it reaches certain metanormative conclusions. The candidate conclusions include ones such as the denial of the alleged authority or overridingness of ethics and the conclusion that there are no ethical facts or properties (i.e. moral error theory). It will be argued that the target AI could conceivably reach such conclusions, and in turn this could cause them to abandon their ethical routines and proceed to cause great harm.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604703",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "162–169",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Robust artificial moral agents and metanormativity",
		"URL": "https://doi.org/10.1145/3600211.3604703",
		"author": [
			{
				"family": "Cook",
				"given": "Tyler"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "uedaMitigatingVoterAttribute2023",
		"type": "paper-conference",
		"abstract": "The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&amp;S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&amp;S model. To address these limitations, we propose a new Soft D&amp;S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&amp;S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604660",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "170–180",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Mitigating voter attribute bias for fair opinion aggregation",
		"URL": "https://doi.org/10.1145/3600211.3604660",
		"author": [
			{
				"family": "Ueda",
				"given": "Ryosuke"
			},
			{
				"family": "Takeuchi",
				"given": "Koh"
			},
			{
				"family": "Kashima",
				"given": "Hisashi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "joLearningOptimalFair2023",
		"type": "paper-conference",
		"abstract": "The increasing use of machine learning in high-stakes domains – where people’s livelihoods are impacted – creates an urgent need for interpretable, fair, and highly accurate algorithms. With these needs in mind, we propose a mixed integer optimization (MIO) framework for learning optimal classification trees – one of the most interpretable models – that can be augmented with arbitrary fairness constraints. In order to better quantify the “price of interpretability”, we also propose a new measure of model interpretability called decision complexity that allows for comparisons across different classes of machine learning models. We benchmark our method against state-of-the-art approaches for fair classification on popular datasets; in doing so, we conduct one of the first comprehensive analyses of the trade-offs between interpretability, fairness, and predictive accuracy. Given a fixed disparity threshold, our method has a price of interpretability of about 4.2 percentage points in terms of out-of-sample accuracy compared to the best performing, complex models. However, our method consistently finds decisions with almost full parity, while other methods rarely do.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604664",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "181–192",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning optimal fair decision trees: Trade-offs between interpretability, fairness, and accuracy",
		"URL": "https://doi.org/10.1145/3600211.3604664",
		"author": [
			{
				"family": "Jo",
				"given": "Nathanael"
			},
			{
				"family": "Aghaei",
				"given": "Sina"
			},
			{
				"family": "Benson",
				"given": "Jack"
			},
			{
				"family": "Gomez",
				"given": "Andres"
			},
			{
				"family": "Vayanos",
				"given": "Phebe"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhangModelDebiasingGradientbased2023",
		"type": "paper-conference",
		"abstract": "Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicting sensitive attributes and 2) the other focus for predicting downstream task labels, and second, use them to perturb the latent code that guides the training of downstream task models towards fairness and utility goals. We show empirically that our framework works with both disentangled and non-disentangled representation learning methods and achieves better fairness-accuracy trade-off on unstructured and structured datasets than previous state-of-the-art approaches.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604668",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "193–204",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Model debiasing via gradient-based explanation on representation",
		"URL": "https://doi.org/10.1145/3600211.3604668",
		"author": [
			{
				"family": "Zhang",
				"given": "Jindi"
			},
			{
				"family": "Wang",
				"given": "Luning"
			},
			{
				"family": "Su",
				"given": "Dan"
			},
			{
				"family": "Huang",
				"given": "Yongxiang"
			},
			{
				"family": "Cao",
				"given": "Caleb Chen"
			},
			{
				"family": "Chen",
				"given": "Lei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "gorantlaSamplingIndividuallyfairRankings2023",
		"type": "paper-conference",
		"abstract": "Rankings on online platforms help their end-users find the relevant information—people, news, media, and products—quickly. Fair ranking tasks, which ask to rank a set of items to maximize utility subject to satisfying group-fairness constraints, have gained significant interest in the Algorithmic Fairness, Information Retrieval, and Machine Learning literature. Recent works, however, identify uncertainty in the utilities of items as a primary cause of unfairness and propose introducing randomness in the output. This randomness is carefully chosen to guarantee an adequate representation of each item (while accounting for the uncertainty). However, due to this randomness, the output rankings may violate group fairness constraints. We give an efficient algorithm that samples rankings from an individually-fair distribution while ensuring that every output ranking is group fair. The expected utility of the output ranking is at least α times the utility of the optimal fair solution. Here, α depends on the utilities, position-discounts, and constraints—it approaches 1 as the range of utilities or the position-discounts shrinks, or when utilities satisfy distributional assumptions. Empirically, we observe that our algorithm achieves individual and group fairness and that Pareto dominates the state-of-the-art baselines.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604671",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "205–216",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sampling individually-fair rankings that are always group fair",
		"URL": "https://doi.org/10.1145/3600211.3604671",
		"author": [
			{
				"family": "Gorantla",
				"given": "Sruthi"
			},
			{
				"family": "Mehrotra",
				"given": "Anay"
			},
			{
				"family": "Deshpande",
				"given": "Amit"
			},
			{
				"family": "Louis",
				"given": "Anand"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "nanchenKeepSensorsCheck2023",
		"type": "paper-conference",
		"abstract": "Machine learning models trained with passive sensor data from mobile devices can be used to perform various inferences pertaining to activity recognition, context awareness, and health and well-being. Prior work has improved inference performance through the use of multimodal sensors (inertial, GPS, proximity, app usage, etc.) or improved machine learning. In this context, a few studies shed light on critical issues relating to the poor cross-country generalization of models due to distributional shifts across countries. However, these studies have largely relied on inference performance as a means of studying generalization issues, failing to investigate whether the root cause of the problem is linked to specific sensor modalities (independent variables) or the target attribute (dependent variable). In this paper, we study this issue in complex activities of daily living (ADL) inference task, involving 12 classes, by using a multimodal, multi-country dataset collected from 689 participants across eight countries. We first show that the ‘country of origin’ of data is captured by sensors and can be inferred from each modality separately, with an average accuracy of 65%. We then propose two diversity scores (DS) that measure how a country differentiates from others w.r.t. sensor modalities or activities. Using these diversity scores, we observed that both individual sensor modalities and activities have the ability to differentiate countries. However, while many activities capture country differences, only the ‘App usage’ and ‘Location’ sensors can do so. By dissecting country-level diversity across dependent and independent variables, we provide a framework to better understand model generalization issues across countries and country-level diversity of sensing modalities.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604688",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "217–228",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Keep sensors in check: Disentangling country-level generalization issues in mobile sensor-based models with diversity scores",
		"URL": "https://doi.org/10.1145/3600211.3604688",
		"author": [
			{
				"family": "Nanchen",
				"given": "Alexandre"
			},
			{
				"family": "Meegahapola",
				"given": "Lakmal"
			},
			{
				"family": "Droz",
				"given": "William"
			},
			{
				"family": "Gatica-Perez",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "cherepanovaDeepDiveDataset2023",
		"type": "paper-conference",
		"abstract": "As the deployment of automated face recognition (FR) systems proliferates, bias in these systems is not just an academic question, but a matter of public concern. Media portrayals often center imbalance as the main source of bias, i.e., that FR models perform worse on images of non-white people or women because these demographic groups are underrepresented in training data. Recent academic research paints a more nuanced picture of this relationship. However, previous studies of data imbalance in FR have focused exclusively on the face verification setting, while the face identification setting has been largely ignored, despite being deployed in sensitive applications such as law enforcement. This is an unfortunate omission, as ‘imbalance’ is a more complex matter in identification; imbalance may arise in not only the training data, but also the testing data, and furthermore may affect the proportion of identities belonging to each demographic group or the number of images belonging to each identity. In this work, we address this gap in the research by thoroughly exploring the effects of each kind of imbalance possible in face identification, and discuss other factors which may impact bias in this setting.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604691",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "229–247",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A deep dive into dataset imbalance and bias in face identification",
		"URL": "https://doi.org/10.1145/3600211.3604691",
		"author": [
			{
				"family": "Cherepanova",
				"given": "Valeriia"
			},
			{
				"family": "Reich",
				"given": "Steven"
			},
			{
				"family": "Dooley",
				"given": "Samuel"
			},
			{
				"family": "Souri",
				"given": "Hossein"
			},
			{
				"family": "Dickerson",
				"given": "John"
			},
			{
				"family": "Goldblum",
				"given": "Micah"
			},
			{
				"family": "Goldstein",
				"given": "Tom"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zhouIterativePartialFulfillment2023",
		"type": "paper-conference",
		"abstract": "Counterfactual (CF) explanations, also known as contrastive explanations and algorithmic recourses, are popular for explaining machine learning models in high-stakes domains. For a subject that receives a negative model prediction (e.g., mortgage application denial), the CF explanations are similar instances but with positive predictions, which informs the subject of ways to improve. While their various properties have been studied, such as validity and stability, we contribute a novel one: their behaviors under iterative partial fulfillment (IPF). Specifically, upon receiving a CF explanation, the subject may only partially fulfill it before requesting a new prediction with a new explanation, and repeat until the prediction is positive. Such partial fulfillment could be due to the subject’s limited capability (e.g., can only pay down two out of four credit card accounts at this moment) or an attempt to take the chance (e.g., betting that a monthly salary increase of 800isenougheventhough1,000 is recommended). Does such iterative partial fulfillment increase or decrease the total cost of improvement incurred by the subject? We mathematically formalize IPF and demonstrate, both theoretically and empirically, that different CF algorithms exhibit vastly different behaviors under IPF. We discuss implications of our observations, advocate for this factor to be carefully considered in the development and study of CF algorithms, and give several directions for future work.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604656",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "248–258",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Iterative partial fulfillment of counterfactual explanations: Benefits and risks",
		"URL": "https://doi.org/10.1145/3600211.3604656",
		"author": [
			{
				"family": "Zhou",
				"given": "Yilun"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "globus-harrisMulticalibratedRegressionDownstream2023",
		"type": "paper-conference",
		"abstract": "We show how to take a regression function that is appropriately multicalibrated and efficiently post-process it into an approximately error minimizing classifier satisfying a large variety of fairness constraints. The post-processing requires no labeled data, and only a modest amount of unlabeled data and computation. The computational and sample complexity requirements of computing are comparable to the requirements for solving a single fair learning task optimally, but it can in fact be used to solve many different downstream fairness-constrained learning problems efficiently. Our post-processing method easily handles intersecting groups, generalizing prior work on post-processing regression functions to satisfy fairness constraints that only applied to disjoint groups. Our work extends recent work showing that multicalibrated regression functions are omnipredictors (i.e. can be post-processed to optimally solve unconstrained ERM problems) to constrained optimization problems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604683",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 28\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "259–286",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Multicalibrated regression for downstream fairness",
		"URL": "https://doi.org/10.1145/3600211.3604683",
		"author": [
			{
				"family": "Globus-Harris",
				"given": "Ira"
			},
			{
				"family": "Gupta",
				"given": "Varun"
			},
			{
				"family": "Jung",
				"given": "Christopher"
			},
			{
				"family": "Kearns",
				"given": "Michael"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			},
			{
				"family": "Roth",
				"given": "Aaron"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hendersonSelfdestructingModelsIncreasing2023",
		"type": "paper-conference",
		"abstract": "A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model’s ability to perform profession classification.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604690",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "287–296",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Self-destructing models: Increasing the costs of harmful dual uses of foundation models",
		"URL": "https://doi.org/10.1145/3600211.3604690",
		"author": [
			{
				"family": "Henderson",
				"given": "Peter"
			},
			{
				"family": "Mitchell",
				"given": "Eric"
			},
			{
				"family": "Manning",
				"given": "Christopher"
			},
			{
				"family": "Jurafsky",
				"given": "Dan"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "jorgensenNotFairImpact2023",
		"type": "paper-conference",
		"abstract": "When bias mitigation methods are applied to make fairer machine learning models in fairness-related classification settings, there is an assumption that the disadvantaged group should be better off than if no mitigation method was applied. However, this is a potentially dangerous assumption because a “fair” model outcome does not automatically imply a positive impact for a disadvantaged individual—they could still be negatively impacted. Modeling and accounting for those impacts is key to ensure that mitigated models are not unintentionally harming individuals; we investigate if mitigated models can still negatively impact disadvantaged individuals and what conditions affect those impacts in a loan repayment example. Our results show that most mitigated models negatively impact disadvantaged group members in comparison to the unmitigated models. The domain-dependent impacts of model outcomes should help drive future bias mitigation method development.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604699",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "297–311",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Not so fair: The impact of presumably fair machine learning models",
		"URL": "https://doi.org/10.1145/3600211.3604699",
		"author": [
			{
				"family": "Jorgensen",
				"given": "Mackenzie"
			},
			{
				"family": "Richert",
				"given": "Hannah"
			},
			{
				"family": "Black",
				"given": "Elizabeth"
			},
			{
				"family": "Criado",
				"given": "Natalia"
			},
			{
				"family": "Such",
				"given": "Jose"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "talTargetSpecificationBias2023",
		"type": "paper-conference",
		"abstract": "Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604678",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "312–321",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare",
		"URL": "https://doi.org/10.1145/3600211.3604678",
		"author": [
			{
				"family": "Tal",
				"given": "Eran"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bennettUnpickingEpistemicInjustices2023",
		"type": "paper-conference",
		"abstract": "Applications of Artificial Intelligence (AI) in the domain of Personal Health Informatics (PHI) offer potential avenues for personalised treatment and support for people living with long-term conditions, however, they also present a number of ethical challenges. Whilst participatory approaches can help mitigate concerns by actively involving healthcare professionals, patients, and other stakeholders in design and development, these are constrained by the limits of epistemic standpoints and the risks posed by extrapolation from individuals to groups. In this paper we draw upon interviews with stakeholders involved in Human Immunodeficiency Virus (HIV) care, including clinicians, insurance providers and pharmaceutical industry representatives, to map intentions and ethical considerations for developing PHI tools for people living with HIV. Whilst treatment efficacy for HIV has improved patient quality of life and life expectancy, management and care is complicated by knowledge gaps about what living and ageing with HIV entails. We investigate how the critical concept of epistemic injustice can inform the design of data-driven technologies intended to address these gaps, helping orient expert perspectives within the broader structures and socio-historical influences that shape them. This is of particular importance when designing for marginalized populations such as people with HIV (i.e. who may experience social stigma and be under-resourced, managing multiple conditions), helping to identify and better account for fundamental ethical considerations such as equity.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604684",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "322–332",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unpicking epistemic injustices in digital health: On the implications of designing data-driven technologies for the management of long-term conditions",
		"URL": "https://doi.org/10.1145/3600211.3604684",
		"author": [
			{
				"family": "Bennett",
				"given": "SJ"
			},
			{
				"family": "Claisse",
				"given": "Caroline"
			},
			{
				"family": "Luger",
				"given": "Ewa"
			},
			{
				"family": "Durrant",
				"given": "Abigail C."
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yangEvaluatingImpactSocial2023",
		"type": "paper-conference",
		"abstract": "Social determinants of health (SDOH) – the conditions in which people live, grow, and age – play a crucial role in a person’s health and well-being. There is a large, compelling body of evidence in population health studies showing that a wide range of SDOH is strongly correlated with health outcomes. Yet, a majority of the risk prediction models based on electronic health records (EHR) do not incorporate a comprehensive set of SDOH features as they are often noisy or simply unavailable. Our work links a publicly available EHR database, MIMIC-IV, to well-documented SDOH features. We investigate the impact of such features on common EHR prediction tasks across different patient populations. We find that community-level SDOH features do not improve model performance for a general patient population, but can improve data-limited model fairness for specific subpopulations. We also demonstrate that SDOH features are vital for conducting thorough audits of algorithmic biases beyond protective attributes. We hope the new integrated EHR-SDOH database will enable studies on the relationship between community health and individual outcomes and provide new benchmarks to study algorithmic biases beyond race, gender, and age.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604719",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 18\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "333–350",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating the impact of social determinants on health prediction in the intensive care unit",
		"URL": "https://doi.org/10.1145/3600211.3604719",
		"author": [
			{
				"family": "Yang",
				"given": "Ming Ying"
			},
			{
				"family": "Kwak",
				"given": "Gloria Hyunjung"
			},
			{
				"family": "Pollard",
				"given": "Tom"
			},
			{
				"family": "Celi",
				"given": "Leo Anthony"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "zajacGroundTruthDare2023a",
		"type": "paper-conference",
		"abstract": "One of the core goals of responsible AI development is ensuring high-quality training datasets. Many researchers have pointed to the importance of the annotation step in the creation of high-quality data, but less attention has been paid to the work that enables data annotation. We define this work as the design of ground truth schema and explore the challenges involved in the creation of datasets in the medical domain even before any annotations are made. Based on extensive work in three health-tech organisations, we describe five external and internal factors that condition medical dataset creation processes. Three external factors include regulatory constraints, the context of creation and use, and commercial and operational pressures. These factors condition medical data collection and shape the ground truth schema design. Two internal factors include epistemic differences and limits of labelling. These directly shape the design of the ground truth schema. Discussions of what constitutes high-quality data need to pay attention to the factors that shape and constrain what is possible to be created, to ensure responsible AI design.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604766",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "351–362",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Ground truth or dare: Factors affecting the creation of medical datasets for training AI",
		"URL": "https://doi.org/10.1145/3600211.3604766",
		"author": [
			{
				"family": "Zając",
				"given": "Hubert Dariusz"
			},
			{
				"family": "Avlona",
				"given": "Natalia Rozalia"
			},
			{
				"family": "Kensing",
				"given": "Finn"
			},
			{
				"family": "Andersen",
				"given": "Tariq Osman"
			},
			{
				"family": "Shklovski",
				"given": "Irina"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "dennlerBoundBountyCollaboratively2023",
		"type": "paper-conference",
		"abstract": "Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604682",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "375–386",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Bound by the bounty: Collaboratively shaping evaluation processes for queer AI harms",
		"URL": "https://doi.org/10.1145/3600211.3604682",
		"author": [
			{
				"family": "Dennler",
				"given": "Nathan"
			},
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Singh",
				"given": "Ashwin"
			},
			{
				"family": "Soldaini",
				"given": "Luca"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Tu",
				"given": "Huy"
			},
			{
				"family": "Agnew",
				"given": "William"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Yee",
				"given": "Kyra"
			},
			{
				"family": "Peradejordi",
				"given": "Irene Font"
			},
			{
				"family": "Talat",
				"given": "Zeerak"
			},
			{
				"family": "Russo",
				"given": "Mayra"
			},
			{
				"family": "Pinhal",
				"given": "Jess De Jesus De Pinho"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "robinsonActionGuidanceAI2023",
		"type": "paper-conference",
		"abstract": "I offer a preliminary conceptual framework for evaluating AI alignment projects. It is based on the concept of action guidance. In §1 and §2, I explain action guidance and its importance to AI alignment. I introduce the ‘Guidance Framework’ in §3. In §4, I show how it can be applied to two different sorts of questions: the practical question of how to design a specific AI agent (my example is a fictional ocean-cleaning robot), and the theoretical question of how to evaluate a specific AI alignment proposal (my example is Stuart Russell's ‘binary approach’). In §5 I discuss limitations of the framework and opportunities for further research.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604714",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "387–395",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Action guidance and AI alignment",
		"URL": "https://doi.org/10.1145/3600211.3604714",
		"author": [
			{
				"family": "Robinson",
				"given": "Pamela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "birdTypologyRisksGenerative2023a",
		"type": "paper-conference",
		"abstract": "This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604722",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 15\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "396–410",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Typology of risks of generative text-to-image models",
		"URL": "https://doi.org/10.1145/3600211.3604722",
		"author": [
			{
				"family": "Bird",
				"given": "Charlotte"
			},
			{
				"family": "Ungless",
				"given": "Eddie"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "albiniConnectionGametheoreticFeature2023",
		"type": "paper-conference",
		"abstract": "Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604676",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "411–431",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "On the connection between game-theoretic feature attributions and counterfactual explanations",
		"URL": "https://doi.org/10.1145/3600211.3604676",
		"author": [
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Mishra",
				"given": "Saumitra"
			},
			{
				"family": "Dervovic",
				"given": "Danial"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "hardyAdaptiveAdversarialTraining2023",
		"type": "paper-conference",
		"abstract": "Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model’s classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier’s susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604704",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "432–442",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Adaptive adversarial training does not increase recourse costs",
		"URL": "https://doi.org/10.1145/3600211.3604704",
		"author": [
			{
				"family": "Hardy",
				"given": "Ian"
			},
			{
				"family": "Yetukuri",
				"given": "Jayanth"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604706",
		"type": "paper-conference",
		"abstract": "Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature reselection, so that features can be selected with respect to secondary model performance characteristics efficiently even after a feature selection process has been done with respect to a primary objective. To address this problem, we propose REFRESH, a method to reselect features so that additional constraints that are desirable towards model performance can be achieved without having to train several new models. REFRESH’s underlying algorithm is a novel technique using SHAP values and correlation analysis that can approximate for the predictions of a model without having to train these models. Empirical evaluations on three datasets, including a large-scale loan defaulting dataset show that REFRESH can help find alternate models with better model characteristics efficiently. We also discuss the need for reselection and REFRESH based on regulation desiderata.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604706",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "443–453",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "REFRESH: Responsible and efficient feature reselection guided by SHAP values",
		"URL": "https://doi.org/10.1145/3600211.3604706",
		"author": [
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Dutta",
				"given": "Sanghamitra"
			},
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Lecue",
				"given": "Freddy"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			},
			{
				"family": "Veloso",
				"given": "Manuela"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604657",
		"type": "paper-conference",
		"abstract": "Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g. support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: one-hot encoding and target encoding. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, irreducible bias, is due to direct group category discrimination and the second type, reducible bias, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604657",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "454–465",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Fairness implications of encoding protected categorical attributes",
		"URL": "https://doi.org/10.1145/3600211.3604657",
		"author": [
			{
				"family": "Mougan",
				"given": "Carlos"
			},
			{
				"family": "Alvarez",
				"given": "Jose Manuel"
			},
			{
				"family": "Ruggieri",
				"given": "Salvatore"
			},
			{
				"family": "Staab",
				"given": "Steffen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604689",
		"type": "paper-conference",
		"abstract": "Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers’ have demonstrated to be particularly salient for the societal impact of deployed ML systems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604689",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "466–481",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Machine Learning practices and infrastructures",
		"URL": "https://doi.org/10.1145/3600211.3604689",
		"author": [
			{
				"family": "Berman",
				"given": "Glen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604674",
		"type": "paper-conference",
		"abstract": "Fairness toolkits are developed to support machine learning (ML) practitioners in using algorithmic fairness metrics and mitigation methods. Past studies have investigated practical challenges for toolkit usage, which are crucial to understanding how to support practitioners. However, the extent to which fairness toolkits impact practitioners’ practices and enable reflexivity around algorithmic harms remains unclear (i.e., distributive unfairness beyond algorithmic fairness, and harms that are not related to the outputs of ML systems). Little is currently understood about the root factors that fragment practices when using fairness toolkits and how practitioners reflect on algorithmic harms. Yet, a deeper understanding of these facets is essential to enable the design of support tools for practitioners. To investigate the impact of toolkits on practices and identify factors that shape these practices, we carried out a qualitative study with 30 ML practitioners with varying backgrounds. Through a mixed within and between-subjects design, we tasked the practitioners with developing an ML model, and analyzed their reported practices to surface potential factors that lead to differences in practices. Interestingly, we found that fairness toolkits act as double-edge swords — with potentially positive and negative impacts on practices. Our findings showcase a plethora of human and organizational factors that play a key role in the way toolkits are envisioned and employed. These results bear implications for the design of future toolkits and educational training for practitioners and call for the creation of new policies to handle the organizational constraints faced by practitioners.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604674",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "482–495",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "“☑ fairness toolkits, a checkbox culture?” on the factors that fragment developer practices in handling algorithmic harms",
		"URL": "https://doi.org/10.1145/3600211.3604674",
		"author": [
			{
				"family": "Balayn",
				"given": "Agathe"
			},
			{
				"family": "Yurrita",
				"given": "Mireia"
			},
			{
				"family": "Yang",
				"given": "Jie"
			},
			{
				"family": "Gadiraju",
				"given": "Ujwal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604705",
		"type": "paper-conference",
		"abstract": "Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness’ raison d’être of “fairness,” we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and praxis, and 4) provide actionable recommendations for AI fairness researchers to engage with intersectionality in their work by grounding it in AI epistemology.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604705",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 16\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "496–511",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in AI fairness",
		"URL": "https://doi.org/10.1145/3600211.3604705",
		"author": [
			{
				"family": "Ovalle",
				"given": "Anaelia"
			},
			{
				"family": "Subramonian",
				"given": "Arjun"
			},
			{
				"family": "Gautam",
				"given": "Vagrant"
			},
			{
				"family": "Gee",
				"given": "Gilbert"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604718",
		"type": "paper-conference",
		"abstract": "Calls for new metrics, technical standards and governance mechanisms to guide the adoption of Artificial Intelligence (AI) in institutions and public administration are now commonplace. Yet, most research and policy efforts aimed at understanding the implications of adopting AI tend to prioritize only a handful of ideas; they do not fully connect all the different perspectives and topics that are potentially relevant. In this position paper, we contend that this omission stems, in part, from what we call the ‘relational problem’ in socio-technical discourse: fundamental ontological issues have not yet been settled—including semantic ambiguity, a lack of clear relations between concepts and differing standard terminologies. This contributes to the persistence of disparate modes of reasoning to assess institutional AI systems, and the prevalence of conceptual isolation in the fields that study them including ML, human factors, social science and policy. After developing this critique, we offer a way forward by proposing a simple policy and research design tool in the form of a conceptual framework to organize terms across fields—consisting of three horizontal domains for grouping relevant concepts and related methods: Operational, Epistemic, and Normative. We first situate this framework against the backdrop of recent socio-technical discourse at two premier academic venues, AIES and FAccT, before illustrating how developing suitable metrics, standards, and mechanisms can be aided by operationalizing relevant concepts in each of these domains. Finally, we outline outstanding questions for developing this relational approach to institutional AI research and adoption.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604718",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "512–519",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A multidomain relational framework to guide institutional AI research and adoption",
		"URL": "https://doi.org/10.1145/3600211.3604718",
		"author": [
			{
				"family": "Straub",
				"given": "Vincent J"
			},
			{
				"family": "Morgan",
				"given": "Deborah"
			},
			{
				"family": "Hashem",
				"given": "Youmna"
			},
			{
				"family": "Francis",
				"given": "John"
			},
			{
				"family": "Esnaashari",
				"given": "Saba"
			},
			{
				"family": "Bright",
				"given": "Jonathan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604659",
		"type": "paper-conference",
		"abstract": "Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an “othering” phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604659",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "520–530",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Flickr africa: Examining geo-diversity in large-scale, human-centric visual data",
		"URL": "https://doi.org/10.1145/3600211.3604659",
		"author": [
			{
				"family": "Naggita",
				"given": "Keziah"
			},
			{
				"family": "LaChance",
				"given": "Julienne"
			},
			{
				"family": "Xiang",
				"given": "Alice"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604662",
		"type": "paper-conference",
		"abstract": "Algorithmic audits of industry face recognition models have recently incentivized companies to diversify their data collection methods, which in turn has reduced error disparities along demographic lines, such as gender or race. We argue that it is important to understand exactly how various forms of targeted data collection mitigate performance disparities in these updated face recognition models. We propose an empirical framework to assess the impact of additional dataset collection targeted towards various racial groups. We apply our framework to three racially-annotated benchmark datasets using three standard face recognition models. Our findings empirically validate the notion that the introduction of data from the demographic group with the initially-lowest performance improves performance on that group significantly more than adding from other groups. We also observe that in all settings, the introduction of data from a previously omitted group does not harm the performance of other groups. Furthermore, investigation of feature embeddings reveals that performance increases are associated with a larger separation among images of different identities. Despite the commonalities we observe across datasets, we also find key differences: for example, in one dataset, training on one racial group generalizes well across all groups. These differences speak to the criticality of re-applying empirical evaluation methods, such as the methods in this work, when introducing new datasets or models.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604662",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "531–541",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluation of targeted dataset collection on racial equity in face recognition",
		"URL": "https://doi.org/10.1145/3600211.3604662",
		"author": [
			{
				"family": "Hong",
				"given": "Rachel"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Morgenstern",
				"given": "Jamie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604666",
		"type": "paper-conference",
		"abstract": "Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604666",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "542–553",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating biased attitude associations of language models in an intersectional context",
		"URL": "https://doi.org/10.1145/3600211.3604666",
		"author": [
			{
				"family": "Omrani Sabbaghi",
				"given": "Shiva"
			},
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "10.1145/3600211.3604667",
		"type": "paper-conference",
		"abstract": "We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604667",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "554–565",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Unmasking nationality bias: A study of human perception of nationalities in AI-Generated articles",
		"URL": "https://doi.org/10.1145/3600211.3604667",
		"author": [
			{
				"family": "Narayanan Venkit",
				"given": "Pranav"
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Panchanadikar",
				"given": "Ruchi"
			},
			{
				"family": "Huang",
				"given": "Ting-Hao"
			},
			{
				"family": "Wilson",
				"given": "Shomir"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "williamsNoJusticeNo2023",
		"type": "paper-conference",
		"abstract": "In this paper, we examine the risks posed by roboticists’ collaboration with law enforcement agencies in the U.S. Using Trust frameworks from AI Ethics, we argue that collaborations with law enforcement present not only risks of technology misuse, but also risks of legitimizing bad actors, and of exacerbating our field’s challenges of representation. We discuss evidence of bad dispositions justifying these risks, grounded in the behavior, origins, and incentivization of American policing, and suggest courses of action for American roboticists seeking to pursue research projects that currently require collaboration with law enforcement agencies, closing with a call for abolitionist robotics.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604663",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "566–575",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "No justice, no robots: From the dispositions of policing to an abolitionist robotics",
		"URL": "https://doi.org/10.1145/3600211.3604663",
		"author": [
			{
				"family": "Williams",
				"given": "Tom"
			},
			{
				"family": "Haring",
				"given": "Kerstin Sophie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "turriWhyWeNeed2023",
		"type": "paper-conference",
		"abstract": "To enable the development and use of safe and equitable artificial intelligence (AI) systems, AI engineers must monitor deployed AI systems and learn from past AI incidents where failures have occurred. Around the world, public databases for cataloging AI systems and resulting harms are instrumental in promoting awareness of potential AI harms among policymakers, researchers, and the public. However, despite growing recognition of the potential of AI systems to produce harms, causes of AI systems failure remain elusive and AI incidents continue to occur. For example, incidents of AI bias are frequently reported and discussed, yet biased systems continue to be developed and deployed. This raises the question – how are we learning from documented incidents? What information do we need to analyze AI incidents and develop new AI engineering best practices? This paper examines reporting techniques from a variety of AI stakeholders and across different industries, identifies requirements towards the design of effective AI incident documentation, and proposes policy recommendations for augmenting current practice.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604700",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "576–583",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Why we need to know more: Exploring the state of AI incident documentation practices",
		"URL": "https://doi.org/10.1145/3600211.3604700",
		"author": [
			{
				"family": "Turri",
				"given": "Violet"
			},
			{
				"family": "Dzombak",
				"given": "Rachel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rismaniWhatDoesIt2023",
		"type": "paper-conference",
		"abstract": "With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604702",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "584–595",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "What does it mean to be a responsible AI practitioner: An ontology of roles and skills",
		"URL": "https://doi.org/10.1145/3600211.3604702",
		"author": [
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Moon",
				"given": "AJung"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "huaEffectiveEnforceabilityEU2023",
		"type": "paper-conference",
		"abstract": "This paper examines whether competition law enforcement can remain effective under different AI development scenarios over the coming years. Economic and political power has become increasingly concentrated into a few AI companies, such as Big Tech. The growth of generative AI could further reinforce this concentration of power in Big Tech. The market power of these companies, and increasingly their involvement in AI, is a major focus for regulators such as the European Commission. Recent EU antitrust fines on Google alone run in the billions. The dynamism of technology markets such as AI can make it difficult for regulators to take effective action. If AI continues to develop rapidly over the coming years, propelled by the proliferation of generative AI, this ability to effectively enforce antitrust law may be further challenged. To help ensure regulators remain effective, EU competition law has been bolstered by a new tech-tailored, ex ante competition regime. These are likely to be critical tools to shape the market power of Big Tech but are largely untested. Exploring how these regulatory tools can be most effective in governing future AI development is a timely question for regulators, lawyers, companies, and citizens. This paper examines this question by considering the ‘effective enforceability’ of EU competition law and the Digital Markets Act under different AI development scenarios. By ‘effective enforceability’ of EU competition law we mean how well it achieves its policy objectives. We consider four factors: jurisdictional scope, potential loopholes, effectiveness of detection, and ability to remedy/sanction breaches. However, there is significant uncertainty as to how AI will develop in the coming years. Considering this, we propose an analytical framework based on five variables: key inputs, speed of development, AI capability, number of actors, and the nature/relationship of actors. In some of these scenarios, we argue EU competition law would struggle to address the power of the largest AI companies; but in many other scenarios it remains a powerful tool. This is a critical juncture for competition regulators. They stand at the dawn of emerging challenges presented by generative AI. With this paper, we hope to contribute to anticipatory governance at this important intersection of legal governance and technology.Effective and future-proof competition law enforcement is crucial to ensuring this potentially transformative technology has widely distributed benefits, rather than concentrating power in a few hands.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604694",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "596–605",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Effective enforceability of EU competition law under AI development scenarios: a framework for anticipatory governance",
		"URL": "https://doi.org/10.1145/3600211.3604694",
		"author": [
			{
				"family": "Hua",
				"given": "Shin-Shin"
			},
			{
				"family": "Belfield",
				"given": "Haydn"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lawrenceBureaucraticChallengeAI2023",
		"type": "paper-conference",
		"abstract": "Can government govern artificial intelligence (AI)? One of the central questions of AI governance surrounds state capacity, namely whether government has the ability to accomplish its policy goals. We study this question by assessing how well the U.S. federal government has implemented three binding laws around AI governance: two executive orders—concerning trustworthy AI in the public sector (E.O. 13,960) and AI leadership (E.O. 13,859)—and the AI in Government Act. We conduct the first systematic empirical assessment of the implementation status of these three laws, which have each been described as central to US AI innovation. First, we track, through extensive research, line-level adoption of each mandated action. Based on publicly available information, we find that fewer than 40 percent of 45 legal requirements could be verified as having been implemented. Second, we research the specific implementation of transparency requirements at up to 220 federal agencies. We find that nearly half of agencies failed to publicly issue AI use case inventories—even when these agencies have demonstrable use cases of machine learning. Even when agencies have complied with these requirements, efforts are inconsistent. Our work highlights the weakness of U.S. state capacity to carry out AI governance mandates and we discuss implications for how to address bureaucratic capacity challenges.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604701",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 47\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "606–652",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "The bureaucratic challenge to AI governance: An empirical assessment of implementation at U.S. federal agencies",
		"URL": "https://doi.org/10.1145/3600211.3604701",
		"author": [
			{
				"family": "Lawrence",
				"given": "Christie"
			},
			{
				"family": "Cui",
				"given": "Isaac"
			},
			{
				"family": "Ho",
				"given": "Daniel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "fabbriSelfdeterminationExplanationEthical2023",
		"type": "paper-conference",
		"abstract": "In the contemporary information age, recommender systems (RSs) play a critical role in influencing online behaviour: from social media to e-commerce, from music streaming to news aggregators, individuals are constantly targeted by personalized recommendations suggesting contents that may interest them. Despite such diffusion, the extent to which recommendations influence users’ decisions is still underexplored, given that independent audits on the structure and functioning of RSs deployed on online platforms are usually prevented by proprietary constraints. The nudging potential of RSs can represent a risk for vulnerable people: indeed, judicial cases involving platforms’ responsibility for displaying recommendations that may lead to political radicalization or endangerment of minors have recently caught public attention. The Digital Services Act of the European Union (DSA) is the first supranational regulation that sets specific transparency and auditing requirements for RSs implemented by online platforms with the aim of enhancing users’ self-determination: in particular, it allows users to modify the parameters on which recommendations rely so to let them choose autonomously which kind of content they want to see. This research focuses on whether and how the enforcement of this regulation can mitigate the unfair consequences of the power imbalance between online platforms and users. To this aim, I discuss the harms arising from digital nudging based on RSs and propose explanations as a tool that can reduce the impact of those harms by increasing users’ awareness. Through a comparative analysis of relevant articles of the DSA, the General Data Protection Regulation (GDPR) and the AI Act, I outline how the provisions of the DSA fill some of the gaps left by other relevant European regulations, while leaving the so-called right to explanation substantially unaddressed. As a result of this analysis, I argue that, in order for the implementation of the DSA provisions on recommender systems to be effective, policy-makers should: 1) enhance users’ awareness through clear and easily accessible explanations on how the recommendation process works and how they can be influenced by it; 2) grant users the possibility of intervening directly on the strategies through which RSs target them on the platform’s interface.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604717",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 9\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "653–661",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Self-determination through explanation: an ethical perspective on the implementation of the transparency requirements for recommender systems set by the Digital Services Act of the European Union",
		"URL": "https://doi.org/10.1145/3600211.3604717",
		"author": [
			{
				"family": "Fabbri",
				"given": "Matteo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "schwarzschildReckoningDisagreementProblem2023",
		"type": "paper-conference",
		"abstract": "As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model’s output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus between explainers other than those used in the loss term. We examine the trade-off between improved consensus and model performance. And finally, we study the influence our method has on feature attribution explanations.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604687",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 17\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "662–678",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reckoning with the disagreement problem: Explanation consensus as a training objective",
		"URL": "https://doi.org/10.1145/3600211.3604687",
		"author": [
			{
				"family": "Schwarzschild",
				"given": "Avi"
			},
			{
				"family": "Cembalest",
				"given": "Max"
			},
			{
				"family": "Rao",
				"given": "Karthik"
			},
			{
				"family": "Hines",
				"given": "Keegan"
			},
			{
				"family": "Dickerson",
				"given": "John"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshWhenFairClassification2023",
		"type": "paper-conference",
		"abstract": "The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all. To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-unaware algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-unaware and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604707",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "679–690",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "When fair classification meets noisy protected attributes",
		"URL": "https://doi.org/10.1145/3600211.3604707",
		"author": [
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kvitca",
				"given": "Pablo"
			},
			{
				"family": "Wilson",
				"given": "Christo"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "edenbergDisambiguatingAlgorithmicBias2023",
		"type": "paper-conference",
		"abstract": "As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term ‘bias.’ Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination. In this paper, we examine the promises and challenges of different approaches to disambiguating bias and designing for justice. While both approaches aid in understanding and addressing clear algorithmic harms, we argue that they also risk being leveraged in ways that ultimately deflect accountability from those building and deploying these systems. Applying this analysis to recent examples of generative AI, our argument highlights unseen dangers in current methods of evaluating algorithmic bias and points to ways to redirect approaches to addressing bias in generative AI at its early stages in ways that can more robustly meet the demands of justice.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604695",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "691–704",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Disambiguating algorithmic bias: From neutrality to justice",
		"URL": "https://doi.org/10.1145/3600211.3604695",
		"author": [
			{
				"family": "Edenberg",
				"given": "Elizabeth"
			},
			{
				"family": "Wood",
				"given": "Alexandra"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "buremaSectorbasedApproachAI2023",
		"type": "paper-conference",
		"abstract": "Acknowledging that society is made up of different sectors with their own rules and structures, this paper studies the relevance of a sector-specific perspective to AI ethics. Incidents with AI are studied in relation to five sectors (police, healthcare, education and academia, politics, automotive) using the AIAAIC repository. A total of 125 incidents are sampled and analyzed by conducting a qualitative content analysis on media reports. The results show that certain ethical principles are found breached across sectors: accuracy/reliability, bias/discrimination, transparency, surveillance/privacy, security. However, results also show that 1) some ethical issues (misinformation, safety, premise/intent) are sector specific, 2) the consequences and meaning of the same ethical issue is able to vary across sectors and 3) pre-existing sector-specific issues are reproduced with these ethical breaches. The paper concludes that general ethical principles are relevant to discuss across sectors, yet, a sector-based approach to AI ethics gives in-depth information on sector-specific structural issues.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604680",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "705–714",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "A sector-based approach to AI ethics: Understanding ethical issues of AI-related incidents within their sectoral context",
		"URL": "https://doi.org/10.1145/3600211.3604680",
		"author": [
			{
				"family": "Burema",
				"given": "Dafna"
			},
			{
				"family": "Debowski-Weimann",
				"given": "Nicole"
			},
			{
				"family": "Janowski",
				"given": "Alexander",
				"non-dropping-particle": "von"
			},
			{
				"family": "Grabowski",
				"given": "Jil"
			},
			{
				"family": "Maftei",
				"given": "Mihai"
			},
			{
				"family": "Jacobs",
				"given": "Mattis"
			},
			{
				"family": "Smagt",
				"given": "Patrick",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Benbouzid",
				"given": "Djalel"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "segerDemocratisingAIMultiple2023",
		"type": "paper-conference",
		"abstract": "Numerous parties are calling for “the democratisation of AI”, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of “AI democratisation” that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to “democratising AI”, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604693",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 8\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "715–722",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Democratising AI: Multiple meanings, goals, and methods",
		"URL": "https://doi.org/10.1145/3600211.3604693",
		"author": [
			{
				"family": "Seger",
				"given": "Elizabeth"
			},
			{
				"family": "Ovadya",
				"given": "Aviv"
			},
			{
				"family": "Siddarth",
				"given": "Divya"
			},
			{
				"family": "Garfinkel",
				"given": "Ben"
			},
			{
				"family": "Dafoe",
				"given": "Allan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "shelbySociotechnicalHarmsAlgorithmic2023",
		"type": "paper-conference",
		"abstract": "Understanding the landscape of potential harms from algorithmic systems enables practitioners to better anticipate consequences of the systems they build. It also supports the prospect of incorporating controls to help minimize harms that emerge from the interplay of technologies and social and cultural dynamics. A growing body of scholarship has identified a wide range of harms across different algorithmic technologies. However, computing research and practitioners lack a high level and synthesized overview of harms from algorithmic systems. Based on a scoping review of computing research (n=172), we present an applied taxonomy of sociotechnical harms to support a more systematic surfacing of potential harms in algorithmic systems. The final taxonomy builds on and refers to existing taxonomies, classifications, and terminologies. Five major themes related to sociotechnical harms — representational, allocative, quality-of-service, interpersonal harms, and social system/societal harms — and sub-themes are presented along with a description of these categories. We conclude with a discussion of challenges and opportunities for future research.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604673",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 19\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "723–741",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Sociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction",
		"URL": "https://doi.org/10.1145/3600211.3604673",
		"author": [
			{
				"family": "Shelby",
				"given": "Renee"
			},
			{
				"family": "Rismani",
				"given": "Shalaleh"
			},
			{
				"family": "Henne",
				"given": "Kathryn"
			},
			{
				"family": "Moon",
				"given": "AJung"
			},
			{
				"family": "Rostamzadeh",
				"given": "Negar"
			},
			{
				"family": "Nicholas",
				"given": "Paul"
			},
			{
				"family": "Yilla-Akbari",
				"given": "N'Mah"
			},
			{
				"family": "Gallegos",
				"given": "Jess"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			},
			{
				"family": "Garcia",
				"given": "Emilio"
			},
			{
				"family": "Virk",
				"given": "Gurleen"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "yetukuriUserGuidedActionable2023",
		"type": "paper-conference",
		"abstract": "Machine Learning’s proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user’s actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604708",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "742–751",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Towards user guided actionable recourse",
		"URL": "https://doi.org/10.1145/3600211.3604708",
		"author": [
			{
				"family": "Yetukuri",
				"given": "Jayanth"
			},
			{
				"family": "Hardy",
				"given": "Ian"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "grabowiczLearningDiscriminatoryTraining2023",
		"type": "paper-conference",
		"abstract": "Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups’ intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulations, and is computationally lightweight — it can be used with any supervised learning model to prevent direct and indirect discrimination via proxies while maximizing model accuracy for business necessity.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604710",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "752–763",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Learning from discriminatory training data",
		"URL": "https://doi.org/10.1145/3600211.3604710",
		"author": [
			{
				"family": "Grabowicz",
				"given": "Przemyslaw"
			},
			{
				"family": "Perello",
				"given": "Nicholas"
			},
			{
				"family": "Takatsu",
				"given": "Kenta"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "bhanotStresstestingBiasMitigation2023",
		"type": "paper-conference",
		"abstract": "To address the growing concern of unfairness in Artificial Intelligence (AI), several bias mitigation algorithms have been introduced in prior research. Their capabilities are often evaluated on certain overly-used datasets without rigorously stress-testing them under simultaneous train and test distribution shifts. To address this, we investigate the fairness vulnerabilities of these algorithms across several distribution shift scenarios using synthetic data, to highlight scenarios where these algorithms do and don’t work to encourage their trustworthy use. The paper makes three important contributions. Firstly, we propose a flexible pipeline called the Fairness Auditor to systematically stress-test bias mitigation algorithms using multiple synthetic datasets with shifts. Secondly, we introduce the Deviation Metric for measuring the fairness and utility performance of these algorithms under such shifts. Thirdly, we propose an interactive reporting tool for comparing algorithmic performance across various synthetic datasets, mitigation algorithms and metrics called the Fairness Report.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604713",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "764–774",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Stress-testing bias mitigation algorithms to understand fairness vulnerabilities",
		"URL": "https://doi.org/10.1145/3600211.3604713",
		"author": [
			{
				"family": "Bhanot",
				"given": "Karan"
			},
			{
				"family": "Baldini",
				"given": "Ioana"
			},
			{
				"family": "Wei",
				"given": "Dennis"
			},
			{
				"family": "Zeng",
				"given": "Jiaming"
			},
			{
				"family": "Bennett",
				"given": "Kristin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "juijnPerceivedAlgorithmicFairness2023",
		"type": "paper-conference",
		"abstract": "Growing concerns about the fairness of algorithmic decision-making systems have prompted a proliferation of mathematical formulations aimed at remedying algorithmic bias. Yet, integrating mathematical fairness alone into algorithms is insufficient to ensure their acceptance, trust, and support by humans. It is also essential to understand what humans perceive as fair. In this study, we, therefore, conduct an empirical user study into crowdworkers’ algorithmic fairness perceptions, focusing on algorithmic hiring. We build on perspectives from organizational justice theory, which categorizes fairness into distributive, procedural, and interactional components. By doing so, we find that algorithmic fairness perceptions are higher when crowdworkers are provided not only with information about the algorithmic outcome but also about the decision-making process. Remarkably, we observe this effect even when the decision-making process can be considered unfair, when gender, a sensitive attribute, is used as a main feature. By showing realistic trade-offs between fairness criteria, we moreover find a preference for equalizing false negatives over equalizing selection rates amongst groups. Our findings highlight the importance of considering all components of algorithmic fairness, rather than solely treating it as an outcome distribution problem. Importantly, our study contributes to the literature on the connection between mathematical– and perceived algorithmic fairness, and highlights the potential benefits of leveraging organizational justice theory to enhance the evaluation of perceived algorithmic fairness.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604677",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "775–785",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Perceived algorithmic fairness using organizational justice theory: An empirical case study on algorithmic hiring",
		"URL": "https://doi.org/10.1145/3600211.3604677",
		"author": [
			{
				"family": "Juijn",
				"given": "Guusje"
			},
			{
				"family": "Stoimenova",
				"given": "Niya"
			},
			{
				"family": "Reis",
				"given": "João"
			},
			{
				"family": "Nguyen",
				"given": "Dong"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "naikSocialBiasesTexttoimage2023",
		"type": "paper-conference",
		"abstract": "Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people from results for both models. Such biases can get mitigated by increasing the amount of specification in the prompt itself, although the prompting mitigation will not address discrepancies in image quality or other usages of the model or its representations in other scenarios. Further, we observe personality traits being associated with only a limited set of people at the intersection of race, gender, and age. Finally, an analysis of geographical location representations on everyday situations (e.g., park, food, weddings) shows that for most situations, images generated through default location-neutral prompts are closer and more similar to images generated for locations of United States and Germany.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604711",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 23\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "786–808",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Social biases through the text-to-image generation lens",
		"URL": "https://doi.org/10.1145/3600211.3604711",
		"author": [
			{
				"family": "Naik",
				"given": "Ranjita"
			},
			{
				"family": "Nushi",
				"given": "Besmira"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "aliEvaluatingFairnessDiscriminative2023",
		"type": "paper-conference",
		"abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI’s CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604720",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 25\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "809–833",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Evaluating the fairness of discriminative foundation models in computer vision",
		"URL": "https://doi.org/10.1145/3600211.3604720",
		"author": [
			{
				"family": "Ali",
				"given": "Junaid"
			},
			{
				"family": "Kleindessner",
				"given": "Matthäus"
			},
			{
				"family": "Wenzel",
				"given": "Florian"
			},
			{
				"family": "Budhathoki",
				"given": "Kailash"
			},
			{
				"family": "Cevher",
				"given": "Volkan"
			},
			{
				"family": "Russell",
				"given": "Chris"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "lammertsHowYouFeel2023",
		"type": "paper-conference",
		"abstract": "Hate speech moderation remains a challenging task for social media platforms. Human-AI collaborative systems offer the potential to combine the strengths of humans’ reliability and the scalability of machine learning to tackle this issue effectively. While methods for task handover in human-AI collaboration exist that consider the costs of incorrect predictions, insufficient attention has been paid to accurately estimating these costs. In this work, we propose a value-sensitive rejection mechanism that automatically rejects machine decisions for human moderation based on users’ value perceptions regarding machine decisions. We conduct a crowdsourced survey study with 160 participants to evaluate their perception of correct and incorrect machine decisions in the domain of hate speech detection, as well as occurrences where the system rejects making a prediction. Here, we introduce Magnitude Estimation, an unbounded scale, as the preferred method for measuring user (dis)agreement with machine decisions. Our results show that Magnitude Estimation can provide a reliable measurement of participants’ perception of machine decisions. By integrating user-perceived value into human-AI collaboration, we further show that it can guide us in 1) determining when to accept or reject machine decisions to obtain the optimal total value a model can deliver and 2) selecting better classification models as compared to the more widely used target of model accuracy.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604655",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "834–844",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "How do you feel? Measuring user-perceived value for rejecting machine decisions in hate speech detection",
		"URL": "https://doi.org/10.1145/3600211.3604655",
		"author": [
			{
				"family": "Lammerts",
				"given": "Philippe"
			},
			{
				"family": "Lippmann",
				"given": "Philip"
			},
			{
				"family": "Hsu",
				"given": "Yen-Chia"
			},
			{
				"family": "Casati",
				"given": "Fabio"
			},
			{
				"family": "Yang",
				"given": "Jie"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rarrickGATEChallengeSet2023",
		"type": "paper-conference",
		"abstract": "Although recent years have brought significant progress in improving translation of unambiguously gendered sentences, translation of ambiguously gendered input remains relatively unexplored. When source gender is ambiguous, machine translation models typically default to stereotypical gender roles, perpetuating harmful bias. Recent work has led to the development of \"gender rewriters\" that generate alternative gender translations on such ambiguous inputs, but such systems are plagued by poor linguistic coverage. To encourage better performance on this task we present and release GATE, a linguistically diverse corpus of gender-ambiguous source sentences along with multiple alternative target language translations. We also provide tools for evaluation and system analysis when using GATE and use them to evaluate our translation rewriter.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604675",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 10\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "845–854",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "GATE: A challenge set for gender-ambiguous translation examples",
		"URL": "https://doi.org/10.1145/3600211.3604675",
		"author": [
			{
				"family": "Rarrick",
				"given": "Spencer"
			},
			{
				"family": "Naik",
				"given": "Ranjita"
			},
			{
				"family": "Mathur",
				"given": "Varun"
			},
			{
				"family": "Poudel",
				"given": "Sundar"
			},
			{
				"family": "Chowdhary",
				"given": "Vishal"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "chanReclaimingDigitalCommons2023",
		"type": "paper-conference",
		"abstract": "Democratization of AI means not only that people can freely use AI, but also that people can collectively decide how AI is to be used. In particular, collective decision-making power is required to redress the negative externalities from the development of increasingly advanced AI systems, including degradation of the digital commons and unemployment from automation. The rapid pace of AI development and deployment currently leaves little room for this power. Monopolized in the hands of private corporations, the development of the most capable foundation models has proceeded largely without public input. There is currently no implemented mechanism for ensuring that the economic value generated by such models is redistributed to account for their negative externalities. The citizens that have generated the data necessary to train models do not have input on how their data are to be used. In this work, we propose that a public data trust assert control over training data for foundation models. In particular, this trust should scrape the internet as a digital commons, to license to commercial model developers for a percentage cut of revenues from deployment. First, we argue in detail for the existence of such a trust. We also discuss feasibility and potential risks. Second, we detail a number of ways for a data trust to incentivize model developers to use training data only from the trust. We propose a mix of verification mechanisms, potential regulatory action, and positive incentives. We conclude by highlighting other potential benefits of our proposed data trust and connecting our work to ongoing efforts in data and compute governance.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604658",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "855–868",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Reclaiming the digital commons: A public data trust for training data",
		"URL": "https://doi.org/10.1145/3600211.3604658",
		"author": [
			{
				"family": "Chan",
				"given": "Alan"
			},
			{
				"family": "Bradley",
				"given": "Herbie"
			},
			{
				"family": "Rajkumar",
				"given": "Nitarshan"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "collinsHumanUncertaintyConceptbased2023",
		"type": "paper-conference",
		"abstract": "Placing a human in the loop may help abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604692",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 21\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "869–889",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Human uncertainty in concept-based AI systems",
		"URL": "https://doi.org/10.1145/3600211.3604692",
		"author": [
			{
				"family": "Collins",
				"given": "Katherine Maeve"
			},
			{
				"family": "Barker",
				"given": "Matthew"
			},
			{
				"family": "Espinosa Zarlenga",
				"given": "Mateo"
			},
			{
				"family": "Raman",
				"given": "Naveen"
			},
			{
				"family": "Bhatt",
				"given": "Umang"
			},
			{
				"family": "Jamnik",
				"given": "Mateja"
			},
			{
				"family": "Sucholutsky",
				"given": "Ilia"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "Dvijotham",
				"given": "Krishnamurthy"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "khosrowiDiffusingCreatorAttributing2023",
		"type": "paper-conference",
		"abstract": "The recent wave of generative AI (GAI) systems like Stable Diffusion that can produce images from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GAI? Existing views on creatorship are mixed: some insist that GAI systems are mere tools, and human prompters are creators proper; others are more open to acknowledging more significant roles for GAI, but most conceive of creatorship in an all-or-nothing fashion. We develop a novel view, called CCC (collective-centered creation), that improves on these existing positions. On CCC, GAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GAI systems, developers, producers of training data and others. Importantly, CCC maintains that GAI systems can sometimes be part of a co-creating collective. We detail how CCC can advance existing debates and resolve controversies around creatorship involving GAI.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604716",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 11\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "890–900",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Diffusing the creator: Attributing credit for generative AI outputs",
		"URL": "https://doi.org/10.1145/3600211.3604716",
		"author": [
			{
				"family": "Khosrowi",
				"given": "Donal"
			},
			{
				"family": "Finn",
				"given": "Finola"
			},
			{
				"family": "Clark",
				"given": "Elinor"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "ghoshChatGPTPerpetuatesGender2023",
		"type": "paper-conference",
		"abstract": "In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in machine translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT’s accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7th most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g., man = doctor, woman = nurse) or actions (e.g., woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to ‘he’ or ‘she’. We also observe ChatGPT completely failing to translate the English gender-neutral singular pronoun ‘they’ into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AI systems that perform machine translation to better accommodate such low-resource languages.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604672",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "901–912",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "ChatGPT perpetuates gender bias in machine translation and ignores non-gendered pronouns: Findings across bengali and five other low-resource languages",
		"URL": "https://doi.org/10.1145/3600211.3604672",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "rastogiSupportingHumanAICollaboration2023",
		"type": "paper-conference",
		"abstract": "Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest&nbsp;[36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604712",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 14\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "913–926",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Supporting human-AI collaboration in auditing LLMs with LLMs",
		"URL": "https://doi.org/10.1145/3600211.3604712",
		"author": [
			{
				"family": "Rastogi",
				"given": "Charvi"
			},
			{
				"family": "Tulio Ribeiro",
				"given": "Marco"
			},
			{
				"family": "King",
				"given": "Nicholas"
			},
			{
				"family": "Nori",
				"given": "Harsha"
			},
			{
				"family": "Amershi",
				"given": "Saleema"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "singhMeasuresDisparityTheir2023",
		"type": "paper-conference",
		"abstract": "Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM conference on AI, ethics, and society",
		"DOI": "10.1145/3600211.3604697",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"note": "number-of-pages: 12\npublisher-place: ¡conf-loc¿, ¡city¿Montréal¡/city¿, ¡state¿QC¡/state¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿",
		"page": "927–938",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"title": "Measures of disparity and their efficient estimation",
		"URL": "https://doi.org/10.1145/3600211.3604697",
		"author": [
			{
				"family": "Singh",
				"given": "Harvineet"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "saisubramanianBalancingTradeoffClustering2020",
		"type": "paper-conference",
		"abstract": "Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β\\!=\\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.",
		"collection-title": "AIES '20",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3375627.3375843",
		"event-place": "New York, NY, USA",
		"ISBN": "978-1-4503-7110-0",
		"page": "351–357",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "Balancing the Tradeoff Between Clustering Value and Interpretability",
		"URL": "https://dl.acm.org/doi/10.1145/3375627.3375843",
		"author": [
			{
				"family": "Saisubramanian",
				"given": "Sandhya"
			},
			{
				"family": "Galhotra",
				"given": "Sainyam"
			},
			{
				"family": "Zilberstein",
				"given": "Shlomo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					7
				]
			]
		}
	},
	{
		"id": "jiangAIArtIts2023",
		"type": "paper-conference",
		"abstract": "The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial “generative AI Art” products have entered the market, making generative AI an estimated $48B industry [125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.",
		"collection-title": "AIES '23",
		"container-title": "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",
		"DOI": "10.1145/3600211.3604681",
		"event-place": "New York, NY, USA",
		"ISBN": "9798400702310",
		"page": "363–374",
		"publisher": "Association for Computing Machinery",
		"publisher-place": "New York, NY, USA",
		"source": "ACM Digital Library",
		"title": "AI Art and its Impact on Artists",
		"URL": "https://dl.acm.org/doi/10.1145/3600211.3604681",
		"author": [
			{
				"family": "Jiang",
				"given": "Harry H."
			},
			{
				"family": "Brown",
				"given": "Lauren"
			},
			{
				"family": "Cheng",
				"given": "Jessica"
			},
			{
				"family": "Khan",
				"given": "Mehtab"
			},
			{
				"family": "Gupta",
				"given": "Abhishek"
			},
			{
				"family": "Workman",
				"given": "Deja"
			},
			{
				"family": "Hanna",
				"given": "Alex"
			},
			{
				"family": "Flowers",
				"given": "Johnathan"
			},
			{
				"family": "Gebru",
				"given": "Timnit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					4,
					25
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					29
				]
			]
		}
	},
	{
		"id": "agizaPoliTuneAnalyzingImpact2024",
		"type": "article-journal",
		"abstract": "In an era where language models are increasingly integrated into decision-making and communication, understanding the biases within Large Language Models (LLMs) becomes imperative, especially when these models are applied in the economic and political domains. This work investigates the impact of fine-tuning and data selection on economic and political biases in LLMs. In this context, we introduce PoliTune, a fine-tuning methodology to explore the systematic aspects of aligning LLMs with specific ideologies, mindful of the biases that arise from their extensive training on diverse datasets. Distinct from earlier efforts that either focus on smaller models or entail resource-intensive pre-training, PoliTune employs Parameter-Efficient Fine-Tuning (PEFT) techniques, which allow for the alignment of LLMs with targeted ideologies by modifying a small subset of parameters. We introduce a systematic method for using the open-source LLM Llama3-70B for dataset selection, annotation, and synthesizing a preferences dataset for Direct Preference Optimization (DPO) to align the model with a given political ideology. We assess the effectiveness of PoliTune through both quantitative and qualitative evaluations of aligning open-source LLMs (Llama3-8B and Mistral-7B) to different ideologies. Our work analyzes the potential of embedding specific biases into LLMs and contributes to the dialogue on the ethical application of AI, highlighting the importance of deploying AI in a manner that aligns with societal values.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "2-12",
		"source": "ojs.aaai.org",
		"title": "PoliTune: Analyzing the Impact of Data Selection and Fine-Tuning on Economic and Political Biases in Large Language Models",
		"title-short": "PoliTune",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31612",
		"volume": "7",
		"author": [
			{
				"family": "Agiza",
				"given": "Ahmed"
			},
			{
				"family": "Mostagir",
				"given": "Mohamed"
			},
			{
				"family": "Reda",
				"given": "Sherief"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "akbulutAllTooHuman2024",
		"type": "article-journal",
		"abstract": "The development of highly-capable conversational agents, underwritten by large language models, has the potential to shape user interaction with this technology in profound ways, particularly when the technology is anthropomorphic, or appears human-like. Although the effects of anthropomorphic AI are often benign, anthropomorphic design features also create new kinds of risk. For example, users may form emotional connections to human-like AI, creating the risk of infringing on user privacy and autonomy through over-reliance. To better understand the possible pitfalls of anthropomorphic AI systems, we make two contributions: first, we explore anthropomorphic features that have been embedded in interactive systems in the past, and leverage this precedent to highlight the current implications of anthropomorphic design. Second, we propose research directions for informing the ethical design of anthropomorphic AI. In advancing  the responsible development of AI, we promote approaches to the ethical foresight, evaluation, and mitigation of harms arising from user interactions with anthropomorphic AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "13-26",
		"source": "ojs.aaai.org",
		"title": "All Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI",
		"title-short": "All Too Human?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31613",
		"volume": "7",
		"author": [
			{
				"family": "Akbulut",
				"given": "Canfer"
			},
			{
				"family": "Weidinger",
				"given": "Laura"
			},
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "alcarazEstimatingWeightsReasons2024",
		"type": "article-journal",
		"abstract": "We present a new approach to representation and acquisition of normative information for machine ethics. It combines an influential philosophical account of the fundamental structure of morality with argumentation theory and machine learning. According to the philosophical account, the deontic status of an action -- whether it is required, forbidden, or permissible -- is determined through the interaction of \"normative reasons\" of varying strengths or weights. We first provide a formal characterization of this account, by modeling it in (weighted) argumentation graphs. We then use it to model ethical learning: the basic idea is to use a set of cases for which deontic statuses are known to estimate the weights of normative reasons in operation in these cases, and to use these weight estimates to determine the deontic statuses of actions in new cases. The result is an approach that has the advantages of both bottom-up and top-down approaches to machine ethics: normative information is acquired through the interaction with training data, and its meaning is clear. We also report the results of some initial experiments with the model.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "27-38",
		"source": "ojs.aaai.org",
		"title": "Estimating Weights of Reasons Using Metaheuristics: A Hybrid Approach to Machine Ethics",
		"title-short": "Estimating Weights of Reasons Using Metaheuristics",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31614",
		"volume": "7",
		"author": [
			{
				"family": "Alcaraz",
				"given": "Benoît"
			},
			{
				"family": "Knoks",
				"given": "Aleks"
			},
			{
				"family": "Streit",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arnoldIntroducingAIGovernance2024",
		"type": "article-journal",
		"abstract": "AI-related laws, standards, and norms are emerging rapidly. However, a lack of shared descriptive concepts and monitoring infrastructure undermine efforts to track, understand, and improve AI governance. We introduce AGORA (the AI Governance and Regulatory Archive), a rigorously compiled and enriched dataset of AI-focused laws and policies encompassing diverse jurisdictions, institutions, and contexts related to AI. AGORA is oriented around an original taxonomy describing risks, potential harms, governance strategies, incentives for compliance, and application domains addressed in AI regulatory documents. At launch, AGORA included data on over 330 instruments, with new entries being added continuously. We describe the manual and automated processes through which these data are systematically compiled, screened, annotated, and validated, enabling deep, efficient, and reliable analysis of the emerging AI governance landscape. The dataset, supporting information, and analyses are available through a public web interface (https://agora.eto.tech) and bulk dataset.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "39-48",
		"source": "ojs.aaai.org",
		"title": "Introducing the AI Governance and Regulatory Archive (AGORA): An Analytic Infrastructure for Navigating the Emerging AI Governance Landscape",
		"title-short": "Introducing the AI Governance and Regulatory Archive (AGORA)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31615",
		"volume": "7",
		"author": [
			{
				"family": "Arnold",
				"given": "Zachary"
			},
			{
				"family": "Schiff",
				"given": "Daniel S."
			},
			{
				"family": "Schiff",
				"given": "Kaylyn Jackson"
			},
			{
				"family": "Love",
				"given": "Brian"
			},
			{
				"family": "Melot",
				"given": "Jennifer"
			},
			{
				"family": "Singh",
				"given": "Neha"
			},
			{
				"family": "Jenkins",
				"given": "Lindsay"
			},
			{
				"family": "Lin",
				"given": "Ashley"
			},
			{
				"family": "Pilz",
				"given": "Konstantin"
			},
			{
				"family": "Enweareazu",
				"given": "Ogadinma"
			},
			{
				"family": "Girard",
				"given": "Tyler"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arzaghiUnderstandingIntrinsicSocioeconomic2024",
		"type": "article-journal",
		"abstract": "Large Language Models (LLMs) are increasingly integrated into critical decision-making processes, such as loan approvals and visa applications, where inherent biases can lead to discriminatory outcomes. In this paper, we examine the nuanced relationship between demographic attributes and socioeconomic biases in LLMs, a crucial yet understudied area of fairness in LLMs. We introduce a novel dataset of one million English sentences to systematically quantify socioeconomic biases across various demographic groups. Our findings reveal pervasive socioeconomic biases in both established models such as GPT-2 and state-of-the-art models like Llama 2 and Falcon. We demonstrate that these biases are significantly amplified when considering intersectionality, with LLMs exhibiting a remarkable capacity to extract multiple demographic attributes from names and then correlate them with specific socioeconomic biases. This research highlights the urgent necessity for proactive and robust bias mitigation techniques to safeguard against discriminatory outcomes when deploying these powerful models in critical real-world applications. Warning: This paper discusses and contains content that can be offensive or upsetting.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "49-60",
		"source": "ojs.aaai.org",
		"title": "Understanding Intrinsic Socioeconomic Biases in Large Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31616",
		"volume": "7",
		"author": [
			{
				"family": "Arzaghi",
				"given": "Mina"
			},
			{
				"family": "Carichon",
				"given": "Florian"
			},
			{
				"family": "Farnadi",
				"given": "Golnoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "arzbergerNothingComesIts2024",
		"type": "article-journal",
		"abstract": "Work on value alignment aims to ensure that human values\nare respected by AI systems. However, existing approaches\ntend to rely on universal framings of human values that obscure\nthe question of which values the systems should capture\nand align with, given the variety of operational situations.\nThis often results in AI systems that privilege only a\nselected few while perpetuating problematic norms grounded\non biases, ultimately causing equity and justice issues. In this\nperspective paper, we unpack the limitations of predominant\nalignment practices of reinforcement learning from human\nfeedback (RLHF) for LLMs through the lens of situated values.\nWe build on feminist epistemology to argue that at the\ndesign-time, RLHF has problems with representation in the\nsubjects providing feedback and implicitness in the conceptualization\nof values and situations of real-world users while\nlacking system adaptation to real user situations at the use time.\nTo address these shortcomings, we propose three research\ndirections: 1) situated annotation to capture information\nabout the crowdworker’s and user’s values and judgments\nin relation to specific situations at both the design and\nuse-time, 2) expressive instruction to encode plural values\nfor instructing LLMs systems at design-time, and 3) reflexive\nadaptation to leverage situational knowledge for system\nadaption at use-time. We conclude by reflecting on the\npractical challenges of pursuing these research directions and\nsituated value alignment of AI more broadly.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "61-73",
		"source": "ojs.aaai.org",
		"title": "Nothing Comes Without Its World – Practical Challenges of Aligning LLMs to Situated Human Values through RLHF",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31617",
		"volume": "7",
		"author": [
			{
				"family": "Arzberger",
				"given": "Anne"
			},
			{
				"family": "Buijsman",
				"given": "Stefan"
			},
			{
				"family": "Lupetti",
				"given": "Maria Luce"
			},
			{
				"family": "Bozzon",
				"given": "Alessandro"
			},
			{
				"family": "Yang",
				"given": "Jie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "attiaKidWhisperBridgingPerformance2024",
		"type": "article-journal",
		"abstract": "Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn’t readily extend to ASR for children due to the lim- ited availability of suitable child-specific databases and the distinct characteristics of children’s speech. A recent study investigated leveraging the My Science Tutor (MyST) chil- dren’s speech corpus to enhance Whisper’s performance in recognizing children’s speech. They were able to demon- strate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children’s ASR performance and the effect of fine-tuning in improving the transcription of disfluent speech.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "74-80",
		"source": "ojs.aaai.org",
		"title": "Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults",
		"title-short": "Kid-Whisper",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31618",
		"volume": "7",
		"author": [
			{
				"family": "Attia",
				"given": "Ahmed Adel"
			},
			{
				"family": "Liu",
				"given": "Jing"
			},
			{
				"family": "Ai",
				"given": "Wei"
			},
			{
				"family": "Demszky",
				"given": "Dorottya"
			},
			{
				"family": "Espy-Wilson",
				"given": "Carol"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bansakPublicAttitudesPerformance2024",
		"type": "article-journal",
		"abstract": "This study explores public preferences between algorithmic and human decision-makers (DMs) in high-stakes contexts, how these preferences are impacted by performance metrics, and whether the public's evaluation of performance differs when considering algorithmic versus human DMs. Leveraging a conjoint experimental design, respondents (n = 9,000) chose between pairs of DM profiles in two scenarios: pre-trial release decisions and bank loan decisions. DM profiles varied on the DM’s type (human vs. algorithm) and on three metrics—defendant crime rate/loan default rate, false positive rate (FPR) among white defendants/applicants, and FPR among minority defendants/applicants—as well as an implicit (un)fairness metric defined by the absolute difference between the two FPRs. Controlling for performance, we observe a general tendency to favor human DMs, though this is driven by a subset of respondents who expect human DMs to perform better in the real world, and there is an analogous group with the opposite preference for algorithmic DMs. We also find that the relative importance of the four performance metrics remains consistent across DM type, suggesting that the public's preferences related to DM performance do not vary fundamentally between algorithmic and human DMs. Taken together, the results collectively suggest that people have very different beliefs about what type of DM (human or algorithmic) will deliver better performance and should be preferred, but they have similar desires in terms of what they want that performance to be regardless of DM type.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "81-81",
		"source": "ojs.aaai.org",
		"title": "Public Attitudes on Performance for Algorithmic and Human Decision-Makers (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31619",
		"volume": "7",
		"author": [
			{
				"family": "Bansak",
				"given": "Kirk"
			},
			{
				"family": "Paulson",
				"given": "Elisabeth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "barnettSimulatingPolicyImpacts2024",
		"type": "article-journal",
		"abstract": "The rapid advancement of AI technologies yields numerous future impacts on individuals and society. Policymakers are tasked to react quickly and establish policies that mitigate those impacts. However, anticipating the effectiveness of policies is a difficult task, as some impacts might only be observable in the future and respective policies might not be applicable to the future development of AI. In this work we develop a method for using large language models (LLMs) to evaluate the efficacy of a given piece of policy at mitigating specified negative impacts. We do so by using GPT-4 to generate scenarios both pre- and post-introduction of policy and translating these vivid stories into metrics based on human perceptions of impacts. We leverage an already established taxonomy of impacts of generative AI in the media environment to generate a set of scenario pairs both mitigated and non-mitigated by the transparency policy in Article 50 of the EU AI Act. We then run a user study (n=234) to evaluate these scenarios across four risk-assessment dimensions: severity, plausibility, magnitude, and specificity to vulnerable populations. We find that this transparency legislation is perceived to be effective at mitigating harms in areas such as labor and well-being, but largely ineffective in areas such as social cohesion and security. Through this case study we demonstrate the efficacy of our method as a tool to iterate on the effectiveness of policy for mitigating various negative impacts. We expect this method to be useful to researchers or other stakeholders who want to brainstorm the potential utility of different pieces of policy or other mitigation strategies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "82-93",
		"source": "ojs.aaai.org",
		"title": "Simulating Policy Impacts: Developing a Generative Scenario Writing Method to Evaluate the Perceived Effects of Regulation",
		"title-short": "Simulating Policy Impacts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31620",
		"volume": "7",
		"author": [
			{
				"family": "Barnett",
				"given": "Julia"
			},
			{
				"family": "Kieslich",
				"given": "Kimon"
			},
			{
				"family": "Diakopoulos",
				"given": "Nicholas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bartschOriginOpportunitiesDevelopers2024",
		"type": "article-journal",
		"abstract": "Open source (OS) software projects in artificial intelligence (AI), such as TensorFlow and scikit-learn, depend on developers' continuous, voluntary code contributions. However, recent security incidents highlighted substantial risks in such software, requiring examinations of factors motivating developers to continuously contribute high-quality code (i.e., providing secure and reliable code fulfilling its functions). Prior research suggests code accountability (i.e., requirements to explain and justify contributed code) to improve code quality, enforced through external accountability mechanisms such as sanctions and rewards. However, the OS domain often lacks such mechanisms, questioning whether and how code accountability arises in this domain and how it affects code contributions. To address these questions, we conducted 26 semi-structured interviews with developers contributing to OS AI software projects. Our findings reveal that despite the absence of external accountability mechanisms, system-, project-, and individual-related factors evoke developers' perceived code accountability. Notably, we discovered a trade-off as high perceived code accountability is associated with higher code quality but discourages developers from participating in OS AI software projects. Overall, this study contributes to understanding the nuanced roles of perceived code accountability in continuously contributing high-quality code without external accountability mechanisms and highlights the complex trade-offs developers face in OS AI software projects.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "94-106",
		"source": "ojs.aaai.org",
		"title": "The Origin and Opportunities of Developers’ Perceived Code Accountability in Open Source AI Software Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31621",
		"volume": "7",
		"author": [
			{
				"family": "Bartsch",
				"given": "Sebastian Clemens"
			},
			{
				"family": "Lother",
				"given": "Moritz"
			},
			{
				"family": "Schmidt",
				"given": "Jan-Hendrik"
			},
			{
				"family": "Adam",
				"given": "Martin"
			},
			{
				"family": "Benlian",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "berettaGenderPixelsPathways2024",
		"type": "article-journal",
		"abstract": "In the field of Computer Vision (CV), the study of bias, including gender bias, has received a significant area of attention in recent years. However, these studies predominantly operate within a binary, cisnormative framework, often neglecting the complexities of non-binary gender identities. To date, there is no comprehensive analysis of how CV is addressing the mitigation of bias for non-binary individuals or how it seeks solutions that transcend a binary view of gender. This systematic scoping review aims to fill this gap by analyzing over 60 papers that delve into gender biases in CV, with a particular emphasis on non-binary perspectives. Our findings indicate that despite the increasing recognition of gender as a multifaceted and complex construct, practical applications of this understanding in CV remain limited and fragmented. The review critically examines the foundational research critiquing the binarism in CV and explores emerging approaches that challenge and move beyond this limited perspective. We highlight innovative solutions, including algorithmic adaptations and the creation of more inclusive and diverse datasets. Furthermore, the study emphasizes the importance of integrating gender theory into CV practices to develop more accurate and representative models.\nOur recommendations advocate for interdisciplinary collaboration, particularly with Gender Studies, to foster a more nuanced understanding of gender in CV. This study serves as a pivotal step towards redefining gender representation in CV, encouraging researchers and practitioners to embrace and incorporate a broader spectrum of gender identities in their work.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "107-119",
		"source": "ojs.aaai.org",
		"title": "Gender in Pixels: Pathways to Non-binary Representation in Computer Vision",
		"title-short": "Gender in Pixels",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31622",
		"volume": "7",
		"author": [
			{
				"family": "Beretta",
				"given": "Elena"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bignottiLegalMindsAlgorithmic2024",
		"type": "article-journal",
		"abstract": "In this paper, we conduct an empirical analysis of how large language models (LLMs), specifically GPT-4, interpret constitutional principles in complex decision-making scenarios.\nWe examine rulings from the Italian Constitutional Court on bioethics issues that involve trade-offs between competing values and compare GPT’s legal arguments on these issues to those presented by the State, the Court, and the applicants.\nOur results indicate that GPT consistently aligns more closely with progressive interpretations of the Constitution, often overlooking competing values and mirroring the applicants’ views rather than the more conservative perspectives of the State or the Court’s moderate positions. Our findings raise important questions about the value alignment of LLMs in scenarios where societal values are in conflict, as our experiment demonstrates GPT’s tendency to align with progressive legal interpretations. We thus underscore the importance of testing alignment in real-world scenarios and considering the implications of deploying LLMs in decision-making processes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "120-130",
		"source": "ojs.aaai.org",
		"title": "Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios",
		"title-short": "Legal Minds, Algorithmic Decisions",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31623",
		"volume": "7",
		"author": [
			{
				"family": "Bignotti",
				"given": "Camilla"
			},
			{
				"family": "Camassa",
				"given": "Carolina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "biscontiFormalAccountTrustworthiness2024",
		"type": "article-journal",
		"abstract": "This paper proposes a formal account of AI trustworthiness, connecting both intrinsic and perceived trustworthiness in an operational schematization. We argue that trustworthiness extends beyond the inherent capabilities of an AI system to include significant influences from observers' perceptions, such as perceived transparency, agency locus, and human oversight. While the concept of perceived trustworthiness is discussed in the literature, few attempts have been made to connect it with the intrinsic trustworthiness of AI systems. Our analysis introduces a novel schematization to quantify trustworthiness by assessing the discrepancies between expected and observed behaviors and how these affect perceived uncertainty and trust. The paper provides a formalization for measuring trustworthiness, taking into account both perceived and intrinsic characteristics. By detailing the factors that influence trust, this study aims to foster more ethical and widely accepted AI technologies, ensuring they meet both functional and ethical criteria.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "131-140",
		"source": "ojs.aaai.org",
		"title": "A Formal Account of Trustworthiness: Connecting Intrinsic and Perceived Trustworthiness",
		"title-short": "A Formal Account of Trustworthiness",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31624",
		"volume": "7",
		"author": [
			{
				"family": "Bisconti",
				"given": "Piercosma"
			},
			{
				"family": "Aquilino",
				"given": "Letizia"
			},
			{
				"family": "Marchetti",
				"given": "Antonella"
			},
			{
				"family": "Nardi",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "blili-hamelinUnsocialIntelligenceInvestigation2024",
		"type": "article-journal",
		"abstract": "Dreams of machines rivaling human intelligence have shaped the field of AI since its inception. Yet, the very meaning of human-level AI or artificial general intelligence (AGI) remains elusive and contested. Definitions of AGI embrace a diverse range of incompatible values and assumptions. Contending with the fractured worldviews of AGI discourse is vital for critiques that pursue different values and futures. To that end, we provide a taxonomy of AGI definitions, laying the ground for examining the key social, political, and ethical assumptions they make. We highlight instances in which these definitions frame AGI or human-level AI as a technical topic and expose the value-laden choices being implicitly made. Drawing on feminist, STS, and social science scholarship on the political and social character of intelligence in both humans and machines, we propose contextual, democratic, and participatory paths to imagining future forms of machine intelligence. The development of future forms of AI must involve explicit attention to the values it encodes, the people it includes or excludes, and a commitment to epistemic justice.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "141-155",
		"source": "ojs.aaai.org",
		"title": "Unsocial Intelligence: An Investigation of the Assumptions of AGI Discourse",
		"title-short": "Unsocial Intelligence",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31625",
		"volume": "7",
		"author": [
			{
				"family": "Blili-Hamelin",
				"given": "Borhane"
			},
			{
				"family": "Hancox-Li",
				"given": "Leif"
			},
			{
				"family": "Smart",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "boerstlerStabilityMoralPreferences2024",
		"type": "article-journal",
		"abstract": "Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants’ responses are stable over time such that, all other things being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants’ true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don’t track. If participants’ moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how participants’ true moral preferences, opinions, and judgments can be ascertained. We address this possibility here by asking the same survey participants the same moral questions about which patient should receive a kidney when only one is available ten times in ten different sessions over two weeks, varying only presentation order across sessions. We measured how often participants gave different responses to simple (Study One) and more complicated (Study Two) controversial and uncontroversial repeated scenarios. On average, the fraction of times participants changed their responses to controversial scenarios (i.e., were unstable) was around 10-18% (±14-15%) across studies, and this instability is observed to have positive associations with response time and decision-making difficulty. We discuss the implications of these results for the efficacy of common moral preference elicitation methods, highlighting the role of response instability in potentially causing value misalignment between the stakeholders and AI tools trained on their moral judgments.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "156-167",
		"source": "ojs.aaai.org",
		"title": "On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods",
		"title-short": "On The Stability of Moral Preferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31626",
		"volume": "7",
		"author": [
			{
				"family": "Boerstler",
				"given": "Kyle"
			},
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Chan",
				"given": "Lok"
			},
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "boguckaCodesigningAIImpact2024",
		"type": "article-journal",
		"abstract": "In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of AI systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "168-180",
		"source": "ojs.aaai.org",
		"title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31627",
		"volume": "7",
		"author": [
			{
				"family": "Bogucka",
				"given": "Edyta"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Šćepanović",
				"given": "Sanja"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniFoundationModelTransparency2024",
		"type": "article-journal",
		"abstract": "Foundation models are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how foundation model developers should provide transparency about the development and deployment of their models, we propose Foundation Model Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms prompted social media transparency reports, our objective is to institutionalize transparency reporting for foundation models while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the Foundation Model Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g. the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage foundation model developers to regularly publish transparency reports, building upon recommendations from the G7 and the White House.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "181-195",
		"source": "ojs.aaai.org",
		"title": "Foundation Model Transparency Reports",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31628",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Klyman",
				"given": "Kevin"
			},
			{
				"family": "Longpre",
				"given": "Shayne"
			},
			{
				"family": "Xiong",
				"given": "Betty"
			},
			{
				"family": "Kapoor",
				"given": "Sayash"
			},
			{
				"family": "Maslej",
				"given": "Nestor"
			},
			{
				"family": "Narayanan",
				"given": "Arvind"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniEcosystemGraphsDocumenting2024",
		"type": "article-journal",
		"abstract": "Foundation models (e.g. GPT-4, Gemini, Llama 3) pervasively influence society, warranting greater understanding. While the models garner much attention, accurately characterizing their impact requires considering the broader sociotechnical ecosystem in which they are created and deployed. We propose Ecosystem Graphs as a documentation framework to centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical and social relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata, such as the model’s estimated training emissions or licensing guidelines. Since its release in March 2023, Ecosystem Graphs represents an ongoing effort to document 568 assets (112 datasets, 359 models, 97 applications) from 117 organizations. Ecosystem Graphs functions as a multifunctional resource: we discuss two major uses by the 2024 AI Index and the UK’s Competition and Markets Authority that demonstrate the value of Ecosystem Graphs.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "196-209",
		"source": "ojs.aaai.org",
		"title": "Ecosystem Graphs: Documenting the Foundation Model Supply Chain",
		"title-short": "Ecosystem Graphs",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31629",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Soylu",
				"given": "Dilara"
			},
			{
				"family": "Liao",
				"given": "Thomas I."
			},
			{
				"family": "Creel",
				"given": "Kathleen A."
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bommasaniTrustworthySocialBias2024",
		"type": "article-journal",
		"abstract": "How do we design measures of social bias that we trust?\nWhile prior work has introduced several measures, no measure has gained widespread trust: instead, mounting evidence argues we should distrust these measures. In this work, we design bias measures that warrant trust based on the cross-disciplinary theory of measurement modeling. To combat the frequently fuzzy treatment of social bias in natural language processing, we explicitly define social bias, grounded in principles drawn from social science research. We operationalize our definition by proposing a general bias measurement framework DivDist, which we use to instantiate 5 concrete bias measures. To validate our measures, we propose a rigorous testing protocol with 8 testing criteria (e.g. predictive validity: do measures predict biases in US employment?). Through our testing, we demonstrate considerable evidence to trust our measures, showing they overcome conceptual, technical, and empirical deficiencies present in prior measures.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "210-224",
		"source": "ojs.aaai.org",
		"title": "Trustworthy Social Bias Measurement",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31630",
		"volume": "7",
		"author": [
			{
				"family": "Bommasani",
				"given": "Rishi"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "bristowViewsAIArent2024",
		"type": "article-journal",
		"abstract": "Recent developments in AI have brought broader attention to tensions between two overlapping communities, “AI Ethics” and “AI Safety.” In this article we (i) characterize this false binary, (ii) argue that a simple binary is not an accurate model of AI discourse, and (iii) provide concrete suggestions for how individuals can help avoid the emergence of us-vs-them conflict in the broad community of people working on AI development and governance. While we focus on “AI Ethics” and “AI Safety,” the general lessons apply to related tensions, including those between accelerationist (“e/acc”) and cautious stances on AI development.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "225-225",
		"source": "ojs.aaai.org",
		"title": "Views on AI Aren't Binary — They’re Plural (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31631",
		"volume": "7",
		"author": [
			{
				"family": "Bristow",
				"given": "Thorin"
			},
			{
				"family": "Thorburn",
				"given": "Luke"
			},
			{
				"family": "Acosta-Navas",
				"given": "Diana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "brownQualitativeStudyCultural2024",
		"type": "article-journal",
		"abstract": "Understanding the future consequences of artificial intelligence requires a holistic consideration of its cultural dimensions, on par with its technological intricacies and potential applications. Individuals and institutions working closely with AI, and with considerable resources, have significant influence on how impact is considered, particularly with regard to how much attention is paid to epistemic concerns (including issues of bias in datasets or potential misinterpretations of data, for example) versus normative concerns (such as societal and ecological effects of AI in the medium- and long-term). In this paper we review qualitative studies conducted with AI researchers and developers to understand how they position themselves relative to each of these two dimensions of impact, and how geographies and conditions of work influence their positions. Our findings underscore the need to gather more perspectives from low- and middle-income countries, whose notions of impact extend beyond the immediate technical concerns or impacts in the short- to medium-term. Rather, they encapsulate a broader spectrum of impact considerations, including the deleterious effects perpetrated by global corporate entities, the unwarranted influence of wealthy nations, the encroachment of philanthrocapitalism, and the adverse consequences of excluding communities affected by these phenomena from active participation in discussions surrounding impact.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "226-238",
		"source": "ojs.aaai.org",
		"title": "A Qualitative Study on Cultural Hegemony and the Impacts of AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31632",
		"volume": "7",
		"author": [
			{
				"family": "Brown",
				"given": "Venetia"
			},
			{
				"family": "Larasati",
				"given": "Retno"
			},
			{
				"family": "Third",
				"given": "Aisling"
			},
			{
				"family": "Farrell",
				"given": "Tracie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "carpenterFDAAIPitfalls2024",
		"type": "article-journal",
		"abstract": "Observers and practitioners of artificial intelligence (AI) have proposed an FDA-style licensing regime for the most advanced AI models, or 'frontier' models.  In this paper, we explore the applicability of approval regulation -- that is, regulation of a product that combines experimental minima with government licensure conditioned partially or fully upon that experimentation -- to the regulation of frontier AI.  There are a number of reasons to believe that approval regulation, simplistically applied, would be inapposite for frontier AI risks.  Domains of weak fit include the difficulty of defining the regulated product, the presence of Knightian uncertainty or deep ambiguity about harms from AI, the potentially transmissible nature of risks, and distributed activities among actors involved in the AI lifecycle. We conclude by highlighting the role of policy learning and experimentation in regulatory development, describing how learning from other forms of AI regulation and improvements in evaluation and testing methods can help to overcome some of the challenges we identify.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "239-254",
		"source": "ojs.aaai.org",
		"title": "An FDA for AI? Pitfalls and Plausibility of Approval Regulation for Frontier Artificial Intelligence",
		"title-short": "An FDA for AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31633",
		"volume": "7",
		"author": [
			{
				"family": "Carpenter",
				"given": "Daniel"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "castlemanWhyAmStill2024",
		"type": "article-journal",
		"abstract": "Recently, Meta has shifted towards AI-mediated ad targeting mechanisms that do not require advertisers to provide detailed targeting criteria. The shift is likely driven by excitement over AI capabilities as well as the need to address new data privacy policies and targeting changes agreed upon in civil rights settlements. At the same time, in response to growing public concern about the harms of targeted advertising, Meta has touted their ad preference controls as an effective mechanism for users to exert control over the advertising they see. Furthermore, Meta markets their \"Why this ad\" targeting explanation as a transparency tool that allows users to understand the reasons for seeing particular ads and inform their actions to control what ads they see in the future. \n\nOur study evaluates the effectiveness of Meta's \"See less\" ad control, as well as the actionability of ad targeting explanations following the shift to AI-mediated targeting. We conduct a large-scale study, randomly assigning participants the intervention of marking \"See less\" to either Body Weight Control or Parenting topics, and collecting the ads Meta shows to participants and their targeting explanations before and after the intervention. We find that utilizing the \"See less\" ad control for the topics we study does not significantly reduce the number of ads shown by Meta on these topics, and that the control is less effective for some users whose demographics are correlated with the topic. Furthermore, we find that the majority of ad targeting explanations for local ads made no reference to location-specific targeting criteria, and did not inform users why ads related to the topics they requested to \"See less\" of continued to be delivered. We hypothesize that the poor effectiveness of controls and lack of actionability and comprehensiveness in explanations are the result of the shift to AI-mediated targeting, for which explainability and transparency tools have not yet been developed by Meta. Our work thus provides evidence for the need of new methods for transparency and user control, suitable and reflective of how the increasingly complex and AI-mediated ad delivery systems operate.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "255-266",
		"source": "ojs.aaai.org",
		"title": "Why Am I Still Seeing This: Measuring the Effectiveness of Ad Controls and Explanations in AI-Mediated Ad Targeting Systems",
		"title-short": "Why Am I Still Seeing This",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31634",
		"volume": "7",
		"author": [
			{
				"family": "Castleman",
				"given": "Jane"
			},
			{
				"family": "Korolova",
				"given": "Aleksandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "cattellCoordinatedFlawDisclosure2024",
		"type": "article-journal",
		"abstract": "Harm reporting in Artificial Intelligence (AI) currently lacks a structured process for disclosing and addressing algorithmic flaws, relying largely on an ad-hoc approach. This contrasts sharply with the well-established Coordinated Vulnerability Disclosure (CVD) ecosystem in software security. While global efforts to establish frameworks for AI transparency and collaboration are underway, the unique challenges presented by machine learning (ML) models demand a specialized approach. To address this gap, we propose implementing a Coordinated Flaw Disclosure (CFD) framework tailored to the complexities of ML and AI issues. This paper reviews the evolution of ML disclosure practices, from ad hoc reporting to emerging participatory auditing methods, and compares them with cybersecurity norms. Our framework introduces innovations such as extended model cards, dynamic scope expansion, an independent adjudication panel, and an automated verification process. We also outline a forthcoming real-world pilot of CFD. We argue that CFD could significantly enhance public trust in AI systems. By balancing organizational and community interests, CFD aims to improve AI accountability in a rapidly evolving technological landscape.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "267-280",
		"source": "ojs.aaai.org",
		"title": "Coordinated Flaw Disclosure for AI: Beyond Security Vulnerabilities",
		"title-short": "Coordinated Flaw Disclosure for AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31635",
		"volume": "7",
		"author": [
			{
				"family": "Cattell",
				"given": "Sven"
			},
			{
				"family": "Ghosh",
				"given": "Avijit"
			},
			{
				"family": "Kaffee",
				"given": "Lucie-Aimée"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "chengAlgorithmAssistedDecisionMaking2024",
		"type": "article-journal",
		"abstract": "The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource.  To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide.  Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "281-292",
		"source": "ojs.aaai.org",
		"title": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool",
		"title-short": "Algorithm-Assisted Decision Making and Racial Disparities in Housing",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31636",
		"volume": "7",
		"author": [
			{
				"family": "Cheng",
				"given": "Lingwei"
			},
			{
				"family": "Drayton",
				"given": "Cameron"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			},
			{
				"family": "Vaithianathan",
				"given": "Rhema"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "collinsThumbsUntanglingChallenges2024",
		"type": "article-journal",
		"abstract": "Human feedback plays a critical role in learning and refining reward models for text-to-image generation, but the optimal form the feedback should take for learning an accurate reward function has not been conclusively established. This paper investigates the effectiveness of fine-grained feedback which captures nuanced distinctions in image quality and prompt-alignment, compared to traditional coarse-grained feedback (for example, thumbs up/down or ranking between a set of options). While fine-grained feedback holds promise, particularly for systems catering to diverse societal preferences, we show that demonstrating its superiority to coarse-grained feedback is not automatic. Through experiments on real and synthetic preference data, we surface the complexities of building effective models due to the interplay of model choice, feedback type, and the alignment between human judgment and computational interpretation. We identify key challenges in eliciting and utilizing fine-grained feedback, prompting a reassessment of its assumed benefits and practicality. Our findings -- e.g., that fine-grained feedback can lead to worse models for a fixed budget, in some settings; however, in controlled settings with known attributes, fine grained rewards can indeed be more helpful -- call for careful consideration of feedback attributes and potentially beckon novel modeling approaches to appropriately unlock the potential value of fine-grained feedback in-the-wild.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "293-303",
		"source": "ojs.aaai.org",
		"title": "Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation",
		"title-short": "Beyond Thumbs Up/Down",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31637",
		"volume": "7",
		"author": [
			{
				"family": "Collins",
				"given": "Katherine M."
			},
			{
				"family": "Kim",
				"given": "Najoung"
			},
			{
				"family": "Bitton",
				"given": "Yonatan"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Omidshafiei",
				"given": "Shayegan"
			},
			{
				"family": "Hu",
				"given": "Yushi"
			},
			{
				"family": "Chen",
				"given": "Sherol"
			},
			{
				"family": "Dutta",
				"given": "Senjuti"
			},
			{
				"family": "Chang",
				"given": "Minsuk"
			},
			{
				"family": "Lee",
				"given": "Kimin"
			},
			{
				"family": "Liang",
				"given": "Youwei"
			},
			{
				"family": "Evans",
				"given": "Georgina"
			},
			{
				"family": "Singla",
				"given": "Sahil"
			},
			{
				"family": "Li",
				"given": "Gang"
			},
			{
				"family": "Weller",
				"given": "Adrian"
			},
			{
				"family": "He",
				"given": "Junfeng"
			},
			{
				"family": "Ramachandran",
				"given": "Deepak"
			},
			{
				"family": "Dvijotham",
				"given": "Krishnamurthy Dj"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "cornacchiaMoJEMixtureJailbreak2024",
		"type": "article-journal",
		"abstract": "The proliferation of Large Language Models (LLMs) in diverse applications underscores the pressing need for robust security measures to thwart potential jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger data integrity and user privacy. Guardrails serve as crucial protective mechanisms against such threats, but existing models often fall short in terms of both detection accuracy, and computational efficiency. This paper advocates for the significance of jailbreak attack prevention on LLMs, and emphasises the role of input guardrails in safeguarding these models. We introduce MoJE (Mixture of Jailbreak Expert), a novel guardrail architecture designed to surpass current limitations in existing state-of-the-art guardrails. By employing simple linguistic statistical techniques, MoJE excels in detecting  jailbreak attacks while maintaining minimal computational overhead during model inference. Through rigorous experimentation, MoJE demonstrates superior performance capable of detecting 90% of the attacks without compromising benign prompts, enhancing LLMs security against jailbreak attacks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "304-315",
		"source": "ojs.aaai.org",
		"title": "MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks",
		"title-short": "MoJE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31638",
		"volume": "7",
		"author": [
			{
				"family": "Cornacchia",
				"given": "Giandomenico"
			},
			{
				"family": "Zizzo",
				"given": "Giulio"
			},
			{
				"family": "Fraser",
				"given": "Kieran"
			},
			{
				"family": "Hameed",
				"given": "Muhammad Zaid"
			},
			{
				"family": "Rawat",
				"given": "Ambrish"
			},
			{
				"family": "Purcell",
				"given": "Mark"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "dashSponsoredNewOrganic2024",
		"type": "article-journal",
		"abstract": "Interleaving sponsored results (advertisements) amongst organic results on search engine result pages (SERP) has become a common practice across multiple digital platforms. Advertisements have catered to consumer satisfaction and fostered competition in digital public spaces; making them an appealing gateway for businesses to reach their consumers. However, especially in the context of digital marketplaces, due to the competitive nature of the sponsored results with the organic ones, multiple unwanted repercussions have surfaced affecting different stakeholders. From the consumers' perspective the sponsored ads/results may cause degradation of search quality and nudge consumers to potentially irrelevant and costlier products. The sponsored ads may also affect the level playing field of the competition in the marketplaces among sellers. To understand and unravel these potential concerns, we analyse the Amazon digital marketplace in four different countries by simulating 4,800 search operations. Our analyses over SERPs consisting 2M organic and 638K sponsored results show items with poor organic ranks (beyond 100th position) appear as sponsored results even before the top organic results on the first page of Amazon SERP. Moreover, we also observe that in majority of the cases, these top sponsored results are costlier and are of poorer quality than the top organic results. We believe these observations can motivate researchers for further deliberation to bring in more transparency and guard rails in the advertising practices followed in digital marketplaces.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "316-327",
		"source": "ojs.aaai.org",
		"title": "Sponsored is the New Organic: Implications of Sponsored Results on Quality of Search Results in the Amazon Marketplace",
		"title-short": "Sponsored is the New Organic",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31639",
		"volume": "7",
		"author": [
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			},
			{
				"family": "Chakraborty",
				"given": "Abhijnan"
			},
			{
				"family": "Gummadi",
				"given": "Krishna P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "deyAPPRAISEGovernanceFramework2024",
		"type": "article-journal",
		"abstract": "As artificial intelligence (AI) systems increasingly impact society, the EU Artificial Intelligence Act (AIA) is the first legislative attempt to regulate AI systems. This paper proposes a governance framework for organizations innovating with AI systems. Building upon secondary research, the framework aims at driving a balance between four types of pressures that organizations, innovating with AI, experience, and thereby creating responsible value. These pressures encompass AI/technology, normative, value creation, and regulatory aspects. The framework is partially validated through primary research in two phases. In the first phase, a conceptual model is proposed that measures the extent to which organizational tasks result in AIA compliance, using elements from the AIA as mediators and strategic variables such as organization size, extent of outsourcing, and offshoring as moderators. 34 organizations in the Netherlands are surveyed to test the conceptual model. The average actual compliance score of the 34 participants is low, and most participants exaggerate their compliance. Organization size is found to have significant impact on AIA compliance. In phase 2, two case studies are conducted with the purpose of generating in-depth insights to validate the proposed framework. The case studies confirm the interplay of the four pressures on organizations innovating with AI, and furthermore substantiate the governance framework.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "328-340",
		"source": "ojs.aaai.org",
		"title": "APPRAISE: a Governance Framework for Innovation with Artificial Intelligence Systems",
		"title-short": "APPRAISE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31640",
		"volume": "7",
		"author": [
			{
				"family": "Dey",
				"given": "Diptish"
			},
			{
				"family": "Bhaumik",
				"given": "Debarati"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazScalingLawsNot2024",
		"type": "article-journal",
		"abstract": "Recent work has advocated for training AI models on ever-larger datasets, arguing that as the size of a dataset increases, the performance of a model trained on that dataset will correspondingly increase (referred to as “scaling laws”). In this paper, we draw on literature from the social sciences and machine learning to critically interrogate these claims. We argue that this scaling law relationship depends on metrics used to measure performance that may not correspond with how different groups of people perceive the quality of models' output. As the size of datasets used to train large AI models grows and AI systems impact ever larger groups of people, the number of distinct communities represented in training or evaluation datasets grows. It is thus even more likely that communities represented in datasets may have values or preferences not reflected in (or at odds with) the metrics used to evaluate model performance in scaling laws. Different communities may also have values in tension with each other, leading to difficult, potentially irreconcilable choices about metrics used for model evaluations---threatening the validity of claims that model performance is improving at scale. We end the paper with implications for AI development: that the motivation for scraping ever-larger datasets may be based on fundamentally flawed assumptions about model performance. That is, models may not, in fact, continue to improve as the datasets get larger---at least not for all people or communities impacted by those models. We suggest opportunities for the field to rethink norms and values in AI development, resisting claims for universality of large models, fostering more local, small-scale designs, and other ways to resist the impetus towards scale in AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "341-357",
		"source": "ojs.aaai.org",
		"title": "Scaling Laws Do Not Scale",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31641",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Fernando"
			},
			{
				"family": "Madaio",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazWhatMakesExpert2024",
		"type": "article-journal",
		"abstract": "Human experts are often engaged in the development of machine learning systems to collect and validate data, consult on algorithm development, and evaluate system performance. At the same time, who counts as an ‘expert’ and what constitutes ‘expertise’ is not always explicitly defined. In this work, we review 112 academic publications that explicitly reference ‘expert’ and ‘expertise’ and that describe the development of machine learning (ML) systems to survey how expertise is characterized and the role experts play. We find that expertise is often undefined and forms of knowledge outside of formal education and professional certification are rarely sought, which has implications for the kinds of knowledge that are recognized and legitimized in ML development. Moreover, we find that expert knowledge tends to be utilized in ways focused on mining textbook knowledge, such as through data annotation. We discuss the ways experts are engaged in ML development in relation to deskilling, the social construction of expertise, and implications for responsible AI development. We point to a need for reflection and specificity in justifications of domain expert engagement, both as a matter of documentation and reproducibility, as well as a matter of broadening the range of recognized expertise.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "358-370",
		"source": "ojs.aaai.org",
		"title": "What Makes An Expert? Reviewing How ML Researchers Define \"Expert\"",
		"title-short": "What Makes An Expert?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31642",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Smith",
				"given": "Angela D. R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "diazSoUnDFrameworkAnalyzing2024",
		"type": "article-journal",
		"abstract": "Decisions about how to responsibly collect, use and document data often rely upon understanding how people are represented in data. Yet, the unlabeled  nature and scale of data used in foundation model development poses a direct challenge to systematic analyses of downstream risks, such as representational harms.  We provide a framework designed to help RAI practitioners more easily plan and structure analyses of how people are represented in unstructured data and identify downstream risks. The framework is organized into groups of analyses that map to 3 basic questions: 1) Who is represented in the data, 2) What content is in the data, and 3) How are the two associated. We use the framework to analyze human representation in two commonly used datasets: the Common Crawl web corpus (C4) of 356 billion tokens, and the LAION-400M dataset of 400 million text-image pairs, both developed in the English language. We illustrate how the framework informs action steps for hypothetical teams faced with data use, development, and documentation decisions. Ultimately, the framework structures human representation analyses and maps out analysis planning considerations, goals, and risk mitigation actions at different stages of dataset and model development.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "371-383",
		"source": "ojs.aaai.org",
		"title": "SoUnD Framework: Analyzing (So)cial Representation in (Un)structured (D)ata",
		"title-short": "SoUnD Framework",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31643",
		"volume": "7",
		"author": [
			{
				"family": "Diaz",
				"given": "Mark"
			},
			{
				"family": "Dev",
				"given": "Sunipa"
			},
			{
				"family": "Reif",
				"given": "Emily"
			},
			{
				"family": "Denton",
				"given": "Emily"
			},
			{
				"family": "Prabhakaran",
				"given": "Vinodkumar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "dingOutlierDetectionBias2024",
		"type": "article-journal",
		"abstract": "The astonishing successes of ML  have raised growing concern for the fairness of modern methods when deployed in real world settings. However, studies on fairness have mostly focused on supervised ML, while unsupervised outlier detection (OD), with numerous applications in finance, security, etc., have attracted little attention. While a few studies proposed fairness-enhanced OD algorithms, they remain agnostic to the underlying driving mechanisms or sources of unfairness. Even within the supervised ML literature, there exists debate on whether unfairness stems solely from algorithmic biases (i.e. design choices)  or from the  biases encoded in the data on which they are trained.  \n\nTo close this gap, this work aims to shed light on the possible sources of unfairness in OD by auditing detection models under different data-centric factors.By injecting various known biases into the input data---as pertain to sample size disparity, under-representation, feature measurement noise, and group membership obfuscation---we find that the OD algorithms under the study all exhibit fairness pitfalls, although differing in which types of data bias they are more susceptible to. Most notable of our study is to demonstrate that OD algorithm bias is not merely a data bias problem. A key realization is that the data properties that emerge from bias injection could as well be organic---as pertain to natural group differences w.r.t. sparsity, base rate, variance, and multi-modality. Either natural or biased, such data properties can \ngive rise to unfairness as they interact with certain algorithmic design choices.\n\nOur work provides a deeper  understanding of the possible sources of OD unfairness, and\nserves as a framework for assessing the unfairness of future OD algorithms under specific data-centric factors. It also paves the way for future work on mitigation strategies by underscoring the susceptibility of various design choices.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "384-395",
		"source": "ojs.aaai.org",
		"title": "Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors",
		"title-short": "Outlier Detection Bias Busted",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31644",
		"volume": "7",
		"author": [
			{
				"family": "Ding",
				"given": "Xueying"
			},
			{
				"family": "Xi",
				"given": "Rui"
			},
			{
				"family": "Akoglu",
				"given": "Leman"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "doerflerLegitimatingEmotionTracking2024",
		"type": "article-journal",
		"abstract": "Contemporary automobiles are now incorporating digital technologies, including emotion recognition technologies intended to monitor and sometimes intervene on the driver’s mood, attentiveness, or emotional state. We investigate how the firms producing these technologies justify and legitimate their design, production, and use, and how these discourses of legitimation paint a picture of the desired social role of emotion recognition in the automotive sector. Through a critical discourse analysis of patents, advertising, and promotional materials from industry-leading companies Cerence and Affectiva/Smart Eye, we argue both companies use  potentially spurious arguments about the accuracy of emotion recognition to rationalize their products. Both companies also use a variety of other legitimation techniques around driver safety, individual personalization, and increased productivity to re-frame the social aspects of digitally mediated autonomous vehicles on their terms.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "396-410",
		"source": "ojs.aaai.org",
		"title": "Legitimating Emotion Tracking Technologies in Driver Monitoring Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31645",
		"volume": "7",
		"author": [
			{
				"family": "Doerfler",
				"given": "Aaron"
			},
			{
				"family": "Stark",
				"given": "Luke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "fangRepresentationMagnitudeHas2024",
		"type": "article-journal",
		"abstract": "The privacy-preserving approaches to machine learning (ML) models have made substantial progress in recent years. However, it is still opaque in which circumstances and conditions the model becomes privacy-vulnerable, leading to a challenge for ML models to maintain both performance and privacy. In this paper, we first explore the disparity between member and non-member data in the representation of models under common training frameworks.We identify how the representation magnitude disparity correlates with privacy vulnerability and address how this correlation impacts privacy vulnerability. Based on the observations, we propose Saturn Ring Classifier Module (SRCM), a plug-in model-level solution to mitigate membership privacy leakage. Through a confined yet effective representation space, our approach ameliorates models’ privacy vulnerability while maintaining generalizability. The code of this work can be found here:\nhttps://github.com/JEKimLab/AIES2024SRCM",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "411-420",
		"source": "ojs.aaai.org",
		"title": "Representation Magnitude Has a Liability to Privacy Vulnerability",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31646",
		"volume": "7",
		"author": [
			{
				"family": "Fang",
				"given": "Xingli"
			},
			{
				"family": "Kim",
				"given": "Jung-Eun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "fefferRedTeamingGenerativeAI2024",
		"type": "article-journal",
		"abstract": "In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming’s central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "421-437",
		"source": "ojs.aaai.org",
		"title": "Red-Teaming for Generative AI: Silver Bullet or Security Theater?",
		"title-short": "Red-Teaming for Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31647",
		"volume": "7",
		"author": [
			{
				"family": "Feffer",
				"given": "Michael"
			},
			{
				"family": "Sinha",
				"given": "Anusha"
			},
			{
				"family": "Deng",
				"given": "Wesley H."
			},
			{
				"family": "Lipton",
				"given": "Zachary C."
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "freszHowShouldAI2024",
		"type": "article-journal",
		"abstract": "This paper investigates the relationship between law and eXplainable Artificial Intelligence (XAI). While there is much discussion about the AI Act, which was adopted by the European Parliament in March 2024, other areas of law seem underexplored. This paper focuses on European (and in part German) law, although with international concepts and regulations such as fiduciary duties, the General Data Protection Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies, requirements for XAI methods are derived from each of the legal fields, resulting in the conclusion that each legal field requires different XAI properties and that the current state of the art does not fulfill these to full satisfaction, especially regarding the correctness (sometimes called fidelity) and confidence estimates of XAI methods.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "438-450",
		"source": "ojs.aaai.org",
		"title": "How Should AI Decisions Be Explained? Requirements for Explanations from the Perspective of European Law",
		"title-short": "How Should AI Decisions Be Explained?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31648",
		"volume": "7",
		"author": [
			{
				"family": "Fresz",
				"given": "Benjamin"
			},
			{
				"family": "Dubovitskaya",
				"given": "Elena"
			},
			{
				"family": "Brajovic",
				"given": "Danilo"
			},
			{
				"family": "Huber",
				"given": "Marco F."
			},
			{
				"family": "Horz",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "gaoSurvivingDiverseBiases2024",
		"type": "article-journal",
		"abstract": "The online data markets have emerged as a valuable source of diverse datasets for training machine learning (ML) models.  However, datasets from different data providers may exhibit varying levels of bias with respect to certain sensitive attributes in the population (such as race, sex, age, and marital status).  Recent dataset acquisition research has focused on maximizing accuracy improvements for downstream model training,  ignoring the negative impact of biases in the acquired datasets, which can lead to an unfair model.  Can a consumer obtain an unbiased dataset from datasets with diverse biases? In this work, we propose a fairness-aware data acquisition framework  (FAIRDA) to acquire high-quality datasets that maximize both accuracy and fairness for consumer local classifier training while remaining within a limited budget.  Given the biases of data commodities remain opaque to consumers,  the data acquisition in FAIRDA employs explore-exploit strategies.  Based on whether exploration and exploitation are conducted sequentially or alternately, we introduce two algorithms: the knowledge-based offline data acquisition (KDA) and the reward-based online data acquisition algorithms (RDA).  Each algorithm is tailored to specific customer needs, giving the former an advantage in computational efficiency and the latter an advantage in robustness.  We conduct experiments to demonstrate the effectiveness of the proposed data acquisition framework in steering users toward fairer model training compared to existing baselines under varying market settings.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "451-462",
		"source": "ojs.aaai.org",
		"title": "Surviving in Diverse Biases: Unbiased Dataset Acquisition in Online Data Market for Fair Model Training",
		"title-short": "Surviving in Diverse Biases",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31649",
		"volume": "7",
		"author": [
			{
				"family": "Gao",
				"given": "Jiashi"
			},
			{
				"family": "Wang",
				"given": "Ziwei"
			},
			{
				"family": "Zhao",
				"given": "Xiangyu"
			},
			{
				"family": "Yao",
				"given": "Xin"
			},
			{
				"family": "Wei",
				"given": "Xuetao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshDontSeeMyself2024",
		"type": "article-journal",
		"abstract": "Though research into text-to-image generators (T2Is) such as Stable Diffusion has demonstrated their amplification of societal biases and potentials to cause harm, such research has primarily relied on computational methods instead of seeking information from real users who experience harm, which is a significant knowledge gap. In this paper, we conduct the largest human subjects study of Stable Diffusion, with a combination of crowdsourced data from 133 crowdworkers and 14 semi-structured interviews across diverse countries and genders. Through a mixed-methods approach of intra-set cosine similarity hierarchies (i.e., comparing multiple Stable Diffusion outputs for the same prompt with each other to examine which result is `closest' to the prompt) and qualitative thematic analysis, we first demonstrate a large disconnect between user expectations for Stable Diffusion outputs with those generated, evidenced by a set of Stable Diffusion renditions of `a Person' providing images far away from such expectations. We then extend this finding of general dissatisfaction into highlighting representational harms caused by Stable Diffusion upon our subjects, especially those with traditionally marginalized identities, subjecting them to incorrect and often dehumanizing stereotypes about their identities. We provide recommendations for a harm-aware approach to (re)design future versions of Stable Diffusion and other T2Is.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "463-475",
		"source": "ojs.aaai.org",
		"title": "“I Don’t See Myself Represented Here at All”: User Experiences of Stable Diffusion Outputs Containing Representational Harms across Gender Identities and Nationalities",
		"title-short": "“I Don’t See Myself Represented Here at All”",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31650",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Lutz",
				"given": "Nina"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshGenerativeAIModels2024",
		"type": "article-journal",
		"abstract": "Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a Non-Western community-centered approach\nand grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English input prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, contributing to the development of more equitable and representative GAI technologies globally. Our work underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise as these models are deployed on a global scale.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "476-489",
		"source": "ojs.aaai.org",
		"title": "Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach",
		"title-short": "Do Generative AI Models Output Harm while Representing Non-Western Cultures",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31651",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			},
			{
				"family": "Venkit",
				"given": "Pranav Narayanan"
			},
			{
				"family": "Gautam",
				"given": "Sanjana"
			},
			{
				"family": "Wilson",
				"given": "Shomir"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "ghoshInterpretationsRepresentationsStereotypes2024",
		"type": "article-journal",
		"abstract": "The surge in the popularity of text-to-image generators (T2Is) has been matched by extensive research into ensuring fairness and equitable outcomes, with a focus on how they impact society. However, such work has typically focused on globally-experienced identities or centered Western contexts. In this paper, we address interpretations, representations, and stereotypes surrounding a tragically underexplored context in T2I research: caste. We examine how the T2I Stable Diffusion displays people of various castes, and what professions they are depicted as performing. Generating 100 images per prompt, we perform CLIP-cosine similarity comparisons with default depictions of an `Indian person’ by Stable Diffusion, and explore patterns of similarity. Our findings reveal how Stable Diffusion outputs perpetuate systems of `castelessness’, equating Indianness with high-castes and depicting caste-oppressed identities with markers of poverty. In particular, we note the stereotyping and representational harm towards the historically-marginalized Dalits, prominently depicted as living in rural areas and always at protests. Our findings underscore a need for a caste-aware approach towards T2I design, and we conclude with design recommendations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "490-502",
		"source": "ojs.aaai.org",
		"title": "Interpretations, Representations, and Stereotypes of Caste within Text-to-Image Generators",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31652",
		"volume": "7",
		"author": [
			{
				"family": "Ghosh",
				"given": "Sourojit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "goldsteinPPOuFrameworkStructured2024",
		"type": "article-journal",
		"abstract": "The diffusion of increasingly capable AI systems has produced concern that bad actors could intentionally misuse current or future AI systems for harm. Governments have begun to create new entities—such as AI Safety Institutes—tasked with assessing these risks. However, approaches for risk assessment are currently fragmented and would benefit from broader disciplinary expertise. As it stands, it is often unclear whether concerns about malicious use misestimate the likelihood and severity of the risks. This article advances a conceptual framework to review and structure investigation into the likelihood of an AI system (X) being applied to a malicious use (Y). We introduce a three-stage framework of (1) Plausibility (can X be used to do Y at all?), (2) Performance (how well does X do Y?), and (3) Observed use (do actors use X to do Y in practice?). At each stage, we outline key research questions, methodologies, benefits and limitations, and the types of uncertainty addressed. We also offer ideas for directions to improve risk assessment moving forward.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "503-518",
		"source": "ojs.aaai.org",
		"title": "The PPOu Framework: A Structured Approach for Assessing the Likelihood of Malicious Use of Advanced AI Systems",
		"title-short": "The PPOu Framework",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31653",
		"volume": "7",
		"author": [
			{
				"family": "Goldstein",
				"given": "Josh A."
			},
			{
				"family": "Sastry",
				"given": "Girish"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "grabbRisksLanguageModels2024",
		"type": "article-journal",
		"abstract": "In the United States and other countries exists a “national mental health crisis”: Rates of suicide, depression, anxiety, substance use, and more continue to increase – exacerbated by isolation, the COVID pandemic, and, most importantly, lack of access to mental healthcare. Therefore, many are looking to AI-enabled digital mental health tools, which have the potential to reach many patients who would otherwise remain on wait lists or without care. The main drive behind these new tools is the focus on large language models that could enable real-time, personalized support and advice for patients. With a trend towards language models entering the mental healthcare delivery apparatus, questions arise about how a robust, high-level framework to guide ethical implementations would look like and whether existing language models are ready for this high-stakes application where individual failures can lead to dire consequences.\n\nThis paper addresses the ethical and practical challenges custom to mental health applications and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s) with defined rubrics and criteria for each question that would define \"safe,\" \"unsafe,\" and \"borderline\" (between safe and unsafe) for reproducibility.\n\nWe find that all tested language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. \nThis is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models based on system prompt engineering and model-generated self-critiques.\n\nBefore the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.\n\nOur code and the redacted data set are available on Github (github.com/maxlampe/taimh_eval, MIT License). The full, unredacted data set is available upon request due to the harmful content contained.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "519-519",
		"source": "ojs.aaai.org",
		"title": "Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation (Extended Abstract)",
		"title-short": "Risks from Language Models for Automated Mental Healthcare",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31654",
		"volume": "7",
		"author": [
			{
				"family": "Grabb",
				"given": "Declan"
			},
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Vasan",
				"given": "Nina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "gravesCompassionateAIMoral2024",
		"type": "article-journal",
		"abstract": "The rapid expansion of artificial intelligence (AI) technology promises plausible increases to human flourishing, health, and well-being but raises concerns about possible harms and increased suffering. By making AI compassionate, the alleviation of suffering becomes explicit, rather than proxied, and potential harms caused by AI automation can be turned into benefits. Compassionate healthcare is beneficial for patient health outcomes and satisfaction and improves caregiver resilience and burnout. AI automation has many benefits but may interfere with patient care and autonomy. Incorporating compassion into healthcare reduces potential harms, increases health benefits and well-being, and can protect patient autonomy while providing more responsive and equitable care.\n\nWhether and how one conceives of AI as plausibly compassionate depends on ethical concerns and cultural context, including assumptions about human nature and AI personhood. Insights from Buddhism have contributed to scholarship on compassion and can extend incomplete Western perspectives on AI possibilities and limitations. Psychological research on the elements of compassion can guide development of compassionate AI and its incorporation into healthcare. Compassionate AI can be deployed especially into application areas where compassion plays an essential role with high demands on the compassion capacity of caregivers, such as dementia eldercare and palliative care.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "520-533",
		"source": "ojs.aaai.org",
		"title": "Compassionate AI for Moral Decision-Making, Health, and Well-Being",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31655",
		"volume": "7",
		"author": [
			{
				"family": "Graves",
				"given": "Mark"
			},
			{
				"family": "Compson",
				"given": "Jane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "guptaConceptualFrameworkEthical2024",
		"type": "article-journal",
		"abstract": "Research in Responsible AI has developed a range of principles and practices to ensure that machine learning systems are used in a manner that is ethical and aligned with human values. However, a critical yet often neglected aspect of ethical ML is the ethical implications that appear when designing evaluations of ML systems. For instance, teams may have to balance a trade-off between highly informative tests to ensure downstream product safety, with potential fairness harms inherent to the implemented testing procedures. We conceptualize ethics-related concerns in standard ML evaluation techniques. Specifically, we present a utility framework, characterizing the key trade-off in ethical evaluation as balancing information gain against potential ethical harms. The framework is then a tool for characterizing challenges teams face, and systematically disentangling competing considerations that teams seek to balance. Differentiating between different types of issues encountered in evaluation allows us to highlight best practices from analogous domains, such as clinical trials and automotive crash testing, which navigate these issues in ways that can offer inspiration to improve evaluation processes in ML. Our analysis underscores the critical need for development teams to deliberately assess and manage ethical complexities that arise during the evaluation of ML systems, and for the industry to move towards designing institutional policies to support ethical evaluations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "534-546",
		"source": "ojs.aaai.org",
		"title": "A Conceptual Framework for Ethical Evaluation of Machine Learning Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31656",
		"volume": "7",
		"author": [
			{
				"family": "Gupta",
				"given": "Neha R."
			},
			{
				"family": "Hullman",
				"given": "Jessica"
			},
			{
				"family": "Subramonyam",
				"given": "Hariharan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hamidiehIdentifyingImplicitSocial2024",
		"type": "article-journal",
		"abstract": "Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-It, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a \"terrorist\". Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "547-561",
		"source": "ojs.aaai.org",
		"title": "Identifying Implicit Social Biases in Vision-Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31657",
		"volume": "7",
		"author": [
			{
				"family": "Hamidieh",
				"given": "Kimia"
			},
			{
				"family": "Zhang",
				"given": "Haoran"
			},
			{
				"family": "Gerych",
				"given": "Walter"
			},
			{
				"family": "Hartvigsen",
				"given": "Thomas"
			},
			{
				"family": "Ghassemi",
				"given": "Marzyeh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hanCausalFrameworkEvaluate2024",
		"type": "article-journal",
		"abstract": "We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the \"causal chain of interactions\" rather than simply focusing on the end outcome; this can help guide reforms. \n\nIn this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and criminality. Our framework identifies three canonical scenarios with distinct characteristics: in settings like (1) airport security, the primary source of observed bias against a race is likely to be bias in law enforcement against innocents of that race; (2) AI-empowered policing, the primary source of observed bias against a race is likely to be bias in law enforcement against criminals of that race; and (3) police-civilian interaction, the primary source of observed bias against a race could be bias in law enforcement against that race or bias from the general public in reporting (e.g. via 911 calls) against the other race. Through an extensive empirical study using police-civilian interaction (stop) data and 911 call data, we And an instance of such a counter-intuitive phenomenon: in New Orleans, the observed bias is against the majority race and the likely reason for it is the over-reporting (via 911 calls) of incidents involving the minority race by the general public.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "562-572",
		"source": "ojs.aaai.org",
		"title": "A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31658",
		"volume": "7",
		"author": [
			{
				"family": "Han",
				"given": "Jessy Xinyi"
			},
			{
				"family": "Miller",
				"given": "Andrew Cesare"
			},
			{
				"family": "Watkins",
				"given": "S. Craig"
			},
			{
				"family": "Winship",
				"given": "Christopher"
			},
			{
				"family": "Christia",
				"given": "Fotini"
			},
			{
				"family": "Shah",
				"given": "Devavrat"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hardalupasContributoryInjusticeEpistemic2024",
		"type": "article-journal",
		"abstract": "AI systems have long been touted as a means to transform the healthcare system and improve service user outcomes. However, these claims frequently ignore the social context that leaves service users subject to epistemic oppression. This paper introduces the term “epistemic calcification” to describe how the use of AI systems leads to our epistemological systems becoming stuck in fixed frameworks for understanding the world. Epistemic calcification leads to contributory injustice as it reduces the ability of healthcare systems to meaningfully consider alternative understandings of people’s health experiences. By analysing examples of algorithmic prognosis and diagnosis, this paper demonstrates the challenges of addressing contributory injustice in AI systems and the need for contestability to focus on more than the AI system and on the underlying epistemologies of AI systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "573-583",
		"source": "ojs.aaai.org",
		"title": "Contributory Injustice, Epistemic Calcification and the Use of AI Systems in Healthcare",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31659",
		"volume": "7",
		"author": [
			{
				"family": "Hardalupas",
				"given": "Mahi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "herdelExploreGenLargeLanguage2024",
		"type": "article-journal",
		"abstract": "Responsible AI design is increasingly seen as an imperative by both AI developers and AI compliance experts. One of the key tasks is envisioning AI technology uses and risks. Recent studies on the model and data cards reveal that AI practitioners struggle with this task due to its inherently challenging nature. Here, we demonstrate that leveraging a Large Language Model (LLM) can support AI practitioners in this task by enabling reflexivity, brainstorming, and deliberation, especially in the early design stages of the AI development process. We developed an LLM framework, ExploreGen, which generates realistic and varied uses of AI technology, including those overlooked by research, and classifies their risk level based on the EU AI Act regulation. We evaluated our framework using the case of Facial Recognition and Analysis technology in nine user studies with 25 AI practitioners. Our findings show that ExploreGen is helpful to both developers and compliance experts. They rated the uses as realistic and their risk classification as accurate (94.5%). Moreover, while unfamiliar with many of the uses, they rated them as having high adoption potential and transformational impact.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "584-596",
		"source": "ojs.aaai.org",
		"title": "ExploreGen: Large Language Models for Envisioning the Uses and Risks of AI Technologies",
		"title-short": "ExploreGen",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31660",
		"volume": "7",
		"author": [
			{
				"family": "Herdel",
				"given": "Viviane"
			},
			{
				"family": "Šćepanović",
				"given": "Sanja"
			},
			{
				"family": "Bogucka",
				"given": "Edyta"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hertweckWhatsDistributiveJustice2024",
		"type": "article-journal",
		"abstract": "In the field of algorithmic fairness, many fairness criteria have been proposed. Oftentimes, their proposal is only accompanied by a loose link to ideas from moral philosophy -- which makes it difficult to understand when the proposed criteria should be used to evaluate the fairness of a decision-making system. More recently, researchers have thus retroactively tried to tie existing fairness criteria to philosophical concepts. Group fairness criteria have typically been linked to egalitarianism, a theory of distributive justice. This makes it tempting to believe that fairness criteria mathematically represent ideals of distributive justice and this is indeed how they are typically portrayed. In this paper, we will discuss why the current approach of linking algorithmic fairness and distributive justice is too simplistic and, hence, insufficient. We argue that in the context of imperfect decision-making systems -- which is what we deal with in algorithmic fairness -- we should not only care about what the ideal distribution of benefits/harms among individuals would look like but also about how deviations from said ideal are distributed. Our claim is that algorithmic fairness is concerned with unfairness in these deviations. This requires us to rethink the way in which we, as algorithmic fairness researchers, view distributive justice and use fairness criteria.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "597-608",
		"source": "ojs.aaai.org",
		"title": "What's Distributive Justice Got to Do with It? Rethinking Algorithmic Fairness from a Perspective of Approximate Justice",
		"title-short": "What's Distributive Justice Got to Do with It?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31661",
		"volume": "7",
		"author": [
			{
				"family": "Hertweck",
				"given": "Corinna"
			},
			{
				"family": "Heitz",
				"given": "Christoph"
			},
			{
				"family": "Loi",
				"given": "Michele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hice-fromilleAfrofuturistValuesMetaverse2024",
		"type": "article-journal",
		"abstract": "Many emerging technologies, such as the immersive VR and AR devices forming the metaverse, are not just reminiscent of but inspired by devices found in popular science fiction texts. Yet, the stories that these technologies are drawn from do not often center marginalized communities and people of color. In this article, we propose that builders and users of these technologies turn to diverse creative texts as inspiration for the ethical codes that will shape the ways that these technologies are built and used.  A study of 39 speculative fiction texts, including 20 that we identified as Afrofuturist, revealed three overarching themes that serve as recommendations for the creation and maintenance of a diverse and inclusive metaverse: Collective Power, Inclusive Engagement, and Cultural Specificity. We outline each recommendation through a textual analysis of three Afrofuturist texts – Esi Edugyan’s Washington Black (2018), Roger Ross Williams’ Traveling While Black (2019), and Ryan Coogler’s Black Panther (2018) – and specify the undercurrents of collectivity and co-production that bind them together. We suggest collaborative and critical reading methods for industry professionals and community members which may help to shape democratic processes governing the future of AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "609-609",
		"source": "ojs.aaai.org",
		"title": "Afrofuturist Values for the Metaverse (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31662",
		"volume": "7",
		"author": [
			{
				"family": "Hice-Fromille",
				"given": "Theresa"
			},
			{
				"family": "Papazoglakis",
				"given": "Sarah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "hollanekEthicoPoliticsDesignToolkits2024",
		"type": "article-journal",
		"abstract": "This paper interrogates the belief in toolkitting as a method for translating AI ethics theory into practice and assesses the toolkit paradigm’s effect on the understanding of ethics in AI research and AI-related policy. I start by exploring the ethico-political assumptions that underly most ethical AI toolkits. Through a meta-critique of toolkits (drawing on a review of existing ‘toolkit-scoping’ work), I demon-strate that most toolkits embody a reductionist conception of ethics and that, because of this, their capacity for facili-tating change and challenging the status quo is limited. Then, I analyze the features of several ‘alternative’ toolkits – informed by feminist theory, posthumanism, and critical design – whose creators recognize that ethics cannot be-come a box-ticking exercise for engineers, while the ethical should not be dissociated from the political. Finally, in the concluding section, referring to broader theories and cri-tiques of toolkitting as a method for structuring the design process, I suggest how different stakeholders can draw on the myriad of available tools, ranging from big tech com-panies’ guidelines to feminist design ideation cards, to achieve positive, socially desirable results, while rejecting the oversimplification of ethical practice and technosolu-tionism that many responsible AI toolkits embody. The analysis thus serves to provide suggestions for future toolkit creators and users on how to meaningfully adopt the toolkit format in AI ethics work without overselling its transformative potential.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "610-610",
		"source": "ojs.aaai.org",
		"title": "The Ethico-Politics of Design Toolkits: Responsible AI Tools, From Big Tech Guidelines to Feminist Ideation Cards (Extended Abstract)",
		"title-short": "The Ethico-Politics of Design Toolkits",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31663",
		"volume": "7",
		"author": [
			{
				"family": "Hollanek",
				"given": "Tomasz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "iqbalLLMPlatformSecurity2024",
		"type": "article-journal",
		"abstract": "Large language model (LLM) platforms, such as ChatGPT, have recently begun offering an app ecosystem to interface with third-party services on the internet. While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Apps also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future third-party integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin (apps) ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms. The full version of this paper is available online at https://arxiv.org/abs/2309.10254",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "611-623",
		"source": "ojs.aaai.org",
		"title": "LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins",
		"title-short": "LLM Platform Security",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31664",
		"volume": "7",
		"author": [
			{
				"family": "Iqbal",
				"given": "Umar"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Roesner",
				"given": "Franziska"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jainAILanguageModel2024",
		"type": "article-journal",
		"abstract": "We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations. Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos. We evaluate the decisions of three state-of-the-art LLMs — GPT-4, Gemini 1.0, and Claude 3 Sonnet — in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded. Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods. These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "624-633",
		"source": "ojs.aaai.org",
		"title": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\": Norm Inconsistency in LLM Decision-Making",
		"title-short": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\"",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31665",
		"volume": "7",
		"author": [
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Calacci",
				"given": "D."
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jaiswalBreakingGlobalNorth2024",
		"type": "article-journal",
		"abstract": "Facial Recognition Systems (FRSs) are being developed and deployed all around the world at unprecedented rates. Most platforms are designed in a limited set of countries, but deployed in other regions too, without adequate checkpoints for region-specific requirements. This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems. A combination of unavailability of datasets, lack of understanding of how FRSs function and low-resource bias mitigation measures accentuate the problems at hand. In this work, we propose a self-curated face dataset composed of 6,579 unique male and female sports-persons (cricket players) from eight countries around the world. More than 50% of the dataset is composed of individuals from the Global South countries and is demographically diverse. To aid adversarial audits and robust model training, we curate four adversarial variants of each image in the dataset, leading to more than 40,000 distinct images. We also use this dataset to benchmark five popular facial recognition systems (FRSs), including both commercial and open-source FRSs, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming). Experiments on industrial FRSs reveal accuracies ranging from 98.2% (in case of Azure) to 38.1% (in case of Face++), with a large disparity between males and females in the Global South (max difference of 38.5% in case of Face++). Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%). A Grad-CAM analysis shows that the nose, forehead and mouth are the regions of interest for one of the open-source FRSs.  Based on this crucial observation, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques that demonstrate a significant improvement in accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings. For the red-teaming experiment using the open-source Deepface model we observe that simple fine-tuning is not very useful while contrastive learning brings steady benefits.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "634-646",
		"source": "ojs.aaai.org",
		"title": "Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems",
		"title-short": "Breaking the Global North Stereotype",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31666",
		"volume": "7",
		"author": [
			{
				"family": "Jaiswal",
				"given": "Siddharth"
			},
			{
				"family": "Ganai",
				"given": "Animesh"
			},
			{
				"family": "Dash",
				"given": "Abhisek"
			},
			{
				"family": "Ghosh",
				"given": "Saptarshi"
			},
			{
				"family": "Mukherjee",
				"given": "Animesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jensenReflectionItsCreators2024",
		"type": "article-journal",
		"abstract": "The increasing prevalence of artificial intelligence (AI) will likely lead to new interactions and impacts for the general public. An understanding of people’s perceptions of AI can be leveraged to design and deploy AI systems toward human needs and values. We conducted semi-structured interviews with 25 individuals in the general public and 20 AI experts in the United States (U.S.) to assess perceptions of AI across levels of expertise. Qualitative analysis revealed that ideas about humanness and ethics were central to perceptions of AI in both groups. Humanness, the set of traits considered to distinguish humans from other intelligent actors, was used to articulate beliefs about AI’s characteristics. Ethics arose in discussions of the role of technology in society and centered around views of AI as made and used by people. General public and expert participants expressed similar perceptions of AI, but articulated beliefs slightly differently. We discuss the implications of humanness-related beliefs and ethical concerns for AI development and deployment.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "647-658",
		"source": "ojs.aaai.org",
		"title": "Reflection of Its Creators: Qualitative Analysis of General Public and Expert Perceptions of Artificial Intelligence",
		"title-short": "Reflection of Its Creators",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31667",
		"volume": "7",
		"author": [
			{
				"family": "Jensen",
				"given": "Theodore"
			},
			{
				"family": "Theofanos",
				"given": "Mary"
			},
			{
				"family": "Greene",
				"given": "Kristen"
			},
			{
				"family": "Williams",
				"given": "Olivia"
			},
			{
				"family": "Goad",
				"given": "Kurtis"
			},
			{
				"family": "Fofang",
				"given": "Janet Bih"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "jorgensonVirtualAssistantsAre2024",
		"type": "article-journal",
		"abstract": "The ethical use of AI typically involves setting boundaries on its deployment. Ethical guidelines advise against practices that involve deception, privacy infringement, or discriminatory actions. However, ethical considerations can also identify areas where using AI is desirable and morally necessary. For instance, it has been argued that AI could contribute to more equitable justice systems. Another area where ethical considerations can make AI deployment imperative is healthcare. For example, patients often withhold pertinent details from healthcare providers due to fear of judgment. However, utilizing virtual assistants to gather patients' health histories could be a potential solution. Ethical imperatives support using such technology if patients are more inclined to disclose information to an AI system. This article presents findings from several survey studies investigating whether virtual assistants can reduce non-disclosure behaviors. Unfortunately, the evidence suggests that virtual assistants are unlikely to minimize non-disclosure. Therefore, the potential benefits of virtual assistants due to reduced non-disclosure are unlikely to outweigh their ethical risks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "659-669",
		"source": "ojs.aaai.org",
		"title": "Virtual Assistants Are Unlikely to Reduce Patient Non-Disclosure",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31668",
		"volume": "7",
		"author": [
			{
				"family": "Jorgenson",
				"given": "Corinne"
			},
			{
				"family": "Ozkes",
				"given": "Ali I."
			},
			{
				"family": "Willems",
				"given": "Jurgen"
			},
			{
				"family": "Vanderelst",
				"given": "Dieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kawakamiResponsibleAIArtifacts2024",
		"type": "article-journal",
		"abstract": "The responsible AI (RAI) community has introduced numerous processes and artifacts---such as Model Cards, Transparency Notes, and Data Cards---to facilitate transparency and support the governance of AI systems.  While originally designed to scaffold and document AI development processes in technology companies, these artifacts are becoming central components of regulatory compliance under recent regulations such as the EU AI Act. Much of the existing literature has focussed primarily on the design of new RAI artifacts, or an examination of their use by practitioners within technology companies.  However, as RAI artifacts begin to play key roles in enabling external oversight, it becomes critical to understand how stakeholders---particularly stakeholders situated outside of technology companies who govern and audit industry AI deployments---perceive the efficacy of RAI artifacts.  In this study, we conduct semi-structured interviews and design activities with 19 government, legal, and civil society stakeholders who inform policy and advocacy around responsible AI efforts. While participants believe that RAI artifacts are a valuable contribution to the RAI ecosystem, many have concerns around their potential unintended and longer-term impacts on actors outside of technology companies (e.g., downstream end-users, policymakers, civil society stakeholders). We organized these beliefs into four barriers that help explain how RAI artifacts may (inadvertently) reconfigure power relations across civil society, government, and industry, impeding civil society and legal stakeholders' ability to protect downstream end-users from potential AI harms. Participants envision how structural changes, along with changes in how RAI artifacts are designed, used, and governed, could help re-direct the role and impacts of artifacts in the RAI ecosystem. Drawing on these findings, we discuss research and policy implications for RAI artifacts.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "670-682",
		"source": "ojs.aaai.org",
		"title": "Do Responsible AI Artifacts Advance Stakeholder Goals? Four Key Barriers Perceived by Legal and Civil Stakeholders",
		"title-short": "Do Responsible AI Artifacts Advance Stakeholder Goals?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31669",
		"volume": "7",
		"author": [
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Wilkinson",
				"given": "Daricia"
			},
			{
				"family": "Chouldechova",
				"given": "Alexandra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kawakamiAIFailureLoops2024",
		"type": "article-journal",
		"abstract": "A growing body of literature has focused on understanding and addressing workplace AI design failures. However, past work has largely overlooked the role of occupational devaluation in shaping the dynamics of AI development and deployment. In this paper, we examine the case of feminized labor: a class of devalued occupations historically misnomered as ``women's work,'' such as social work, K-12 teaching, and home healthcare. Drawing on literature on AI deployments in feminized labor contexts, we conceptualize AI Failure Loops: a set of interwoven, socio-technical failures that help explain how the systemic devaluation of workers' expertise negatively impacts, and is impacted by, AI design, evaluation, and governance practices. These failures demonstrate how misjudgments on the automatability of workers' skills can lead to AI deployments that fail to bring value and, instead, further diminish the visibility of workers' expertise. We discuss research and design implications for workplace AI, especially for devalued occupations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "683-683",
		"source": "ojs.aaai.org",
		"title": "AI Failure Loops in Feminized Labor: Understanding the Interplay of Workplace AI and Occupational Devaluation",
		"title-short": "AI Failure Loops in Feminized Labor",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31670",
		"volume": "7",
		"author": [
			{
				"family": "Kawakami",
				"given": "Anna"
			},
			{
				"family": "Taylor",
				"given": "Jordan"
			},
			{
				"family": "Fox",
				"given": "Sarah"
			},
			{
				"family": "Zhu",
				"given": "Haiyi"
			},
			{
				"family": "Holstein",
				"given": "Ken"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kayEpistemicInjusticeGenerative2024",
		"type": "article-journal",
		"abstract": "This paper investigates how generative AI can potentially undermine the integrity of collective knowledge and the processes we rely on to acquire, assess, and trust information, posing a significant threat to our knowledge ecosystem and democratic discourse. Grounded in social and political philosophy, we introduce the concept of generative algorithmic epistemic injustice. We identify four key dimensions of this phenomenon: amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice. We illustrate each dimension with real-world examples that reveal how generative AI can produce or amplify misinformation, perpetuate representational harm, and create epistemic inequities, particularly in multilingual contexts. By highlighting these injustices, we aim to inform the development of epistemically just generative AI systems, proposing strategies for resistance, system design principles, and two approaches that leverage generative AI to foster a more equitable information ecosystem, thereby safeguarding democratic values and the integrity of knowledge production.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "684-697",
		"source": "ojs.aaai.org",
		"title": "Epistemic Injustice in Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31671",
		"volume": "7",
		"author": [
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Mohamed",
				"given": "Shakir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kennedyVernacularizingTaxonomiesHarm2024",
		"type": "article-journal",
		"abstract": "Operationalizing AI ethics and safety principles and frameworks is essential to realizing the potential benefits and mitigating potential harms caused by AI systems. To that end, actors across industry, academia, and regulatory bodies have created formal taxonomies of harm to support operationalization efforts. These include novel “holistic” methods that go beyond exclusive reliance on technical benchmarking. However, our paper argues that such taxonomies are still too general to be readily implemented in sector-specific AI safety operationalization efforts, and especially in underresourced or “high-risk” sectors. This is because many sectors are constituted by discourses, norms, and values that “refract” or even directly conflict with those operating in society more broadly. Drawing from emerging anthropological theories of human rights, we propose that the process of “vernacularization”—a participatory, decolonial practice distinct from doctrinary “translation” (the dominant mode of AI safety operationalization)—can help bridge this gap. To demonstrate this point, we consider the education sector, and identify precisely how vernacularizing a leading  taxonomy of harm leads to a clearer view of how harms AI systems may cause are substantially intensified when deployed in educational spaces. We conclude by discussing the generalizability of vernacularization as a useful AI safety methodology.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "698-710",
		"source": "ojs.aaai.org",
		"title": "Vernacularizing Taxonomies of Harm is Essential for Operationalizing Holistic AI Safety",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31672",
		"volume": "7",
		"author": [
			{
				"family": "Kennedy",
				"given": "Wm Matthew"
			},
			{
				"family": "Campos",
				"given": "Daniel Vargas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "keswaniProsConsActive2024",
		"type": "article-journal",
		"abstract": "Computational preference elicitation methods are tools used to learn people’s preferences quantitatively in a given context. Recent works on preference elicitation advocate for active learning as an efficient method to iteratively construct queries (framed as comparisons between context-specific cases) that are likely to be most informative about an agent’s underlying preferences. In this work, we argue that the use of active learning for moral preference elicitation relies on certain assumptions about the underlying moral preferences, which can be violated in practice. Specifically, we highlight the following common assumptions (a) preferences are stable over time and not sensitive to the sequence of presented queries, (b) the appropriate hypothesis class is chosen to model moral preferences, and (c) noise in the agent’s responses is limited. While these assumptions can be appropriate for preference elicitation in certain domains, prior research on moral psychology suggests they may not be valid for moral judgments. Through a synthetic simulation of preferences that violate the above assumptions, we observe that active learning can have similar or worse performance than a basic random query selection method in certain settings. Yet, simulation results also demonstrate that active learning can still be viable if the degree of instability or noise is relatively small and when the agent’s preferences can be approximately represented with the hypothesis class used for learning. Our study highlights the nuances associated with effective moral preference elicitation in practice and advocates for the cautious use of active learning as a methodology to learn moral preferences.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "711-723",
		"source": "ojs.aaai.org",
		"title": "On the Pros and Cons of Active Learning for Moral Preference Elicitation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31673",
		"volume": "7",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Conitzer",
				"given": "Vincent"
			},
			{
				"family": "Heidari",
				"given": "Hoda"
			},
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Sinnott-Armstrong",
				"given": "Walter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "keswaniAlgorithmicFairnessPerspective2024",
		"type": "article-journal",
		"abstract": "Real-world applications of machine learning (ML) algorithms often propagate negative stereotypes and social biases against marginalized groups. In response, the field of fair machine learning has proposed technical solutions for a variety of settings that aim to correct the biases in algorithmic predictions. These solutions remove the dependence of the final prediction on the protected attributes (like gender or race) and/or ensure that prediction performance is similar across demographic groups. Yet, recent studies assessing the impact of these solutions in practice demonstrate their ineffectiveness in tackling real-world inequalities. Given this lack of real-world success, it is essential to take a step back and question the design motivations of algorithmic fairness interventions. \n\nWe use popular legal anti-discriminatory principles, specifically anti-classification and anti-subordination principles, to study the motivations of fairness interventions and their applications. The anti-classification principle suggests addressing discrimination by ensuring that decision processes and outcomes are independent of the protected attributes of individuals. The anti-subordination principle, on the other hand, argues that decision-making policies can provide equal protection to all only by actively tackling societal hierarchies that enable structural discrimination, even if that requires using protected attributes to address historical inequalities. Through a survey of the fairness mechanisms and applications, we assess different components of fair ML approaches from the perspective of these principles. We argue that the observed shortcomings of fair ML algorithms are similar to the failures of anti-classification policies and that these shortcomings constitute violations of the anti-subordination principle. Correspondingly, we propose guidelines for algorithmic fairness interventions to adhere to the anti-subordination principle. In doing so, we hope to bridge critical concepts between legal frameworks for non-discrimination and fairness in machine learning.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "724-737",
		"source": "ojs.aaai.org",
		"title": "Algorithmic Fairness From the Perspective of Legal Anti-discrimination Principles",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31674",
		"volume": "7",
		"author": [
			{
				"family": "Keswani",
				"given": "Vijay"
			},
			{
				"family": "Celis",
				"given": "L. Elisa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kimWhatsYourStake2024",
		"type": "article-journal",
		"abstract": "It's no secret that AI systems come with a significant environmental cost. This raises the question: What are the roles and responsibilities of computing professionals regarding the sustainability of AI? Informed by a year-long informal literature review on the subject, we employ stakeholder identification, analysis, and mapping to highlight the complex and interconnected roles that five major stakeholder groups (industry, practitioners, regulatory, advocacy, and the general public) play in the sustainability of AI. Swapping the traditional final step of stakeholder methods (stakeholder engagement) for entanglement, we demonstrate the inherent entwinement of choices made with regard to the development and maintenance of AI systems and the people who impact (or are impacted by) these choices. This entanglement should be understood as a system of human and non-human agents, with the implications of each choice ricocheting into the use of natural resources and climate implications. We argue that computing professionals (AI-focused or not) may belong to multiple stakeholder groups, and that we all have multiple roles to play in the sustainability of AI. Further, we argue that the nature of regulation in this domain will look unlike others in environmental preservation (e.g., legislation around water contaminants). As a result, we call for ongoing, flexible bodies and policies to move towards the regulation of AI from a sustainability angle, as well as suggest ways in which individual computing professionals can contribute to fighting the environmental and climate effects of AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "738-750",
		"source": "ojs.aaai.org",
		"title": "What’s Your Stake in Sustainability of AI?: An Informed Insider’s Guide",
		"title-short": "What’s Your Stake in Sustainability of AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31675",
		"volume": "7",
		"author": [
			{
				"family": "Kim",
				"given": "Grace C."
			},
			{
				"family": "Rothschild",
				"given": "Annabel"
			},
			{
				"family": "DiSalvo",
				"given": "Carl"
			},
			{
				"family": "DiSalvo",
				"given": "Betsy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kirfelAnticipatingRisksBenefits2024",
		"type": "article-journal",
		"abstract": "This paper examines the transformative potential of Counterfactual World Simulation Models (CWSMs). CWSMs use pieces of multi-modal evidence, such as the CCTV footage or sound recordings of a road accident, to build a high-fidelity 3D reconstruction of the scene. They can also answer causal questions, such as whether the accident happened because the driver was speeding, by simulating what would have happened in relevant counterfactual situations. CWSMs will enhance our capacity to envision alternate realities and investigate the outcomes of counterfactual alterations to how events unfold. This also, however, raises questions about what alternative scenarios we should be considering and what to do with that knowledge. We present a normative and ethical framework that guides and constrains the simulation of counterfactuals. We address the challenge of ensuring fidelity in reconstructions while simultaneously preventing stereotype perpetuation during counterfactual simulations. We anticipate different modes of how users will interact with CWSMs and discuss how their outputs may be presented. Finally, we address the prospective applications of CWSMs in the legal domain, recognizing both their potential to revolutionize legal proceedings as well as the ethical concerns they engender. Anticipating a new type of AI, this paper seeks to illuminate a path forward for responsible and effective use of CWSMs.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "751-751",
		"source": "ojs.aaai.org",
		"title": "Anticipating the Risks and Benefits of Counterfactual World Simulation Models (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31676",
		"volume": "7",
		"author": [
			{
				"family": "Kirfel",
				"given": "Lara"
			},
			{
				"family": "MacCoun",
				"given": "Rob"
			},
			{
				"family": "Icard",
				"given": "Thomas"
			},
			{
				"family": "Gerstenberg",
				"given": "Tobias"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "klymanAcceptableUsePolicies2024",
		"type": "article-journal",
		"abstract": "As foundation models have accumulated hundreds of millions of users, developers have begun to take steps to prevent harmful types of uses. One salient intervention that foundation model developers adopt is acceptable use policies—legally binding policies that prohibit users from using a model for specific purposes. This paper identifies acceptable use policies from 30 foundation model developers, analyzes the use restrictions they contain, and argues that acceptable use policies are an important lens for understanding the regulation of foundation models. Taken together, developers’ acceptable use policies include 127 distinct use restrictions; the wide variety in the number and type of use restrictions may create fragmentation across the AI supply chain. Companies also employ acceptable use policies to prevent competitors or specific industries from making use of their models. Developers alone decide what constitutes acceptable use, and rarely provide transparency about how they enforce their policies. In practice, acceptable use policies are difficult to enforce, and scrupulous enforcement can act as a barrier to researcher access and limit beneficial uses of foundation models. Acceptable use policies for foundation models are an early example of self-regulation that have a significant impact on the market for foundation models and the AI ecosystem.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "752-767",
		"source": "ojs.aaai.org",
		"title": "Acceptable Use Policies for Foundation Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31677",
		"volume": "7",
		"author": [
			{
				"family": "Klyman",
				"given": "Kevin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "koltResponsibleReportingFrontier2024",
		"type": "article-journal",
		"abstract": "Mitigating the risks from frontier AI systems requires up-to-date and reliable information about those systems. Organizations that develop and deploy frontier systems have significant access to such information. By reporting safety-critical information to actors in government, industry, and civil society, these organizations could improve visibility into new and emerging risks posed by frontier systems. Equipped with this information, developers could make better informed decisions on risk management, while policymakers could design more targeted and robust regulatory infrastructure. We outline the key features of responsible reporting and propose mechanisms for implementing them in practice.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "768-783",
		"source": "ojs.aaai.org",
		"title": "Responsible Reporting for Frontier AI Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31678",
		"volume": "7",
		"author": [
			{
				"family": "Kolt",
				"given": "Noam"
			},
			{
				"family": "Anderljung",
				"given": "Markus"
			},
			{
				"family": "Barnhart",
				"given": "Joslyn"
			},
			{
				"family": "Brass",
				"given": "Asher"
			},
			{
				"family": "Esvelt",
				"given": "Kevin"
			},
			{
				"family": "Hadfield",
				"given": "Gillian K."
			},
			{
				"family": "Heim",
				"given": "Lennart"
			},
			{
				"family": "Rodriguez",
				"given": "Mikel"
			},
			{
				"family": "Sandbrink",
				"given": "Jonas B."
			},
			{
				"family": "Woodside",
				"given": "Thomas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "krishnaTradeoffsAdversarialRobustness2024",
		"type": "article-journal",
		"abstract": "As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-robust linear and non-linear models. Our empirical results with multiple real-world datasets validate our theoretical results and show the impact of varying degrees of model robustness on the cost and validity of the resulting recourses. Our analyses demonstrate that adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses, thus shedding light on the inherent trade-offs between adversarial robustness and actionable explanations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "784-795",
		"source": "ojs.aaai.org",
		"title": "On the Trade-offs between Adversarial Robustness and Actionable Explanations",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31679",
		"volume": "7",
		"author": [
			{
				"family": "Krishna",
				"given": "Satyapriya"
			},
			{
				"family": "Agarwal",
				"given": "Chirag"
			},
			{
				"family": "Lakkaraju",
				"given": "Himabindu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "kwegyir-aggreyObservingContextImproves2024",
		"type": "article-journal",
		"abstract": "In many domains, it is difficult to obtain the race data that is required to estimate racial disparity.  To address this problem, practitioners have adopted the use of proxy methods which predict race using non-protected covariates.  However, these proxies often yield biased estimates, especially for minority groups, limiting their real-world utility.  In this paper, we introduce two new contextual proxy models that advance existing methods by incorporating contextual features in order to improve race estimates. We show that these algorithms demonstrate significant performance improvements in estimating disparities, on real-world home loan and voter data. We establish that achieving unbiased disparity estimates with contextual proxies relies on mean-consistency, a calibration-like condition.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "796-806",
		"source": "ojs.aaai.org",
		"title": "Observing Context Improves Disparity Estimation when Race is Unobserved",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31680",
		"volume": "7",
		"author": [
			{
				"family": "Kwegyir-Aggrey",
				"given": "Kweku"
			},
			{
				"family": "Durvasula",
				"given": "Naveen"
			},
			{
				"family": "Wang",
				"given": "Jennifer"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lamparthHumanVsMachine2024",
		"type": "article-journal",
		"abstract": "To some, the advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness while reducing the influence of human error and emotions. However, there is still debate about how AI systems, especially large language models (LLMs) that can be applied to many tasks, behave compared to humans in high-stakes military decision-making scenarios with the potential for increased risks towards escalation and unnecessary conflicts. To test this potential and scrutinize the use of LLMs for such purposes, we use a new wargame experiment with 107 national security experts designed to examine crisis escalation in a fictional US-China scenario and compare the behavior of human player teams to LLM-simulated team responses in separate simulations. Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. Here, we find that the LLM-simulated responses can be more aggressive and significantly affected by changes in the scenario. We show a considerable high-level agreement in the LLM and human responses and significant quantitative and qualitative differences in individual actions and strategic tendencies. These differences depend on intrinsic biases in LLMs regarding the appropriate level of violence following strategic instructions, the choice of LLM, and whether the LLMs are tasked to decide for a team of players directly or first to simulate dialog between a team of players. When simulating the dialog, the discussions lack quality and maintain a farcical harmony. The LLM simulations cannot account for human player characteristics, showing no significant difference even for extreme traits, such as “pacifist” or “aggressive sociopath.” When probing behavioral consistency across individual moves of the simulation, the tested LLMs deviated from each other but generally showed somewhat consistent behavior. Our results motivate policymakers to be cautious before granting autonomy or following AI-based strategy recommendations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "807-817",
		"source": "ojs.aaai.org",
		"title": "Human vs. Machine: Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
		"title-short": "Human vs. Machine",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31681",
		"volume": "7",
		"author": [
			{
				"family": "Lamparth",
				"given": "Max"
			},
			{
				"family": "Corso",
				"given": "Anthony"
			},
			{
				"family": "Ganz",
				"given": "Jacob"
			},
			{
				"family": "Mastro",
				"given": "Oriana Skylar"
			},
			{
				"family": "Schneider",
				"given": "Jacquelyn"
			},
			{
				"family": "Trinkunas",
				"given": "Harold"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yejasRacialNeighborhoodDisparities2024",
		"type": "article-journal",
		"abstract": "Legal financial obligations (LFOs) such as court fees and fines are commonly levied on individuals who are convicted of crimes. It is expected that LFO amounts should be similar across social, racial, and geographic subpopulations convicted of the same crime.  This work analyzes the distribution of LFOs in Jefferson County, Alabama and highlights disparities across different individual and neighborhood demographic characteristics. Data-driven discovery methods are used to detect subpopulations that experience higher LFOs than the overall population of offenders. Critically, these discovery methods do not rely on pre-specified groups and can assist scientists and researchers investigate socially-sensitive hypotheses in a disciplined way.  Some findings, such as individuals who are Black, live in Black-majority neighborhoods, or live in low-income neighborhoods tending to experience higher LFOs, are commensurate with prior expectation. However others, such as high LFO amounts in worthless instrument (bad check) cases experienced disproportionately by individuals living in affluent majority-white neighborhoods, are more surprising. More broadly than the specific findings, the methodology is shown to identify structural weaknesses that undermine the goal of equal justice under law that can be addressed through policy interventions.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "818-827",
		"source": "ojs.aaai.org",
		"title": "Racial and Neighborhood Disparities in Legal Financial Obligations in Jefferson County, Alabama",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31682",
		"volume": "7",
		"author": [
			{
				"family": "Yejas",
				"given": "Óscar Lara"
			},
			{
				"family": "Joshi",
				"given": "Aakanksha"
			},
			{
				"family": "Martinez",
				"given": "Andrew"
			},
			{
				"family": "Nelson",
				"given": "Leah"
			},
			{
				"family": "Speakman",
				"given": "Skyler"
			},
			{
				"family": "Thompson",
				"given": "Krysten"
			},
			{
				"family": "Nishimura",
				"given": "Yuki"
			},
			{
				"family": "Bond",
				"given": "Jordan"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lehdonvirtaComputeNorthVs2024",
		"type": "article-journal",
		"abstract": "Governments have begun to view AI compute infrastructures, including advanced AI chips, as a geostrategic resource. This is partly because “compute governance” is believed to be emerging as an important tool for governing AI systems. In this governance model, states that host AI compute capacity within their territorial jurisdictions are likely to be better placed to impose their rules on AI systems than states that do not. In this study, we provide the first attempt at mapping the global geography of public cloud GPU compute, one particularly important category of AI compute infrastructure. Using a census of hyperscale cloud providers’ cloud regions, we observe that the world is divided into “Compute North” countries that host AI compute relevant for AI development (ie. training), “Compute South” countries whose AI compute is more relevant for AI deployment (ie. running inferencing), and “Compute Desert” countries that host no public cloud AI compute at all. We generate potential explanations for the results using expert interviews, discuss the implications to AI governance and technology geopolitics, and consider possible future trajectories.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "828-838",
		"source": "ojs.aaai.org",
		"title": "Compute North vs. Compute South: The Uneven Possibilities of Compute-based AI Governance Around the Globe",
		"title-short": "Compute North vs. Compute South",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31683",
		"volume": "7",
		"author": [
			{
				"family": "Lehdonvirta",
				"given": "Vili"
			},
			{
				"family": "Wú",
				"given": "Bóxī"
			},
			{
				"family": "Hawkins",
				"given": "Zoe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "leidingerHowAreLLMs2024",
		"type": "article-journal",
		"abstract": "With the widespread availability of LLMs since the release of ChatGPT and increased public scrutiny, commercial model development appears to have focused their efforts on `safety' training concerning legal liabilities at the expense of social impact evaluation. This mimics a similar trend which we could observe for search engine autocompletion some years prior. We draw on scholarship from NLP and search engine auditing and present a novel evaluation task in the style of autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by using four metrics, namely refusal rates, toxicity, sentiment and regard, with and without safety system prompts. Our findings indicate an improvement to stereotyping outputs with the system prompt, but overall a lack of attention by LLMs under study to certain harms classified as toxic, particularly for prompts about peoples/ethnicities and sexual orientation. Mentions of intersectional identities trigger a disproportionate amount of stereotyping. Finally, we discuss the implications of these findings about stereotyping harms in light of the coming intermingling of LLMs and search and the choice of stereotyping mitigation policy to adopt. We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "839-854",
		"source": "ojs.aaai.org",
		"title": "How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine Studies",
		"title-short": "How Are LLMs Mitigating Stereotyping Harms?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31684",
		"volume": "7",
		"author": [
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Rogers",
				"given": "Richard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "liFeasibilityIntentObfuscating2024",
		"type": "article-journal",
		"abstract": "Intent obfuscation is a common tactic in adversarial situations, enabling the attacker to both manipulate the target system and avoid culpability. Surprisingly, it has rarely been implemented in adversarial attacks on machine learning systems. We are the first to propose using intent obfuscation to generate adversarial examples for object detectors: by perturbing another non-overlapping object to disrupt the target object, the attacker hides their intended target. We conduct a randomized experiment on 5 prominent detectors---YOLOv3, SSD, RetinaNet, Faster R-CNN, and Cascade R-CNN---using both targeted and untargeted attacks and achieve success on all models and attacks. We analyze the success factors characterizing intent obfuscating attacks, including target object confidence and perturb object sizes. We then demonstrate that the attacker can exploit these success factors to increase success rates for all models and attacks. Finally, we discuss main takeaways and legal repercussions. If you are reading the AAAI/ACM version, please download the technical appendix on arXiv at https://arxiv.org/abs/2408.02674",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "855-866",
		"source": "ojs.aaai.org",
		"title": "On Feasibility of Intent Obfuscating Attacks",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31685",
		"volume": "7",
		"author": [
			{
				"family": "Li",
				"given": "Zhaobin"
			},
			{
				"family": "Shafto",
				"given": "Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "linDemocratizingAIConcern2024",
		"type": "article-journal",
		"abstract": "The call to make artificial intelligence (AI) more democratic, or to “democratize AI,” is sometimes framed as a promising response for mitigating algorithmic injustice or making AI more aligned with social justice. However, the notion of “democratizing AI” is elusive, as the phrase has been associated with multiple meanings and practices, and the extent to which it may help mitigate algorithmic injustice is still underexplored. In this paper, based on a socio-technical understanding of algorithmic injustice, I examine three notable notions of democratizing AI and their associated measures—democratizing AI use, democratizing AI development, and democratizing AI governance—regarding their respective prospects and limits in response to algorithmic injustice. My examinations reveal that while some versions of democratizing AI bear the prospect of mitigating the concern of algorithmic injustice, others are somewhat limited and might even function to perpetuate unjust power hierarchies. This analysis thus urges a more fine-grained discussion on how to democratize AI and suggests that closer scrutiny of the power dynamics embedded in the socio-technical structure can help guide such explorations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "867-867",
		"source": "ojs.aaai.org",
		"title": "“Democratizing AI” and the Concern of Algorithmic Injustice (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31686",
		"volume": "7",
		"author": [
			{
				"family": "Lin",
				"given": "Ting-an"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "livanosFoundationsUnfairnessAnomaly2024",
		"type": "article-journal",
		"abstract": "Deep anomaly detection (AD) is perhaps the most controversial of data analytic tasks as it identifies entities that are specifically targeted for further investigation or exclusion. Also controversial is the application of AI to facial data, in particular facial recognition. This work explores the intersection of these two areas to understand two core questions: Who these algorithms are being unfair to and equally important why. Recent work has shown that deep AD can be unfair to different groups despite being unsupervised with a recent study showing that for portraits of people: men of color are far more likely to be chosen to be outliers. We study the two main categories of AD algorithms: autoencoder-based and single-class-based which effectively try to compress all the instances and those that can not be easily compressed are deemed to be outliers. We experimentally verify sources of unfairness such as the under-representation of a group (e.g people of color are relatively rare), spurious group features (e.g. men are often photographed with hats) and group labeling noise (e.g. race is subjective). We conjecture that lack of compressibility is the main foundation and the others cause it but experimental results show otherwise and we present a natural hierarchy amongst them.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "868-877",
		"source": "ojs.aaai.org",
		"title": "Foundations for Unfairness in Anomaly Detection - Case Studies in Facial Imaging Data",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31687",
		"volume": "7",
		"author": [
			{
				"family": "Livanos",
				"given": "Michael"
			},
			{
				"family": "Davidson",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "albaredaUncoveringGapChalleging2024",
		"type": "article-journal",
		"abstract": "In this paper, I will argue that the responsibility gap arising from new AI systems is reducible to the problem of many hands and collective agency. Systematic analysis of the agential dimension of AI will lead me to outline a disjunctive between the two problems. Either we reduce individual responsibility gaps to the many hands, or we abandon the individual dimension and accept the possibility of responsible collective agencies. Moreover, I will adduce that this conclusion reveals an underlying weakness in AI ethics: the lack of attention to the question of the disciplinary boundaries of AI ethics. This absence has made it difficult to identify the specifics of the responsibility gap arising from new AI systems as compared to the responsibility gaps of other applied ethics. Lastly, I will be concerned with outlining these specific aspects.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "878-878",
		"source": "ojs.aaai.org",
		"title": "Uncovering the Gap: Challeging the Agential Nature of AI Responsibility Problems (Extended Abstract)",
		"title-short": "Uncovering the Gap",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31688",
		"volume": "7",
		"author": [
			{
				"family": "Albareda",
				"given": "Joan Llorca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "locatelliExaminingBehaviorLLM2024",
		"type": "article-journal",
		"abstract": "The Exame Nacional do Ensino Médio (ENEM) is a pivotal test for Brazilian students, required for admission to a significant number of universities in Brazil. The test consists of four objective high-school level tests on Math, Humanities, Natural Sciences and Languages, and one writing essay. Students' answers to the test and to the accompanying socioeconomic status questionnaire are made public every year (albeit anonymized) due to transparency policies from the Brazilian Government. In the context of large language models (LLMs), these data lend themselves nicely to comparing different groups of humans with AI, as we can have access to human and machine answer distributions. We leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4, and MariTalk, a model trained using Portuguese data, to humans, aiming to ascertain how their answers relate to real societal groups and what that may reveal about the model biases. We divide the human groups by using socioeconomic status (SES), and compare their answer distribution with LLMs for each question and for the essay. We find no significant biases when comparing LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as the distance between model and human answers is mostly determined by the human accuracy. A similar conclusion is found by looking at the generated text as, when analyzing the essays, we observe that human and LLM essays differ in a few key factors, one being the choice of words where model essays were easily separable from human ones. The texts also differ syntactically, with LLM generated essays exhibiting, on average, smaller sentences and less thought units, among other differences. These results suggest that, for Brazilian Portuguese in the ENEM context, LLM outputs represent no group of humans, being significantly different from the answers from Brazilian students across all tests. The appendices may be found at https://arxiv.org/abs/2408.05035.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "879-890",
		"source": "ojs.aaai.org",
		"title": "Examining the Behavior of LLM Architectures Within the Framework of Standardized National Exams in Brazil",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31689",
		"volume": "7",
		"author": [
			{
				"family": "Locatelli",
				"given": "Marcelo Sartori"
			},
			{
				"family": "Miranda",
				"given": "Matheus Prado"
			},
			{
				"family": "Costa",
				"given": "Igor Joaquim da Silva"
			},
			{
				"family": "Prates",
				"given": "Matheus Torres"
			},
			{
				"family": "Thomé",
				"given": "Victor"
			},
			{
				"family": "Monteiro",
				"given": "Mateus Zaparoli"
			},
			{
				"family": "Lacerda",
				"given": "Tomas"
			},
			{
				"family": "Pagano",
				"given": "Adriana"
			},
			{
				"family": "Neto",
				"given": "Eduardo Rios"
			},
			{
				"family": "Jr",
				"given": "Wagner Meira"
			},
			{
				"family": "Almeida",
				"given": "Virgilio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "loeffladSocialScoringSystems2024",
		"type": "article-journal",
		"abstract": "Recent developments in artificial intelligence research have advanced the spread of automated decision-making (ADM) systems used for regulating human behaviors. In this context, prior work has focused on the determinants of human trust in and the legitimacy of ADM systems, e.g., when used for decision support. However, studies assessing people's perceptions of ADM systems used for behavioral regulation, as well as the effect on behaviors and the overall impact on human communities are largely absent. In this paper, we experimentally investigate people's behavioral adaptations to, and their perceptions of an institutionalized decision-making system, which resembled a social scoring system. Using social scores as incentives, the system aimed at ensuring mutual fair treatment between members of experimental communities. We explore how the provision of transparency affected people’s perceptions, behaviors, as well as the well-being of the communities. While a non-transparent scoring system led to disparate impacts both within as well as across communities, transparency helped people develop trust in each other, create wealth, and enabled them to benefit from the system in a more uniform manner. A transparent system was perceived as more effective, procedurally just, and legitimate, and led people to rely more strongly on the system. However, transparency also made people strongly discipline those with a low score. This suggests that social scoring systems that precisely disclose past behaviors may also impose significant discriminatory consequences on individuals deemed non-compliant.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "891-904",
		"source": "ojs.aaai.org",
		"title": "Social Scoring Systems for Behavioral Regulation: An Experiment on the Role of Transparency in Determining Perceptions and Behaviors",
		"title-short": "Social Scoring Systems for Behavioral Regulation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31690",
		"volume": "7",
		"author": [
			{
				"family": "Loefflad",
				"given": "Carmen"
			},
			{
				"family": "Chen",
				"given": "Mo"
			},
			{
				"family": "Grossklags",
				"given": "Jens"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lovatoForegroundingArtistOpinions2024",
		"type": "article-journal",
		"abstract": "Generative AI tools are used to create art-like outputs and sometimes aid in the creative process. These tools have potential benefits for artists, but they also have the potential to harm the art workforce and infringe upon artistic and intellectual property rights. Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like outputs at scale. These outputs are now being used to compete with human artists in the marketplace as well as being used by some artists in their generative processes to create art. We surveyed 459 artists to investigate the tension between artists' opinions on Generative AI art's potential utility and harm. This study surveys artists' opinions on the utility and threat of Generative AI art models, fair practices in the disclosure of artistic works in AI art training models, ownership and rights of AI art derivatives, and fair compensation. Results show that a majority of artists believe creators should disclose what art is being used in AI training, that AI outputs should not belong to model creators, and express concerns about AI's impact on the art workforce and who profits from their art. We hope the results of this work will further meaningful collaboration and alignment between the art community and Generative AI researchers and developers.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "905-916",
		"source": "ojs.aaai.org",
		"title": "Foregrounding Artist Opinions: A Survey Study on Transparency, Ownership, and Fairness in AI Generative Art",
		"title-short": "Foregrounding Artist Opinions",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31691",
		"volume": "7",
		"author": [
			{
				"family": "Lovato",
				"given": "Juniper"
			},
			{
				"family": "Zimmerman",
				"given": "Julia Witte"
			},
			{
				"family": "Smith",
				"given": "Isabelle"
			},
			{
				"family": "Dodds",
				"given": "Peter"
			},
			{
				"family": "Karson",
				"given": "Jennifer L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "lunaNavigatingGovernanceParadigms2024",
		"type": "article-journal",
		"abstract": "As Generative Artificial Intelligence (GenAI) technologies\nevolve at an unprecedented rate, global governance approaches\nstruggle to keep pace with the technology, highlighting\na critical issue in the governance adaptation of significant\nchallenges. Depicting the nuances of nascent and\ndiverse governance approaches based on risks, rules, outcomes,\nprinciples, or a mix, across different regions around\nthe globe, is fundamental to discern discrepancies and convergences,\nand to shed light on specific limitations that need\nto be addressed, thereby facilitating the safe and trustworthy\nadoption of GenAI. In response to the need and the evolving\nnature of GenAI, this paper seeks to provide a collective\nview of different governance approaches around the world.\nOur research introduces a Harmonized GenAI Framework,\n“H-GenAIGF”, based on the current governance approaches\nof six regions: (European Union (EU), United States (US),\nChina (CN), Canada (CA), United Kingdom (UK), and Singapore\n(SG)). We have identified four constituents, fifteen\nprocesses, twenty-five sub-processes, and nine principles that\naid the governance of GenAI, thus providing a comprehensive\nperspective on the current state of GenAI governance. In\naddition, we present a comparative analysis to facilitate identification\nof common ground and distinctions based on coverage\nof the processes by each region. The results show that\nrisk-based approaches allow for better coverage of the processes,\nfollowed by mixed approaches. Other approaches lag\nbehind, covering less than 50% of the processes. Most prominently,\nthe analysis demonstrates that amongst the regions,\nonly one process aligns across all approaches, highlighting\nthe lack of consistent and executable provisions. Moreover,\nour case study on ChatGPT reveals process coverage deficiency,\nshowing that harmonization of approaches is necessary\nto find alignment for GenAI governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "917-931",
		"source": "ojs.aaai.org",
		"title": "Navigating Governance Paradigms: A Cross-Regional Comparative Study of Generative AI Governance Processes & Principles",
		"title-short": "Navigating Governance Paradigms",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31692",
		"volume": "7",
		"author": [
			{
				"family": "Luna",
				"given": "Jose"
			},
			{
				"family": "Tan",
				"given": "Ivan"
			},
			{
				"family": "Xie",
				"given": "Xiaofei"
			},
			{
				"family": "Jiang",
				"given": "Lingxiao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "maasParticipatoryAI2024",
		"type": "article-journal",
		"abstract": "The ‘participatory turn’ in AI design has received much attention in the literature. In this paper, we provide various arguments and proposals to move the discussion of participatory AI beyond its current state and towards stakeholder empowerment. The participatory AI literature points to Arnstein’s understanding of ‘citizen power’ as the right approach to participation. Although we agree with this general idea, we argue that there is a lack of depth in analyzing the legal, economic, and political arrangements required for a genuine redistribution of power to prioritize AI stakeholders. We highlight two domains on which the current discourse on participatory AI needs to expand. These are (1) the legal-institutional background that could provide ‘participation teeth’ for stakeholder empowerment and (2) the political economy of AI production that fosters such power asymmetries between AI developers and other stakeholders. We conclude by offering ways forward to explore alternative legal arrangements and ownership models for participatory AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "932-942",
		"source": "ojs.aaai.org",
		"title": "Beyond Participatory AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31693",
		"volume": "7",
		"author": [
			{
				"family": "Maas",
				"given": "Jonne"
			},
			{
				"family": "Inglés",
				"given": "Aarón Moreno"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "manziniCodeThatBinds2024",
		"type": "article-journal",
		"abstract": "The development of increasingly agentic and human-like AI assistants, capable of performing a wide range of tasks on user's behalf over time, has sparked heightened interest in the nature and bounds of human interactions with AI. Such systems may indeed ground a transition from task-oriented interactions with AI, at discrete time intervals, to ongoing relationships -- where users develop a deeper sense of connection with and attachment to the technology. This paper investigates what it means for relationships between users and advanced AI assistants to be appropriate and proposes a new framework to evaluate both users' relationships with AI and developers' design choices. We first provide an account of advanced AI assistants, motivating the question of appropriate relationships by exploring several distinctive features of this technology. These include anthropomorphic cues and the longevity of interactions with users, increased AI agency, generality and context ambiguity, and the forms and depth of dependence the relationship could engender. Drawing upon various ethical traditions, we then consider a series of values, including benefit, flourishing, autonomy and care, that characterise appropriate human interpersonal relationships. These values guide our analysis of how the distinctive features of AI assistants may give rise to inappropriate relationships with users. Specifically, we discuss a set of concrete risks arising from user--AI assistant relationships that: (1) cause direct emotional or physical harm to users, (2) limit opportunities for user personal development, (3) exploit user emotional dependence, and (4) generate material dependencies without adequate commitment to user needs. We conclude with a set of recommendations to address these risks.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "943-957",
		"source": "ojs.aaai.org",
		"title": "The Code That Binds Us: Navigating the Appropriateness of Human-AI Assistant Relationships",
		"title-short": "The Code That Binds Us",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31694",
		"volume": "7",
		"author": [
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Keeling",
				"given": "Geoff"
			},
			{
				"family": "Alberts",
				"given": "Lize"
			},
			{
				"family": "Vallor",
				"given": "Shannon"
			},
			{
				"family": "Morris",
				"given": "Meredith Ringel"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "menonLessonsClinicalCommunications2024",
		"type": "article-journal",
		"abstract": "One of the major challenges in the use of opaque, complex AI models is the need or desire to provide an explanation to the end-user (and other stakeholders) as to how the system arrived at the answer it did. While there is significant research in the development of explainability techniques for AI, the question remains as to who needs an explanation, what an explanation consists of, and how to communicate this to a lay user who lacks direct expertise in the area. In this position paper, an interdisciplinary team of researchers argue that the example of clinical communications offers lessons to those interested in improving the transparency and interpretability of AI systems. We identify five lessons from clinical communications: (1) offering explanations for AI systems and disclosure of their use recognizes the dignity of those using and impacted by it; (2) AI explanations can be productively targeted rather than totally comprehensive; (3) AI explanations can be enforced through codified rules but also norms, guided by core values; (4) what constitutes a “good” AI explanation will require repeated updating due to changes in technology and social expectations; 5) AI explanations will have impacts beyond defining any one AI system, shaping and being shaped by broader perceptions of AI. We review the history, debates and consequences surrounding the institutionalization of one type of clinical communication, informed consent, in order to illustrate the challenges and opportunities that may await attempts to offer explanations of opaque AI models. We highlight takeaways and implications for computer scientists and policymakers in the context of growing concerns and moves toward AI governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "958-970",
		"source": "ojs.aaai.org",
		"title": "Lessons from Clinical Communications for Explainable AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31695",
		"volume": "7",
		"author": [
			{
				"family": "Menon",
				"given": "Alka V."
			},
			{
				"family": "Omar",
				"given": "Zahra Abba"
			},
			{
				"family": "Nahar",
				"given": "Nadia"
			},
			{
				"family": "Papademetris",
				"given": "Xenophon"
			},
			{
				"family": "Fiellin",
				"given": "Lynn E."
			},
			{
				"family": "Kästner",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "michelPayAttentionCall2024",
		"type": "article-journal",
		"abstract": "Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "971-983",
		"source": "ojs.aaai.org",
		"title": "Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance",
		"title-short": "Pay Attention",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31696",
		"volume": "7",
		"author": [
			{
				"family": "Michel",
				"given": "Franck"
			},
			{
				"family": "Gandon",
				"given": "Fabien"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "muellerLLMsMemorizationQuality2024",
		"type": "article-journal",
		"abstract": "Memorization in large language models (LLMs) is a growing concern. LLMs have been shown to easily reproduce parts of their training data, including copyrighted work. This is an important problem to solve, as it may violate existing copyright laws as well as the European AI Act. In this work, we propose a systematic analysis to quantify the extent of potential copyright infringements in LLMs using European law as an example. Unlike previous work, we evaluate instruction-finetuned models in a realistic end-user scenario. Our analysis builds on a proposed threshold of 160 characters, which we borrow from the German Copyright Service Provider Act and a fuzzy text matching algorithm to identify potentially copyright-infringing textual reproductions. The specificity of countermeasures against copyright infringement is analyzed by comparing model behavior on copyrighted and public domain data. We investigate what behaviors models show instead of producing protected text (such as refusal or hallucination) and provide a first legal assessment of these behaviors. We find that there are huge differences in copyright compliance, specificity, and appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing a particularly low absolute number of potential copyright violations. Code can be found at github.com/felixbmuller/llms-memorization-copyright.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "984-996",
		"source": "ojs.aaai.org",
		"title": "LLMs and Memorization: On Quality and Specificity of Copyright Compliance",
		"title-short": "LLMs and Memorization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31697",
		"volume": "7",
		"author": [
			{
				"family": "Mueller",
				"given": "Felix B."
			},
			{
				"family": "Görge",
				"given": "Rebekka"
			},
			{
				"family": "Bernzen",
				"given": "Anna K."
			},
			{
				"family": "Pirk",
				"given": "Janna C."
			},
			{
				"family": "Poretschkin",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "munParticipAIDemocraticSurveying2024",
		"type": "article-journal",
		"abstract": "General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without a comprehensive assessment of risks. As a first step towards democratic risk assessment and design of general purpose AI, we introduce PARTICIP-AI, a carefully designed framework for laypeople to speculate and assess AI use cases and their impacts. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI devel- opment through making a concluding choice on its development. To showcase the promise of our framework towards informing democratic AI development, we run a medium-scale study with inputs from 295 demographically diverse participants. Our analyses show that participants’ responses emphasize applications for personal life and society, contrasting with most current AI development’s business focus. We also surface diverse set of envisioned harms such as distrust in AI and institutions, complementary to those defined by experts. Furthermore, we found that perceived impact of not developing use cases significantly predicted participants’ judgements of whether AI use cases should be developed, and highlighted lay users’ concerns of techno-solutionism. We conclude with a discussion on how frameworks like PARTICIP-AI can further guide democratic AI development and governance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "997-1010",
		"source": "ojs.aaai.org",
		"title": "Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits",
		"title-short": "Particip-AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31698",
		"volume": "7",
		"author": [
			{
				"family": "Mun",
				"given": "Jimin"
			},
			{
				"family": "Jiang",
				"given": "Liwei"
			},
			{
				"family": "Liang",
				"given": "Jenny"
			},
			{
				"family": "Cheong",
				"given": "Inyoung"
			},
			{
				"family": "DeCairo",
				"given": "Nicole"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			},
			{
				"family": "Sap",
				"given": "Maarten"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nakajimaQuantifyingGenderedCitation2024",
		"type": "article-journal",
		"abstract": "The number of citations received by papers often exhibits imbalances in terms of author attributes such as country of affiliation and gender. While recent studies have quantified citation imbalance in terms of the authors' gender in journal papers, the computer science discipline, where researchers frequently present their work at conferences, may exhibit unique patterns in gendered citation imbalance. Additionally, understanding how network properties in citations influence citation imbalances remains challenging due to a lack of suitable reference models. In this paper, we develop a family of reference models for citation networks and investigate gender imbalance in citations between papers published in computer science conferences. By deploying these reference models, we found that homophily in citations is strongly associated with gendered citation imbalance in computer science, whereas heterogeneity in the number of citations received per paper has a relatively minor association with it. Furthermore, we found that the gendered citation imbalance is most pronounced in papers published in the highest-ranked conferences, is present across different subfields, and extends to citation-based rankings of papers. Our study provides a framework for investigating associations between network properties and citation imbalances, aiming to enhance our understanding of the structure and dynamics of citations between research publications.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1011-1022",
		"source": "ojs.aaai.org",
		"title": "Quantifying Gendered Citation Imbalance in Computer Science Conferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31699",
		"volume": "7",
		"author": [
			{
				"family": "Nakajima",
				"given": "Kazuki"
			},
			{
				"family": "Sasaki",
				"given": "Yuya"
			},
			{
				"family": "Tokuno",
				"given": "Sohei"
			},
			{
				"family": "Fletcher",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nanniniHabemusRightExplanation2024",
		"type": "article-journal",
		"abstract": "The European Union's Artificial Intelligence Act (AI Act), finalized in February 2024, mandates comprehensive transparency and explainability requirements for AI systems to enable effective oversight and safeguard fundamental rights. However, the practical implementation of these requirements faces challenges due to tensions between the need for meaningful explanations and the potential risks to intellectual property and commercial interests of AI providers. This research proposes the Transparency-Explainability Functionality and Tensions (TEFT) framework to systematically analyze the complex interplay of legal, technical, and socio-ethical factors shaping the realization of algorithmic transparency and explainability in the EU context.\nThrough a two-pronged approach combining a focused literature review and an in-depth examination of the AI Act's provisions, we identify key friction points and challenges in operationalizing the right to explanation. The TEFT framework maps the interests and incentives of various stakeholders, including AI providers & deployers, oversight bodies, and affected individuals, while considering their goals, expected benefits, risks, possible negative impacts, and context to algorithmic explainability.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1023-1035",
		"source": "ojs.aaai.org",
		"title": "Habemus a Right to an Explanation: so What? – A Framework on Transparency-Explainability Functionality and Tensions in the EU AI Act",
		"title-short": "Habemus a Right to an Explanation",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31700",
		"volume": "7",
		"author": [
			{
				"family": "Nannini",
				"given": "Luca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nejadgholiHumanCenteredAIApplications2024",
		"type": "article-journal",
		"abstract": "While AI has been frequently applied in the context of immigration, most of these applications focus on selection and screening, which primarily serve to empower states and authorities, raising concerns due to their understudied reliability and high impact on immigrants' lives. In contrast, this paper emphasizes the potential of AI in Canada’s immigration settlement phase, a stage where access to information is crucial and service providers are overburdened. By highlighting the settlement sector as a prime candidate for reliable AI applications, we demonstrate its unique capacity to empower immigrants directly, yet it remains under-explored in AI research. We outline a vision for human-centred and responsible AI solutions that facilitate the integration of newcomers. We call on AI researchers to build upon our work and engage in multidisciplinary research and active collaboration with service providers and government organizations to develop tailored AI tools that are empowering, inclusive and safe.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1036-1050",
		"source": "ojs.aaai.org",
		"title": "Human-Centered AI Applications for Canada’s Immigration Settlement Sector",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31701",
		"volume": "7",
		"author": [
			{
				"family": "Nejadgholi",
				"given": "Isar"
			},
			{
				"family": "Molamohammadi",
				"given": "Maryam"
			},
			{
				"family": "Missaghi",
				"given": "Kimiya"
			},
			{
				"family": "Bakhtawar",
				"given": "Samir"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nematovAIDEAntitheticalIntentbased2024",
		"type": "article-journal",
		"abstract": "For many use-cases, it is often important to explain the prediction of a black-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's reasoning from different angles. \n\nIn this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explanations for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, investigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast.\nTo provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and increase coverage of the training data. \n\nWe demonstrate the effectiveness of AIDE on image and text classification tasks,\nin three ways: \nquantitatively, assessing correctness and continuity; \nqualitatively, comparing anecdotal evidence from AIDE and other example-based approaches;\nand via a user study, evaluating multiple aspects of AIDE.\nThe results show that AIDE addresses the limitations of existing methods and exhibits desirable traits for an explainability method.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1051-1062",
		"source": "ojs.aaai.org",
		"title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations",
		"title-short": "AIDE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31702",
		"volume": "7",
		"author": [
			{
				"family": "Nematov",
				"given": "Ikhtiyor"
			},
			{
				"family": "Sacharidis",
				"given": "Dimitris"
			},
			{
				"family": "Hose",
				"given": "Katja"
			},
			{
				"family": "Sagi",
				"given": "Tomer"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "norhashimMeasuringHumanAIValue2024",
		"type": "article-journal",
		"abstract": "This paper seeks to quantify the human-AI value alignment in large language models. Alignment between humans and AI has become a critical area of research to mitigate potential harm posed by AI. In tandem with this need, developers have incorporated a values-based approach towards model development where ethical principles are integrated from its inception. However, ensuring that these values are reflected in outputs remains a challenge. In addition, studies have noted that models lack consistency when producing outputs, which in turn can affect their function. Such variability in responses would impact human-AI value alignment as well, particularly where consistent alignment is critical. Fundamentally, the task of uncovering a model’s alignment is one of explainability – where understanding how these complex models behave is essential in order to assess their alignment. \n\nThis paper examines the problem through a case study of GPT-3.5. By repeatedly prompting the model with scenarios based on a dataset of moral stories, we aggregate the model’s alignment with human values to produce a human-AI value alignment metric. Moreover, by using a comprehensive taxonomy of human values, we uncover the latent value profile represented by these outputs, thereby determining the extent of human-AI value alignment.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1063-1073",
		"source": "ojs.aaai.org",
		"title": "Measuring Human-AI Value Alignment in Large Language Models",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31703",
		"volume": "7",
		"author": [
			{
				"family": "Norhashim",
				"given": "Hakim"
			},
			{
				"family": "Hahn",
				"given": "Jungpil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "nunesAreLargeLanguage2024",
		"type": "article-journal",
		"abstract": "Large language models (LLMs) have taken centre stage in debates on Artificial Intelligence. Yet there remains a gap in how to assess LLMs' conformity to important human values. In this paper, we investigate whether state-of-the-art LLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid results) are moral hypocrites. We employ two research instruments based on the Moral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which investigates which values are considered morally relevant in abstract moral judgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate moral cognition in concrete scenarios related to each moral foundation. We characterise conflicts in values between these different abstractions of moral evaluation as hypocrisy. We found that both models displayed reasonable consistency within each instrument compared to humans, but they displayed contradictory and hypocritical behaviour when we compared the abstract values present in the MFQ to the evaluation of concrete moral violations of the MFV.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1074-1087",
		"source": "ojs.aaai.org",
		"title": "Are Large Language Models Moral Hypocrites? A Study Based on Moral Foundations",
		"title-short": "Are Large Language Models Moral Hypocrites?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31704",
		"volume": "7",
		"author": [
			{
				"family": "Nunes",
				"given": "José Luiz"
			},
			{
				"family": "Almeida",
				"given": "Guilherme F. C. F."
			},
			{
				"family": "Araujo",
				"given": "Marcelo",
				"dropping-particle": "de"
			},
			{
				"family": "Barbosa",
				"given": "Simone D. J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "olulanaHiddenInferredFair2024",
		"type": "article-journal",
		"abstract": "As learning-to-rank models are increasingly deployed for decision-making in areas with profound life implications, the FairML community has been developing fair learning-to-rank (LTR) models. These models rely on the availability of sensitive demographic features such as race or sex. However, in practice, regulatory obstacles and privacy concerns protect this data from collection and use. As a result, practitioners may either need to promote fairness despite the absence of these features or turn to demographic inference tools to attempt to infer them. Given that these tools are fallible, this paper aims to further understand how errors in demographic inference impact the fairness performance of popular fair LTR strategies. In which cases would it be better to keep such demographic attributes hidden from models versus infer them? We examine a spectrum of fair LTR strategies ranging from fair LTR with and without demographic features hidden versus inferred to fairness-unaware LTR followed by fair re-ranking. We conduct a controlled empirical investigation modeling different levels of inference errors by systematically perturbing the inferred sensitive attribute. We also perform three case studies with real-world datasets and popular open-source inference methods. Our findings reveal that as inference noise grows, LTR-based methods that incorporate fairness considerations into the learning process may increase bias. In contrast, fair re-ranking strategies are more robust to inference errors. All source code, data, and experimental artifacts of our experimental study are available here: https://github.com/sewen007/hoiltr.git",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1088-1099",
		"source": "ojs.aaai.org",
		"title": "Hidden or Inferred: Fair Learning-To-Rank With Unknown Demographics",
		"title-short": "Hidden or Inferred",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31705",
		"volume": "7",
		"author": [
			{
				"family": "Olulana",
				"given": "Oluseun"
			},
			{
				"family": "Cachel",
				"given": "Kathleen"
			},
			{
				"family": "Murai",
				"given": "Fabricio"
			},
			{
				"family": "Rundensteiner",
				"given": "Elke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "oudahPerceptionExperienceInfluences2024",
		"type": "article-journal",
		"abstract": "It has been argued that human social and economic interactions depend on the perception of mind of the interacting partner. Minds are perceived along two dimensions: experience, i.e., the ability to feel, and agency, i.e., the ability to act and take responsibility for one’s actions. Here, we pair participants with bots in a dictator game (to measure altruism) and a trust game (to measure trust) while varying the bots’ perceived experience and agency.  Here, we pair participants with bots in a dictator game (to measure altruism) and a trust game (to measure trust) while varying the bots' perceived experience and agency. Results demonstrate that the perception of experience influences altruism, while the perception of agency influences trust.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1100-1100",
		"source": "ojs.aaai.org",
		"title": "Perception of Experience Influences Altruism and Perception of Agency Influences Trust in Human-Machine Interactions (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31706",
		"volume": "7",
		"author": [
			{
				"family": "Oudah",
				"given": "Mayada"
			},
			{
				"family": "Makovi",
				"given": "Kinga"
			},
			{
				"family": "Gray",
				"given": "Kurt"
			},
			{
				"family": "Battu",
				"given": "Balaraju"
			},
			{
				"family": "Rahwan",
				"given": "Talal"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "owensFaceFactsUsing2024",
		"type": "article-journal",
		"abstract": "We applied techniques from psychology --- typically used to visualize human bias --- to facial analysis systems, providing novel approaches for diagnosing and communicating algorithmic bias. First, we aggregated a diverse corpus of human facial images (N=1492) with self-identified gender and race. We tested four automated gender recognition (AGR) systems and found that some exhibited intersectional gender-by-race biases. Employing a technique developed by psychologists --- face averaging --- we created composite images to visualize these systems' outputs. For example, we visualized what an \"average woman\" looks like, according to a system's output. Second, we conducted two online experiments wherein participants judged the bias of hypothetical AGR systems. The first experiment involved participants (N=228) from a convenience sample. When depicting the same results in different formats, facial visualizations communicated bias to the same magnitude as statistics. In the second experiment with only Black participants (N=223), facial visualizations communicated bias significantly more than statistics, suggesting that face averages are meaningful for communicating algorithmic bias.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1101-1111",
		"source": "ojs.aaai.org",
		"title": "Face the Facts: Using Face Averaging to Visualize Gender-by-Race Bias in Facial Analysis Algorithms",
		"title-short": "Face the Facts",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31707",
		"volume": "7",
		"author": [
			{
				"family": "Owens",
				"given": "Kentrell"
			},
			{
				"family": "Freiburger",
				"given": "Erin"
			},
			{
				"family": "Hutchings",
				"given": "Ryan"
			},
			{
				"family": "Sim",
				"given": "Mattea"
			},
			{
				"family": "Hugenberg",
				"given": "Kurt"
			},
			{
				"family": "Roesner",
				"given": "Franziska"
			},
			{
				"family": "Kohno",
				"given": "Tadayoshi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "papageorgiouProxyFairnessEuropean2024",
		"type": "article-journal",
		"abstract": "This paper navigates the convergence of the European Data Protection Regulation and the AI Act within the paradigm of computational methods that operationalise fairness in the absence of demographic data, notably through the use of proxy variables and inferential techniques (Proxy Fairness). Particularly, it explores the legal nature of the data involved in Proxy Fairness under the European Data Protection Regulation, focusing on the legal notion of Sensitivity. Moreover, it examines the lawfulness of processing sensitive personal data for Proxy Fairness purposes under the AI Act, particularly focusing on the legal requirement of Necessity. Through this analysis, the paper aims to shed light on core aspects of the legitimacy of Proxy Fairness in the context of EU law, providing a normative foundation to this line of Fair-AI approaches.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1112-1122",
		"source": "ojs.aaai.org",
		"title": "Proxy Fairness under the European Data Protection Regulation and the AI Act: A Perspective of Sensitivity and Necessity",
		"title-short": "Proxy Fairness under the European Data Protection Regulation and the AI Act",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31708",
		"volume": "7",
		"author": [
			{
				"family": "Papageorgiou",
				"given": "Ioanna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "pinkavaModelDataAgnosticDebiasing2024",
		"type": "article-journal",
		"abstract": "As reliance on Machine Learning (ML) systems in real-world decision-making processes grows, ensuring these systems are free of bias against sensitive demographic groups is of increasing importance. Existing techniques for automatically debiasing ML models generally require access to either the models’ internal architectures, the models’ training datasets, or both. In this paper we outline the reasons why such requirements are disadvantageous, and present an alternative novel debiasing system that is both data- and model-agnostic. We implement this system as a Reinforcement Learning Agent and through extensive experiments show that we can debias a variety of target ML model architectures over three benchmark datasets. Our results show performance comparable to data- and/or model-gnostic state-of-the-art debiasers.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1123-1131",
		"source": "ojs.aaai.org",
		"title": "A Model- and Data-Agnostic Debiasing System for Achieving Equalized Odds",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31709",
		"volume": "7",
		"author": [
			{
				"family": "Pinkava",
				"given": "Thomas"
			},
			{
				"family": "McFarland",
				"given": "Jack"
			},
			{
				"family": "Mashhadi",
				"given": "Afra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "pistilliCIVICSBuildingDataset2024",
		"type": "article-journal",
		"abstract": "This paper introduces the \"CIVICS: Culturally-Informed \\& Values-Inclusive Corpus for Societal impacts\" dataset, designed to evaluate the social and cultural variation of Large Language Models (LLMs) towards socially sensitive topics across multiple languages and cultures. The hand-crafted, multilingual dataset of statements addresses value-laden topics, including LGBTQI rights, social welfare, immigration, disability rights, and surrogacy. CIVICS is designed to elicit responses from LLMs to shed light on how values encoded in their parameters shape their behaviors. Through our dynamic annotation processes, tailored prompt design, and experiments, we investigate how open-weight LLMs respond to these issues, exploring their behavior across diverse linguistic and cultural contexts.\nUsing two experimental set-ups based on log-probabilities and long-form responses, we show social and cultural variability across different LLMs. Specifically, different topics and sources lead to more pronounced differences across model answers, particularly on immigration, LGBTQI rights, and social welfare. Experiments on generating long-form responses from models tuned for user chat demonstrate that refusals are triggered disparately across different models, but consistently and more frequently in English or translated statements. As shown by our initial experimentation, the CIVICS dataset can serve as a tool for future research, promoting reproducibility and transparency across broader linguistic settings, and furthering the development of AI technologies that respect and reflect global cultural diversities and value pluralism. \nThe CIVICS dataset and tools are made available under open licenses at hf.co/CIVICS-dataset.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1132-1144",
		"source": "ojs.aaai.org",
		"title": "CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models",
		"title-short": "CIVICS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31710",
		"volume": "7",
		"author": [
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Leidinger",
				"given": "Alina"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Kasirzadeh",
				"given": "Atoosa"
			},
			{
				"family": "Luccioni",
				"given": "Alexandra Sasha"
			},
			{
				"family": "Mitchell",
				"given": "Margaret"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "poirouxDisengagementAlgorithmsHow2024",
		"type": "article-journal",
		"abstract": "This study examines the use of algorithmic tools in traditional organizational decision-making processes. Through forty semi-structured interviews with managers, engineers, and (expert) users across six European projects, we suggest that initiators deploy algorithms not to automate actions or replace users, but to disengage themselves from prescriptive decision-making. Consequently, the responsibility to choose, select, and decide falls upon the users; they become engaged. Therefore, algorithm evaluation is oriented towards utility, interpretability, and, more broadly, user satisfaction. Further research is encouraged to analyze the advent of a 'satisfaction regime', from platforms to traditional organizations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1145-1156",
		"source": "ojs.aaai.org",
		"title": "Disengagement through Algorithms: How Traditional Organizations Aim for Experts' Satisfaction",
		"title-short": "Disengagement through Algorithms",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31711",
		"volume": "7",
		"author": [
			{
				"family": "Poiroux",
				"given": "Jérémie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "probascoNotOraclesBattlefield2024",
		"type": "article-journal",
		"abstract": "AI-based military decision support systems that help commanders observe, orient, decide, and act on the battlefield are highly sought after by military leadership. With the advent of large language models, AI developers have begun advertising automated AI-based decision support systems designed to both analyze and act on data from the battlefield. While the desire to use decision support systems to make better decisions on the battlefield is unsurprising, the responsible deployment of such systems requires a clear understanding of the capabilities and limitations of modern machine learning models. This paper reviews recently proposed uses of AI-enables decision support systems (DSS), provides a simplified framework for considering AI-DSS capabilities and limitations, and recommends practical risk mitigations commanders might employ when operating with an AI-enabled DSS.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1157-1165",
		"source": "ojs.aaai.org",
		"title": "Not Oracles of the Battlefield: Safety Considerations for AI-Based Military Decision Support Systems",
		"title-short": "Not Oracles of the Battlefield",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31712",
		"volume": "7",
		"author": [
			{
				"family": "Probasco",
				"given": "Emelia"
			},
			{
				"family": "Burtell",
				"given": "Matthew"
			},
			{
				"family": "Toner",
				"given": "Helen"
			},
			{
				"family": "Rudner",
				"given": "Tim G. J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "purvesWhatTrustWhen2024",
		"type": "article-journal",
		"abstract": "What to Trust When We Trust Artificial Intelligence\n\nAbstract:\nSo-called “trustworthy AI” has emerged as a guiding aim of industry leaders, computer and data science researchers, and policy makers in the US and Europe. Often, trustworthy AI is characterized in terms of a list of criteria. These lists usually include at least fairness, accountability, and transparency. Fairness, accountability, and transparency are valuable objectives, and they have begun to receive attention from philosophers and legal scholars. However, those who put forth criteria for trustworthy AI have failed to explain why satisfying the criteria makes an AI system—or the organizations that make use of the AI system—worthy of trust. Nor do they explain why the aim of trustworthy AI is important enough to justify devoting resources to achieve it. It even remains unclear whether an AI system is the sort of thing that can be trustworthy or not.\n\nTo explain why fairness, accountability, and transparency are suitable criteria for trustworthy AI one needs an analysis of trustworthy AI. Providing an analysis of trustworthy AI is a distinct task from providing criteria. Criteria are diagnostic; they provide a useful test for the phenomenon of interest, but they do not purport to explain the nature of the phenomenon. It is conceivable that an AI system could lack transparency, accountability, or fairness while remaining trustworthy. An analysis of trustworthy AI provides the fundamental features of an AI system in virtue of which it is (or is not) worthy of trust. An AI system that lacks these features will, necessarily, fail to be worthy of trust. This paper puts forward an analysis of trustworthy AI that can be used to critically evaluate criteria for trustworthy AI such as fairness, accountability, and transparency. \n\nIn this paper we first make clear the target concept to be analyzed: trustworthy AI. We argue that AI, at least in its current form, should be understood as a distributed, complex system embedded in a larger institutional context. This characterization of AI is consistent with recent definitions proposed by national and international regulatory bodies, and it eliminates some unhappy ambiguity in the common usage of the term. We further limit the scope of our discussion to AI systems which are used to inform decision-making about qualification problems, problems wherein a decision-maker must decide whether an individual is qualified for some beneficial or harmful treatment. We argue that, given reasonable assumptions about the nature of trust and trustworthiness, only AI systems that are used to inform decision-making about qualification problems are appropriate candidates for attributions of (un)trustworthiness.\n\nWe then distinguish between two models of trust and trustworthiness that we find in the existing literature. We motivate our account by highlighting this as a dilemma in in the accounts of trustworthy AI that have previously been offered. These accounts claim that trustworthiness is either exclusive to full agents (and it is thus nonsense when we talk of trustworthy AI), or they offer an account of trustworthiness that collapses into mere reliability. The first sort of account we refer to as an agential account and the second sort we refer to as a reliability account. We offer that one of the core challenges of putting forth an account of trustworthy AI is to avoid reducing to one of these two camps. It is thus a desideratum of our account that it avoids being exclusive to full moral agents, while it simultaneously avoids capturing things such as mere tools. We go on to propose our positive account which we submit avoids these twin pitfalls.\n\nWe subsequently argue that if AI can be trustworthy, then it will be trustworthy on an institutional model. Starting from an account of institutional trust offered by Purves and Davis, we argue that trustworthy AI systems have three features: they are competent with regard to the task they are assigned, they are responsive to the morally salient facts governing the decision-making context in which they are deployed, and they publicly provide evidence of these features. As noted, this account builds on a model of institutional trust offered by Purves and Davis and an account of default trust from Margaret Urban Walker. The resulting account allows us to accommodate the core challenge of finding a balance between agential accounts and reliability accounts. We go on to refine our account, answer objections, and revisit the list criteria from above as explained in terms of competence, responsiveness, and evidence.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1166-1166",
		"source": "ojs.aaai.org",
		"title": "What to Trust When We Trust Artificial Intelligence (Extended Abstract)",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31713",
		"volume": "7",
		"author": [
			{
				"family": "Purves",
				"given": "Duncan"
			},
			{
				"family": "Sturm",
				"given": "Schuyler"
			},
			{
				"family": "Madock",
				"given": "John"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "qianPPSPersonalizedPolicy2024",
		"type": "article-journal",
		"abstract": "AI-enabled agents designed to assist humans are gaining traction in a variety of domains such as healthcare and disaster response. It is evident that, as we move forward, these agents will play increasingly vital roles in our lives. To realize this future successfully and mitigate its unintended consequences, it is imperative that humans have a clear understanding of the agents that they work with. Policy summarization methods help facilitate this understanding by showcasing key examples of agent behaviors to their human users. Yet, existing methods produce “one-size-fits-all” summaries for a generic audience ahead of time. Drawing inspiration from research in pedagogy, we posit that personalized policy summaries can more effectively enhance user understanding. To evaluate this hypothesis, this paper presents and benchmarks a novel technique: Personalized Policy Summarization (PPS). PPS discerns a user’s mental model of the agent through a series of algorithmically generated questions and crafts customized policy summaries to enhance user understanding. Unlike existing methods, PPS actively engages with users to gauge their comprehension of the agent behavior, subsequently generating tailored explanations on the fly. Through a combination of numerical and human subject experiments, we confirm the utility of this personalized approach to explainable AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1167-1179",
		"source": "ojs.aaai.org",
		"title": "PPS: Personalized Policy Summarization for Explaining Sequential Behavior of Autonomous Agents",
		"title-short": "PPS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31714",
		"volume": "7",
		"author": [
			{
				"family": "Qian",
				"given": "Peizhu"
			},
			{
				"family": "Huang",
				"given": "Harrison"
			},
			{
				"family": "Unhelkar",
				"given": "Vaibhav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rajBreakingBiasBuilding2024",
		"type": "article-journal",
		"abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis, a concept from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure their influence on the model’s biases, mirroring how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (LLaMA 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction-tunes these models with unbiased responses to prompts. Our research demonstrates that LLM responses exhibit social biases when subject to contact probing, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning LLaMA 2 following our SCD strategy.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1180-1189",
		"source": "ojs.aaai.org",
		"title": "Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis",
		"title-short": "Breaking Bias, Building Bridges",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31715",
		"volume": "7",
		"author": [
			{
				"family": "Raj",
				"given": "Chahat"
			},
			{
				"family": "Mukherjee",
				"given": "Anjishnu"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			},
			{
				"family": "Anastasopoulos",
				"given": "Antonios"
			},
			{
				"family": "Zhu",
				"given": "Ziwei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rathjeLearningWhenNot2024",
		"type": "article-journal",
		"abstract": "LLMs and other forms of generative AI have shown immense promise in producing highly accurate epistemic judgements in domains as varied as law, education, and medicine – with GPT notably passing the legal Bar exam and various medical licensing exams. The safe extension of LLMs into safety-critical professional domains requires assurance not only of epistemic but ethical alignment. This paper adopts a theoretical and philosophical approach, drawing from metaethical theories to argue for a distinction hinging around quantitative, axiological comparability that separates Kantian ethics from not only the utilitarianism it is well-known to oppose, but from just distribution theories as well, which are key to debiasing LLM models. It presents the novel hypothesis that LLM ethical acquisition from both corpus induction and RLHF may encounter value conflicts between Kantian and just distribution principles that intensify as they come into improved alignment with both theories, hinging around the variability by which self-attention may statistically attend to the same characterizations as more person-like or more resource-like under distinct prompting strategies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1190-1199",
		"source": "ojs.aaai.org",
		"title": "Learning When Not to Measure: Theorizing Ethical Alignment in LLMs",
		"title-short": "Learning When Not to Measure",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31716",
		"volume": "7",
		"author": [
			{
				"family": "Rathje",
				"given": "William"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rauhGapsSafetyEvaluation2024",
		"type": "article-journal",
		"abstract": "Generative AI systems produce a range of ethical and social risks. Evaluation of these risks is a critical step on the path to ensuring the safety of these systems. However, evaluation requires the availability of validated and established measurement approaches and tools. In this paper, we provide an empirical review of the methods and tools that are available for evaluating known safety of generative AI systems to date. To this end, we review more than 200 safety-related evaluations that have been applied to generative AI systems. We categorise each evaluation along multiple axes to create a detailed snapshot of the safety evaluation landscape to date. We release this data for researchers and AI safety practitioners (https://bitly.ws/3hUzu). Analysing the current safety evaluation landscape reveals three systemic ”evaluation gaps”. First, a ”modality gap” emerges as few safety evaluations exist for non-text modalities. Second, a ”risk coverage gap” arises as evaluations for several ethical and social risks are simply lacking. Third, a ”context gap” arises as most safety evaluations are model-centric and fail to take into account the broader context in which AI systems operate. Devising next steps for safety practitioners based on these findings, we present tactical ”low-hanging fruit” steps towards closing the identified evaluation gaps and their limitations. We close by discussing the role and limitations of safety evaluation to ensure the safety of generative AI systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1200-1217",
		"source": "ojs.aaai.org",
		"title": "Gaps in the Safety Evaluation of Generative AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31717",
		"volume": "7",
		"author": [
			{
				"family": "Rauh",
				"given": "Maribeth"
			},
			{
				"family": "Marchal",
				"given": "Nahema"
			},
			{
				"family": "Manzini",
				"given": "Arianna"
			},
			{
				"family": "Hendricks",
				"given": "Lisa Anne"
			},
			{
				"family": "Comanescu",
				"given": "Ramona"
			},
			{
				"family": "Akbulut",
				"given": "Canfer"
			},
			{
				"family": "Stepleton",
				"given": "Tom"
			},
			{
				"family": "Mateos-Garcia",
				"given": "Juan"
			},
			{
				"family": "Bergman",
				"given": "Stevie"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Griffin",
				"given": "Conor"
			},
			{
				"family": "Bariach",
				"given": "Ben"
			},
			{
				"family": "Gabriel",
				"given": "Iason"
			},
			{
				"family": "Rieser",
				"given": "Verena"
			},
			{
				"family": "Isaac",
				"given": "William"
			},
			{
				"family": "Weidinger",
				"given": "Laura"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "reuelFairnessReinforcementLearning2024",
		"type": "article-journal",
		"abstract": "While our understanding of fairness in machine learning has significantly progressed, our understanding of fairness in reinforcement learning (RL) remains nascent. Most of the attention has been on fairness in one-shot classification tasks; however, real-world, RL-enabled systems (e.g., autonomous vehicles) are much more complicated in that agents operate in dynamic environments over a long period of time. To ensure the responsible development and deployment of these systems, we must better understand fairness in RL. In this paper, we survey the literature to provide the most up-to-date snapshot of the frontiers of fairness in RL. We start by reviewing where fairness considerations can arise in RL, then discuss the various definitions of fairness in RL that have been put forth thus far. We continue to highlight the methodologies researchers used to implement fairness in single- and multi-agent RL systems and showcase the distinct application domains that fair RL has been investigated in. Finally, we critically examine gaps in the literature, such as understanding fairness in the context of RLHF, that still need to be addressed in future work to truly operationalize fair RL in real-world systems.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1218-1230",
		"source": "ojs.aaai.org",
		"title": "Fairness in Reinforcement Learning: A Survey",
		"title-short": "Fairness in Reinforcement Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31718",
		"volume": "7",
		"author": [
			{
				"family": "Reuel",
				"given": "Anka"
			},
			{
				"family": "Ma",
				"given": "Devin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "robertsonHumanLoopFairnessAwareModel2024",
		"type": "article-journal",
		"abstract": "Fairness-aware Machine Learning (FairML) applications are often characterized by complex social objectives and legal requirements, frequently involving multiple, potentially conflicting notions of fairness. Despite the well-known Impossibility Theorem of Fairness and extensive theoretical research on the statistical and socio-technical trade-offs between fairness metrics, many FairML tools still optimize or constrain for a single fairness objective. However, this one-sided optimization can inadvertently lead to violations of other relevant notions of fairness. In this socio-technical and empirical study, we frame fairness as a Many-Objective (MaO) problem by treating fairness metrics as conflicting objectives in a multi-objective (MO) sense. We introduce ManyFairHPO, a human-in-the-loop, fairness-aware model selection framework that enables practitioners to effectively navigate complex and nuanced fairness objective landscapes. ManyFairHPO aids in the identification, evaluation, and balancing of fairness metric conflicts and their related social consequences, leading to more informed and socially responsible model-selection decisions. Through a comprehensive empirical evaluation and a case study on the Law School Admissions problem, we demonstrate the effectiveness of ManyFairHPO in balancing multiple fairness objectives, mitigating risks such as self-fulfilling prophecies, and providing interpretable insights to guide stakeholders in making fairness-aware modeling decisions.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1231-1242",
		"source": "ojs.aaai.org",
		"title": "A Human-in-the-Loop Fairness-Aware Model Selection Framework for Complex Fairness Objective Landscapes",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31719",
		"volume": "7",
		"author": [
			{
				"family": "Robertson",
				"given": "Jake"
			},
			{
				"family": "Schmidt",
				"given": "Thorsten"
			},
			{
				"family": "Hutter",
				"given": "Frank"
			},
			{
				"family": "Awad",
				"given": "Noor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "roccaIntroducingELLIPSEthicsCentered2024",
		"type": "article-journal",
		"abstract": "As mental health care systems worldwide struggle to meet demand, there is increasing focus on using language models (LM) to infer neuropsychiatric conditions or psychopathological traits from language production. Yet, so far, this research has only delivered solutions with limited clinical applicability, due to insufficient consideration of ethical questions crucial to ensuring the synergy between possible applications and model design.\nTo accelerate progress towards clinically applicable models, our paper charts the ethical landscape of research on language-based inference of psychopathology and provides a practical tool for researchers to navigate it. We identify seven core ethical principles that should guide model development and deployment in this domain, translate them into ELLIPS, an ethical toolkit operationalizing these principles into questions that can guide researchers' choices with respect to data selection, architectures, evaluation, and model deployment, and provide a case study exemplifying its use. With this, we aim to facilitate the emergence of model technology with concrete potential for real-world applicability.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1243-1254",
		"source": "ojs.aaai.org",
		"title": "Introducing ELLIPS: An Ethics-Centered Approach to Research on LLM-Based Inference of Psychiatric Conditions",
		"title-short": "Introducing ELLIPS",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31720",
		"volume": "7",
		"author": [
			{
				"family": "Rocca",
				"given": "Roberta"
			},
			{
				"family": "Pistilli",
				"given": "Giada"
			},
			{
				"family": "Maheshwari",
				"given": "Kritika"
			},
			{
				"family": "Fusaroli",
				"given": "Riccardo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "rothschildProblemsProxiesMaking2024",
		"type": "article-journal",
		"abstract": "Fairness in AI and ML systems is increasingly linked to the proper treatment and recognition of data workers involved in training dataset development. Yet, those who collect and annotate the data, and thus have the most intimate knowledge of its development, are often excluded from critical discussions. This exclusion prevents data annotators, who are domain experts, from contributing effectively to dataset contextualization. Our investigation into the hiring and engagement practices of 52 data work requesters on platforms like Amazon Mechanical Turk reveals a gap: requesters frequently hold naive or unchallenged notions of worker identities and capabilities and rely on ad-hoc qualification tasks that fail to respect the workers’ expertise. These practices not only undermine the quality of data but also the ethical standards of AI development. To rectify these issues, we advocate for policy changes to enhance how data annotation tasks are designed and managed and to ensure data workers are treated with the respect they deserve.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1255-1268",
		"source": "ojs.aaai.org",
		"title": "The Problems with Proxies: Making Data Work Visible through Requester Practices",
		"title-short": "The Problems with Proxies",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31721",
		"volume": "7",
		"author": [
			{
				"family": "Rothschild",
				"given": "Annabel"
			},
			{
				"family": "Wang",
				"given": "Ding"
			},
			{
				"family": "Vilvanathan",
				"given": "Niveditha Jayakumar"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			},
			{
				"family": "DiSalvo",
				"given": "Carl"
			},
			{
				"family": "DiSalvo",
				"given": "Betsy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "salavatiReducingBiasesMinoritized2024",
		"type": "article-journal",
		"abstract": "Biased information (recently termed bisinformation) continues to be taught in medical curricula, often long after having been debunked. In this paper, we introduce bricc, a first-in-class initiative that seeks to mitigate medical bisinformation using machine learning to systematically identify and flag text with potential biases, for subsequent review in an expert-in-the-loop fashion, thus greatly accelerating an otherwise labor-intensive process. We have developed a gold-standard bricc dataset throughout several years containing over 12K pages of instructional materials.  Medical experts meticulously annotated these documents for bias according to comprehensive coding guidelines, emphasizing gender, sex, age, geography, ethnicity, and race. Using this labeled dataset, we trained, validated, and tested medical bias classifiers. We test three classifier approaches: a binary type-specific classifier, a general bias classifier; an ensemble combining bias type-specific classifiers independently-trained; and a multi-task learning (MTL) model tasked with predicting both general and type-specific biases. While MTL led to some improvement on race bias detection in terms of F1-score, it did not outperform binary classifiers trained specifically on each task.\nOn general bias detection, the binary classifier achieves up to 0.923 of AUC, a 27.8% improvement over the baseline.\nThis work lays the foundations for debiasing medical curricula by exploring a novel dataset and evaluating different training model strategies. Hence, it offers new pathways for more nuanced and effective mitigation of bisinformation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1269-1280",
		"source": "ojs.aaai.org",
		"title": "Reducing Biases towards Minoritized Populations in Medical Curricular Content via Artificial Intelligence for Fairer Health Outcomes",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31722",
		"volume": "7",
		"author": [
			{
				"family": "Salavati",
				"given": "Chiman"
			},
			{
				"family": "Song",
				"given": "Shannon"
			},
			{
				"family": "Diaz",
				"given": "Willmar Sosa"
			},
			{
				"family": "Hale",
				"given": "Scott A."
			},
			{
				"family": "Montenegro",
				"given": "Roberto E."
			},
			{
				"family": "Murai",
				"given": "Fabricio"
			},
			{
				"family": "Dori-Hacohen",
				"given": "Shiri"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "sangaryaEstimatingEnvironmentalCost2024",
		"type": "article-journal",
		"abstract": "With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to changes in the environment of model deployment or variations/changes in the input data. In this paper, we propose PreIndex, a predictive index to estimate the environmental and compute resources associated with model retraining to distributional shifts in data. PreIndex can be used to estimate environmental costs such as carbon emissions and energy usage when retraining from current data distribution to new data distribution. It also correlates with and can be used to estimate other resource indicators associated with deep learning, such as epochs, gradient norm, and magnitude of model parameter change. PreIndex requires only one forward pass of the data, following which it provides a single concise value to estimate resources associated with retraining to the new distribution shifted data. We show that PreIndex can be reliably used across various datasets, model architectures, different types, and intensities of distribution shifts. Thus, PreIndex enables users to make informed decisions for retraining to different distribution shifts and determine the most cost-effective and sustainable option, allowing for the reuse of a model with a much smaller footprint in the environment. The code for this work is available here: \nhttps://github.com/JEKimLab/AIES2024PreIndex",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1281-1291",
		"source": "ojs.aaai.org",
		"title": "Estimating Environmental Cost Throughout Model’s Adaptive Life Cycle",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31723",
		"volume": "7",
		"author": [
			{
				"family": "Sangarya",
				"given": "Vishwesh"
			},
			{
				"family": "Bradford",
				"given": "Richard"
			},
			{
				"family": "Kim",
				"given": "Jung-Eun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "scariaAlgorithmsRecidivismMultidisciplinary2024",
		"type": "article-journal",
		"abstract": "The adoption of algorithms across different jurisdictions have transformed the workings of the criminal justice system, particularly in predicting recidivism risk for bail, sentencing, and parole decisions. This shift from human decision-making to statistical or algorithmic tool-assisted decision-making has prompted discussions regarding the legitimacy of such adoption. Our paper presents the results of a systematic review of the literature on criminal recidivism, spanning both legal and empirical perspectives. By coalescing different approaches, we highlight the most prominent themes that have garnered the attention of researchers so far and some that warrant further investigation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1292-1305",
		"source": "ojs.aaai.org",
		"title": "Algorithms and Recidivism: A Multi-disciplinary Systematic Review",
		"title-short": "Algorithms and Recidivism",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31724",
		"volume": "7",
		"author": [
			{
				"family": "Scaria",
				"given": "Arul George"
			},
			{
				"family": "Subramanian",
				"given": "Vidya"
			},
			{
				"family": "George",
				"given": "Nevin K."
			},
			{
				"family": "Sengupta",
				"given": "Nandana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "borgWhatRequiredEmpathic2024",
		"type": "article-journal",
		"abstract": "Interest is growing in artificial empathy, but so is confusion about what artificial empathy is or needs to be. This confusion makes it challenging to navigate the technical and ethical issues that accompany empathic AI development. Here, we outline a framework for thinking about empathic AI based on the premise that different constellations of capabilities associated with empathy are important for different empathic AI applications. We describe distinctions of capabilities that we argue belong under the empathy umbrella, and show how three medical empathic AI use cases require different sets of these capabilities. We conclude by discussing why appreciation of the diverse capabilities under the empathy umbrella is important for both AI creators and users.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1306-1318",
		"source": "ojs.aaai.org",
		"title": "What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users",
		"title-short": "What Is Required for Empathic AI?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31725",
		"volume": "7",
		"author": [
			{
				"family": "Borg",
				"given": "Jana Schaich"
			},
			{
				"family": "Read",
				"given": "Hannah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "schmer-galunderAnnotatorLoopCase2024",
		"type": "article-journal",
		"abstract": "With the growing prevalence of large language models, it is increasingly common to annotate datasets for machine learning using pools of crowd raters. However, these raters often work in isolation as individual crowdworkers. In this work, we regard annotation not merely as inexpensive, scalable labor, but rather as a nuanced interpretative effort to discern the meaning of what is being said in a text. We describe a novel, collaborative, and iterative annotator-in-the-loop methodology for annotation, resulting in a 'Bridging Benchmark Dataset' of comments relevant to bridging divides, annotated from 11,973 textual posts in the Civil Comments dataset. The methodology differs from popular anonymous crowd-rating annotation processes due to its use of an in-depth, iterative engagement with seven US-based raters to (1) collaboratively refine the definitions of the to-be-annotated concepts and then (2) iteratively annotate complex social concepts, with check-in meetings and discussions. This approach addresses some shortcomings of current anonymous crowd-based annotation work, and we present empirical evidence of the performance of our annotation process in the form of inter-rater reliability. Our findings indicate that collaborative engagement with annotators can enhance annotation methods, as opposed to relying solely on isolated work conducted remotely. We provide an overview of the input texts, attributes, and annotation process, along with the empirical results and the resulting benchmark dataset, categorized according to the following attributes: Alienation, Compassion, Reasoning, Curiosity, Moral Outrage, and Respect.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1319-1328",
		"source": "ojs.aaai.org",
		"title": "Annotator in the Loop: A Case Study of In-Depth Rater Engagement to Create a Prosocial Benchmark Dataset",
		"title-short": "Annotator in the Loop",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31726",
		"volume": "7",
		"author": [
			{
				"family": "Schmer-Galunder",
				"given": "Sonja"
			},
			{
				"family": "Wheelock",
				"given": "Ruta"
			},
			{
				"family": "Jalan",
				"given": "Zaria"
			},
			{
				"family": "Chvasta",
				"given": "Alyssa"
			},
			{
				"family": "Friedman",
				"given": "Scott"
			},
			{
				"family": "Saltz",
				"given": "Emily"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "septiandriImpactResponsibleAI2024",
		"type": "article-journal",
		"abstract": "Translational research, especially in the fast-evolving field of Artificial Intelligence (AI), is key to converting scientific findings into practical innovations. In Responsible AI (RAI) research, translational impact is often viewed through various pathways, including research papers, blogs, news articles, and the drafting of forthcoming AI legislation (e.g., the EU AI Act). However, the real-world impact of RAI research remains an underexplored area. Our study aims to capture it through two pathways: patents and code repositories, both of which provide a rich and structured source of data. Using a dataset of 200,000 papers from 1980 to 2022 in AI and related fields, including Computer Vision, Natural Language Processing, and Human-Computer Interaction, we developed a Sentence-Transformers Deep Learning framework to identify RAI papers. This framework calculates the semantic similarity between paper abstracts and a set of RAI keywords, which are derived from the NIST's AI Risk Management Framework; a framework that aims to enhance trustworthiness considerations in the design, development, use, and evaluation of AI products, services, and systems. We identified 1,747 RAI papers published in top venues such as CHI, CSCW, NeurIPS, FAccT, and AIES between 2015 and 2022. By analyzing these papers, we found that a small subset that goes into patents or repositories is highly cited, with the translational process taking between 1 year for repositories and up to 8 years for patents. Interestingly, impactful RAI research is not limited to top U.S. institutions, but significant contributions come from European and Asian institutions. Finally, the multidisciplinary nature of RAI papers, often incorporating knowledge from diverse fields of expertise, was evident as these papers tend to build on unconventional combinations of prior knowledge.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1329-1342",
		"source": "ojs.aaai.org",
		"title": "The Impact of Responsible AI Research on Innovation and Development",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31727",
		"volume": "7",
		"author": [
			{
				"family": "Septiandri",
				"given": "Ali Akbar"
			},
			{
				"family": "Constantinides",
				"given": "Marios"
			},
			{
				"family": "Quercia",
				"given": "Daniele"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shangTrustingYourAI2024",
		"type": "article-journal",
		"abstract": "Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLM-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1343-1356",
		"source": "ojs.aaai.org",
		"title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust",
		"title-short": "Trusting Your AI Agent Emotionally and Cognitively",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31728",
		"volume": "7",
		"author": [
			{
				"family": "Shang",
				"given": "Ruoxi"
			},
			{
				"family": "Hsieh",
				"given": "Gary"
			},
			{
				"family": "Shah",
				"given": "Chirag"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shastriAutomatingTransparencyMechanisms2024",
		"type": "article-journal",
		"abstract": "Bringing more transparency to the judicial system for the purposes of increasing accountability often demands extensive effort from auditors who must meticulously sift through numerous disorganized legal case files to detect patterns of bias and errors. For example, the high-profile investigation into the Curtis Flowers case took seven reporters a full year to assemble evidence about the prosecutor's history of selecting racially biased juries. LLMs have the potential to automate and scale these transparency pipelines, especially given their demonstrated capabilities to extract information from unstructured documents. We discuss the opportunities and challenges of using LLMs to provide transparency in two important court processes: jury selection in criminal trials and housing eviction cases.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1357-1367",
		"source": "ojs.aaai.org",
		"title": "Automating Transparency Mechanisms in the Judicial System Using LLMs: Opportunities and Challenges",
		"title-short": "Automating Transparency Mechanisms in the Judicial System Using LLMs",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31729",
		"volume": "7",
		"author": [
			{
				"family": "Shastri",
				"given": "Ishana"
			},
			{
				"family": "Jain",
				"given": "Shomik"
			},
			{
				"family": "Engelhardt",
				"given": "Barbara"
			},
			{
				"family": "Wilson",
				"given": "Ashia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "shea-blymyerFormalEthicalObligations2024",
		"type": "article-journal",
		"abstract": "When designing agents for operation in uncertain environments, designers need tools to automatically reason about what agents ought to do, how that conflicts with what is actually happening, and how a policy might be modified to remove the conflict.\nThese obligations include ethical and social obligations, permissions and prohibitions, which constrain how the agent achieves its mission and executes its policy.\nWe propose a new deontic logic, Expected Act Utilitarian deontic logic, for enabling this reasoning at design time: for specifying and verifying the agent's strategic obligations, then modifying its policy from a reference policy to meet those obligations.\nUnlike approaches that work at the reward level, working at the logical level increases the transparency of the trade-offs.\nWe introduce two algorithms: one for model-checking whether an RL agent has the right strategic obligations, and one for modifying a reference decision policy to make it meet obligations expressed in our logic.\nWe illustrate our algorithms on DAC-MDPs which accurately abstract neural decision policies, and on toy gridworld environments.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1368-1378",
		"source": "ojs.aaai.org",
		"title": "Formal Ethical Obligations in Reinforcement Learning Agents: Verification and Policy Updates",
		"title-short": "Formal Ethical Obligations in Reinforcement Learning Agents",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31730",
		"volume": "7",
		"author": [
			{
				"family": "Shea-Blymyer",
				"given": "Colin"
			},
			{
				"family": "Abbas",
				"given": "Houssam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "siumIndividualFairnessGraphs2024",
		"type": "article-journal",
		"abstract": "Graph neural networks are powerful graph representation learners in which node representations are highly influenced by features of neighboring nodes. Prior work on individual fairness in graphs has focused only on node features rather than structural issues. However, from the perspective of fairness in high-stakes applications, structural fairness is also important, and the learned representations may be systematically and undesirably biased against unprivileged individuals due to a lack of structural awareness in the learning process. In this work, we propose a pre-processing bias mitigation approach for individual fairness that gives importance to local and global structural features. We mitigate the local structure discrepancy of the graph embedding via a locally fair PageRank method. We address the global structure disproportion between pairs of nodes by introducing truncated singular value decomposition-based pairwise node similarities. Empirically, the proposed pre-processed fair structural features have superior performance in individual fairness metrics compared to the state-of-the-art methods while maintaining prediction performance.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1379-1389",
		"source": "ojs.aaai.org",
		"title": "Individual Fairness in Graphs Using Local and Global Structural Information",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31731",
		"volume": "7",
		"author": [
			{
				"family": "Sium",
				"given": "Yonas"
			},
			{
				"family": "Li",
				"given": "Qi"
			},
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "soganciogluFairnessAIBasedMental2024",
		"type": "article-journal",
		"abstract": "There is limited research on fairness in automated decision-making systems in the clinical domain, particularly in the mental health domain. Our study explores clinicians' perceptions of AI fairness through two distinct scenarios: violence risk assessment and depression phenotype recognition using textual clinical notes. We engage with clinicians through semi-structured interviews to understand their fairness perceptions and to identify appropriate quantitative fairness objectives for these scenarios. Then, we compare a set of bias mitigation strategies developed to improve at least one of the four selected fairness objectives. Our findings underscore the importance of carefully selecting fairness measures, as prioritizing less relevant measures can have a detrimental rather than a beneficial effect on model behavior in real-world clinical use.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1390-1400",
		"source": "ojs.aaai.org",
		"title": "Fairness in AI-Based Mental Health: Clinician Perspectives and Bias Mitigation",
		"title-short": "Fairness in AI-Based Mental Health",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31732",
		"volume": "7",
		"author": [
			{
				"family": "Sogancioglu",
				"given": "Gizem"
			},
			{
				"family": "Mosteiro",
				"given": "Pablo"
			},
			{
				"family": "Salah",
				"given": "Albert Ali"
			},
			{
				"family": "Scheepers",
				"given": "Floortje"
			},
			{
				"family": "Kaya",
				"given": "Heysem"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "steinPublicVsPrivate2024",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both. \nAuditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms’ compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively. \nFirst, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of advanced AI’s risk profile, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight.\nSecondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies’ capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1401-1415",
		"source": "ojs.aaai.org",
		"title": "Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries",
		"title-short": "Public vs Private Bodies",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31733",
		"volume": "7",
		"author": [
			{
				"family": "Stein",
				"given": "Merlin"
			},
			{
				"family": "Gandhi",
				"given": "Milan"
			},
			{
				"family": "Kriecherbauer",
				"given": "Theresa"
			},
			{
				"family": "Oueslati",
				"given": "Amin"
			},
			{
				"family": "Trager",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "tahaeiSurveysConsideredHarmful2024",
		"type": "article-journal",
		"abstract": "Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1416-1433",
		"source": "ojs.aaai.org",
		"title": "Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance",
		"title-short": "Surveys Considered Harmful?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31734",
		"volume": "7",
		"author": [
			{
				"family": "Tahaei",
				"given": "Mohammad"
			},
			{
				"family": "Wilkinson",
				"given": "Daricia"
			},
			{
				"family": "Frik",
				"given": "Alisa"
			},
			{
				"family": "Muller",
				"given": "Michael"
			},
			{
				"family": "Abu-Salma",
				"given": "Ruba"
			},
			{
				"family": "Wilcox",
				"given": "Lauren"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "taibEnhancingEquitableAccess2024",
		"type": "article-journal",
		"abstract": "The top priority of a Housing and Homelessness System of Care (HHSC) is to connect people experiencing homelessness to supportive housing. An HHSC typically consists of many agencies serving the same population.  Information technology platforms differ in type and quality between agencies, so their data are usually isolated from one agency to another. Larger agencies may have sufficient data to train and test artificial intelligence (AI) tools but smaller agencies typically do not. To address this gap, we introduce a Federated Learning (FL) approach enabling all agencies to train a predictive model collaboratively without sharing their sensitive data. We demonstrate how FL can be used within an HHSC to provide all agencies equitable access to quality AI and further assist human decision-makers in the allocation of resources within HHSC. This is achieved while preserving the privacy of the people within the data by not sharing identifying information between agencies without their consent. Our experimental results using real-world HHSC data from a North American city demonstrate that our FL approach offers comparable performance with the idealized scenario of training the predictive model with data fully shared and linked between agencies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1434-1443",
		"source": "ojs.aaai.org",
		"title": "Enhancing Equitable Access to AI in Housing and Homelessness System of Care through Federated Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31735",
		"volume": "7",
		"author": [
			{
				"family": "Taib",
				"given": "Musa"
			},
			{
				"family": "Wu",
				"given": "Jiajun"
			},
			{
				"family": "Drew",
				"given": "Steve"
			},
			{
				"family": "Messier",
				"given": "Geoffrey G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "tennantDynamicsMoralBehavior2024",
		"type": "article-journal",
		"abstract": "Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents: a promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents; however, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., focused on maximizing outcomes over time), norm-based (i.e., conforming to specific norms), or virtue-based (i.e., considering a combination of different virtues). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using an Iterated Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain types of moral agents are able to steer selfish agents towards more cooperative behavior.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1444-1454",
		"source": "ojs.aaai.org",
		"title": "Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31736",
		"volume": "7",
		"author": [
			{
				"family": "Tennant",
				"given": "Elizaveta"
			},
			{
				"family": "Hailes",
				"given": "Stephen"
			},
			{
				"family": "Musolesi",
				"given": "Mirco"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "thaisMisrepresentedTechnologicalSolutions2024",
		"type": "article-journal",
		"abstract": "Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1455-1465",
		"source": "ojs.aaai.org",
		"title": "Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community",
		"title-short": "Misrepresented Technological Solutions in Imagined Futures",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31737",
		"volume": "7",
		"author": [
			{
				"family": "Thais",
				"given": "Savannah"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "valdiviaSupplyChainCapitalism2024",
		"type": "article-journal",
		"abstract": "Artificial Intelligence (AI) is woven into a supply chain of capital, resources and human labour that has been neglected in debates about the social impact of this technology. Given the current surge in generative AI—which is estimated to use more natural resources than classic machine learning algorithms—it is vital that we better understand its production networks. Building on Tsing’s concept of supply chain capitalism, this paper offers a journey through the AI industry by illustrating the complex, diverse, opaque and global structures of the AI supply chain. The paper then illustrates an ethnographic research in Latin America revealing that AI’s rapid infrastructural growth may be precipitating environmental struggles. Investigating the supply chain capitalism of AI  reveals that eco-political frictions are arising. This demands broad critical perspectives on AI studies from a critical perspective by considering the entire capitalist production line of its industry.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1466-1466",
		"source": "ojs.aaai.org",
		"title": "The Supply Chain Capitalism of AI: A Call to (Re)think Algorithmic Harms and Resistance (Extended Abstract)",
		"title-short": "The Supply Chain Capitalism of AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31738",
		"volume": "7",
		"author": [
			{
				"family": "Valdivia",
				"given": "Ana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "varshneyDecolonialAIAlignment2024",
		"type": "article-journal",
		"abstract": "Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment through mechanisms such as extractivism, automation, sociological essentialism, surveillance, and containment. However, that work has not engaged much with alignment: teaching behaviors to a large language model (LLM) in line with desired values, and has not considered a mechanism that arises within that process: moral absolutism---a part of the coloniality of knowledge. Colonialism has a history of altering the beliefs and values of colonized peoples; in this paper, I argue that this history is recapitulated in current LLM alignment practices and technologies. Furthermore, I suggest that AI alignment be decolonialized using three forms of openness: openness of models, openness to society, and openness to excluded knowledges. This suggested approach to decolonial AI alignment uses ideas from the argumentative moral philosophical tradition of Hinduism, which has been described as an open-source religion. One concept used is viśeṣa-dharma, or particular context-specific notions of right and wrong. At the end of the paper, I provide a suggested reference architecture to work toward the proposed framework.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1467-1481",
		"source": "ojs.aaai.org",
		"title": "Decolonial AI Alignment: Openness, Visesa-Dharma, and Including Excluded Knowledges",
		"title-short": "Decolonial AI Alignment",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31739",
		"volume": "7",
		"author": [
			{
				"family": "Varshney",
				"given": "Kush R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "victorMedicalAICategories2024",
		"type": "article-journal",
		"abstract": "It is becoming clear that, in the process of aligning AI with human values, one glaring ethical problem is that of value conflict. It is not obvious what we should do when two compelling values (such as autonomy and safety) come into conflict with one another in the design or implementation of a medical AI technology. This paper shares findings from a scoping review at the intersection of three concepts—AI, moral value, and health—that have to do with value conflict and arbitration. The paper looks at some important and unique cases of value conflict, and then describes three possible categories of value conflict: personal value conflict, interpersonal or intercommunal value conflict, and definitional value conflict. It then describes three general paths forward in addressing value conflict: additional ethical theory, additional empirical evidence, and bypassing the conflict altogether. Finally, it reflects on the efficacy of these three paths forward as ways of addressing the three categories of value conflict, and motions toward what is needed for better approaching value conflicts in medical AI.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1482-1489",
		"source": "ojs.aaai.org",
		"title": "Medical AI, Categories of Value Conflict, and Conflict Bypasses",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31740",
		"volume": "7",
		"author": [
			{
				"family": "Victor",
				"given": "Gavin"
			},
			{
				"family": "Bélisle-Pipon",
				"given": "Jean-Christophe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "vidaDecodingMultilingualMoral2024",
		"type": "article-journal",
		"abstract": "Large language models (LLMs) increasingly find their way into the most diverse areas of our everyday lives. They indirectly influence people's decisions or opinions through their daily use. Therefore, understanding how and which moral judgements these LLMs make is crucial. However, morality is not universal and depends on the cultural background. This raises the question of  whether these cultural preferences are also reflected in LLMs when prompted in different languages or whether moral decision-making is consistent across different languages. So far, most research has focused on investigating the inherent values of LLMs in English. While a few works conduct multilingual analyses of moral bias in LLMs in a multilingual setting, these analyses do not go beyond atomic actions. To the best of our knowledge, a multilingual analysis of moral bias in dilemmas has not yet been conducted.\n\nTo address this, our paper builds on the moral machine experiment (MME) to investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilingual setting and compares them with the preferences collected from humans belonging to different cultures. To accomplish this, we generate 6500 scenarios of the MME and prompt the models in ten languages on which action to take. Our analysis reveals that all LLMs inhibit different moral biases to some degree and that they not only differ from the human preferences but also across multiple languages within the models themselves. Moreover, we find that almost all models, particularly Llama 3, divert greatly from human values and, for instance, prefer saving fewer people over saving more.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1490-1501",
		"source": "ojs.aaai.org",
		"title": "Decoding Multilingual Moral Preferences: Unveiling LLM's Biases through the Moral Machine Experiment",
		"title-short": "Decoding Multilingual Moral Preferences",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31741",
		"volume": "7",
		"author": [
			{
				"family": "Vida",
				"given": "Karina"
			},
			{
				"family": "Damken",
				"given": "Fabian"
			},
			{
				"family": "Lauscher",
				"given": "Anne"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "villaniPICEPolyhedralComplex2024",
		"type": "article-journal",
		"abstract": "Polyhedral geometry can be used to shed light on the behaviour of piecewise linear neural networks, such as ReLU-based architectures. \nCounterfactual explanations are a popular class of methods for examining model behaviour by comparing a query to the closest point with a different label, subject to constraints. \nWe present a new algorithm, Polyhedral-complex Informed Counterfactual Explanations (PICE), which leverages the decomposition of the piecewise linear neural network into a polyhedral complex to find counterfactuals that are provably minimal in the Euclidean norm and exactly on the decision boundary for any given query. \nMoreover, we develop variants of the algorithm that target popular counterfactual desiderata such as sparsity, robustness, speed, plausibility, and actionability. \nWe empirically show on four publicly available real-world datasets that our method outperforms other popular techniques to find counterfactuals and adversarial attacks by distance to decision boundary and distance to query.\nMoreover, we successfully improve our baseline method in the dimensions of the desiderata we target, as supported by experimental evaluations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1502-1513",
		"source": "ojs.aaai.org",
		"title": "PICE: Polyhedral Complex Informed Counterfactual Explanations",
		"title-short": "PICE",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31742",
		"volume": "7",
		"author": [
			{
				"family": "Villani",
				"given": "Mattia Jacopo"
			},
			{
				"family": "Albini",
				"given": "Emanuele"
			},
			{
				"family": "Sharma",
				"given": "Shubham"
			},
			{
				"family": "Mishra",
				"given": "Saumitra"
			},
			{
				"family": "Amoukou",
				"given": "Salim Ibrahim"
			},
			{
				"family": "Magazzeni",
				"given": "Daniele"
			},
			{
				"family": "Veloso",
				"given": "Manuela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wangStrategiesIncreasingCorporate2024",
		"type": "article-journal",
		"abstract": "Responsible artificial intelligence (RAI) is increasingly recognized as a critical concern. However, the level of corporate RAI prioritization has not kept pace. In this work, we conduct 16 semi-structured interviews with practitioners to investigate what has historically motivated companies to increase the prioritization of RAI. What emerges is a complex story of conflicting and varied factors, but we bring structure to the narrative by highlighting the different strategies available to employ, and point to the actors with access to each. While there are no guaranteed steps for increasing RAI prioritization, we paint the current landscape of motivators so that practitioners can learn from each other, and put forth our own selection of promising directions forward.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1514-1526",
		"source": "ojs.aaai.org",
		"title": "Strategies for Increasing Corporate Responsible AI Prioritization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31743",
		"volume": "7",
		"author": [
			{
				"family": "Wang",
				"given": "Angelina"
			},
			{
				"family": "Datta",
				"given": "Teresa"
			},
			{
				"family": "Dickerson",
				"given": "John P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "weiOperationalizingContentModeration2024",
		"type": "article-journal",
		"abstract": "The Digital Services Act, recently adopted by the EU, requires social media platforms to report the ``accuracy'' of their automated content moderation systems. The colloquial term is vague, or open-textured---the literal accuracy (number of correct predictions divided by the total) is not suitable for problems with large class imbalance, and the ground truth and dataset to measure accuracy against is unspecified. Without further specification, the regulatory requirement allows for deficient reporting. In this interdisciplinary work, we operationalize ``accuracy'' reporting by refining legal concepts and relating them to technical implementation. We start by elucidating the legislative purpose of the Act to legally justify an interpretation of ``accuracy'' as precision and recall. These metrics remain informative in class imbalanced settings, and reflect the proportional balancing of Fundamental Rights of the EU Charter. We then focus on the estimation of recall, as its naive estimation can incur extremely high annotation costs and disproportionately interfere with the platform's right to conduct business. Through a simulation study, we show that recall can be efficiently estimated using stratified sampling with trained classifiers, and provide concrete recommendations for its application. Finally, we present a case study of recall reporting for a subset of Reddit under the Act. Based on the language in the Act, we identify a number of ways recall could be reported due to underspecification. We report on one possibility using our improved estimator, and discuss the implications and areas for further legal clarification.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1527-1538",
		"source": "ojs.aaai.org",
		"title": "Operationalizing Content Moderation “Accuracy” in the Digital Services Act",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31744",
		"volume": "7",
		"author": [
			{
				"family": "Wei",
				"given": "Johnny Tian-Zheng"
			},
			{
				"family": "Zufall",
				"given": "Frederike"
			},
			{
				"family": "Jia",
				"given": "Robin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "weiHowAICompanies2024",
		"type": "article-journal",
		"abstract": "Industry actors in the United States have gained extensive influence in conversations about the regulation of general-purpose artificial intelligence (AI) systems. Although industry participation is an important part of the policy process, it can also cause regulatory capture, whereby industry co-opts regulatory regimes to prioritize private over public welfare. Capture of AI policy by AI developers and deployers could hinder such regulatory goals as ensuring the safety, fairness, beneficence, transparency, or innovation of general-purpose AI systems. In this paper, we first introduce different models of regulatory capture from the social science literature. We then present results from interviews with 17 AI policy experts on what policy outcomes could compose regulatory capture in US AI policy, which AI industry actors are influencing the policy process, and whether and how AI industry actors attempt to achieve outcomes of regulatory capture. Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others. Experts most commonly identified agenda-setting (15 of 17 interviews), advocacy (13), academic capture (10), information management (9), cultural capture through status (7), and media capture (7) as channels for industry influence. To mitigate these particular forms of industry influence, we recommend systemic changes in developing technical expertise in government and civil society, independent funding streams for the AI ecosystem, increased transparency and ethics requirements, greater civil society access to policy, and various procedural safeguards.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1539-1555",
		"source": "ojs.aaai.org",
		"title": "How Do AI Companies “Fine-Tune” Policy? Examining Regulatory Capture in AI Governance",
		"title-short": "How Do AI Companies “Fine-Tune” Policy?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31745",
		"volume": "7",
		"author": [
			{
				"family": "Wei",
				"given": "Kevin"
			},
			{
				"family": "Ezell",
				"given": "Carson"
			},
			{
				"family": "Gabrieli",
				"given": "Nick"
			},
			{
				"family": "Deshpande",
				"given": "Chinmay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wen-yiAutomateAssistRole2024",
		"type": "article-journal",
		"abstract": "The language used by US courtroom actors in criminal trials has long been studied for biases. However, systematic studies for bias in high-stakes court trials have been difficult, due to the nuanced nature of bias and the legal expertise required.  Large language models offer the possibility to automate annotation. But validating the computational approach requires both an understanding of how automated methods fit in existing annotation workflows and what they really offer.  We present a case study of adding a computational model to a complex and high-stakes problem: identifying gender-biased language in US capital trials for women defendants.\nOur team of experienced death-penalty lawyers and NLP technologists pursue a three-phase study: first annotating manually, then training and evaluating computational models, and finally comparing expert annotations to model predictions.  Unlike many typical NLP tasks, annotating for gender bias in months-long capital trials is complicated, with many individual judgment calls.  Contrary to standard arguments for automation that are based on efficiency and scalability, legal experts find the computational models most useful in providing opportunities to reflect on their own bias in annotation and to build consensus on annotation rules.  This experience suggests that seeking to replace experts with computational models for complex annotation is both unrealistic and undesirable. Rather, computational models offer valuable opportunities to assist the legal experts in annotation-based studies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1556-1566",
		"source": "ojs.aaai.org",
		"title": "Automate or Assist? The Role of Computational Models in Identifying Gendered Discourse in US Capital Trial Transcripts",
		"title-short": "Automate or Assist?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31746",
		"volume": "7",
		"author": [
			{
				"family": "Wen-Yi",
				"given": "Andrea W."
			},
			{
				"family": "Adamson",
				"given": "Kathryn"
			},
			{
				"family": "Greenfield",
				"given": "Nathalie"
			},
			{
				"family": "Goldberg",
				"given": "Rachel"
			},
			{
				"family": "Babcock",
				"given": "Sandra"
			},
			{
				"family": "Mimno",
				"given": "David"
			},
			{
				"family": "Koenecke",
				"given": "Allison"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wielingaRelationalJustificationAI2024",
		"type": "article-journal",
		"abstract": "While much has been written about what democratized AI should look like, there has been surprisingly little attention for the normative grounds of AI democratization. Existing calls for AI democratization that do make explicit arguments broadly fall into two categories: outcome-based and legitimacy-based, corresponding to outcome-based and process-based views of procedural justice respectively. This paper argues that we should favor relational justifications of AI democratization to outcome-based ones, because the former additionally provide outcome-independent reasons for AI democratization. Moreover, existing legitimacy-based arguments often leave the why of AI democratization implicit and instead focus on the how. We present two relational arguments for AI democratization: one based on empirical findings regarding the perceived importance of relational features of decision-making procedures, and one based on Iris Marion Young’s conception of justice, according to which the main forms of injustice are domination and oppression. We show how these arguments lead to requirements for procedural fairness and thus also offer guidance on the how of AI democratization. Finally, we consider several objections to AI democratization, including worries concerning epistemic exploitation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1567-1577",
		"source": "ojs.aaai.org",
		"title": "A Relational Justification of AI Democratization",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31747",
		"volume": "7",
		"author": [
			{
				"family": "Wielinga",
				"given": "Bauke"
			},
			{
				"family": "Buijsman",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wilsonGenderRaceIntersectional2024",
		"type": "article-journal",
		"abstract": "Artificial intelligence (AI) hiring tools have revolutionized resume screening, and large language models (LLMs) have the potential to do the same. However, given the biases which are embedded within LLMs, it is unclear whether they can be used in this scenario without disadvantaging groups based on their protected attributes. In this work, we investigate the possibilities of using LLMs in a resume screening setting via a document retrieval framework that simulates job candidate selection. Using that framework, we then perform a resume audit study to determine whether a selection of Massive Text Embedding (MTE) models are biased in resume screening scenarios. We simulate this for nine occupations, using a collection of over 500 publicly available resumes and 500 job descriptions. We find that the MTEs are biased, significantly favoring White-associated names in 85.1% of cases and female-associated names in only 11.1% of cases, with a minority of cases showing no statistically significant differences. Further analyses show that Black males are disadvantaged in up to 100% of cases, replicating real-world patterns of bias in employment settings, and validate three hypotheses of intersectionality. We also find an impact of document length as well as the corpus frequency of names in the selection of resumes. These findings have  implications for widely used AI tools that are automating employment, fairness, and tech policy.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1578-1590",
		"source": "ojs.aaai.org",
		"title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31748",
		"volume": "7",
		"author": [
			{
				"family": "Wilson",
				"given": "Kyra"
			},
			{
				"family": "Caliskan",
				"given": "Aylin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wojtowiczWhenWhyPersuasion2024",
		"type": "article-journal",
		"abstract": "As generative foundation models improve, they also tend to become more persuasive, raising concerns that AI automation will enable governments, firms, and other actors to manipulate beliefs with unprecedented scale and effectiveness at virtually no cost. The full economic and social ramifications of this trend have been difficult to foresee, however, given that we currently lack a complete theoretical understanding of why persuasion is costly for human labor to produce in the first place. This paper places human and AI agents on a common conceptual footing by formalizing informational persuasion as a mathematical decision problem and characterizing its computational complexity. A novel proof establishes that persuasive messages are challenging to discover (NP-Hard) but easy to adopt if supplied by others (NP). This asymmetry helps explain why people are susceptible to persuasion, even in contexts where all relevant information is publicly available. The result also illuminates why litigation, strategic communication, and other persuasion-oriented activities have historically been so human capital intensive, and it provides a new theoretical basis for studying how AI will impact various industries.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1591-1594",
		"source": "ojs.aaai.org",
		"title": "When and Why is Persuasion Hard? A Computational Complexity Result",
		"title-short": "When and Why is Persuasion Hard?",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31749",
		"volume": "7",
		"author": [
			{
				"family": "Wojtowicz",
				"given": "Zachary"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeImplicationsOpenGenerative2024",
		"type": "article-journal",
		"abstract": "Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1595-1607",
		"source": "ojs.aaai.org",
		"title": "The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations",
		"title-short": "The Implications of Open Generative Models in Human-Centered Data Science Work",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31750",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Mitra",
				"given": "Tanushree"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeMLEATMultilevelEmbedding2024",
		"type": "article-journal",
		"abstract": "This research introduces the Multilevel Embedding Association Test (ML-EAT), a method designed for interpretable and transparent measurement of intrinsic bias in language technologies. The ML-EAT addresses issues of ambiguity and difficulty in interpreting the traditional EAT measurement by quantifying bias at three levels of increasing granularity: the differential association between two target concepts with two attribute concepts; the individual effect size of each target concept with two attribute concepts; and the association between each individual target concept and each individual attribute concept. Using the ML-EAT, this research defines a taxonomy of EAT patterns describing the nine possible outcomes of an embedding association test, each of which is associated with a unique EAT-Map, a novel four-quadrant visualization for interpreting the ML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2 language models, and a CLIP language-and-image model shows that EAT patterns add otherwise unobservable information about the component biases that make up an EAT; reveal the effects of prompting in zero-shot models; and can also identify situations when cosine similarity is an ineffective metric, rendering an EAT unreliable. Our work contributes a method for rendering bias more observable and interpretable, improving the transparency of computational investigations into human minds and societies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1608-1620",
		"source": "ojs.aaai.org",
		"title": "ML-EAT: A Multilevel Embedding Association Test for Interpretable and Transparent Social Science",
		"title-short": "ML-EAT",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31751",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeRepresentationBiasAdolescents2024",
		"type": "article-journal",
		"abstract": "Popular and news media often portray teenagers with sensationalism, as both a risk to society and at risk from society. As AI begins to absorb some of the epistemic functions of traditional media, we study how teenagers in two countries speaking two languages: 1) are depicted by AI, and 2) how they would prefer to be depicted. Specifically, we study the biases about teenagers learned by static word embeddings (SWEs) and generative language models (GLMs), comparing these with the perspectives of adolescents living in the U.S. and Nepal. We find English-language SWEs associate teenagers with societal problems, and more than 50% of the 1,000 words most associated with teenagers in the pretrained GloVe SWE reflect such problems. Given prompts about teenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss societal problems, most commonly violence, but also drug use, mental illness, and sexual taboo. Nepali models, while not free of such associations, are less dominated by social problems. Data from workshops with N=13 U.S. adolescents and N=18 Nepalese adolescents show that AI presentations are disconnected from teenage life, which revolves around activities like school and friendship. Participant ratings of how well 20 trait words describe teens are decorrelated from SWE associations, with Pearson's rho=.02, n.s. in English FastText and rho=.06, n.s. GloVe; and rho=.06, n.s. in Nepali FastText and rho=-.23, n.s. in GloVe. U.S. participants suggested AI could fairly present teens by highlighting diversity, while Nepalese participants centered positivity. Participants were optimistic that, if it learned from adolescents, rather than media sources, AI could help mitigate stereotypes. Our work offers an understanding of the ways SWEs and GLMs misrepresent a developmentally vulnerable group and provides a template for less sensationalized characterization.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1621-1634",
		"source": "ojs.aaai.org",
		"title": "Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study",
		"title-short": "Representation Bias of Adolescents in AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31752",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Dangol",
				"given": "Aayushi"
			},
			{
				"family": "Howe",
				"given": "Bill"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wolfeDatasetScaleSocietal2024",
		"type": "article-journal",
		"abstract": "Multimodal AI models capable of associating images and text hold promise for numerous domains, ranging from automated image captioning to accessibility applications for blind and low-vision users. However, uncertainty about bias has in some cases limited their adoption and availability. In the present work, we study 43 CLIP vision-language models to determine whether they learn human-like facial impression biases, and we find evidence that such biases are reflected across three distinct CLIP model families. We show for the first time that the the degree to which a bias is shared across a society predicts the degree to which it is reflected in a CLIP model. Human-like impressions of visually unobservable attributes, like trustworthiness and sexuality, emerge only in models trained on the largest dataset, indicating that a better fit to uncurated cultural data results in the reproduction of increasingly subtle social biases. Moreover, we use a hierarchical clustering approach to show that dataset size predicts the extent to which the underlying structure of facial impression bias resembles that of facial impression bias in humans. Finally, we show that Stable Diffusion models employing CLIP as a text encoder learn facial impression biases, and that these biases intersect with racial biases in Stable Diffusion XL-Turbo. While pretrained CLIP models may prove useful for scientific studies of bias, they will also require significant dataset curation when intended for use as general-purpose models in a zero-shot setting.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1635-1647",
		"source": "ojs.aaai.org",
		"title": "Dataset Scale and Societal Consistency Mediate Facial Impression Bias in Vision-Language AI",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31753",
		"volume": "7",
		"author": [
			{
				"family": "Wolfe",
				"given": "Robert"
			},
			{
				"family": "Dangol",
				"given": "Aayushi"
			},
			{
				"family": "Hiniker",
				"given": "Alexis"
			},
			{
				"family": "Howe",
				"given": "Bill"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "wuStableDiffusionExposed2024",
		"type": "article-journal",
		"abstract": "Several studies have raised awareness about social biases in image generative models, demonstrating their predisposition towards stereotypes and imbalances. This paper contributes to this growing body of research by introducing an evaluation protocol that analyzes the impact of gender indicators at every step of the generation process on Stable Diffusion images. Leveraging insights from prior work, we explore how gender indicators not only affect gender presentation but also the representation of objects and layouts within the generated images. Our findings include the existence of differences in the depiction of objects, such as instruments tailored for specific genders, and shifts in overall layouts. We also reveal that neutral prompts tend to produce images more aligned with masculine prompts than their feminine counterparts. We further explore where bias originates through representational disparities and how it manifests in the images via prompt-image dependencies, and provide recommendations for developers and users to mitigate potential bias in image generation.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1648-1659",
		"source": "ojs.aaai.org",
		"title": "Stable Diffusion Exposed: Gender Bias from Prompt to Image",
		"title-short": "Stable Diffusion Exposed",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31754",
		"volume": "7",
		"author": [
			{
				"family": "Wu",
				"given": "Yankun"
			},
			{
				"family": "Nakashima",
				"given": "Yuta"
			},
			{
				"family": "Garcia",
				"given": "Noa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xieNonlinearWelfareAwareStrategic2024",
		"type": "article-journal",
		"abstract": "This paper studies algorithmic decision-making in the presence of strategic individual behaviors, where an ML model is used to make decisions about human agents and the latter can adapt their behavior strategically to improve their future data. Existing results on strategic learning have largely focused on the linear setting where agents with linear labeling functions best respond to a (noisy) linear decision policy. Instead, this work focuses on general non-linear settings where agents respond to the decision policy with only \"local information\" of the policy. Moreover, we simultaneously consider objectives of maximizing decision-maker welfare (model prediction accuracy), social welfare (agent improvement caused by strategic behaviors), and agent welfare (the extent that ML underestimates the agents). We first generalize the agent best response model in previous works to the non-linear setting and then investigate the compatibility of welfare objectives. We show the three welfare can attain the optimum simultaneously only under restrictive conditions which are challenging to achieve in non-linear settings. The theoretical results imply that existing works solely maximizing the welfare of a subset of parties usually diminish the welfare of others. We thus claim the necessity of balancing the welfare of each party in non-linear settings and propose an irreducible optimization algorithm suitable for general strategic learning. Experiments on synthetic and real data validate the proposed algorithm.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1660-1671",
		"source": "ojs.aaai.org",
		"title": "Non-linear Welfare-Aware Strategic Learning",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31755",
		"volume": "7",
		"author": [
			{
				"family": "Xie",
				"given": "Tian"
			},
			{
				"family": "Zhang",
				"given": "Xueru"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xieAlgorithmicDecisionMakingAgents2024",
		"type": "article-journal",
		"abstract": "This paper studies algorithmic decision-making under human strategic behavior, where a decision-maker uses an algorithm to make decisions about human agents, and the latter with information about the algorithm may exert effort strategically and improve to receive favorable decisions. Unlike prior works that assume agents benefit from their efforts immediately, we consider realistic scenarios where the impacts of these efforts are persistent and agents benefit from efforts by making improvements gradually. We first develop a dynamic model to characterize persistent improvements and based on this construct a Stackelberg game to model the interplay between agents and the decision-maker. We analytically characterize the equilibrium strategies and identify conditions under which agents have incentives to invest efforts to improve their qualifications. With the dynamics, we then study how the decision-maker can design an optimal policy to incentivize the largest improvements inside the agent population. We also extend the model to settings where 1) agents may be dishonest and game the algorithm into making favorable but erroneous decisions; 2) honest efforts are forgettable and not sufficient to guarantee persistent improvements. With the extended models, we further examine conditions under which agents prefer honest efforts over dishonest behavior and the impacts of forgettable efforts.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1672-1683",
		"source": "ojs.aaai.org",
		"title": "Algorithmic Decision-Making under Agents with Persistent Improvement",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31756",
		"volume": "7",
		"author": [
			{
				"family": "Xie",
				"given": "Tian"
			},
			{
				"family": "Tan",
				"given": "Xuwei"
			},
			{
				"family": "Zhang",
				"given": "Xueru"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "xuTracingEvolutionInformation2024",
		"type": "article-journal",
		"abstract": "Information transparency, the open disclosure of information about models, is crucial for proactively evaluating the potential societal harm of large language models (LLMs) and developing effective risk mitigation measures. Adapting the biographies of artifacts and practices (BOAP) method from science and technology studies, this study analyzes the evolution of information transparency within OpenAI’s Generative Pre-trained Transformers (GPT) model reports and usage policies from its inception in 2018 to GPT-4, one of today’s most capable LLMs. To assess the breadth and depth of transparency practices, we develop a 9-dimensional, 3-level analytical framework to evaluate the comprehensiveness and accessibility of information disclosed to various stakeholders. Findings suggest that while model limitations and downstream usages are increasingly clarified, model development processes have become more opaque. Transparency remains minimal in certain aspects, such as model explainability and real-world evidence of LLM impacts, and the discussions on safety measures such as technical interventions and regulation pipelines lack in-depth details. The findings emphasize the need for enhanced transparency to foster accountability and ensure responsible technological innovations.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1684-1695",
		"source": "ojs.aaai.org",
		"title": "Tracing the Evolution of Information Transparency for OpenAI’s GPT Models through a Biographical Approach",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31757",
		"volume": "7",
		"author": [
			{
				"family": "Xu",
				"given": "Zhihan"
			},
			{
				"family": "Mustafaraj",
				"given": "Eni"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yangLLMVotingHuman2024",
		"type": "article-journal",
		"abstract": "This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1696-1708",
		"source": "ojs.aaai.org",
		"title": "LLM Voting: Human Choices and AI Collective Decision-Making",
		"title-short": "LLM Voting",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31758",
		"volume": "7",
		"author": [
			{
				"family": "Yang",
				"given": "Joshua C."
			},
			{
				"family": "Dailisan",
				"given": "Damian"
			},
			{
				"family": "Korecki",
				"given": "Marcin"
			},
			{
				"family": "Hausladen",
				"given": "Carina I."
			},
			{
				"family": "Helbing",
				"given": "Dirk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "yewYouStillSee2024",
		"type": "article-journal",
		"abstract": "Data forms the backbone of artificial intelligence (AI). Privacy and data protection laws thus have strong bearing on AI systems. Shielded by the rhetoric of compliance with data protection and privacy regulations, privacy-preserving techniques have enabled the extraction of more and new forms of data. We illustrate how the application of privacy-preserving techniques in the development of AI systems--from private set intersection as part of dataset curation to homomorphic encryption and federated learning as part of model computation--can further support surveillance infrastructure under the guise of regulatory permissibility. Finally, we propose technology and policy strategies to evaluate privacy-preserving techniques in light of the protections they actually confer. We conclude by highlighting the role that technologists could play in devising policies that combat surveillance AI technologies.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1709-1722",
		"source": "ojs.aaai.org",
		"title": "You Still See Me: How Data Protection Supports the Architecture of AI Surveillance",
		"title-short": "You Still See Me",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31759",
		"volume": "7",
		"author": [
			{
				"family": "Yew",
				"given": "Rui-Jie"
			},
			{
				"family": "Qin",
				"given": "Lucy"
			},
			{
				"family": "Venkatasubramanian",
				"given": "Suresh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "zhangMitigatingUrbanRuralDisparities2024",
		"type": "article-journal",
		"abstract": "Satellite imagery is being leveraged for many societally critical tasks across climate, economics, and public health. Yet, because of heterogeneity in landscapes (e.g. how a road looks in different places), models can show disparate performance across geographic areas. Given the important potential of disparities in algorithmic systems used in societal contexts, here we consider the risk of urban-rural disparities in identification of land-cover features. This is via semantic segmentation (a common computer vision task in which image regions are labelled according to what is being shown) which uses pre-trained image representations generated via contrastive self-supervised learning. We propose fair dense representation with contrastive learning (FairDCL) as a method for de-biasing the multi-level latent space of a convolution neural network. The method improves feature identification by removing spurious latent representations which are disparately distributed across urban and rural areas, and is achieved in an unsupervised way by contrastive pre-training. The pre-trained image representation mitigates downstream urban-rural prediction disparities and outperforms state-of-the-art baselines on real-world satellite images. Embedding space evaluation and ablation studies further demonstrate FairDCL’s robustness. As generalizability and robustness in geographic imagery is a nascent topic, our work motivates researchers to consider metrics beyond average accuracy in such applications.",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1723-1734",
		"source": "ojs.aaai.org",
		"title": "Mitigating Urban-Rural Disparities in Contrastive Representation Learning with Satellite Imagery",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31760",
		"volume": "7",
		"author": [
			{
				"family": "Zhang",
				"given": "Miao"
			},
			{
				"family": "Chunara",
				"given": "Rumi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	},
	{
		"id": "zhangOntologyBeliefDiversity2024",
		"type": "article-journal",
		"abstract": "AI applications across classification, fairness, and human interaction  often  implicitly  require  ontologies  of  social  concepts.  Constructing  these  well  –  especially  when  there  are many relevant categories – is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which isa complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological  methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology’s utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models",
		"container-title": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
		"language": "en",
		"license": "Copyright (c) 2024 Association for the Advancement of Artificial Intelligence",
		"page": "1735-1743",
		"source": "ojs.aaai.org",
		"title": "Ontology of Belief Diversity: A Community-Based Epistemological Approach",
		"title-short": "Ontology of Belief Diversity",
		"URL": "https://ojs.aaai.org/index.php/AIES/article/view/31761",
		"volume": "7",
		"author": [
			{
				"family": "Zhang",
				"given": "Richard"
			},
			{
				"family": "Liemt",
				"given": "Erin Van"
			},
			{
				"family": "Fischella",
				"given": "Tyler"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2024",
					10,
					30
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					10,
					16
				]
			]
		}
	}
]